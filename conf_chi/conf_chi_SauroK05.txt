Current methods to represent system or task usability in a single metric do not include all the ANSI and ISO defined usability aspects: effectiveness, efficiency & satisfaction.
We propose a method to simplify all the ANSI and ISO aspects of usability into a single, standardized and summated usability metric .
In four data sets, totaling 1860 task observations, we show that these aspects of usability are correlated and equally weighted and present a quantitative model for usability.
Using standardization techniques from Six Sigma, we propose a scalable process for standardizing disparate usability metrics and show how Principal Components Analysis can be used to establish appropriate weighting for a summated model.
SUM provides one continuous variable for summative usability evaluations that can be used in regression analysis, hypothesis testing and usability reporting.
As usability analysts encourage business leaders to track "usability" against other indicators of company performance--such as revenue growth, customer support expenditures or product abandonment rate--the various metrics we depend upon become clumsy and difficult to use.
Each metric is measured on its own scale and yet each must be represented in the analysis and reporting process if we are to be true to the accepted industry definition of usability.
Furthermore, differences in the scales make it difficult to compare the relative usability of different features or products.
This complexity in analysis and reporting makes usability data hard to digest.
The analyst is challenged to present multiple usability metrics that clearly delineates usable and unusable aspects in a product without overwhelming business leaders or inadvertently promoting one metric over another.
To increase the meaningfulness and strategic influence of usability data, analysts need to be able to represent the entire construct of usability as a single dependent variable without sacrificing precision.
In a summative usability evaluation, several metrics are available to the analyst for benchmarking the usability of a product.
There is general agreement from the standards boards ANSI 2001 and ISO 9241 pt.11 as to what the dimensions of usability are  and to a lesser extent which metrics are most commonly used to quantify those dimensions.
There have been attempts to derive a single measure for the construct of usability.
Babiker et al  derived a single metric for usability in hypertext systems using objective performance measures only.
They found their metric correlated to subjective assessment measures but could not generalize their model to other systems.
Questionnaires such as the SUMI , PSSUQ, QUIS and SUS have users provide a subjective assessment of recently completed tasks or specific product issues and claim to derive a reliable and low-cost standardized measure of the overall usability or quality of use of a system.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Cordes  and McGee  used a method of magnitude estimation derived from methods in psychophysics as outlined by Stevens .
Specifically, McGee uses a geometric averaging procedure  to standardize ratios of participants' subjective assessment ratings on tasks to derive a single score for task usability.
His research identifies the potential for a standardized measure of usability to support usability comparisons across products, the same product over time, at lower levels of detail, and of tasks common to multiple products.
Lewis used a rank-based system when assessing competing products .
This approach creates a rank score comprised of both users' objective performance measures and subjective assessment, but the resulting metric only represents a relative comparison between like-products with similar tasks.
It does not result in an absolute measure of usability that can be compared across products or different task-sets.
These methods provide helpful information to the analyst in making decisions about usability; however, one must question the ability of methods relying solely on objective or subjective measures to effectively describe the entire construct of usability in light of the guidance set by ISO 9241 and ANSI 354-2001 .
Additionally, the reliance on relative ranking falls short of an absolute measure that can be freely compared as a standardized measure.
Yet, the existence and usage of all these methods demonstrates the need to represent the complex construct of usability into a succinct and manageable form.
Four summative usability tests were conducted to collect the common metrics as described above  as well as several other metrics as suggested in Dumas and Redish , and Nielsen .
For measuring satisfaction we created a questionnaire containing semantic distance scales with five points, similar to the ASQ created by Lewis  .
The questionnaire included questions on task experience, ease of task, time on task, and overall task satisfaction.
The questionnaires were administered immediately after each task to improve accuracy .
The four usability tests were conducted in a controlled usability lab setting over a two-year period.
Participants were asked to complete the tasks to the best of their ability and the administrator only intervened when the participant indicated they were done or gave up.
At the end of the test session, "post-test" satisfaction questions similar to those in SUMI and SUS that asked about overall product usability were given to users.
The applications tested were all from the financial and accounting industry.
Three were Windows-based and one application was web-based.
One application was tested twice one year apart.
Each test used different test administrators  in five geographic locations within the US.
Data was collected from 129 total participants completing a total of 57 tasks.
Participants varied in their application experience, gender, and industries.
In an attempt to fully represent the entire construct of usability as well as creating a single usability metric we began with a high-level model of usability starting with the ISO/ANSI dimensions .
We used the following four metrics to represent these dimensions--task completion, error counts, task times and satisfaction scores 
To investigate the relationship among the metrics to properly build the model and a single score, we set up a data collection plan for four summative usability evaluations.
To attempt to combine the metrics into a single usability score we examined the relationship among the four primary variables for each task observation.
We generated a correlation matrix with all four variables from all four data sets plus a combined data set containing data from all tests.
As can be seen in the lower right cell of Table 1, the Pearson Product Moment correlation coefficients between satisfaction and task completion are consistent with prior correlation analyses  .
What's more, the positive correlation between subjective measures  and objective measures  are also consistent with Nielsen's 1994 metaanalysis  .
PCA is not to be confused with Factor Analysis, another common multivariate technique that can use a method of Principal Components.
A smaller set of uncorrelated variables can be much easier to understand and use in further analyses than a larger set of correlated variables.
The variables used in a PCA need not be normally distributed or all continuous.
This flexibility makes PCA an especially helpful procedure in interpreting usability data that takes the form of continuous , ordinal  and binary .
The technique has been used to summarize behavioral data in the social sciences , and .
For example, task completion or satisfaction may account for more variance than time or errors.
Frokjaer et al  earlier has made the case for including all aspects  when measuring the usability of a system since it was found that these aspects did not always correlate in the data they reviewed.
We agree with Frokjaer et al's conclusion to measure all aspects of usability, however, not because they do not correlate with each other , but because each measure adds additional information not contained in the other measures.
The average values from the post-test satisfaction questions also showed low to moderate and significant correlations with average task performance by user .
The correlations were not as strong or as significant as the post-task satisfaction questions but still showed a similar relationship.
We used the values from the post-task questions since the focus of our analysis was at task-level usability and this provided us with the same number of observations for all four variables.
The first step in interpreting the results of a Principal Components Analysis is to determine which components to retain.
There are several methods and none are definitive, with each method requiring some level of judgment.
Some of the more commonly used methods include: 1.
Kaiser's Rule: Only retain principal components  with eigenvalues greater than 1.
Jolliffe recommends .7 as more rigid cutoff .
Scree Plot Test: Stop retaining components at the point in a plot of the eigenvalues when the line levels off more or less horizontally similar to a pile of rocks at the bottom of a hill .
Cumulative Variance: Stop retaining when the cumulative variance of the PC's reach a certain predetermined level.
This level fluctuates depending on the goal of the analysis.
At a minimum the majority  should be accounted for by the PCs and ideally 70%-90% .
When variables that are ostensibly measuring the same event correlate with each other, there is redundant information making analysis more complicated.
Principal Components Analysis   is a statistical technique that is commonly used in such situations.
PCA linearly transforms an original set of variables into a smaller set of uncorrelated variables that represents most of the information in the original set of variables.
The results of the analysis revealed similar results for all five data sets.
We retained the first PC based on it meeting all three criteria listed above.
Only the first PC contained eignervalues greater than 1 in all data sets .
As per method 2, the Scree plots of the eigenvalues also indicate only retaining the first PC .
After determining the number of components to be retained, the next step is to identify the construct that the retained PCs measure and assess which variables account for more of the variance.
As can be seen in Table 4, all four variables are significant .
Since all the variables are showing significant coefficients it indicates that each variable adds new information not contained in the other variables.
That is, if we saw the coefficients for errors consistently falling below .3 we would conclude that errors are not adding a sufficient amount of new information to the combined model.
An interpretation of the coefficients would read that as errors and time decrease, completion and satisfaction increase.
This relationship is generally regarded as the construct of "usability."
Since all four variables have roughly equal coefficients on the first principal component across all five data sets we concluded that all four variables account for the same amount of variance--they are equally weighted.
We examined the relationship between the variables in the remaining three principal components in all data sets and did not find a pattern that was interpretable.
While the second component contained as much as 21% of the variance , there was no consistent discernable pattern.
PC's 3 and 4 accounted for very little variance and could not be interpreted, as is common with the last PCs .
Due to our desire for parsimony, the aforementioned inconsistencies and the first PC meeting the requirements in methods 1-3, we retained only the first principal component.
Next we stored values  of the first PC and used these values as surrogate variables.
PC scores are created by multiplying the variable coefficients by the raw variable data in standardized form and summing the products .
This creates one surrogate value instead of four.
The surrogate variable is a composite of the four raw variables and accounts for between 52% and 60% of the variance.
This variable represents the best mathematical combination of all four variables.
It can be thought of as a "usability score" and can be used in the same way as any of the four variables can.
If the usability analyst has access to statistical software to run a PCA and store the scores from the first component, then he or she can use those scores for regression analysis, hypothesis testing and drawing conclusions in experimental analyses.
The major drawback to using the stored scores is that they are dependent on the raw data used for that study and therefore cannot be compared to other component scores from other data sets.
To compare scores across tests a summated scale needs to be created that duplicates the relationship built from the PCA .
Doing this requires standardizing all variables and then multiplying them times the coefficients from the first PC.
Since the coefficients were consistently equal across the data sets, taking the arithmetic mean of the four standardized variables  will provide similar values as the PCA scores.
This is similar to the method succinctly described by Martin and Bateson : Measures that are to be combined usually need to be standardized so that they have the same mean and variation.
One-way is to calculate for each raw value its z score: the score for that  minus the mean score for the sample divided by the standard deviation.
Scores standardized in this way have a mean of zero and a standard deviation of 1.
The composite score for an individual is then the average of the z scores of the separate measures.
This procedure gives the same statistical weight to each measure.
If different weights are to be given to the separate measures, this is best done explicitly by multiplying the z score of each measure by an amount that can be specified; for instance, by its loading on a principal component, obtained by principal component analysis.
Unlike the calculation for task completion, it is insufficient to define "error opportunities" as simply each instance of a user in the sample attempting a task.
This is because not all tasks are equal when it comes to error potential and users can commit more than one error per task .
Complex tasks with many required components for task success have a greater potential for error then less complex tasks .
Our standardization process needs to account for this variation in error potential when trying to calculate the error probability.
Therefore, we defined a task's "error opportunities" as the number of sub-tasks that a user must conform to in order to complete a task error-free.
This method is similar to calculating the Human Error Probability  as described in  and .
To standardize each of the usability metrics we created a zscore type value or z-equivalent.
For the continuous data , we subtracted the mean value from a specification limit and divided by the standard deviation.
For discrete data  we divided the unacceptable conditions  by all opportunities for defects.
This method of standardization was adapted from the process sigma metric used in Six Sigma ,, .
See Sauro & Kindlund  for a more detailed discussion on how to standardize these metrics from raw usability data.
We can assume that all users want to successfully complete tasks, so a defect in task completion can be identified as an instance of a user failing a task.
An opportunity for a defect in task completion is simply each instance of a user attempting a task.
Therefore, we standardized task completion as the ratio of failed tasks to attempted tasks.
This proportion of defects per opportunities has a corresponding zequivalent that can be looked up in a standard normal table.
For example, a task completion rate of 80% would have the z-equivalent of .841.
While there are 6 opportunities for the user to make errors, there can be multiple ways an error can be committed.
It's important to note that identifying opportunities does not mean identifying ideal paths through the software.
Users may take many paths or choose many directions to accomplish certain tasks.
If certain required operations are not completed, it's an error regardless of how the user arrived at the screen.
For the analysis we created a composite satisfaction score by averaging the responses from questions of overall ease, satisfaction and perceived task time  .The three questions had high internal-reliability .
The average of the responses  provided a less error-prone score and one more descriptive of the users' perceived sense of usability, see , ,  and .
To standardize the composite score we looked to the literature for a logical specification limit.
Prior research across numerous usability studies suggests that systems with "good-usability" typically have a mean rating of 4 on a 1-5 scale and 5.6 on a 1-7 scale .
Therefore we set the specification limit to 4.
To arrive at a standardized zequivalent for composite satisfaction we subtracted the average rating of a user's satisfaction score from 4 and divided by the standard deviation.
While the specification limits of 4  and 5.6  are good guideposts for setting specification limits they should be used as starting points.
How would you describe how difficult or easy it was to complete this task?
Very Difficult Very Easy 1 2 3 4 5 How satisfied are you with using this application to complete this task?
Very Unsatisfied Very Satisfied 1 2 3 4 5 How would you rate the amount of time it took to complete this task?
We created a single, standardized and summated usability metric for each task by averaging together the four standardized values based on the equal weighting of the coefficients from the Principal Components Analysis.
To be sure our method of standardization was properly reflecting the relationship built from the PCA, we regressed the scores from the 1st PC with the average of our four standardized metrics for each data set .
As can be seen from the Fitted Line Plot in Figure 3, there is a very strong positive correlation between the scores calculated from the PCA and from the method of standardization .
Identifying ideal task times presents an interesting challenge: how long is too long for any given task?
When comparing task times between products, a simple T-Test of the means will identify significant differences.
For looking at only one set of times, the point at which a task takes too long is not as easy to define.
It is not indefinable, just difficult to define in an absolute sense without some arbitrariness.
This strong relationship suggests that using the average of the four standardized metrics will adequately mimic the relationship built from the Principal Components Analysis.
This standardized and summated usability metric can now be used for analysis as well as for comparisons across tasks and studies.
Our Quantitative Model of Usability  now reflects the equal weighting of the standardized component metrics to summarize the construct of usability.
Four data sets and 1860 observations provide a starting point for further investigating this relationship between usability metrics.
It is encouraging that similar results were obtained under different testing conditions, with different products and using different test administrators with slightly different testing protocols.
Testing a greater variety of products with a broad spectrum of users will provide more insight into the validity of this model and approach.
Future analyses are necessary to provide an indication of how versatile this model is in different domains and with different interfaces .
As stated by Molich, et.
We acknowledge that the reliability of any metrics procured from a summative evaluation can be equally dependant on these factors.
However, having a model for deriving a standard measure is also a powerful tool to evaluate differences in testing procedures.
We also examined the relationship when including two additional metrics--help access and click counts.
The two metrics have significant and moderate to strong correlations with the existing four metrics .
We included each variable in the PCA for the respective data sets to see how their inclusion affected the variable weights .
In data set B, the addition of click counts moderately affected the coefficients and slightly reduced the variance.
In data set C'04, Help was accessed in 72 of the 778 observations or about 10% of the time.
Its inclusion also slightly changed the coefficients and brought the amount of variance down below 50% for the 1st PC .
Our method summarizes the majority of variance in four metrics commonly used to assess the usability of a product in a summative evaluation.
Whether these metrics properly quantify "usability" is a much larger discussion and we do not claim to be definitively measuring the construct of usability.
A summated usability metric is only as good as its underlying component metrics and to the extent that ISO and ANSI have properly identified those is certainly worth discussion.
Others might add more metrics to a summative model, such as measures for learnability or memorability , .
Still others might argue for fewer measures for the sake of expediency or to remove subjectivity.
For example, identifying errors and error opportunities is both time consuming and arguably the most subjectively built metric-- not all analysts will agree on what constitutes an error or error opportunity.
Errors are also not always included in models of usability ,.
There are strong opinions both for and against including errors in a summative model.
We excluded errors from our PCA analysis and found that the 1st PC can still summarize the majority of variance in the three remaining variables.
The three-variable model also had roughly equally weighted variables  - see Table 6.
Error analysis plays a crucial role in formative evaluations when the goal is to uncover usability problems in an interface.
In our data sets, users performed a task successfully, quickly and reported a high level of satisfaction yet committed some undesirable errors.
Only the error measurement reflected this "unusable" aspect of the task.
If the goals of a summative evaluation require certain metrics to be evaluated, then this method of combining standardized metrics can still be used.
The analyst should check the correlation of the metrics and run a PCA to assess the coefficients for weighting and amount of variance explained.
All things being equal, it's better to include more variables than less in a summative metric.
The point of diminishing returns occurs when variables added reduce the amount of variance accounted for by one PC to below 50%.
This did not occur with the addition of click counts in data set B  but did with the addition of Help Access in Data Set C'04 .
The major drawback with adding or subtracting variables would be that a score created with 3 variables cannot be compared to a score created with 4 or 5 variables.
Adopting a standard that captures the majority of the variance based on the most universal metrics is recommended.
A single, standardized and summated usability metric  cannot and should not take the place of diagnostic qualitative usability improvements typically found in formative evaluations.
When a summative evaluation is used to quantitatively assess the "before and after" impact of design changes, the advantage of one score is in its ability to summarize the majority of variance in four integral summative usability measures.
SUM has two additional advantages.
First it provides one continuous variable that can be used in regression analysis, hypothesis testing and in the same ways existing metrics are used to report usability.
Second, a single metric based on logical specification limits provides an idea of how usable a task or product is without having to reference historical data.
This score can then be used to report against other key business metrics.
SUM can never replace all the information inherent in the component metrics, but like a FICO score, an IQ score or even the Richter scale, the ability to provide high-level summary information about a complex construct with one number should prove helpful for informing and making decisions about usability.
Data from four summative evaluations indicates that our model provides a versatile method that can be used to develop a single, standardized and summated score for analyzing and reporting usability metrics.
The authors would like to thank Intuit, Inc. for providing the facilities and opportunity to conduct research within usability testing.
We further thank Grace Pariante, Shara Barnett and Jen Moore for their support collecting data used in our analysis, Jim Lewis, Lynda Finn, Rolf Molich, Wayne Gray and Kaaren Hanson for reviewing our research and providing feedback on previous versions of this paper.
Consolidating the ISO Usability Models.
Paper presented at 11th annual International Software Quality Management Conference.
Common industry format for usability test reports .
Washington, DC: American National Standards Institute.
A metric for hypertext usability.
11th Annual International Conference on Systems documentation, .
Implementing Six Sigma: Smarter Solutions Using Statistical Methods.
SUS: A "quick and dirty" usability scale.
The scree test for the number of factors.
Development of an instrument measuring user satisfaction of the human-computer interface.
Application of Magnitude Estimation for Evaluating Software Ease of Use.
First USA-Japan Conference on Human Computer Interaction, Amsterdam: Elsevier Science Publishers.
Dunteman, George H,  Principal Components Analysis.
In Sage University Papers Series Quantitative Applications in the Social Sciences ; No.
A practical guide to usability testing.
Frokjaer, E., Hertzum, M., and Hornbaek, K.  Measuring usability: are effectiveness, efficiency, and satisfaction really correlated?
Calculating, Interpreting, and Reporting Cronbach's Alpha Reliability Coefficient for Likert-Type Scales.
In 2003 Midwest Research to Practice Conference in Adult, Continuing and Community Education.
Hair, Anderson, Tatham, Black  Multivariate Data Analysis Fifth Edition.
Human Reliability Analysis, control and instrumentation.
From Mental Effort to Perceived Usability: Transforming Experiences into Summary Assessments.
In the Extended Abstracts of the 2004 conference on Human Factors and Computing Systems .
The Nature of Six Sigma Quality.
Technical Report, Government Electronics Group, Motorola Inc. Scottsdale, AZ.
Discarding variables in a principal component analysis 1: Artificial data.
The application of electronic computers to factor analysis.
The Software Usability Measurement Inventory: Background and usage.
London, UK: Taylor and Francis.
SUMI: The Software Usability Measurement Inventory.
Lewis, J  A Rank-Based Method for the Usability Comparison of Competing Products.
In Proceedings of the Human Factors and Ergonomics Society 35th Annual Meeting San Francisco California .
Psychometric evaluation of an afterscenario questionnaire for computer usability studies: The ASQ.
Psychometric evaluation of the Post-Study System Usability Questionnaire: The PSSUQ.
Atlanta, GA: Human Factors Society.
International Journal of Human-Computer Interaction, 7, 57-78.
Handbook of Human Factors and Ergonomics 3rd Edition.
Martin,P and Bateson, P  Measuring Behaviour.
Master usability scaling: magnitude estimation and master scaling applied to usability measurement.
Nielsen, J. and Levy, J.
Identifying Spec Limits for Task Times in Usability Tests.
Retrieved September 13, 2004, from Measuring Usability Web site : http://measuringusability.com/time_specs.htm 43.
Retrieved September 13, 2004, from Measuring Usability Web site: http://measuringusability.com/z_calc.htm 44.
Sauro, J & Kindlund E.  Making Sense of Usability Metrics: Usability and Six Sigma, in Proceedings of the 14th Annual Conference of the Usability Professionals Association, Montreal, Canada 45.
Psychophysics: Introduction to its Perceptual, Neural, and Social Prospects.
