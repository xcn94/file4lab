Visual exploration of volume data often requires the user to manipulate the orientation and position of a slicing plane in order to observe, annotate or measure its internal structures.
Such operations, with its many degrees of freedom in 3D space, map poorly into interaction modalities afforded by mouse-keyboard interfaces or flat multi-touch displays alone.
We addressed this problem using a what-you-see-is-what-you-feel  approach, which integrates the natural user interface of a multi-touch wall display with the untethered physical dexterity provided by a handheld device with multi-touch and 3D-tilt sensing capabilities.
A slicing plane can be directly and intuitively manipulated at any desired position within the displayed volume data using a commonly available mobile device such as the iPod touch.
2D image slices can be transferred wirelessly to this small touch screen device, where a novel fast fat finger annotation technique  is proposed to perform accurate and speedy contour drawings.
Our user studies support the efficacy of our proposed visual exploration and annotation interaction designs.
Since volume data has internal structures that could be highly complex, visualization and interaction methods are needed to accentuate occluded structures and their relationships.
Despite the immense research effort and progress in volume visualization techniques, interactions with such applications are still very much through mouse-keyboard controls, or relatively large and expensive virtual-reality apparatus such as the virtual workbench and haptic devices.
In this paper, we describe our efforts in developing an affordable volume data visualization and interaction system that uses interaction devices that are widely available in the consumer market.
Our system employs a standard upright multi-touch display, which can be a custom-made multitouch display  or a multi-touch LCD display, together with a handheld device with multitouch and 3D-tilt sensing capabilities, like the iPhone or iPod touch, for example.
By combining the strengths of both devices into a seamless integrated system, we are able to provide direct and tangible manipulation of volume data in ways that are intuitive, efficient and easy to use.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To our knowledge, this is the first visualization system that fully integrates a commodity handheld device  and an upright multi-touch display to support direct and tangible exploration and annotation of volume data.
We present several novel interaction methods that provide efficient exploration and annotation of volume data on this WYSIWYF system .
They are summarized as follows:
Firstly, we propose using a tangible handheld device such as an iPod touch to slice the volume data displayed on the upright multitouch screen.
By directly touching the wall display with the handheld device, we can initialize a 3D slicing plane at the very locality of the touch.
Using the tilt sensing capability of the iPod touch, we can also synchronize the slicing plane's orientation to that of the handheld device during the contact.
In the same way, the slicing plane can be translated by directly sliding the device over the wall display, with the visible volume data acting as a useful reference image.
Such direct manipulation provides an intuitive and efficient means to walk through various cross-sections in the volume data.
We extended the WYSIWYF design principle further to allow the user to remotely manipulate the slicing plane on the wall display.
A combination of local fingers and tilt gestures on the handheld device allows near full 3D control of the slicing plane.
Being able to move a distance away from the large multi-touch wall display helps alleviate the problems of limited viewing angle, hand occlusion and annoying pixilation artifacts .
A unique feature implemented in our system is the ability to use the multi-touch capability of the handheld device to anchor carefully chosen point and line constraints about which the slicing plane can be rotated in 3D space.
Such constrained rotation is helpful in simplifying the complex manipulation of a plane with many degrees of freedom.
Lastly, with the ability to transfer 2D sliced images to the small screen on the handheld device, users can draw simple visual annotations on these images and have these annotations transferred immediately back to the volume data visualization on the wall display.
To support this feature, we proposed a novel technique for fast and accurate finger tracing of smooth continuous contours in a 2D image that is displayed on a small touch screen.
Avila and Sobierajski  proposed a haptic interaction approach for volume visualization; they simulated a virtual tool by using the position and orientation provided by a haptic device.
Szalavari and Gervautz  introduced the Personal Interaction Panel system consisting of a tracked simple clipboard and pen; this setup enables a natural twohanded interface for assorted applications.
Their work supports constrained rotation on 3D objects by inserting a virtual pin on the 2D workspace via a digital pen.
Unlike , we do not require a passive prop and its associated tracking system.
Instead, we integrated various functionalities available on off-the-shelf handheld devices, such as the multi-touch screen and tilt-sensing, to perform volume data annotation and manipulation.
Similar to , we also support constrained rotation of a slicing plane by direct specification of up to two rotation points, but we use touch inputs on handheld device's multi-touch display.
Tangible interfaces seek to interact with virtual objects by manipulating tracked props in physical space.
Qi and Martens  investigated the problem of positioning a slicing plane within a volume data set by using a wooden cube and a pen/frame in physical space; tracked by a vision-based system, the wooden cube controls the volume data orientation and zoom factor while the pen/frame controls the orientation and position of the clipping plane.
According to Ratti et al.
Since this work relates more to the virtual reality and tangible aspects in this scheme, we review mainly these two categories: Virtual reality approach.
This approach focuses on immersing users in a virtual environment with the 3D volume data and optionally some 3D user interface elements and virtual simulation tools.
The use of tracked props in existing works of this approach requires users to switch visual foci between the handmanipulated tangible interface and the computer screen.
Our use of a large interactive surface for both image display and user interaction with a tangible interface provides a single visual focus for both visualization and interaction, thus removing the stressful time-shared visual foveation.
Moreover, the use of a computation-capable tangible device with a built-in multi-touch surface enables instantaneous transfer of desired 2D slice images for volume data annotation and portable take-away storage.
Tangible approach with handheld devices.
Today, many consumer handheld devices come with built-in accelerometers to sense 3D tilting.
These devices enable us to develop a low-cost system for volume data exploration.
Zsaki  made use of a two-axis accelerometer as a tangible input to slice a given volume data set in a natural manner.
Besides using the tangible device to slice volume data, the tangible interface in our system is able to receive and display the 2D sliced images.
These reference images displayed on the handheld device are useful for supporting visual annotation as well as allowing users to locate and specify anchor point for constrained point and line rotation of the slicing plane.
Applications with handheld device interaction.
Wilson and Sarin  developed an interactive surface called BlueTable; it can automatically connect to mobile devices, allowing us to conveniently transfer photos and show them on the table.
Since mobile devices have higher visual and input resolution than local regions on large displays, Olwal and Feiner  suggested tracking mobile devices over large displays to take advantage of mobile devices to support better graphics and user interaction.
Another application class that often integrates handheld/mobile devices and large displays is multi-player games.
Various natural interactions such as throwing, touching the table, tilting and shaking are explored in this work.
Contacting handheld devices on upright displays.
Directly touching a display with a mobile device as a form of interaction is a relatively new concept.
Hardy and Rukzio  proposed using such a touch action to perform position selection, data picking or dropping over a large display.
Compared to these works, the main novelty of our work is the use of a direct touch contact between the handheld device and upright multi-touch display to enhance the control of the slicing plane during the exploration of the volume data.
The convergence of these two types of interactive devices allows the design of useful interactive features such as the ability to initiate constrained slice plane rotation and fast visual annotation from a remote multi-touch display on the tangible interface.
There are numerous works that involve interaction between handheld devices and an assortment of display systems.
Handheld devices as remote controllers.
Similar to TV and VCR remote controllers, handheld devices equipped with multi-touch and 3D-tilt sensing capabilities can also serve as remote controllers for supporting user interaction with a co-located display system.
Dachselt and Buchholz  proposed the throw and tilt gestures on handheld devices to interact with a large display.
More recently, Katzakis and Hori  applied the accelerometer and magnetometer on smart phones to manipulate the rotation on 3D virtual objects.
Our system setup consists of an iPod touch , a large vertical multi-touch wall display, a wireless router and a server PC .
Wireless connection with low-latency UDP is used to transmit sensor data and files between the iPod touch and server.
The multi-touch wall display transmits touch information to the server through UDP over wired Ethernet link.
For the tangible device, we used an iPod touch for rapid prototyping, and employed many of its built-in features, which include a 3-axis accelerometer, multi-touch screen, wireless network, and 3D graphics capability.
In our usage configuration, the large multi-touch wall display is employed for general volume data visualization purposes.
Its large screen format is amenable to group viewing, such as when a doctor is teaching human anatomy to a small group of medical students.
Basic 3D data manipulation operations can be done on this display using finger gestures.
The handheld device, e.g., an iPod touch, is used as a tangible interface that allows the user to interactively position a slicing plane, control its orientation, and perform visual annotations.
The next section details the different groups of interaction designs involved in realizing this integrated system.
Our volumetric data visualization application uses the Visualization Toolkit   to render the volume data.
A small pop-up window on top-left of the screen shows sliced image as we manipulate the slicing plane, see Figure 1.
An integrated interaction module receives and processes input data both from the multi-touch wall display and the remote handheld device.
Wall display interaction-sensing is implemented as a thread, which records all multi-touch events and stores them in a linked-list data structure shared with the application thread.
The iPod touch interactionsensing is implemented as another thread that receives and processes user input from the mobile device, as well as sends 2D sliced image data to the device upon request.
For completeness, we briefly describe how the volume data is manipulated on the multi-touch wall display.
These walldisplay-based operations help set the reference of the volume data image to a convenient 3D orientation so that the user can begin to use the tangible handheld device to specify the desired slicing plane.
Since the interactive surface is essentially 2D, a combination of single finger, two-finger and multiple-finger gestures are required to perform objected-centered rotation, scaling, and translation operations, respectively, as summarized in Figures 3.
The iPhone SDK is used to build a client program to sense the touch and tilt events, and transmit them to the server PC.
At the same time, the client program also receives and displays the small-sized 2D sliced image received from the server program.
Since the iPod touch is used for slicing plane manipulation and also visual annotation, the client program on the mobile device operates under three modes: remote control, image manipulation and annotation.
In remote control mode, the device sends both accelerometer and multi-touch signals to the server for controlling the slicing plane.
Moving-window low pass filtering is used on the device to reduce the jitter inherent in the accelerometer signal.
2D sliced images can be automatically loaded to the device when the tilt activation button  is released.
The image manipulation mode allows the 2D sliced image to be translated, rotated and scaled using simple finger gestures on the device's multi-touch screen.
Finally, the annotation mode allows users to quickly draw outlines or contours on the 2D sliced image and to send their sketches back to the server PC for visualization.
Basic volume data manipulation on the wall display.
Visual exploration of volume data often requires a user to manipulate the position and orientation of a slicing plane in order to observe, annotate or measure internal structures within the 3D volume data.
The flexible and direct manner by which this slicing plane can be manipulated would greatly enhance the exploratory power of the visualization system.
The primary strength and novelty of our system is the combined use of a multi-touch wall display and a tangible tilt-aware interface.
We describe novel and intuitive interaction design afforded by this system.
This is exactly the type of direct manipulation afforded by our system.
The physical contact between the mobile device and the volume data that is being visualized on the multi-touch display provides an intuitive way for a user to specify the initial position and orientation of a slicing plane within the 3D data.
In order to enable a standard multi-touch wall display to detect the contact of the tangible device, we attached a simple wooden base to the iPod touch, thus providing it with two rounded protruding prongs .
These prongs allow the mobile device to contact the multitouch display with a simulated two-finger touch gesture of fixed Euclidean spacing d, which the application has been calibrated to detect.
The slicing plane's new position on the x-y plane can be specified by directly touching the wall display at any desired location with the mobile device .
Once in contact with the wall display, the user can press the tilt activation button  on the mobile device to inform the client program to continuously transmit tilt sensor data to the server.
The orientation of the slicing plane can now be computed, and will follow the iPod touch as the device is rotated against the wall display.
This way of controlling the slicing plane is intuitive and gives the user the sense that he is holding a physical slicer in his hand.
Making two-pronged touch contact and moving the iPod touch along the wall display's x-y plane will translate the slicer on its x-y plane .
The flexibility of our integrated system also provides users the option of positioning the slicing plane by translating the 3D volume data on the wall display using the panning gesture .
The translation of the slicing plane about its coordinate system can be done by  remotely using various finger gestures on the iPod touch and/or  sliding the iPod touch against the wall display.
The handheld device's 3-axis accelerometer is unable to detect rotation about the z-axis  since no gravitational force change is experienced by the device's inertial sensor when it is held horizontally and facing up.
In order to perform constrained rotation about the slicing plane normal, manipulation of the reference volume data on the wall display is needed.
This is achieved when one finger is held stationary on the multi-touch surface while another finger moves.
The direction of the moving finger determines clockwise or counterclockwise rotation.
The slicing plane is a virtual window with boundary in the 3D space, corresponding to what is shown on the handheld device screen.
The slicing plane can be translated either along its own coordinate axes  or the wall display coordinate axes .
Incremental translation about the slicing plane coordinate system is more intuitive and has proven useful when there is a need to move the slicer to a precise position to acquire a 2D image slice for geometric measurements.
Figure 5a shows how the multi-touch capability of the mobile device is used as a convenient remote interface for controlling the slicing plane translation.
One finger gesture on the mobile device's multi-touch screen translates the slicer in x-z plane while two fingers moving gesture translates the slicer along the y-axis.
The slicing plane's 3D orientation can be manipulated by performing corresponding tilt gestures on the handheld device, while the tilt activation button is being touched.
The slicing plane in the virtual space will rotate in unison with the tangible interface, matching angle for angle, axis for axis.
Unfortunately, a horizontally held iPod touch cannot detect rotations about it z-axis  since no gravity changes occur.
Thus, the slicing plane can only be rotated along the x and y axes.
The constrained rotation of the 3D object about the slicing plane normal, which can be performed using finger gestures on the wall display  can compensate for this limitation.
By default, the slicing plane will rotate around an anchor point located at the mid-point of the slicing plane's edge that is the closest to the iPod touch when contact was made with the wall display.
6, which shows a user trying to orientate the slicing plane such that it flushes and touches the sloping cut surface of a 3D cylindrical object.
Due to the many axes of freedom in controlling the 3D orientation of a plane in free space, this is not an easy feat to achieve for a novice.
By providing the ability to interactively shift the anchor point about which the constrained plane rotation can occur, this greatly simplifies the exploratory process required to tilt the slicer to the desired 3D orientation.
Once the slicer has intersected a strategic position in the 3D object, the user merely needs to use the multi-touch screen on the iPod touch to visually move an anchor ball on the slicing plane to any desired 2D location on the plane.
As far as we are aware, this is the first interactive volume visualization system that uses a single tangible device to provide tangible slicing plane manipulation and the ability to interactively shift the point of constrained rotation.
An additional feature of our interaction design is the ability to sequentially place  anchor points on the slicing plane, thereby allowing constrained rotation about a line.
This is especially useful in the visualization of fabricated 3D machine parts where line geometry is prevalent.
Once the slicing plane is in position and the relevant 2D slice image data is acquired, the next task that may be performed is visual annotation.
Releasing the tilt activation button on the iPod touch after manipulating the slicing plane will wirelessly transfer the 2D sliced image data.
With the 2D image conveniently in the hand of the user, he can now perform the visual annotation operation remotely while seated comfortably away from the wall display.
Tracing annotation outline over the slice image on a small multi-touch screen can be inaccurate and tedious.
We next discuss the issues related to this problem and propose a novel technique to perform fast visual annotation on a small multi-touch screen.
Easy manipulation of slicer to desired 3D orientation using a novel multi-touch technique for specifying an arbitrary point on the slicing plane, about which the constrained plane rotation can be performed.
With one finger touching the tilt activate button, another finger from the other hand can be used to slide the anchor ball to a new location  from its initial default location .
The fat finger problem  is a well-known problem related to the difficulty to accurately locate a desired point on a touch display by using one's finger.
Unlike a pen or stylus that has a sharp tip, the human finger is soft and compresses to a large blob-like contact point when pressed against a multi-touch surface.
The centroid of this detected blob may not coincide with the point on the image where the user wishes to draw the desired outline.
Using cross-hair visual feedback does not help because of the occlusion of the human finger over the visual area of interest.
The two finger technique of Benko et al.
An interesting solution to the fat finger problem is LucidTouch by Wigdor et al.
By sensing finger touches from behind the display and augmenting the screen visuals with point cursors and pseudo translucent shadow fingers, users can select accurate screen locations without finger occlusions.
Unfortunately, this is not currently a solution on the ubiquitous iPhone or iPod touch.
In fact, the simplest way to alleviate this problem is to zoom up the visual region of interest so that the object boundary region where the outline is to be drawn is now of size commensurate to that of the human finger tip .
However, this image zooming gives rise to another problem.
In order to complete the outline drawing of the entire shape, the user has to go through many individual zoom-up sections of the image .
When the user removes his finger to switch mode and then places it back to continue the drawing from where he last stopped, overlaps and small discontinuities in the outline can result due to the inaccurate placement of the user's finger over the last drawn point .
There are basically two interaction modes during visual annotation .
The PAN mode allows the user to pan the image to reveal out-of-view regions that may require visual annotation.
A thick green boundary around the image is used to indicate PAN mode.
The DRAW mode allows the user to annotate by tracing an outline along the desired object boundary in the image.
In this mode, the image remains stationary and a yellow line is drawn under the sliding finger.
The DRAW mode is indicated by a thick red boundary around the image.
The users can switch between the two modes by touching the thick red or green border with the same tracing finger  or with the non-gripping thumb of the hand that is holding the handheld device .
Our user performance evaluation on the visual annotation task shown in Figure 12 revealed that the two-handed switching technique is on average about 28% less accurate and 15% slower than the one-handed technique.
Moreover, some users complained that coordinating repeated PAN-DRAW switching using two hands was confusing and their finger that performs the mode switching sometimes occlude relevant image regions.
We propose a novel technique called the Fast Fat Finger Annotation Technique  to address this problem.
The user normally starts annotation with an image view that displays the complete object boundary.
When he touches the boundary area where he wishes to start the drawing, the image can automatically zoom up  so that enough visual resolution is now available to avoid the fat finger inaccuracy when drawing.
The image is now in DRAW mode.
Figure 10 illustrates the various steps in using F3AT.
One finger is used to trace the desired outline along the boundary  until it reaches the red border , where no more drawing can be done since the remaining boundary in the image is out of view.
At this instant , the annotation mode automatically switches to PAN mode .
The border changes its color to green, indicating to the user that he can freely pan the image in the opposite direction to the drawing direction to reveal more image boundary region to draw .
The main novelty in the proposed F3AT is the constant monitoring of the direction of finger movement during the image pan.
V t > 0 where V1 is the motion vector of the finger when it last touched the red border and Vt is the current motion vector of the finger in PAN mode.
Essentially, any motion vector with an angular difference of 90 from V1 is sufficient to trigger a switch back to DRAW mode.
The main advantage of using the F3AT to trace lines on a small touch screen is the fact that the user can complete the contour drawing along the boundary of a large image region without lifting up the drawing finger even once.
This avoids overlaps and discontinuities in the outline .
Additionally, since the F3AT automatically switches between PAN and DRAW mode based on the context of the user's finger position  and motion direction , this allows the user to perform annotation more speedily than the traditional approach of manual toggling between PAN and DRAW mode as is evident from our user study results.
The task completion time using the WYSIWYF system is compared to a traditional keyboard-mouse system, which uses a combination of mouse  and keyboard control .
A graphical depiction of the keyboard and mouse functions was made available for user reference.
Users could practice performing task 1 with the interface currently under trial for up to 5 minutes.
After the practice, the slicing plane was reset to its initial state and the time taken to perform task 1 was recorded.
The user was then given another task 2 without the possibility of practice to see if they can transfer their newly acquired slicer manipulation skills to a novel situation.
As expected, the direct and tangible 3D manipulation of the slicing plane afforded by our system allowed the user to perform the tasks much more speedily.
For the familiar task 1, it was 97% faster and for unfamiliar task 2, it was 142% faster.
The increase of about 15s when moving from task 1 to task 2 using the proposed system compared to a much longer 42s using the mousekeyboard approach suggests that users are better at transferring the practice skills to another new task using the WYSIWYF approach.
The intuitive and direct manner in which the slicing plane can be positioned  and orientated  makes it easier for the users to figure out how to perform the necessary operations to manipulate the slicing plane to the target.
The comparatively lower variances in timing for our proposed system also suggest that the more intuitive integrated WYSIWYF mode of interaction allows a larger cross-section of users to acquire similar level of competency with limited usage exposure on the system.
Two series of user studies were conducted.
A total of 10 participants  aged between 22 and 30 were involved.
Prior to the trial, none of the participants have used the systems in our study.
To remove biases arising from task familiarization, equal numbers of participants used one technique first to complete the required task and then the other and vice-versa.
The goals of this study are to observe how well users of the proposed interface are able to transfer their practice skills on one task to another new task and the variability in the acquired competency level among a small population of naive users.
A traditional mouse-and-keyboard interface was used to obtain a set of baseline timing measures that allows us to gauge the quantum of improvement afforded by our proposed WYSIWYF approach.
The study compares the speed performance in manipulating the position and orientation of the slicing plane from an initial state to a new one.
The second user study compares the timing performance in doing contour tracing on a small touch screen device  using the manual approach of toggling between PAN and DRAW modes with one hand and the novel F3AT.
Table 2 shows that the proposed F3AT method is 49% faster than the manual method.
Every single user drew the required contour faster using F3AT.
User studies conducted support the efficacy of our proposed visual exploration and annotation interaction designs.
In future, we also would like to explore our interaction techniques in astrophysical visualization settings .
In fact, our observations were that for faster drawing speed, which F3AT affords, the more likely it is that the user gets negligent in keeping to within the stipulated line trace width.
However, the greatest advantage of F3AT is its ability to draw an entire outline without lifting one's finger.
The benefits are clearly illustrated in comparison of the visual quality of drawings produced by the two techniques .
The contour drawn using F3AT is smooth and continuous while that drawn using the manual method has many overlaps and discontinuities.
We have described a novel interactive visual exploration and annotation system that combines the strengths of a standard upright multi-touch display and a commonly available handheld device that acted as an intuitive tangible interface.
We described how the WYSIWYF concept was realized using the 3D-tilt sensing and multi-touch capability on the handheld device.
A novel feature of this concept is the ability to directly and efficiently manipulate a slicing plane on the multi-touch wall display by making direct contact with the handheld device.
We can also use this same handheld device to remotely manipulate the slicing plane while activating the constrained rotation capability to selectively reduce the degree of freedom of the slice plane.
W. Schroeder, K. Marti, and B. Lorensen.
The Visualization Toolkit: An object oriented approach to 3D graphics.
A. Sears and B. Shneiderman.
High precision touchscreens: Design strategies and comparisons with a mouse.
Poker surface: Combining a multi-touch table and mobile phones in interactive card games.
M. Spindler, S. Stellmach, and R. Dachselt.
PaperLens: Advanced magic lens interaction above the tabletop.
F. Steinicke, K. Hinrichs, J. Schoning, and A. Kruger.
Multi-touching 3D data: Towards direct interaction in stereoscopic display environments coupled with mobile devices.
In AVI 2008, Workshop on Designing Multi-Touch Interaction Techniques for Coupled Public and Private Displays, 46-49, 2008.
S. Subramanian, D. Aliakseyeu, and J.-B.
Empirical evaluation of performance in hybrid 3D and 2D interfaces.
Z. Szalavari and M. Gervautz.
The personal interaction panel - A two-handed interface for augmented reality.
S. Voida, M. Tobiasz, J. Stromer, P. Isenberg, and S. Carpendale.
Getting practical with interactive tabletop displays: Designing for dense data, "fat fingers," diverse interactions, and face-to-face collaboration.
D. Weiskopf, T. Schafhitzel, and T. Ertl.
Texturebased visualization of unsteady 3D flow by real-time advection and volumetric illumination.
D. Wigdor, C. Forlines, P. Baudisch, J. Barnwell, and C. Shen.
LucidTouch: A see-through mobile device.
BlueTable: Connecting wireless mobile devices on interactive surfaces using vision-based handshaking.
Y. Yokokohji, R. L. Hollis, and T. Kanade.
WYSIWYF display: A visual/haptic interface to virtual environment.
Cutting | Plane: An interactive tool for exploration of 3D datasets via slicing.
