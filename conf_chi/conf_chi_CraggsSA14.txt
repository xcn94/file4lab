During online search, the user's expectations often differ from those of the author.
This is known as the `intention gap' and is particularly problematic when searching for and discriminating between online video content.
An author uses description and meta-data tags to label their content, but often cannot predict alternate interpretations or appropriations of their work.
To address this intention gap, we present ThumbReels, a concept for query-sensitive video previews generated from crowdsourced, temporally defined semantic tagging.
Further, we supply an open-source tool that supports on-the-fly temporal tagging of videos, whose output can be used for later search queries.
A first user study validates the tool and concept.
We then present a second study that shows participants found ThumbReels to better represent search terms than contemporary preview techniques.
For example, of the top five YouTube search results for the term "apple juice"  only one returns a contextually relevant thumbnail .
Only by watching the video can the user know the actual content; undermining the purpose of the surrogate objects in providing a mechanism for discrimination.
To address this problem, we created ThumbReels.
The ThumbReel concept involves two steps.
First, viewers  temporally tag videos whilst watching them--these tags are stored on the search server.
Second, when a search query is issued, the tag data, along with traditional meta-data is used to create a result list.
A ThumbReel is then generated for each result.
ThumbReel frames are extracted from points on the video timeline where the crowd's tags match the query terms.
These are then animated to create a query-sensitive preview of the video.
We validate the ThumbReel concept in two steps.
First, we crowdsource the tagging of three videos using our opensource tool.
Second, we evaluate the generated ThumbReels against traditional preview mechanisms.
This paper therefore contributes:  The ThumbReel concept,  An opensource tool for crowdsourcing temporal video tags, with accompanying proof of operation, and  An evaluation of ThumbReels; highlighting a significant proportion of participants found ThumbReels to better represent search terms in comparison to contemporary preview techniques.
Thumbnail previews are surrogate objects used by VideoSharing Websites  to provide users with concise representations of content.
Video generated over half of all IP traffic in 2012; Cisco predict this will account for 8090% of global consumer traffic by 20171 --underlining the importance of techniques that allow users to discover and discrimintate relevant content.
VSWs, in response to user-defined queries, typically return search results as a populated list of surrogate objects, composed of a thumbnail and author defined metadata such as a title and description.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
The process of producing surrogate objects to represent previews of video content is widely studied.
Poster frames are contemporary thumbnails and filmstrips are animations of multiple poster frames extracted at set intervals from the video.
Most VSWs present static thumbnails, however adult websites commonly use filmstrips to preview content.
When seeking online videos, users engage a search engine with the intention of discovering specific content using query terms rather than explicitly stating their intent .
When authors provide video content and meta-data  their intent is to gain an audience.
Allowing the author to curate thumbnails returned in a search may present an `intention gap': a discrepancy between the intended information sought by the user and the perceived intention by the content author .
While the observed intention gap may purely be due to the author mistakenly choosing the wrong thumbnail, or indeed that only one thumbnail is available for the whole content, it is well known that the affordance of author curated surrogates can invite deliberate misrepresentation or intentional abuse, such as the meme of "Rick-Rolling."
Previous work has shown that users would discover relevant content more rapidly when thumbnails are dependent on context  and that these are preferable to thumbnails chosen by the video authors .
Despite this, none of the popular VSWs appear to return query-sensitive thumbnails when populating search results, employing algorithmic or author-curated thumbnails instead.
ThumbReels address the `intention gap' by providing a crowdsourced set of semantic temporal tags for the video content.
These tags  can be used to create a preview that is tailored to a user's query terms when searching for content.
To create a ThumbReel we need be able to both gather tags about a video and to generate a preview.
Video content on VSWs contains both surface and semantic features, the latter including metadata such as the title and description.
There are additional semantic metadata schemes for captioning, subtitles and categorisation; but "these elements are not constrained to any framework or ontology, making automated interpretation difficult" .
Whilst researchers have applied "considerable effort" into making video content on the Web more accessible, it still remains opaque on websites .
The lack of semantic information  creates problems related to synonymy and polysemy and can reduce the efficiency of content search .
Given the use of semantic query terms when searching for content, the need for better metadata about the actual video content is critical to discoverability.
To source tags we created a Web based tool using HTML and Javascript and hosted this as a Web page on publicly accessible servers .
The tool enables a viewer to watch a video and describe what is happening, using keywords.
When they start to type, the video is paused and the tag along with timestamp of the current frame is recorded.
Throughout the process the viewer can see a list of their previous tags along with respective timestamps; the tags of other viewers are hidden so as to not distract or influence.
We have made this tool open-source and is available for download from: http://thumbreels.com.
Enriching semantic data algorithmically is problematic.
Image processing solutions can identify pixel-arrangements to a great degree of accuracy, however they are unable to tell us whether a picture is beautiful or whether an instruction is useful.
Crowdsourcing has successfully enriched the semantic metadata of video  by asking users to describe the content through tagging.
Estell es and Guevara  suggest that people will volunteer their support in return for personal esteem or wellbeing.
ThumbReels consist of frames that are extracted from points on the video timeline where a search term matches previous viewer's tags.
These frames are then animated to create a query-sensitive preview of the video.
With no previously formalised method for constructing ThumbReels, we derived a specification by analysing and replicating the frame rates, sizes, and inter-frame delays used in filmstrips.
Further, observations suggest that users primarily search for videos using semantic query terms  and so we only use these in our preview generation.
We break ThumbReel creation into two steps:  Tag Processing and,  Preview Generation.
The user-generated temporal tags need processing before they are suitable for search and preview generation.
Overall, the crowd-sourced gathering of temporal tags was deemed successful.
Participants created a large number of tags which  were suitable to generate ThumbReels.
While real-world deployment is necessary to understand true crowd dynamics, this study shows promise for this within-video tagging technique.
Tag processing would typically happen `offline' from the user query, while preview generation would occur `on-the-fly' so ThumbReels could best match the user's search terms.
This process consists of:  Extracting a single frame from each point on the video timeline where the query term matches the crowdsourced tags stored in the matrix generated above,  Selecting 10 evenly distributed frames from this extracted frameset in chronological order,  Compile these frames into an animation using an inter-frame delay of 500 ms .
For each of the test videos, we selected a pair of tags from the top ten most frequent.
This pair:  Represented a reasonable set of query terms when searching for the test video, and  Yielded search results that did not return the test video on YouTube.
The chosen terms are shown in Table 1.
Three preview surrogates were produced for each video, with following formats:  A copy of the static thumbnail from YouTube,  A filmstrip by extracting 10 equally spaced frames from the total length of each video, animated at 500 ms intervals, and  A ThumbReel constructed using the chosen query terms and the method described above.
To evaluate the ThumbReel concept, we conducted two user evaluations.
The first aimed to assess the effectiveness of gathering temporal, semantic video tags.
The second aimed to compare the ThumbReel visualisation against traditional preview techniques.
For both evaluations we used three short raw news footage videos.
These were selected to require only a small time commitment from participants given the average YouTube video is approximately 5 minutes.
The videos we used are listed in Table 1.
Task group one  compared thumbnails with filmstrips; task group two  compared filmstrips with ThumbReels.
Each task group consisted of three preview comparisons, six in total.
Respondents were assigned to groups with a formula that balanced responses across tasks and allowed completion of up to six tasks.
In each task the participant was asked to observe a pair of surrogates matching the query terms and to state whether the second was `worse', `the same', or `better' than the first.
The respondent was also asked to supply a qualitative statement elaborating the reason for this rating.
Anonymous participants were recruited by invitation using social networks.
This attracted 95 participants who were directed to the tool.
Participants were offered the ability to tag, using keywords, one or all of three of the videos.
The participants created a total of 904 tags on the three videos.
The crowdsourced tags were manually processed independently by the two primary investigators.
Interpretation disagreements were negotiated where possible; providing an 86% correlation of the 889 usable tags, after erroneous responses were removed .
Those tags not agreed upon  were used as provided by participants.
A summary of the collected tags is presented in Table 2.
A meta-analysis of the was tags was conducted to leave only those categorised as semantic .
As this was a repeated-measures design with non-trivial within-subject correlations across comparison tasks, Generalised Estimating Equations  allowed valid statistical inference.
We collapsed the `worse' and `the same' responses into a single `not better' category and estimated, for each task group separately, the GEE model of the resulting binary dependent variable using a logit link function.
Task Group 1: Across all three tasks, 81% of responses rated the filmstrip as being the better representation of the query terms than the thumbnail displayed by YouTube .
However, the GEE model's two-sided Wald tests strongly rejected the null hypothesis of H0: Exp=1 across all three TG1 tasks.
Two user studies evaluated the effectiveness of crowdsourcing temporal video tagging  and participant perception of ThumbReels in representing query search terms .
Planned future work includes algorithmically creating ThumbReels based on natural-language interpretation of crowd-sourced tags and further evolution to using querysensitive  skims.
Work is also planned to test the viability of dynamic creation versus cached ThumbReels.
Task Group 2: Across all three tasks, 68% of responses rated the ThumbReel as being the better representation of the query terms than the filmstrip .
The GEE model's two-sided Wald tests strongly rejected the null hypothesis of H0: Exp=1 for V1 and V3, but not for V2.
Overall: This study showed that participants found ThumbReels to better represent query serch terms than FilmStrips, which were in turn where better than thumbnails.
Whilst it did not test `real world' searching, it does provide positive support for the ThumbReels surrogate concept.
We wish to thank Kim Kaivanto for his invaluable help with statistical evaluation and Curtis Kennington for help with the creation of the software tools used in this study.
This research was supported by HighWire, a post-disciplinary Doctoral Training Centre at Lancaster University funded by the RCUK Digital Economy Programme through the EPSRC .
Scalability: Given time, and as the number of tags naturally increases, it may become difficult to identify distinct clusters of a specific tag within a video.
One solution is to apply a weighting to clusters of tags, so that these become the primary area for frame extraction.
This results in the discarding of lesser-clustered tags until they reach a determined level of significance as the tag distribution stabilises .
A stabilised tag distribution would also help to mitigate the risk that malicious users will join the crowd and corrupt the work of other volunteers.
Generalisability: Inviting participants to the study via social networking websites, has both a selection bias  and a self-selection bias .
We acknowledge there may be an element of video selection bias present and future work is required to examine whether the preference for ThumbReels can be generalised to videos of different genre, varying length, across different demographics, and different varieties of VSWs.
The use of thumbnail surrogates is not exclusive to VSWs.
Thumbnails are used online by a wide variety of digital content providers, with video and crowdsourced images for product descriptions prevalent amongst online retailers.
These retailers increasingly rely on customer-sourced content to provide thumbnail previews for their expanding catalogues of products.
This presents an alternative use case for ThumbReels by returning previews, images or video frames that are contextually relevant.
Christel, M. G. Evaluation and user studies with respect to video summarization and browsing.
Christel, M. G., Winkler, D. B., and Taylor, C. R. Improving Access to a Digital Video Library.
Estell es-Arolas, E., and Gonzalez-Ladron-de Guevara, F. Towards an integrated crowdsourcing definition.
Halpin, H., Robu, V., and Shepherd, H. The complex dynamics of collaborative tagging.
Haubold, A., and Natsev, A. Web-based information content and its application to concept-based video retrieval.
Content-based Image and Video Retrieval, ACM Press .
Liu, C., Huang, Q., and Jiang, S. Query sensitive dynamic web video thumbnail generation.
Marchetti, A., Tesconi, M., and Ronzano, F. Semkey: A semantic collaborative tagging system.
Workshop on Tagging and Metadata for Social Information Organization at WWW , 8-12.
Steiner, T., Verborgh, R., Van de Walle, R., Hausenblas, M., and Vall es, J. G. Crowdsourcing event detection in YouTube video.
Detection, Representation, and Exploitation of Events in the Semantic Web , 58-67.
Yamamoto, D., Masuda, T., Ohira, S., and Nagao, K. Video Scene Annotation Based on Web Social Activities.
