Such models would help the design and evaluation of existing or future gesture interfaces by quantitatively predicting their efficiency before running extensive user studies.
The existing theoretical tools in user interface design at a motor control level are a set of so-called "laws of action".
They model human performance on tasks including pointing , crossing  and path steering .
However, the existing laws only apply to visually guided performance and therefore may not be appropriate to model freehand open-loop stroke gestures.
A highly desirable goal is to extend the family of laws of action to pen stroke gestures.
However, freehand gesturing is an inherently complicated behavior that involves planning, chunking, and the variability of behavior across different people and different types of gestures.
As we move up the ladder of motor control skills, simple regularities that can be called a law may not exist.
The goal of our current work is to build an approximate "computational" model that can predict the production time of single pen-strokes as a function of the stroke's composition, within a required level of error-tolerance..
This paper presents a quantitative human performance model of making single-stroke pen gestures within certain error constraints in terms of production time.
Computed from the properties of Curves, Line segments, and Corners  in a gesture stroke, the model may serve as a foundation for the design and evaluation of existing and future gesture-based user interfaces at the basic motor control efficiency level, similar to the role of previous "laws of action" played to pointing, crossing or steeringbased user interfaces.
We report and discuss our experimental results on establishing and validating the CLC model, together with other basic empirical findings in stroke gesture production.
Pen stroke gestures have been widely used on many penbased computing devices such as PDAs, tablet PCs or electronic whiteboards for entering text  or triggering commands .
With the increasing availability and popularity of pen-based devices, especially mobile devices, it is likely that we will see even more applications of pen gestures in various environments.
In order to enhance the \user experience of gesture-based interaction systems, numerous research works have been conducted to improve the performance of gesture recognizers  and to design gestures that are easy to learn and remember for users .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Isokoski  proposed a model for stroke gestures that used the number of approximating straight line segments in a gesture as a predictor of complexity correlating to production time.
The underlying assumption is that drawing a straight line segment takes constant time, regardless of the length of the segment.
The model's best correlation result was R2 = 0.85 on Unistroke characters , and it achieved R2 between 0.5 and 0.8 for other gesture sets.
This model provides a useful quick prediction, with the attractive merits of simplicity and ease of application.
However, defining the number of straight line segments needed to approximate a curved gesture is ambiguous.
Furthermore, it does not provide an estimation of the magnitude of the actual production time.
While we will maintain the same first order approximation spirit as this model, our goal is to more closely reflect the complexity in the actual strokes.
At a more fundamental motor control theory level, Viviani and colleagues  investigated human handwriting and drawing behavior in terms of instant movement velocity as a function of curvature, and proposed a power-
A special case of smooth curves is a circular arc with a radius r and a sweep angle , which has a constant radius of curvature R = r .
The total length of the arc is S=r.
The total production time is then:
The model, known as the power law of curvature, indicates that the larger curvature the trajectory has at a given point, the slower the pen motion will be at that point.
This model has been tested in different settings, including drawing trajectories with or without visual guidance .
We will derive some basic assumptions in our model based on this movement law.
Lank and Saund  also employed the power law of curvature and its interaction with the steering law  to study pen-based gestures.
However, their work is more focused on inferring user's target selection intention from the encircling gestures.
The approach we took was to first find common "building blocks" of pen strokes at an appropriate level.
Our model is based on the assumption that any gesture stroke can be decomposed into several "elements", each of which can be modeled by a lower-level model.
The total model is represented by integrating the elemental models.
While making this "reductionism" assumption, we recognize the possibility that the interaction between elements and the user's planning as a whole will result in a shorter or longer gesture production time than the simple sum of all elements.
However, we hypothesize that the sum of elements may still give a first order approximation or a baseline prediction useful for many applications.
In what follows we first build a set of lower-level models of the common building blocks of stroke gestures, based on information from the existing literature and intuition, to be verified by later experiments.
However, V cannot be  in practice.
Three plausible candidate models may be proposed for the production time of a straight line.
Let L be the length of the line:  Constant time model: T = c which is the underlying assumption of Isokoski's model.
Later this power model proved most valid according to our experimental data.
R = R: radius of curvature.
S: total curve length of G .
T: total production time of G 
Pastel  studied human performance of steering through paths with corners, and found that people spent more time steering through paths with 90 corners than with paths with 45 and 135 corners.
However, since a steering task is fundamentally different from the open-loop movement in making gestures, the same trend may not exist in gesture performance on corners.
Since a corner is an abrupt change of stroke direction  and only exists with the arms that form it, it is difficult to define the operational boundaries of a corner.
We therefore define T as "the net contribution of the abrupt direction change to the total production time".
This value may not necessarily be positive .
For generality, we tentatively represent the corner production time as: T = f  f is an arbitrary function of the corner angle , to be found through experiments.
We postulate that any given single stroke gesture can be decomposed into these three types of elements: smooth curves,  straight line segments and corners , each of which can be modeled by their respective elemental models to be established by experiments.
We then put these elements together to form a general summative CLC model of total stroke production time:
Otherwise, it is displayed in red, and the user must repeat the input until it is accepted.
This overlaying feedback helps the user to determine how good the input is, and which part needs to be improved.
The feedback disappears before the user can make a second attempt, in order to prevent the user from visually tracing it.
The gesture samples presented in the experiments were generated using a semi-automatic authoring program, with which the decomposition of the gestures can be interactively specified.
Unlike previous modeling work on visually guided actions, such as target pointing, goal crossing or path steering with explicitly defined error behavior , the error of a stroke gesture is more ambiguous, subjective , or system dependent .
There are potentially infinite numbers of error metrics that can measure the difference between two instances  of a stroke gesture.
Hence it is difficult, if not impossible, to select the "best" error metric that reflects human's  sensitivity and habits in all situations.
Our tactic in this regard is to define an operational procedure that can capture the essence of holding all gesture strokes to the same error tolerance criterion.
We aim to select an error metric that is intuitive, relatively simple , and insensitive to scale.
In our procedure the input gesture U and the sample gesture V are first normalized by scaling and translating so that both of them have a bounding box with the larger side measuring 1 length unit, and centered at .
Then both U and V are re-sampled into M  equidistant sample points.
All experiments were executed on an Acer TravelMate C110 tablet PC using stylus input.
A program was developed for pen gesture study.
In each trial, the program displays a sample  gesture in the top window, with a red dot and an arrowhead indicating the starting and ending points respectively.
The user then draws the prompted gesture with the stylus in the bottom window.
Determined by an error metric , the system decides whether the user's input is acceptable.
Since all gestures are normalized in size, scaling the gestures does not affect E. E effectively measures the "relative" or "percentage" shape distance between the input and the sample.
In our experiments, the acceptance threshold for E was set at 13.5% of the sample stroke length.
Since spatial scaling may also affect the production time of a gesture, we also required the size of the input gesture within 1/1.3 ~ 1.3 times of the sample gesture to be accepted.
These thresholds were selected based on pilot trials.
For each successful trial, we recorded the production time T , percentage error E, and the number of attempts  made until accepted by the system.
Polylines are gestures that consist of only connected straight line segments.
This is the type of gesture used in the ShapeWriter text input system .
Its gesture prototypes consist of straight lines connecting letters on a soft keyboard.
We selected 36 representative gestures from the most commonly used words in the ShapeWriter system as the samples in this category.
Out of these 36 samples, 6 of them consisted of 1 line, 6 consisted of 2 lines, ..., 6 consisted of 6 lines.
The sizes of  the gestures varied between 40 ~ 80mm.
36 samples were selected from the Graffiti gesture set, including representatives of letters, numbers, and functional gestures.
However, in order to reduce the familiarity to the user that may bias the result, all gestures were rotated by 90.
The sizes of  the gestures were set to be roughly equal .
A fully crossed within-participant factorial design was used for each phase.
Each experiment phase was executed in 3 repeated blocks.
Within each block the participant conducted one trial for each sample .
The presentation order of the samples was randomized in each block.
Ten practice trials were performed before each phase started.
Ten right-handed volunteers, aged 20~59, participated in the experiment.
The experiment consisted of 5 categories of pen gestures, each tested in a separate phase of the experiment, as shown in Figure 5.
The first three phases of the experiment used elemental gestures to determine the elemental models, and the last two used composite gestures to test the validity of the summative model .
We sought to find the parameters  and K in equation  by regression on equation  with the experimental data on arcs.
Movements on the primary axes  tended to be more accurate.
Note the different trends of production time and input error as related to orientation.
Although the 45 and 225 lines were faster to produce, they were also among the most error-prone compared with other orientations.
No significant interaction between length and orientation were observed for either E or A.
In summary, the length of a straight line was the predominant determinant of the line's production time and accuracy.
Within a set accuracy threshold, a power model best described the production time as a function of the length.
The orientation of a line also affected time and accuracy, although secondarily.
A diagonal line along the wrist rotation direction was somewhat faster to produce, but horizontal and vertical lines were more accurate.
Note that both models have 2 degrees of freedom; therefore any difference between their fitness with experimental data should not be due to model complexity difference.
The Power model best describes the relationship between length and production time.
Therefore T = mLn  is the best validated elemental model for straight lines although the specific parameters m and n may be subject to individual differences.
Both the number of attempts A and percentage error E  are informative measures of input accuracy.
Both E and A decreased as the length increased, which suggests it is harder to maintain the same relative accuracy with smallersized gestures .
Angles in the middle of the range  were more error prone than the more extreme angles.
The high regression coefficient validated our derivation of equation .
Both radius and sweep angle also had significant effects  on E and A.
Similar to the straight line cases, both error measures decrease as the radius increases.
Arcs with a sweep angle of 180  introduced the least error.
To compute a corner's "net time contribution", we subtracted the time spent on drawing two corner arms  from the total production time of the corner gesture.
Drawing from the experimental data of the straight lines, the participants spent an average of 370ms on straight lines of 30mm.
Therefore T = sample production time - 740ms.
T fluctuates around zero, with corners of 0, 22.5 and 90 contributing negative mean time, and all other corners contributing positive mean time.
However, the absolute values of these contributions were less than 40ms for all angles.
We therefore chose to omit this element in our model.
This prediction was then compared with the mean value of the actual polyline gesture input time Td collected in the polyline tests under the same error tolerance criterion.
Figure 13 plots the correlation between Tp and Td.
The presence of a corner implicitly affects the production time by breaking one stroke segment into two, causing acceleration and deceleration in movement that are reflected in the linear or non-linear forms of T .
Drawing two straight lines with length L1, L2 connected by a corner takes longer time than drawing one straight line with length L1+L2.
For the linear form T model, the constant b  is reflected twice in the sum of two lines  joined by a corner, while only once in a single line with length L1+L2.
Similar effect exists with the power form T model.
R2 value is the common validity metric for testing laws of action, reflecting how tightly empirical data regress to the model predictions.
Such an approach tests the form of a model, but not its specific parameters, since the parameters are left floating, and are used as performance indicators of individuals, different muscle groups, devices, or interface methods.
By such a metric the validity of the proposed model is quite strong.
Next, we challenge the model to the next level, to examine the specific value predictions against actual instances of gesture production.
The average value of Tp across all polyline gestures is 1208ms.
In other words, although on average the magnitude of the prediction is similar to the actual data, it underestimates the participants' production time.
Recall that the experiment only had three blocks of tests and within a block each sample gesture was tested only once.
For complex and unfamiliar polyline gestures this might not be enough practice to reach the fluid gesturing behavior that equation  intends to model.
The more complex a gesture is, the more likely the model would underestimate the actual time when the gesture is still unfamiliar.
This could have caused the slight upward bend of the data in Figure 13.
We predict that the model's underestimation could be reduced or eliminated  if the participants had enough practice with the gesture set.
In the following experiment we had the opportunity to test this prediction.
In contrast to the polyline predictions, the model predictions of the tested arbitrary gestures were longer than actual production time.
The average value of Tp across all arbitrary gestures was 1343ms, and the average value of Td was 1045ms.
Incidentally  Tp-avg = 1.285 Td-avg held exactly the same for both subsets.
This eliminated the concern that this overestimation of time was caused by the model's bias towards either the straight elements or curved elements.
Instead, since the samples used in the arbitrary category were from the Graffiti gesture set, we suspect that the participants' familiarity with Roman letter-like symbols  may have helped the participants spend less time producing them.
It is conceivable that for a highly familiar gesture, rather than slowing down or pausing to plan for the next move , one may chunk the entire gesture  and therefore achieve faster speed than what our model predicts.
To summarize, the gesture production time predicted by the proposed model were consistent with the experimental data with the magnitude within 30% in average, and correlation R2 > 0.9.
Overall, given its high correlation with and the similar magnitude to the experimental data, the proposed model is validated as a good first-order approximation.
Similarly, we verified the proposed model with the arbitrary gestures that consisted of both straight and curved segments.
The Tp predictions were computed by equation , and verified against Td, the mean value of the experimental data for each gesture.
The integrals for each curve segments were calculated numerically.
Figure 14 plots the correlation between Tp and Td.
Again going beyond the correlation between the model and actual human performance, one potential application of the model is to numerically evaluate and compare the efficiency of existing and future gesture set design in terms of expected gesture production time.
As an exercise we conducted a small-scale experiment comparing the performance of two pairs of existing gesture sets: Unistrokes vs. Graffiti, and ShapeWriter  gestures on two different soft keyboard layouts.
We calculated model predictions and collected empirical data for each set of the gestures.
We compared two ShapeWriter gesture sets for English words defined on two different keyboard layouts: the traditional QWERTY layout vs. the optimized ATOMIK layout  as illustrated in .
We selected gestures for the 24 most frequently used words in spoken English .
A within-participant design was used.
In each phase gesture samples from the two contrasting sets were mixed and presented in random order.
To reach "expert" behavior, each sample was repeated 8 consecutive times, which was in contrast with experiment 1 and gave us a chance to test the prediction that the under-estimation of production time by the model can be reduced by practice.
The order of the two phases was counterbalanced among participants.
The same error tolerance criterion as in experiment 1 was enforced.
5 volunteers, who were all participants of experiment 1 and still available, participated in experiment 2.
This was to ensure that the data from the two experiments were comparable, given the possible large individual differences in the elemental model parameters.
We emphasize that running the experiment at such a small-scale was only to empirically test the model's prediction power for a given group.
Weighted by word frequencies estimated from the ANC  we calculated the expected gesture production time for the top 24 words on both keyboard layouts.
In addition to the 24 words tested for ShapeWriter in Experiment 2, we can also theoretically estimate an expected production time for the gestures in a complete dictionary, which is difficult to do empirically.
Based on 27628 common spoken English words and their frequencies, extracted from the ANC, our model predicts that the expected gesture production time is 903 ms for the ATOMIK layout and 1139 ms for the QWERTY layout.
This type of analysis, even theoretically and approximately, had not been possible previously due to the lack of quantitative gesture models.
To compare model predictions with empirical data, we can look at three increasingly stringent indicators.
First, the model predictions and empirical data correlated well, with similar R2 as in experiment 1.
Second, between two contrasting designs, the performance order as predicted by the model was always correct and the ratio predicted was similar to that of the empirical data.
Third, contrasting with experiment 1, we can see that the difference in production time magnitude between model prediction and empirical data was indeed influenced by practice and previous familiarity.
Recall that much more repetition of each gesture was given in this experiment.
Consequently, the underestimation of time for ShapeWriter gestures in the experiment was eliminated.
In fact, the model overestimated the time that the well-practiced ShapeWriter gestures actually took, presumably due to chunking.
For the more familiar or simpler Graffiti or Unistrokes characters, more intense practice in this experiment further enlarged the difference between prediction and actual data.
We will discuss chunking behavior further in next section.
In summary, the proposed model could approximately predict the comparative performances of different gesture sets for the same set of users.
Using the above parameters, we computed time predictions Tp for each gesture sample from Unistrokes and Graffiti.
Correlation between Tp and experimental data Td is plotted in Figure 15, with R2 = 0.920.
Conceptual and mathematical analyses as well as knowledge in the motor control literature, particularly Viviani's power law of curvature, led us to a set of candidate elemental models  and the summative CLC model for gestures.
We tested these models in two multi-phase experiments.
To our knowledge such a level of precision has never been previously achieved for such a wide range of gestures in varying complexity.
In the more demanding tests  that not only required correlation between prediction and empirical data, but also a specific a priori time value prediction for each gesture, the proposed model may either underestimate or overestimate the time needed, but nonetheless gave a similar magnitude prediction which can be still useful as a baseline prediction for many design purposes.
The order and ratio of predicted performances between comparative pairs were always consistent with the empirical data.
We reason that two factors are key to the over or under estimation of time value.
First, individual difference may influence the parameters of the model.
More accurate parameter estimations are needed in future work from a very large pool of participants.
Second and more importantly, familiarity and the amount of practice may drive the actual empirical data away from the model's prediction.
For unfamiliar and little practiced gestures, such as the polyline gestures in experiment 1, actual articulation of the gesture may be slower than the model prediction due to online visual perception, planning and decision making.
For familiar or well practiced gestures, as in the rest of the gestures sets tested in the two experiments, actual production tended to be more rapid than the model prediction.
This was likely caused by "chunking" behavior - linking two or multiple elements of a gesture into one action, which can also be explained by our models.
This function is plotted in Figure 17 , with a maximum  0.8 at   1 .
The fact that T2/T1 is less than 1 indicates that cutting the corner will always save time.
However, this time saving is less noticeable at moderate angles around 60 when compared with the more extreme angles.
Note that this is a simplified analysis.
A more complete and strict analysis should concern the error tolerance criterion and the context around the corner, etc.
Nonetheless, it provides us with a way to more "correctly" apply the summative model  than simply adding elements together.
Instead, one may consider the chunking behavior and calculate the elemental and summative models accordingly.
For example two connected elements may be considered as one for a well practiced user.
Alternatively, we may use the simple summation as a baseline and add a compensatory term to equation  to reflect the degree of chunking impact.
However, both approaches require further research to be practical.
In addition to the summative CLC model as a contribution toward enlarging the theoretical tool box of UI research, design and evaluation, the elemental empirical findings from this work can also be relevant to gesture interface design and interesting to HCI researchers.
Due to the space limitation we will only point out two examples.
One is that  users are faster at drawing straight lines in the 45 and 225 orientations.
However, the diagonal directions were also found to be more error-prone, probably due to humans' lesser perceptual sensitivity to these angles than primary axes directions.
Therefore gesture interface designers can exploit these directions for faster interaction, but only if the precise orientation is not essential to the interaction.
The second example is that our data showed that it was harder to maintain the same relative accuracy for small gestures, suggesting special challenges for UI design for very small devices.
For a more detailed study about the scaling effect on production time, please refer to our extended research report .
The current work also revealed many differences between visually guided movement and open-loop gestures.
For example, for visually guided motor movement the impact on time performance from scale change in the moderate range is relatively low, as reflected by, for example, the flat bottom of a U shaped function over several orders of magnitude scale change .
Another difference is that while a previous study on visually guided steering performance by Pastel  found that the degree of a corner significantly influences movement time, the current study found that the presence of a corner is a major determinant of gesture production time; however, the difference between different degrees of corner is negligible.
In sum, the current work shows that findings based on visually guided motor control tasks cannot be taken for granted when applied to gesture stroke analysis.
The current work on stroke gesture modeling is probably the most comprehensive to date.
Previous work in this domain is rare and has different goals or makes different levels of prediction.
The most successful previous model is that of Isokoski's "line counting" model, which has the merits of simplicity and ease of calculation.
However, it makes no specific quantitative time prediction for a given gesture.
Our proposed model goes beyond the prior work, enabling us to, for example, predict the performance difference between two layouts for ShapeWriter .
Our model focused purely on the motor control aspect of gesture strokes; thus it does not model the mental complexity in perceiving and planning the gestures or the transition from novice to expert behavior.
In practice these are all important factors that influence the overall user experience.
Furthermore, given the limited data we collected, although we have confidence in the form of the model, we do not claim to have found "universal" parameters in the elemental models.
Modeling stroke gestures is an important, complex and challenging task.
The current work should certainly not be viewed as a definitive investigation on the topic, but rather as one of the first systematic attempts toward the ultimate modeling and understanding of pen stroke gestures as a human-computer interaction medium.
Accot, J. and Zhai, S., Beyond Fitts' Law: Models for trajectory-based HCI tasks.
Accot, J. and Zhai, S., More than dotting the i's foundations for crossing-based interfaces.
Accot, J. and Zhai, S. Performance evaluation of input devices in trajectory-based tasks: an application of the steering law.
Accot, J. and Zhai, S. Scale effects in steering law tasks.
ACM CHI Conference on Human Factors in Computing Systems , 1-8.
The information capacity of the human motor system in controlling the amplitude of movement.
Goldberg, D. and Richardson, C., Touch-typing with a stylus.
Hinckley, K., Ramos, G., Guimbretiere, F., Baudisch, P. and Smith, M., Stitching: pen gestures that span multiple displays.
Isokoski, P., Model for unistroke writing time.
Lank, E., Saund, E., Sloppy selection: Providing an accurate interpretation of imprecise selection gestures.
Visual similarity of pen gestures.
Tables of single-letter and digram frequency counts for various word-length and letter-position combinations.
Pastel, R., Measuring the difficulty of steering through corners.
Viviani, P. and Flash, T. Minimum-jerk, two-thirds power law, and isochrony: converging approaches to movement planning.
Journal of Experimental Psychology: Human Perception and Performance, 21, 1 .
Viviani, P. and Terzuolo, C. Trajectory determines movement dynamics.
Zhai, S., Accot, J. and Woltjer, R. Human Action Laws in Electronic Virtual Worlds - an empirical study pf path steering performance in VR.
Performance optimization of virtual keyboards.
Zhai, S., Kong, J. and Ren, X. Speed-accuracy trade-off in Fitts' law tasks - on the equivalency of actual and nominal pointing precision.
International Journal of HumanComputer Studies .
Zhao, R., Incremental recognition in gesture-based and syntax-directed diagram editors.
