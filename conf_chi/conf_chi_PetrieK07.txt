For usability, there is a precise and widely accepted definition now provided by ISO 9241: "the extent to which a product  can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use" .
For accessibility, the situation is less clear .
The Web Accessibility Initiative , founded by the World Wide Web Consortium  in 1997 to promote the accessibility of the Web, gives a widely accepted general definition of Web accessibility as "people with disabilities can use the Web ... more specifically  can perceive, understand, navigate, and interact with the Web" .
This might be termed the "usability for people with disabilities" or "usable accessibility"  definition of Web accessibility, as it appears to be promoting a user-based definition similar to that provided by ISO 9241.
However, rather than defining more precise user-based criteria, WAI has promoted conformance to the Web Content Accessibility Guidelines   as the criteria for achieving and measuring accessibility .
This might be termed the "technical accessibility" definition of Web accessibility, as it relies largely on meeting technical criteria in the underlying Web code .
The relationship between the usable accessibility definition and the technical accessibility definition is unclear.
Little empirical data have been gathered to show that websites that achieve higher conformance to WCAG are also more usable by people with disabilities and what the criteria for usability for people with disabilities should be.
For example, the study of 1000 websites conducted for the Disability Rights Commission  found no significant relationship between conformance with WCAG and a number of measures of user performance and satisfaction for five different categories of disabled people.
We believe that the ultimate criteria for accessibility should be userbased and we can adapt the ISO 9241 definition for this purpose: the extent to which a product/website can be used by specified users with specified disabilities to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use.
Accessibility and usability are well established concepts for user interfaces and websites.
Usability is precisely defined, but there are different approaches to accessibility.
In addition, different possible relationships could exist between problems encountered by disabled and nondisabled users, yet little empirical data have been gathered on this question.
Guidelines for accessibility and usability of websites provide ratings of the importance of problems for users, yet little empirical data have been gathered to validate these ratings.
A study investigated the accessibility of two websites with 6 disabled  and 6 non-disabled  people.
Problems encountered by the two groups comprised two intersecting sets, with approximately 15% overlap.
For one of the two websites, blind people rated problems significantly more severely than sighted people.
There was high agreement between participants as to the severity of problems, and agreement between participants and researchers.
However, there was no significant agreement between either participants or researchers and the importance/priority ratings provided by accessibility and usability guidelines.
Practical and theoretical implications of these results are discussed.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Usability can also be defined as the lack of usability problems in using a product or website; this is important, as in measuring usability, one can either measure effectiveness, efficiency and so on, or one can measure the problems that a user encounters or might encounter .
Similarly, accessibility can be defined as the lack of accessibility problems.
But this is not the same as saying that usability problems are only encountered by people without disabilities and accessibility problems are only encountered by users with disabilities.
The relationship between accessibility and usability and accessibility and usability problems are rarely explicitly analysed, either in the context of the Web or other computer-based systems.
Thatcher et al  propose that accessibility is a subset of usability, suggesting that accessibility problems are particular types of usability problems.
However they also state that usability problems affect all users equally, regardless of ability or disability, whereas accessibility problems hinder access for people with disabilities and put people with disabilities at a disadvantage relative to people without disabilities.
These latter statements suggest a more complex relationship between accessibility and usability than the former being a subset of the latter.
Shneiderman  proposes "universal usability" as a term to encompass both accessibility and usability, which he defines as "having more than 90% of households as successful users of information and communication technologies at least once a week" .
Shneiderman  notes that "access is not sufficient to ensure successful usage", suggesting that accessibility is a first but not sufficient step towards universal usability, but does not analyse the relationship between the two concepts further.
If we consider the problems that disabled and non-disabled people respectively encounter in using a website, we can propose that a number of possible relationships might hold between these two sets of problems.
Firstly, the problems might be two distinct, non-intersecting sets, meaning that there are no problems that disabled people encounter that are also encountered by non-disabled people and vice versa .
In fact, this is the way accessibility and usability are usually dealt with in the development of most websites.
The processes for conceptualizing, assessing and removing problems encountered by each group of users are completely distinct, most likely dealt with by different individuals within an organization, at different times in the development process.
Secondly, as noted above, Thatcher et al  propose that accessibility problems  might be a subset of usability problems.
This definition is attractive in that accessibility can be dealt with as part of the usability evaluations process.
But it also suggests the possibility that some problems that we typically think of as accessibility problems also affect non-disabled users.
For example, providing an informative set of headings can make a webpage much more usable for blind people using screenreading technologies, but it is also very helpful for non-disabled people as well.
However, some problems appear to only affect people with specific disabilities.
For example, having a "submit" button with green text on a red background will not pose any problems for people with full color vision, but will be a catastrophic problem for people with red-green color vision deficiency.
So not all accessibility problems affect non-disabled users, and are therefore not within the scope of usability problems.
Thirdly, Shneiderman's concept of universal usability might be thought of as expanding the scope of what we traditionally think of as usability to include disabled users, so that usability problems become a subset of accessibility problems.
This can account for the color vision problem discussed above, as some accessibility problems are beyond the scope of usability.
But this formulation suggests that all usability problems are within the scope of accessibility, meaning that people with disabilities encounter all the same problems that people without disabilities encounter.
Finally, we believe that accessibility and usability problems can be seen as two overlapping sets, which would include three categories: x x x Problems that only affect disabled people; these can be termed "pure accessibility" problems; Problems that only affect non-disabled people; these can be termed "pure usability" problems; Problems that affect both disabled and nondisabled people; these can be termed "universal usability" problems.
It should also be noted that one could expand this analysis, taking users with each specific disability separately, as the problems encountered by the different disability groups can have a range of relationships with each other.
Nonetheless, all these possible basic relationships between the problem sets highlight useful aspects of the situation.
However, we lack empirical data on the actual breakdown of problems into these sets on websites.
This paper sets out to investigate these relationships.
In addition, there is the question of whether a particular problem affects disabled and non-disabled people equally.
As noted above, Thatcher et al  define usability problems as those which affect disabled and non-disabled people equally, whereas accessibility problems hinder access to a website for disabled people.
Thus, problems encountered by nondisabled people  appear be amplified or intensified for people with disabilities.
This is a particularly interesting effect.
It suggests that usability problems could be detected more easily by conducting evaluations with disabled people rather than with the non-disabled people currently used in usability evaluations.
However, this anecdotal evidence needs a sound empirical basis, so this relationship will also be investigated in this paper.
Guidelines for both accessibility and usability attempt to quantify the importance of particular problems.
For example, the usability guidelines originally developed by the US National Cancer Institute and then extended by the Department of Health and Human Sciences   provide two five point ratings for each guideline: the "relative importance" of the guideline, and the "strength of evidence" used in making that judgement.
The WCAG divides the checkpoints  into three groups: Priority 1: a Web content developer must satisfy this checkpoint.
Otherwise, one or more groups will find it impossible to access information in the document.
Satisfying this checkpoint is a basic requirement for some groups to be able to use Web documents.
Priority 2: A Web content developer should satisfy this checkpoint.
Otherwise, one or more groups will find it difficult to access information in the document.
Satisfying this checkpoint will remove significant barrier to accessing Web documents.
Priority 3: A Web content developer may address this checkpoint.
Otherwise, one or more groups will find it somewhat difficult to access information in the document.
Satisfying this checkpoint will improve access to Web documents.
The WCAG documentation states that the priority levels were assigned by the Working Group , but does not elaborate on the process or evidence used to define the priorities.
Harrison and Petrie  investigated the relationship between the WCAG priority levels for a set of problems encountered by disabled and non-disabled Web users and the users' own ratings of the severity of the problems and an expert's rating of the problems.
However, this was a small study with only two blind and two dyslexic participants and one expert.
Further evidence on the relationship between the importance ratings provided by the guidelines and user experience is particularly interesting and important because of the practical and legal relevance of these ratings.
With both the accessibility and the usability guidelines, developers may use these ratings to prioritize their work in improving a website.
In the case of the accessibility guidelines, legislation and directives in a number of countries requires websites to meet guidelines with Priority 1 and 2 ratings.
The above discussion a number of issues about the relationship between accessibility and usability of websites and the importance ratings of accessibility and usability problems provided by guidelines.
This study will explore these issues with a user-based study of two websites.
Given the extensive data collection required, it was decided to concentrate on the experience of blind people interacting with the Web using screenreaders, as they encounter the most difficulties in using the Web , and compare their experience with that of a matched group of non-disabled people.
The following research questions were investigated: 1.What is the nature of the relationship of the problems encountered by non-disabled  people and disabled  people?
2.If the same problems are encountered by both blind and sighted people, are they more severe for the blind people than the sighted people?
In order to answer this second research question, the relationship between different measures of severity of accessibility and usability problems needs to be further investigated, an area also of interest in itself.
So the third research question was: 3.What is the relationship between users' ratings of the severity of problems, expert's ratings and the ratings provided by accessibility and usability guidelines?
Six sighted and six blind participants undertook the evaluation study.
The six sighted participants were 5 men and 1 woman with a median age of 30.
The six blind participants were 4 men and 2 women with a median age of 40.
The two groups of participants were matched as far as possible on age, gender and most importantly, general computer and Internet experience and expertise.
Participants were asked to rate their computer experience and expertise on 5 point scales.
They were also asked to estimate how many hours a week they spend using the Web.
Figure 1: Orange website Table 1 shows the ranges and median values on each of these variables.
The blind participants were all experienced users of a screen reader, either JAWS  or Window-Eyes .
Table 1: Computer and Internet experience and expertise of participants Blind participants Computer Experience Computer Expertise Hours/week using WWW Range: 3 - 5 Median: 5 Range: 3 - 5 Median: 5 Range: 2 - 5 Median: 4 Sighted participants Range: 3 - 5 Median: 5 Range: 2 - 5 Median: 4 Range: 2 - 5 Median: 4.5 well known mobile companies in the U.K., but their web solutions are quite different.
None of the participants were familiar with the websites, they had not used either of the websites frequently or visited them recently.
Figures 1 and 2 show screenshots of the home pages of the two websites at the time of the evaluation.
Each participant was asked to attempt seven tasks with each website, the tasks being the same for each site.
It was thought that repeating the same tasks on both websites was not a problem, as the two websites structured their information and functionality quite differently.
Nonetheless, the order in which participants evaluated the sites was counter-balanced within each user group, to minimize order effects.
The tasks were organized into a scenario of choosing a new mobile phone, exploring different options of phones and payment plans, finding the closest shop to go and try the phone, and finding information about video call coverage and the use of the phone abroad.
The order of tasks was not counterbalanced, as the sequence of tasks formed a meaningful scenario and began simply and then progressed to more complex tasks.
Two mobile telephone company websites were evaluated, Orange  and T-Mobile .
Initially, we planned to evaluate three such websites, but a pilot study showed that the individual evaluation sessions would be too long and be too tiring, particularly for the blind participants.
This class of website was chosen as they have a complex range of functionality and interaction styles, but allow participants to do the same range of tasks on each website.
Before commencing the evaluation, participants were briefed about the study and procedures to be used and their written consent was obtained.
They were assured that the evaluation was of the websites, and not their ability to use the Web.
With their permission, all evaluation session were recorded using Morae  for later viewing and analysis.
Participants were given the tasks one at a time and told that they could ask to be reminded of the task at any point .
Figure 2: T-Mobile website "think aloud" as they did the tasks , particularly to articulate whenever there was a problem with the website.
Every time that occurred, the researcher conducting the evaluation  asked the participant to briefly pause in doing the task, and to rate the severity of the problem on a four point scale, taken from Nielsen's heuristic evaluation method : Cosmetic, Minor, Major or Catastrophic problem.
The researcher conducting the evaluation also rated the severity of the problem using the same four point scale.
The procedure was that the researcher rated the problem first  and noted it on a coding sheet and then asked the participant for their rating, which was also noted.
Occasionally the participant spontaneously gave their rating first, then the research tried not to be affected by the participant's rating.
This did not happen often enough to affect the independence of the two sets of ratings.
Due to technical difficulties, Morae recordings for one sighted and one blind participant were incomplete; where this affected results, this is indicated.
Table 2, below, shows a summary of the problem results for the Orange website.
The difference between "distinct problems" and "problem instances" is that any particular problem may have been encountered by more than one participant or by the same participant on more than one occasion.
Table 3 shows a summary of the performance of blind and sighted participants with the Orange website.
This reflects the different strategies typically used by sighted and blind participants.
Blind participants tended to spend longer on each page looking for the appropriate information and were less likely to return to a page.
Sighted participants also encountered far fewer problems than blind participants.
However, the mean severity of the problems was very similar for blind and sighted participants, both as rated by the participants themselves and the researchers.
Tables 4 and 5 show the same information for the TMobile website.
There was a significant difference between the two groups in their success rate , with sighted participants having a significantly higher success rate of 70.2%  compared to blind participants success rate of 50.7% .
There was no significant interaction between the website and group factors.
Figure 3 shows this result graphically.
Table 3: Participants' performance with the Orange website Sighted Ps Success rate  Mean number of distinct pages visited Mean number of page visits Mean number of problem instances Mean severity of problems as rated by participants  Mean severity of problems as rated by researchers  57% 28.0 82.6 15.0 2.8  2.7  Blind Ps 36% 23.8 64.4 28.0 2.6  2.8 
Figure 3: Success rate  for sighted and blind participants on the two websites For the ratings given by the participants, for any problem in a particular condition there might be between one and six ratings, depending on the number of participants who encountered the problem.
The key variable is how much agreement there is between the participants when more than one participant encountered the same problem.
Figure 4 shows the levels of agreement for the 32 cases  when 3 or more participants encountered the same problem.
The ratings agreements were scored in the following way: x "Total Agreement"  - all participants gave the same rating of severity;
To investigate the relationships between the different measures of the severity ratings of problems encountered by participants, correlations were calculated between the severity ratings provided by the relevant Guidelines and the severity ratings provided by the participants and the researchers.
Before these correlations could be calculated,
Nor were there any significant correlations between the priority levels given by WCAG and the ratings given by the participants and the researchers.
Figure 4: % Agreement between participants in severity ratings Figure 4 shows that the majority of ratings agreements were of the "1 Difference" category , and when the "Total agreement" and "1 Difference" agreements are combined, this accounted for over 80% of all problems encountered by 3 or more participants, a very high level of agreement was achieved.
Therefore it was decided to take the mean value for the ratings of the participants who encountered a particular problem and consider that to be the measure of participant severity rating.
Finally, for both the HHS and WCAG Guidelines, for a particular problem, a number of the guidelines might have been considered relevant to any particular problem.
Again, a mean was taken of the severity ratings  of the different guidelines considered relevant as the measure of severity as indicated by the Guidelines.
Table 6 shows that for the HHS Guidelines, there were no significant correlations between the severity ratings provided by the guidelines and those given by either the sighted participants or the researchers.
Priority levels in WCAG use lower numbers to indicate greater priority and higher numbers to indicate lower priority, whereas the ratings of severity given by participants and researchers use the reverse system.
So for the correlations between participant/researcher ratings and WCAG priority levels, a significant negative correlation indicates agreement between the two sets of ratings.
To investigate the nature of the relationship between problems encountered by blind and sighted participants, all problems were tabulated for whether they were encountered by sighted participants only, by blind participants only or by both blind and sighted participants.
For these analyses, only pages on the websites that had been visited by at least three blind and three sighted people were included, to create equity in the potential for detecting problems.
For the Orange website, there were 106 distinct problems encountered by both groups.
Table 7 shows the breakdown of these problems between the three categories.
17% of problems were encountered by sighted participants only.
66% of problems were encountered by blind participants only.
17% of problems were encountered by both blind and sighted participants.
Just over half  of problems were encountered by blind participants only.
Just over 10% of problems  were encountered by both blind and sighted participants.
To investigate whether the problems encountered by sighted and blind participants were more severe for the blind participants than for the sighted participants on the Orange website, the ratings of the severity of the problems encountered by both participant groups as given by both the participants themselves, the researchers and the guidelines were analysed.
The mean rating of severity given by the blind participants was 2.60  and the mean rating by the sighted participants was 2.61 .
Table 8 shows the mean ratings for the researchers, again this difference was not significant.
Table 8: Mean ratings  of severity of problems encountered by both blind and sighted participants 
To investigate whether the problems encountered by sighted and blind participants were more severe for the blind participants than for sighted participants on the TMobile website, the ratings of the severity of the problems encountered by both participant groups as given by both the participants themselves, the researchers and the guidelines were analysed.
This difference was significant , with blind participants giving more severe ratings of their problems when compared to their sighted peers.
Table 10 shows the mean ratings for the researchers, and both researchers individually, none of these differences were significant.
For both websites, the problems encountered by the blind and sighted participants constituted intersecting sets, with some problems only encountered by blind participants , some problems encountered by only sighted people and some problems, although not a large percentage, encountered by both groups.
These results were not simply a consequence of sampling, that is the fact that a particular problem happened to be encountered by a blind person or persons rather than a sighted person or persons.
It was clear that some of the problems resulted specifically from the fact that the blind people used screenreaders which results in problems not encountered when using the sites visually.
For example, although both websites provided clear headings within pages to divide text into identifiable chunks, neither site indicated these headings in the html code, so they could not be detected by the blind participants.
Conversely, some of the problems encountered by sighted users related to the visual presentation of the information and thus did not affect blind participants.
For example, on the T-Mobile website, information about price plans was set out in a table, but the headings for the 12 month and 18 month plans were poorly grouped visually, so some sighted participants were unsure whether the information was a heading for one section of the table, or a footnote to the previous section.
This problem did not affect blind participants, as the information was before the price group, so they correctly assumed it related to the prices that followed.
Thus accessibility problems were not a complete sub-set of usability problems, as suggested by Thatcher et al  nor were usability problems a complete sub-set of accessibility problems, as might be inferred from Shneiderman .
However a more detailed analysis of the nature of the problems encountered by each group and the problems shared by both groups, will be undertaken in a future paper.
The degree of overlap between the problems encountered by the two groups was not large, certainly not as great as we had predicted.
However, it should be remembered that the disabled group studied here were blind people using screenreaders, whose interaction with the Web is conceptually most different from that of non-disabled, sighted people.
For those problems encountered by both blind and sighted participants, it was hypothesized that they would be more severe for blind people than for sighted people and this would be reflected in the ratings given by participants, researchers and the guidelines.
Only limited support was found for this hypothesis, with a significant difference between participants' ratings on only one of the two websites, and no differences in the ratings given by researchers and the guidelines.
However, it is the participants' ratings that are of most interest here.
Unfortunately, in spite of a large number of user problems studied, the number of problems encountered by both blind and sighted participants was quite small, so the test of this research question is not highly robust.
Further evidence to support this hypothesis is needed.
But if it can be found, it would be particularly useful in accessibility and usability evaluations, as it would show that studying the accessibility problems on a website highlight and amplify the usability problems.
This is both useful practically and of theoretical interest in understanding the relationship between accessibility and usability.
Perhaps the most interesting findings from this study concern the relationships between the different measures of importance or severity of the problems encountered by participants.
Firstly, the fact that there is strong agreement between participants in the severity of a particular problem is encouraging.
Although the participants for this study were chosen to be relatively homogenous in their computing and internet expertise, they had different strategies for interacting with the sites and different knowledge that they brought to the tasks.
Nonetheless they broadly agreed on which problems were important and which problems were not.
However, the most important finding on this research question is the lack of any significant relationship between the ratings given by the participants  of the importance of the problems they encountered and the ratings provided by the HHS and WCAG guidelines of the severity of problems.
This study confirms the preliminary findings of Harrison and Petrie  who failed to find a relationship between participants' ratings, one expert's ratings and the guidelines ratings, but the current study is a considerable larger study, both in terms of numbers of participants and numbers of user problems.
In the case of the WCAG guidelines, the problem is that there has been little detailed research into the ways screenreader users interact with the Web and the problems they encounter, so the empirical basis for the Priority Levels in WCAG was actually quite weak.
Nonetheless, it should not be thought that this study is suggesting that providing importance ratings on guidelines is not possible, rather that considerably more research on the relationship between users' experiences of problems and the ratings to be given to those problems is required.
We would like to thank all the participants who took part in the study for their patience and enthusiasm.
We would also like to thank Fraser Hamilton, Chandra Harrison and Christopher Power for both practical assistance and useful discussions in the preparation of this paper.
We would like to thank the Darzentas and Scott-Surridge clans for ensuring the paper actually got written and submitted and the anonymous reviewers for very helpful comments.
Usable accessibility to the Web for blind users.
Lecture Notes in Computer Science, No 3196.
Web accessibility testing: when the method is the culprit.
Proceedings of the 10th International Conference on Computers Helping People with Special Needs.
Lecture Notes in Computer Science, No 4061.
Chisholm, W., Vanderheiden, G., and Jacobs, I.
Understanding Inspection Methods: Lessons from an Assessment of Heuristic Evaluation.
In A. Blandford, J. Vanderdonckt and P.D.
The Web: access and inclusion for disabled people.
Impact of usability and accessibility problems in e-commerce and e-
