Therefore skin has great potential to act as a companion surface for mobile devices.
However, skin is fundamentally different from conventional, off-body touch surfaces.
As skin is stretchable, it allows for additional input modalities, such as pulling, pressing and squeezing.
This increases the input space for on-skin interactions and enables more varied forms of interaction, for instance more varied gestures.
Moreover, interaction on skin has a strong personal and strong emotional component , enabling a more personal way of interaction.
In addition, since the physiological properties of skin vary across body locations, input location is likely to be very influential - even more as people have different mental associations with different parts of their body.
This opens up a new interaction space, which is largely unexplored.
We aim to contribute to the systematic understanding of skin as an input modality and of its specific capabilities.
This paper contributes results from the first study on multimodal on-skin input.
It empirically addresses on-skin input from three main perspectives, which impact immediate usability as well as the design of future sensors and applications:  What are characteristics of skin-specific input modalities, and what modalities do people use?
What kinds of gestures do users perform on their skin for mobile computing?
What are the mental models associated with them?
What are preferred locations on the upper limb for multi-modal skin input?
The study followed an elicitation methodology similar to Wobbrock et al.
This approach has proven successful in prior work on a range of novel interfaces  for providing "insights into users' mental models" and "implications for technology and UI design" .
This paper contributes results from an empirical study of on-skin input, an emerging technique for controlling mobile devices.
Skin is fundamentally different from off-body touch surfaces, opening up a new and largely unexplored interaction space.
We investigate characteristics of the various skin-specific input modalities, analyze what kinds of gestures are performed on skin, and study what are preferred input locations.
Our main findings show that  users intuitively leverage the properties of skin for a wide range of more expressive commands than on conventional touch surfaces;  established multi-touch gestures can be transferred to on-skin input;  physically uncomfortable modalities are deliberately used for irreversible commands and expressing negative emotions; and  the forearm and the hand are the most preferred locations on the upper limb for on-skin input.
We detail on users' mental models and contribute a first consolidated set of on-skin gestures.
Our findings provide guidance for developers of future sensors as well as for designers of future applications of on-skin input.
Mobile computing; on-skin input; touch input; skin gestures; deformable surface; elicitation study.
User-Interfaces: User-centered design An emerging stream of research proposes skin as an input surface for mobile computing .
Miniaturization of electronic components enables increasingly small mobile devices, e.g.
This generates new challenges for input, since these devices tend to offer too little surface area for effective touch input.
Skin provides a large input surface,
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
This accounts for the expressive nature of skin.
Moreover, we elicited mappings to specific on-skin locations.
In addition, we systematically investigated ease and comfort of input modalities across locations.
We opted for not using any specific sensing technology and not providing any form of output.
This allowed us to investigate the full input design space independently of constraints that would be imposed by present-day technology.
The main findings and implications of our study are:  Participants intuitively performed skin-specific gestures, leveraging physical affordances and the richer expressiveness of skin, and taking inspiration from interpersonal touch.
This allowed them to better express emotions, variations of commands, as well as standard commands, which relate to interpersonal communication.
For many standard commands, conventional multitouch gestures were successfully transferred from touchinput devices to skin.
Overall this demonstrates the wide spectrum of skin as an input surface, which is highly compatible with existing forms of multi-touch input, but in addition enables substantially novel forms of input.
Physical discomfort was explicitly desired for some types of commands.
Participants performed physically uncomfortable gestures for irreversible actions, to avoid accidental input, and for expressing negative emotions.
Half of all user-defined gestures were located on the forearm, showing that the forearm a very well suited location for on-skin input.
The palm should be considered for precise or private interactions.
To provide guidance for designers, we derived a first user-defined set of skin-specific gestures.
This comprises skin-specific alternatives for conventional gestures as well as gestures for interpersonal communication and expression of emotional state.
These findings provide guidance for researchers and practitioners in developing future sensors as well as in designing novel interaction techniques and applications for on-skin input.
Skin is the largest human organ.
Human skin is composed of two layers: The outer layer is the epidermis, which forms the boundary between the body and its environment.
The inner layer, the dermis, contains tear-resistant and flexible cells and comprises most sensory cells of the skin .
These cells are able to detect pressure, touch, vibration, temperature and pain and therefore allow for sensing of expressive tactile cues.
Their density varies on body locations; it ranges from rather low density, e.g.
Prior work has suggested skin as an input surface for applications in mobile computing.
Likewise, it could be used as input for mobile devices that are in the pocket , for interacting with content on a head-mounted display or on a smart watch.
Our empirical study results inform these application scenarios.
Only little previous work has empirically investigated input on skin, mostly through pointing studies.
The authors investigated pointing performance and user preferences of touching different body locations.
Touching the upper limb was found to have high social acceptance and was rated positively by the participants.
Mean pointing time was faster than on locations on the lower body, but slower than on the torso.
Other pointing studies revealed that people are able to distinguish a maximum of eight to ten different locations on their forearm without or with visual cues .
Findings of studies on palm-based imaginary interfaces showed that people can effectively interact on skin without visual output.
A first study demonstrated that users are able to point to imaginary icons on their palm, by leveraging spatial memory from prior interaction with a smart phone .
A further study revealed that when users are blindfolded, tactile cues can replace visual cues for precise pointing on imaginary interface elements .
All these previous studies investigated only touch input, i.e.
However, studies of interpersonal touch highlighted that people can distinguish much more varied forms of touch .
These findings, and generally the richness of tactile sensing, inspired us to investigate on-skin input beyond simple touch.
In contrast to previous work on skin input, we study how users interact with a broader set of skin-specific input modalities.
Furthermore, this is the first study investigating what gestures users perform on their skin for controlling electronic devices.
Previous work has presented promising non-invasive solutions for sensing of on-skin input.
Some sensors are a flexible, skin-like overlay that is worn as an additional layer on top of skin; other work captures input on textiles  or on the surface of robots .
In the following, we are focusing on input on bare skin, as this retains the natural feeling of interacting on skin and preserves full tactile feedback.
One class of sensors that do not interfere with tactile feedback relies on optical capture of input by using body-worn RGB cameras  IR cameras , or depth-sensors .
This supports real-time capture of single- or multitouch input on skin.
However, to our knowledge, sensing of further input modalities than touch has not been investigated yet.
Problems of camera-based approaches involve a limited resolution of sensing, occlusion, and ambient light in mobile use cases.
A second class of sensors captures input through direct skin contact, using acoustic, optical, EMG or capacitive approaches.
Such sensors can be worn as wristbands or even embedded into smart watches.
Skinput senses the location of tapping on the skin through acoustic transmission and reflection within skin and bones .
Other work proposed sensing skin deformation on the forearm using two armbands with multiple infraredreflective sensors .
Capacitive detection of in-air gestures  or EMG measurement  of interactions that are performed with the touching hand might allow for sensing more expressive input.
On the long run, sensors might even be implanted, which however raises questions about user acceptance .
Participants could freely choose between the dominant and the non-dominant upper limb for performing input.
They were seated at a desk and did not hold anything in their hands.
Participants who wore long-sleeved clothing turned both sleeves up, such that skin on all locations below the shoulder was uncovered and freely accessible.
The flexible nature of skin affords not only touching, but also pulling, shearing, squeezing, and twisting.
Skin is capable of sensing various levels of contact force, which enables pressing.
Lastly, the physiological properties of the touching finger or hand further add to the expressiveness: touch can be performed with the fingernails, resulting in scratching, or the full hand can enclose another body part, resulting in grabbing.
The resulting set of eight modalities is shown in Figure 1.
It was derived from established modalities of conventional touch interfaces and from results of studies on the biomechanics of skin .
These modalities are ranging from on-surface interaction to intense skin deformations.
Note that these modalities are defined from a user perspective and not from a technology-centered one.
For keeping the study focused, we restricted input to the upper limb.
We excluded the shoulder, as this is typically covered by clothing.
Participants were asked to perform input directly on their bare skin without any instrumentation of the body, to preserve tactile feedback.
As existing sensor hardware can capture only some few of the input modalities that are possible on skin , we opted for not using any specific sensing technology.
This allowed us to observe participants' unrevised behavior, free of the restrictions of current hardware.
This method prove helpful in previous work for deriving implications for future hardware and system designs to accommodate this user behavior .
Moreover, to avoid biasing participants by a specific form or location of output, we opted against providing any system output.
The study comprised three tasks, in a single-user setting: Task 1 : This task was designed for investigating properties of on-skin gestures.
The participant was sequentially presented 40 different referents.
For each of them, the task was to invent a corresponding gesture and perform it anywhere on the skin of his or her upper limb.
Figure 3 gives an overview of all referents.
Inspired by the human ability to express emotions through touch , we added a set of emotional expressions covering all four main classes on Schacter's twodimensional spectrum of emotions .
These emotional expressions could support a more personal way of input for remote communication.
They could also support novel ways of interacting with computer contents through affective computing , e.g.
Task 2 : This task specifically focused on usability of input modalities across different locations on the upper limb.
The participant was asked to perform input using each of the 8 modalities introduced above on the six different locations.
For each of the combinations, the participant rated the perceived ease of use and comfort of use on two fivepoint Likert scales.
Task 3 : This task was designed to investigate other forms of input than gestures.
We presented a set of input types derived from established interactions with mobile devices , e.g.
We asked the participant for each of them sequentially what is the location on the upper limb where they would intuitively most like to provide input for the input widget.
We also investigated how participants arrange virtual items using different orders and levels of privacy .
The study followed a think-aloud protocol to obtain rich qualitative data of the mental models of the participants.
We specifically encouraged participants to verbally describe the gestures they performed and to describe their reasoning as accurately as possible.
To avoid bias, the order of items was randomized in each task.
Moreover, the order of T1 and T2 was counterbalanced.
T3 was performed as last task, to avoid biasing the intuitive choice of location in T1.
At the end of each session, we conducted a semi-structured interview and handed out a questionnaire to collect demographic data.
Each session took around 70 minutes and was video-recorded.
We collected a total of 880 gestures  during T1, 1056 ratings of input modalities  in T2, and 198 location preferences for input widgets and orders  during T3.
We used grounded theory  for the qualitative analysis of the dataset.
Each received a compensation of 10 Euros.
18 participants were righthanded, 2 left-handed and 2 mixed-handed.
Participants had various cultural backgrounds .
Their occupations included teacher, editor, researcher and students in biology, education, law, computer science, tourism and psychology.
All participants were frequently using computing devices.
Seventeen participants owned a device with a touch screen.
In the following, we investigate what kinds of gestures participants have defined.
Are they similar to gestures from conventional multi-touch devices or specific to the affordances of skin?
We discuss their characteristics as well as the reasons for performing skin-specific gestures.
This is followed by an investigation of what are preferred input locations on the upper limb and what meanings are associated with different locations.
In our analysis, we manually classified each user-defined gesture qualitatively using the following dimensions: input modalities, location on the body, and properties of the gesture .
The remaining gestures were classified as conventional multi-touch gestures.
The Cohen kappa coefficient of the inter-rater agreement was 0.746, indicating a substantial to excellent agreement on the definition.
Figure 3 depicts main results for all referents of the three gesture sets of task 1 regarding the distribution between skin-specific gestures and conventional multi-touch gestures.
It also gives the agreement score as defined by .
Our scores are comparable with those in prior work  despite the larger input space of our study.
While the set of standard commands involved only an average of 21% of skin-specific gestures, the variation set comprised 46% and the emotional set 66%.
Bonferroni corrected post-hoc tests found significant differences between all sets.
In-line with this finding, we identified a monotonous increase in the number of referents, for which the most frequent gesture was skin-specific: this held true for only two referents in the standard set, but for 5 of the 10 referents in the variation set, and even for 5 out of the 7 referents in the emotional set.
To characterize usage of input modalities, Figure 4a depicts for each modality the percentage of all user-defined gestures that involved this modality.
Multi-touch is used in 72.3% of all gestures.
It is very likely that the higher familiarity of multi-touch gestures partially influenced these results.
However, even despite the novelty of skin-specific modalities, they were consistently used for expressive interactions.
The most frequently used skin-specific modalities were pressing and grabbing, followed by twisting.
Even though participants were allowed to use any hand for interaction, all preferred to interact with the dominant hand on the non-dominant upper limb.
Mixed-handed people switched between both hands.
These outliers will be discussed below.
For variations, participants used skin-specific gestures more frequently.
The most frequently performed gesture was skin-specific for five of the ten referents.
Figure 5a gives an overview of important skin-specific gestures, which we identified for standard commands and for their variations.
Some of them were the most frequent gesture performed for the respective command; some were skin-specific alternatives to the most frequent multi-touch gesture.
We included only alternatives for commands where the most frequent skin-specific gesture was performed by at least 3 participants.
We opted against depicting the most frequent multi-touch gestures, since these were in-line with the findings reported in .
Participants used a skin-specific gesture for the majority of emotional expressions.
In the semi-structured interviews, all participants stated that they could express emotions better on their skin than on a touch screen.
One main reason was that this allows them to draw inspiration from typical ways of expressing emotions when touching other people.
Only happiness and boredom turned out to be easier to express with multi-touch gestures.
Here, people took inspiration from facial expressions  and bored tapping on a surface.
Figure 5b shows a conflict-free user-defined gesture set for all emotional expressions.
For each expression, it contains the most frequently performed gesture.
Following , whenever the same gesture was used for different emotions, the conflict was resolved by assigning it to the larger group and selecting the second most frequent gesture for the smaller group.
Most gestures performed for referents in the standard set were conventional multitouch gestures.
For ten referents of the standard set, the most frequent gesture was identical with the one found by Wobbrock et al.
These findings show that participants transferred conventional multi-touch gestures to on-skin input.
In conclusion, our findings show that conventional multitouch gestures for standard commands are transferred from touch screens to on-skin input.
Skin-specific gestures are preferred for expressing emotions.
They are also frequently used for expressing variations of a command.
Skin-specific gestures were used less frequently than multitouch gestures, but added expressiveness.
The analysis of the user-defined gestures revealed two main mental models, which explain why participants opted for using skinspecific gestures: Most gestures performed for emotional expressions were inspired from how one touches another person to convey emotion.
To express sympathy, 73% of participants rubbed or stroked their arm, as if they consoled another person.
To express anger, six participants hit their palm with the fist, five grabbed and squeezed their skin, and others used twisting or scratching.
However, also conventional computer commands were inspired by interactions with other people.
For help, 32% of participants performed a poking gesture, as if they poked a nearby person.
Another common gesture was grabbing their upper arm, as if they grabbed another person.
Also ten participants seek attention either by making a sound using clapping to "direct the attention of the person to myself" or by poking the person virtually through their own arm as if they said "Hey, there!"
Participants made use of tactile feedback and leveraged the expressiveness of skin modalities and locations.
For instance, 27% of participants used twisting for rotation due to the affordance involved: "It feels like actually grabbing the object and rotating it" .
45% of participants varied the pressure for distinguishing between temporary close and force close; the latter gesture was performed with much stronger pressure, which provides a distinct tactile feedback.
Affordances of specific body locations were also leveraged for selection: 36% of participants touched one of their fingers of the non-dominant hand to select a numbered item.
These mental models show that skin-specific interaction has great potential to enhance the user experience of on-skin input.
Participants used skin-specific modalities to add expressiveness to their gestures and mimic established interpersonal gestures.
These gestures can be taken as a source of inspiration for on-skin gestures to encourage users to interact in a more personal way with electronic devices.
They rated perceived ease and comfort of use on two independent Likertscales.
The aggregated results for input modalities across all six locations are given in Figure 4b.
All input modalities were perceived as being rather easy to perform.
The means for perceived comfort of use followed the same order, with somewhat lower means.
The only outlier was scratching.
This is explained by qualitative feedback: although participants did not perceive scratching as physically uncomfortable, it was perceived as a socially unaccepted and awkward input modality: "I feel like a monkey" .
Figure 4b shows a clear relation between perceived ease/comfort of use and the degree to which skin is deformed: the more the input modality deforms the skin, the lower its rating.
Multi-touch, grabbing and pressing have the highest means.
This corresponds to the order of frequency in which participants have used these modalities in their user-defined gestures.
The modality with the lowest mean ratings, both in ease and in comfort of use, was twisting.
Interestingly, this modality was used much more frequently in user-defined gestures than scratching, shearing, squeezing and pulling, even though these latter modalities had higher ratings.
This finding will be discussed in the next section.
All three tasks allowed us to investigate characteristics of input locations on the upper limb.
Figure 6a shows the locations where user-defined gestures were performed.
Half of all gestures were performed on the forearm.
Also back of the hand and the palm were frequently used location, while the upper arm and elbow were rarely used.
Figure 6b shows the mean values for perceived ease and comfort of use for each location, aggregated for all input modalities.
As expected and in-line with Fig.
6a, the forearm showed the highest perceived ease and comfort of all locations, followed by the back of the hand.
Surprisingly the palm received the lowest value for perceived ease, contradicting to the findings depicted in Fig.
This finding can be explained by a high variance: separate analyses for each input modality revealed that input modalities which include no or only slight deformation of the skin, i.e.
In contrast, input modalities that involve strong deformation, as twisting and pulling, were perceived as particularly hard to perform.
Elbow and upper arm received the lowest scores for perceived comfort.
Participants mentioned that the elbow was hard to reach and that they perceive interaction on the elbow to be socially uncomfortable: "I would not like to interact with anything on my elbow".
For all three ordering criteria  we found two mutually contradicting concepts: The majority of participants  placed frequently used and most important/liked items close to the hand.
Their reasoning was to have them ready-at-hand.
Items extended in decreasing order towards the elbow and the upper arm.
In contrast, a minority of participants  chose the reverse order: most frequently used, most important or most liked items were placed close to the body.
The arrangement extended from the upper arm towards the hand.
These participants wanted the highestranked items "to be near to me"  or "close to my heart" , or to give them a "kind of protection"  by placing them close to their body.
In T3 we asked participants where they would like to interact with private and public information.
For private, all participants preferred the inner side of their upper limb, which is not easily visible to others.
The outer side was mainly used for public interactions.
41% of participants preferred specifically the palm for private interactions, because it can be closed: "We used to write on the palm for cheating in an exam.
It's possible to hide things there" .
This finding lends empirical support to prior research on visibility on forearm-worn displays .
Surprisingly, participants deliberately chose uncomfortable input modalities to perform some specific commands.
This involved quite intense pressing, pulling, twisting and squeezing, which created some slight sensation of physical pain.
Uncomfortable interactions were chosen for actions that are very important and not reversible, e.g.
They ensured a higher degree of consciousness while performing the action: "You have to be conscious while deleting" .
Participants also used uncomfortable gestures to express intense emotions, e.g.
Participants stated: "It needs to hurt to express anger"  and "it should hurt" , while they were twisting or squeezing their skin to express anger.
However, participants mentioned that the gestures were performed "more gently than I would on another person" .
These results add to the understanding of how uncomfortable interactions can improve user experience .
The gesture for `accept a call` was performed more than twice as often on the palm  than on the back of the hand .
In contrast, reject call was preferably mapped to the back of the hand .
Also the thumb was associated with positive actions  due to the common `thumbs up'-gesture.
In contrast, the pinky was associated with negative actions, since it is farthest away from the thumb.
Some referents of Task 1 contained variations that differentiate between temporary and permanent actions, e.g.
These variations were expressed by 27% of participants using different directions: Movement on the upper limb towards the body, i.e.
This confirms prior design on forearm-worn displays, which uses movement towards the sleeve to store information for later usage .
The same participants associated movement away from the body, i.e.
This is similar to dragging the element off-screen as found in prior user-centric tabletop studies , but accounts for the different input location.
Based on the above findings, we derive the following implications for on-skin input.
These provide guidance to developers of future on-skin sensors and to interface designers.
Results of the study show that participants intuitively made use of the added possibilities provided by on-skin input.
Skin-specific gestures, which involved more input modalities than multi-touch alone, were frequently used for distinguishing between variations of a command as well as for performing emotional or interpersonal commands.
In particular if an interface comprises functionality that relates to interpersonal or emotional dimensions, it should provide support for gestures that go beyond multi-touch.
Irreversible commands can be mapped to uncomfortable modalities , in order to prevent accidental input.
Social acceptance needs to be taken into account; in particular scratching needs to be considered with care.
Furthermore, results of the study show that users transfer established multi-touch gestures from conventional touch displays to on-skin input.
Therefore, on-skin interfaces should support multi-touch input for established standard commands.
We contribute a first user-defined set of skin-specific gestures.
These gestures increase the input space with expressive gestures, reducing the need for menus or explicit interface elements, which might take up valuable screen space.
For instance in a picture gallery touching can be used for selection, while scratching deletes the picture.
Skin-specific modalities also allow for fast access to critical commands, e.g.
Deployed in mobile computing, such gestures could support a more personal way of input for remote communication.
They could also enable novel ways of interacting with computer contents.
For instance, user interfaces could offer emotional skin gestures for commands that imply some emotional semantics, e.g.
As a general rule of thumb, the non-dominant forearm is the location to consider first when designing for on-skin input on the upper limb.
50.0% of all gestures were performed on the non-dominant forearm.
Moreover, the forearm has the highest values of perceived ease and comfort.
However, 19% of gestures were performed on the back of the hand and 18% on the palm.
Precise interaction took benefit from the accurate tactile feedback in the palm.
Applications that require high precision input, such as sketching and handwriting, would benefit from a biologically inspired sensor that provides a higher sensing resolution at this location.
Prior work has contributed non-invasive optical techniques for sensing multi-touch gestures on skin .
The two most frequently used skin-specific input modalities were press and grab.
In consequence, a very large subset of gestures could be sensed by combining multi-touch sensing with a pressure sensor.
This accounts for 87.5% of all skinspecific gestures performed in the study and for 19 out of the 23 gestures of the consolidated set.
Three gestures comprise shearing, squeezing and twisting.
This requires detecting lateral forces.
These could be captured by a shear sensor presented in  or by a high accuracy depth camera that performs a detailed capture of the deformed skin's surface topology.
One gesture involves shaking, which could be detected using an accelerometer.
Second, our results show that the forearm and the hand are most preferred locations for on-skin input; both areas are in direct vicinity of a smart watch.
However, it can be assumed that some location preferences would differ from our findings, given the fact that output is provided right on the body and within the input area.
On-skin projection was proposed in prior work as a compelling form of output on the forearm and on the hand .
Our findings provide additional empirical support for the locations chosen in this previous work.
Since in this scenario input is fully co-located with output, those input modalities that strongly deform the skin might interfere with output, as they distort the projection surface.
It can be expected that this decreases their perceived ease and comfort.
Furthermore, we expect some gestures might change when users perform them directly on visual output.
These might be perceived as being too aggressive if they are performed on a photo or live video of another person.
The study was conducted indoors during summertime.
Most participants were short-sleeved or could easily uncover the skin of their upper limb.
No participant mentioned clothing as an issue during the study.
Clothes might lower the accessibility of some locations or make them inaccessible, e.g.
In these cases, on-skin input is restricted to the uncovered areas while cloth replaces skin as interaction surface  on the covered areas.
Participants were seated during the study.
While this allowed for elicitation of mental models in a comfortable setting, gestures and locations might vary in other conditions, e.g.
This should be investigated in future work.
This paper contributed findings from the first study of multi-modal skin input and derived implications for the development of future sensors and the design of future interaction techniques and applications.
Our findings open up several important avenues for future research.
Empirical studies should investigate the performance of skin-specific input modalities, explore user preference for a wider range of emotions and interpersonal commands and study cultural effects.
Future sensors for on-skin input would benefit from integrating multi-touch with pressure sensing, since this would allow for sensing the largest part of the skin-specific gestures that were performed in our study.
Future applications can make use of the proposed gesture set to interact in a more personal and more expressive way with electronic devices.
In our study setup, we have deliberately opted against providing any system output, to avoid biasing participants by a specific form or a specific location of output.
In the following, we discuss implications from our findings for several promising classes of devices that can complement on-skin input by providing output to the user.
Off-skin output: All gestures we have identified can be performed in an eyes-free manner, due to proprioception and tactile feedback.
Hence, our results inform most directly those application cases in which skin is used for input only, while a complementary device provides visual, auditory or haptic off-skin output.
Handheld mobile devices: For handheld devices with a touch display, such as mobile phones or tablets, the lower arm, hand and fingers can provide complementary input space.
This can be used for more expressive or more personal ways of input than possible on the touch display.
Smart watches: Our results show that on-skin input is most compatible with smart watches for several reasons.
