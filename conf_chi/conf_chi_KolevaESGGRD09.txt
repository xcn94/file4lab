Koleva, Boriana and Rennick-Egglestone, Stefan and Schnadelbach, Holger and Glover, Kevin and Greenhalgh, Chris and Rodden, Tom and DadeRobertson, Martin  Supporting the creation of hybrid museum experiences.
Access from the University of Nottingham repository: http://eprints.nottingham.ac.uk/27875/1/artect_author.pdf Copyright and reuse: The Nottingham ePrints service makes this work by researchers of the University of Nottingham available open access under the following conditions.
This article is made available under the University of Nottingham End User licence and may be reused according to the conditions of the licence.
For more details see: http://eprints.nottingham.ac.uk/end_user_agreement.pdf A note on versions: The version presented here may differ from the published version or from the version of record.
If you wish to cite this item you are advised to consult the publisher's version.
Please see the repository url above for details on accessing the published version and note that access may require a subscription.
This paper presents the evolution of a tool to support the rapid prototyping of hybrid museum experiences by domain professionals.
The developed tool uses visual markers to associate digital resources with physical artefacts.
We present the iterative development of the tool through a user centred design process and demonstrate its use by domain experts to realise two distinct hybrid exhibits.
The process of design and refinement of the tool highlights the need to adopt an experience oriented approach allowing authors to think in terms of the physical and digital "things" that comprise a hybrid experience rather than in terms of the underlying technical components.
A number of techniques have been used to augment the visitors experience.
Some of these track the visitors  in order to provide them with digital augmentations.
In the HIPS project visitors to the Museum Civico in Siena received audio messages on their hand-held devices that were related to the closest object .
The ARCHEOGUIDE project  explored the visual integration of the physical and the digital using see-through head-mounted displays to allow visitors to see reconstructions of missing artefacts and damaged parts in the context of cultural heritage sites.
The Augurscope  is a sharable mobile AR display that makes use of a variety of positioning technologies to allow it to visualize a medieval castle as it used to appear in relation to its current, quite different site.
The Telescope , a similar AR device, was deployed in the One Rock exhibition to show visitors the unseen world of the rock - its microscopic life and substance.
An equally popular approach has been the development of hybrid artefacts where physical artefacts within a museum collection are augmented with digital media to allow content to be actively explored in order to motivate visitors and enhance their understanding .
To realize these hybrid artefacts in museums physical objects are usually tagged in order to associate them with digital content.
For example, in an exhibition held in the Hunt Museum in Limerick each of a selection of artefacts had a corresponding RFID tagged key card, which visitors could pick up and use to trigger the provision of information about the object .
In the Electronic Guidebook project visitors to the Exploratorium in San Francisco scanned barcodes and RFID tags near objects of interest in order to access web pages about them on their handheld devices .
The use of visual tags is exemplified in The Mobile Augmented Reality Quest, a PDA-based AR tour guide that allows visitors to see visualizations and animations superimposed onto real exhibits tagged with markers .
This paper explores how we might best support the creation of hybrid exhibits that merge both physical and digital elements in a museum environment.
We focus on the use of visual markers to tag artifacts as this approach is relatively cheap and robust, thus very suitable for prototyping.
Museums and visitor centres have often been at the forefront of deploying interactive technologies to provide a richer visitor experience.
Collections often have a variety of associated interpretations and background materials that museum creators wish to present and it is becoming increasingly popular to augment museum artefacts with digital information in order to educate, entertain and engage visitors.
Furthermore the belief that visitors do not sufficiently engage with static text interpretations  has motivated a growing interest in interactivity to encourage engagement.
This paper presents the development of the tool in partnership with museum staff through a user-centred design process.
This approach has allowed museum experts to easily prototype two distinctive museum exhibits and in doing so to explore locally appropriate uses of hybrid exhibits to enhance the museum experience rather than getting overwhelmed by the technical details of digitally augmenting physical artefacts.
The paper concludes by reflecting on our user-oriented authoring approach highlighting the importance of an experience oriented perspective that allows users to reason about the physical and digital "things" that comprise a hybrid experience.
Similarly, the AMIRE project allows users to associate and configure components by changing their properties .
While reducing the cost of development, these different approaches still require significant amounts of technical expertise - either knowledge of a scripting language or some understanding of the programmatic interfaces to a set of components.
These approaches are also fundamentally concerned with the software of the hybrid artefact, rather than the artefact as a whole.
We are interested in how we might best support the museum professional in the development of interesting and engaging hybrid artefacts.
Our approach is to build upon systems such as d.tools to provide facilities accessible to museum domain experts and stakeholders with no software background that allow the prototyping of hybrid artefacts through a simple visual interface.
In the same way we wish to represent and manipulate key elements of the experience rather than its underlying software realisation, and we are therefore attempting to provide an experiencecentred, rather than a software-centred toolkit.
Irrespective of the approach adopted, the development of hybrid artefacts is a complex technical process involving the creation of digital assets , defining behaviours and manipulations, and associating these with physical objects in the real world.
Currently the most widely adopted approach involves the use of a programming language, such as C++ or Java, whilst special purpose libraries such as ARToolkit  and Studierstube  can be utilized in the process of constructing hybrid artefacts that are based on augmented reality technologies.
VRPN  is a technology that that has been used to interface to physical devices such as trackers, whilst a number of specialized toolkits, including Phidgets , iStuff  and d.tools  provide for integration with tangible input and output devices.
Unfortunately, however, the technical expertise required by programming-based approaches means that the vast majority of domain professionals are not able to directly experiment with potential combinations of physical and digital media.
Instead they need to communicate their ideas to and collaborate with computer programmers in order to develop effective and engaging hybrid artefacts, reducing the ability to creatively explore a range of alternative solutions.
Researchers have therefore begun to explore how the creativity of domain professionals can be supported through tools with alternative authoring paradigms.
Scripting and toolkit approaches have been developed to reduce the "cost" of programming for designers.
For example, DART   is built on top of Macromedia Director which provides a drag-anddrop timeline-based authoring model with scripting of behaviours in Lingo.
This allows skilled designers familiar with Lingo  to experiment with the AR medium.
In the ubiquitous computing domain the d.tools architecture and design editor  has been developed to allow design students to construct novel interactive arrangements using a graph based editor.
Configuration approaches have drawn upon techniques from interface development environments  to support application development by instantiating and setting the properties of components.
In order to develop a tool that allows museum domain experts to create hybrid artefacts from their own experience-oriented perspective, we have adopted an iterative user-centred development process.
Our tool has evolved through three iterations, each with a different emphasis, and guided by feedback from museum experts.
The development stages can be summarized as follows: - Iteration 1 was a proof-of-concept demonstrator that allowed simple hybrid artefacts to be constructed by linking software components properties.
Feedback was received from a small group of domain experts about the viability of such a tool.
The concept was validated through a workshop with ten domain experts where the focus was on exploring the breadth of possibilities offered by the tool.
It was evaluated through a workshop with two teams of museum professionals where each created a hybrid artefact, enabling deeper assessment of the approach and of the tools suitability for prototyping for museum settings.
Our starting point was a proof-of-concept demonstrator to show how hybrid artefacts can be quickly constructed by visually linking software components in a manner akin to that used in emerging systems such as d.tools.
It was based on an integration of the ARToolkit  video tracking libraries  and the Equator Component Toolkit   giving the name ARTECT to our tool.
ECT is a platform which allows distributed systems to be constructed by linking instances of self-contained software components .
This early work involved the development of a set of AR-orientated components.
These could then be manipulated by a non-programming user within the ECT environment.
Figure 1 shows a screenshot of an example system constructed using these components.
ECT fits with our aim to support interactive prototyping: all authoring and configuration changes in ECT are made interactively to the running system, giving immediate feedback and allowing rapid, incremental exploration of different options and behaviours.
While no actual programming is required, the author must still work with a representation of ECTs internal state: software components, their properties  and data-flow links.
This approach is similar to systems such as ARCO  and AMIRE .
For example, to create a simple 2-marker AR system requires the assembled of component shown in figure 1.
The system provides access to help documentation, but is still framed in terms of software components, properties, etc.
This version was demonstrated at an initial meeting involving a researcher who works within the museum sector and a representative of a company that offers interactive multimedia systems for museums and science centres.
The feedback from the meeting was that a system of this kind does indeed provide in principle new possibilities for creating hybrid artefacts and that it would be of interest to museum professionals.
However, it was also apparent that the software component oriented interface required a significant amount of specific technical expertise, and a different approach would be necessary to make it accessible to museum professionals such as curators.
Our hypothesis was that, for a prototyping interface to be useful to museum domain experts and stakeholders with no software background, it should allow the user to author in terms of the "things" and relationships that comprise the system from their own experience-oriented perspective.
In general, we assume domain experts have little direct interest in software components per se, but rather in the interactivity and experience which they afford.
The general approach taken was to develop a new kind of graphical editor which would support this form of authoring.
To support interactive prototyping we make use of ECTs runtime environment which is synchronized with this editor.
In the following we provide an overview of the design and use of the editor before detailing its implementation.
We then describe the feedback we collected from the museum professionals that took part in a validation workshop.
Figure 2a  shows a representative view of the visual editor.
The "Possible Things" panel  lists the resources which can be utilized in a hybrid artefact.
This iteration of the software supported 3D objects and sounds , physical sliders and webcams , and AR markers .
This selection of resources lets us illustrate the authoring approach and allows us to create example hybrid artefacts that use visual markers to augment physical objects.
Instances of available resources are dragged onto the main editor panel  to be used in the construction of a hybrid artefact.
The authoring process involves making visual connections between iconic representations of these resources.
In the case of the visual marker based hybrid artefacts we are considering here, one of the first steps is to specify that a web-cam is to be used with the machine running ARTECT .
The support for web-cams is represented by the web-cam item in the "Possible Things" panel.
This can be dragged to the editing panel and linked to the computer icon.
In response an output window is launched which displays the cameras video feed .
In the same way visual markers can be connected to the web-cam to specify that they should be tracked and 3D models and sounds can be associated with these markers.
Figure 2a shows a simple arrangement with 2 markers connected to the web-cam and each of these linked to a 3D model and a sound sample.
As a result, whenever one of these markers is in the field of view of the web-cam they are digitally augmented.
For example, a 3D model of a drum kit appears on top of the "Hiro" marker in the 3D output window  and a sound sample of a drum is played whenever the marker is visible.
ARTECT can also recognize when two markers are close to each other, and by making a link between them the author can specify what should then happen.
Figure 2c shows an example of this where the interaction point between two markers has been configured with a specific sound sample to be played when they are close together.
User-facing aspects of the visual editor were implemented using the Eclipse GUI toolkit .
Internally, the editor uses Resource Description Framework  and Web Ontology Language  to realize an internal "world" model and a rule engine.
We h ave defined an ontology which specifies the resources that are available to users, the ways in which they can be configured, and the relationships that can exist between them.
Whilst managing its graphical user interface, the editor has to frequently interrogate this ontology for information.
For example, if a user attempts to establish a relationship between icons representing two resources, the editor will check with the ontology to determine if such a relationship is allowable, and will only draw a link between the icons if it is valid.
Bridging between the abstract user-centred view and the underlying software-oriented view is an extensible and automated rule-based system.
This determines both the connection between software components, and the configuration of individual components that are required to implement any system specified through the editor.
For example when the user connects the web-cam and computer icons in figure 2a, the ARTECT rule-base responds to the establishment of this abstract relationship in the world model by causing the instantiation, configuration and linkage of software components to handle video capture from the webcam and create the 3D output window.
It responds to the introduction and linking of a 3D model resource by creating and configuring a software component to represent and render that object within the 3D scene, and so on.
This second iteration of the ARTECT editor was refined through a process of regular testing by researchers not involved in the technical implementation of ARTECT and meetings with the museum domain experts who provided feedback on the first prototype.
Enhancements made at this stage included making  links between resources non-directional, automatically launching the 3D output window  whenever a web camera component is connected, and specifying actions for when two markers are close together.
We then tested our approach to authoring hybrid artefacts with potential end-users from the museums sector.
A one day workshop was held with 10 external participants, who included staff from the Hatton Gallery, Newcastle Museum of Antiquities, The Centre for Life and South Shields Museum as well as representatives from The International Centre for Cultural and Heritage Studies and the Great North Museum scheme .
We first presented the ARTECT tool and then participants had a hands-on session for 90 minutes.
There were three trial stations including the one shown in figure 3.
Each included a dual-display PC , a web camera attached to a configurable desk light stand , visual markers on cardboard paddles  and physical sliders.
A set of 3D models and audio clips were also pre-loaded into the tool.
The attendees were split into three groups and asked to experiment with the interface and create different configurations.
A developer was on hand to deal with any problems.
However, the participants remained in control of all interaction including the on-screen interface, the physical markers and sliders.
Each group was joined by a member of our team who observed the session, took notes and occasionally asked prompting questions.
Additionally one of the groups was video recorded.
The participants had no problems using the interface and quickly created working configurations.
All three groups adopted a similar approach of exploring all available aspects of the tool, constructing experiences that were as complex as possible, for example by making many connections between the available resources.
Overall feedback was very positive.
The museum domain experts liked the approach of graphically linking representations of physical and digital resources and found it intuitive.
All three groups suggested extensions to ARTECT to allow richer experiences to be constructed.
These included support for more types of resources such as text and images that could be configured to display on markers and/or in a separate window, as well as support for more types of physical sensors and actuators.
Additionally the museum staff wanted more control over how the 3D models appeared on the markers.
Finally, the workshop also provided an opportunity for the curators to present objects that are difficult to display and interpret for visitors and to discuss how digital augmentations may help.
The Hatton Gallery and the Museum of Antiquities were particularly interested in further exploring the use of our tool to enhance the presentation of artefacts in their collections and agreed to participate in a future prototyping workshop.
The next iteration of the ARTECT tool realized new functionality based on participant suggestions from the second iteration workshop.
These enhancements focused on greater presentation control, extending the set of available digital resources, providing a broader set of input and output capabilities and support for richer interactive arrangements of hybrid artefacts.
Museum experts stressed the importance of fine control over the presentation of digital resources.
Authors were given the freedom to decide how a 3D object should be overlaid on a marker by translating, rotating and scaling the object from its original position.
The result is immediately visible in the 3D output window.
Textures are available as one of the subcategories under 3D objects and can be selected and visually connected in the same way as other resources.
Additionally new custom textures and sounds can easily be introduced while authoring by using the properties panel to configure an appropriate resource to be loaded from a local texture or sound file.
To respond to the need for a greater diversity of digital resources a Document resource type was added to ARTECT.
These are HTML pages which can be loaded from local files or URLs.
If a document resource is connected to a maker then whenever that marker is visible to the webcam, the documents content is displayed in a browser as a secondary window, allowing the authors to display supplementary information.
When multiple documents are used, for example one for each marker and one for each interaction point, it is the document that has been triggered last which is being displayed.
The need to have a richer set of input and output capabilities resulted in extensions to the support for the physical sliders.
If a slider is connected to a sound sample then it will control its volume when it is playing.
If a link is made between a slider and a 3D object then a wizard appears allowing the author to choose which properties of the model are to be controlled .
Extensions to output capabilities focused on the incorporation of X10 units  that control the power supplied to a domestic power socket.
If an X10 icon is connected to a marker icon then the power is turned on when the marker is first recognized, and turned off when the marker disappears from view.
For example, such a configuration was used to turn a spotlight on and off.
Finally, we responded to the need for richer interactive arrangements by allowing authors to link an interaction point  to different types of resources, for example to play a sound, display a document or control a power socket.
It should be clear from the above that this iterative development process involving potential users has allowed us to evolve resources and functionality within our tool.
The following illustrations summarise the changes that occurred as a result of these processes.
Figure 4 lists the resource types and their relationships that were available during iteration 1.
It is important to note that lines in this diagram denote relationships that can have multiple instances.
For example, the number of markers that an ARTECT System can support is not limited in ARTECT itself, but depends on the processing power of the computer.
Equally, multiple 3D objects and sounds can be attached to the same marker.
Figure 5 then illustrates ARTECT resources and their relationships as they were available after iteration 3.
Here relationships that can have multiple instantiations are still represented as single lines.
Relationships where the number of instances is limited have been labelled with that limit.
When comparing the two graphs above, it becomes clear that many new resource types have been added, which in turn resulted in new types of logical relationships.
In addition, new relationships have been added for interactional reasons: some to make entirely new interactions possible such as the interaction points, but some to adapt the tool to the expectations of its end-users.
For example, the fact that sounds can be attached to 3D objects , derived from the workshop participants finding this more intuitive.
To understand how well ARTECT meets the need of museum professionals, we organized a two day workshop during which museum staff used the tool to create their own hybrid exhibits.
The main participants were from the Hatton Gallery and the Museum of Antiquities.
Prior to the workshop we communicated with both groups to confirm which objects from their collections they wished to focus on, allowing us to create physical replicas in the workshop space and to prepare initial digital materials.
The workshop began with a demonstration of the functionality of ARTECT and the extended resource set, followed by a group discussion about the interests of the museum staff.
The participants then split into two groups  to design and implement hybrid exhibits.
The workshop was videotaped for later analysis and each team was joined by a researcher who observed the process and took notes.
Participants were also interviewed after the workshop about the utility of ARTECT and digital augmentation more generally.
The Museum of Antiquities selected three artefacts - a stone inscription from Milecastle, the tombstone of Aurelia Caula, and the head from a stone statue of the god Antenociticus.
Replicas were made and placed in a corner of the workshop space in order to allow free experimentation .
The group included 3 museum staff and 2 museum visitors who had taken part in previous museum projects.
The team was interested in how digital augmentations could help the museum to become more "alive" by putting objects in context and telling stories about them.
A key idea was to provide different layers of information that visitors could choose to reveal based on their interests.
Museum staff felt that this might be a good solution for providing individually relevant information to their cross-generation visitors and to provide the "right" amount of information for each visitor.
The marker was placed immediately below each object  as experimentation revealed that this allowed the most effective integration of the visual information with the object.
It also meant that the visitors attention was not drawn away from the artefact.
It was envisaged that the visual marker would be printed as part of an objects label.
Specific ideas about how the artefacts could be augmented included showing the objects original context , revealing relationships between objects, making inscriptions more accessible by highlighting and translating them, and incorporating modern interpretations Available resources included images, web pages about the collection, and audio recordings of poems about the objects .
It was decided that some additional audio information would be useful and one of the curators recorded a description for each of the three objects, a reading of the associated Latin inscriptions and their corresponding translations in English.
The starting point for the workshop team was the physical set up of the space.
They quickly explored a number of potential marker and device arrangements before deciding to have a visual marker fixed near each object with visitors carrying mobile displays with them.
This arrangement was very appealing to the museum staff because an extensive computing infrastructure did not have to be built into the exhibition space.
Instead, only labels need to be attached at appropriate places, making the physical set-up very easily reconfigurable.
To prototype this arrangement we used lightweight wired displays but envisaged that when deployed in the museum, the installation would make use of mobile devices such as hand-held computers.
The dominant emphasis of the first day was in establishing the overall physical arrangement of both artefacts and digital devices to be used.
Once these broad decisions were made the hybrid artefacts were created over the course of the second day of the workshop.
At this point ARTECT was used to prototype and test the different ideas until settling upon the final arrangement.
Three additional audio trails were also created by the team each reflecting different perspectives on the artefacts .
Each trail was associated with a movable marker that users could carry with them.
Whenever visitors placed one of these movable markers next to the marker of a particular object the associated audio trail was played .
This allowed visitors to choose what aspects of the collection they were interested in and reveal that information.
The design team tended to focus on establishing the physical arrangement of each artefact and visual marker before encoding the links required within ARTECT for the hybrid artefact.
As the team worked through the exhibit they added to the overall connection diagram.
Figure 8 shows a screenshot of the final configuration in ARTECT to realize these hybrid artefacts.
It is worth noting that this team made no use of 3D models.
The Hutton Gallery chose to explore ways to augment Schwitters Merzbarn wall .
It is a collagesculpture, originally situated in a barn in the Lake District, England, incorporating a variety of found objects .
In his Merz compositions, Schwitters treated found objects in the same way as other artistic material, such as paint or clay, and used them together to create room-sized installations.
In this case the found objects include a rubber ball, the rose of a childrens watering can and the rim of a cart wheel among many others, which are all partially embedded into the plaster background and therefore not always immediately recognizable.
For preservation reasons, the wall was carefully moved to the present location in the Hatton Gallery in 1965, where it remains today, detached from its original context.
This final physical arrangement involved projecting the digital data onto a near-full-scale reproduction of the artwork  as shown in Figure 10.
Once this physical arrangement had been agreed the team then set up an ARTECT interaction space on a low table in front of the Merzbarn wall, with the camera looking down on its surface.
Pre-authored 3D models of seven of the objects embedded in the Merzbarn were linked to distinct markers.
Flat black 3D objects were also associated with each physical marker, hiding the marker in the projected view and minimizing the impact on the art work.
The team working with the Merzbarn wall consisted of 2 curators from the Hatton Gallery and 3 staff members from the International Centre for Cultural and Heritage Studies.
They were particularly interested in enabling each visitor to form their own personal relationship with the artwork and it was important that the technology not obstruct this.
The physical set-up of the exhibit and the relationship between the visitor, the artefact and the surrounding technology dominated initial design deliberations for this team.
The curators saw it as imperative that the eventual installation did not form a barrier between visitors and the artwork.
If they preferred visitors should be able to appreciate the Merzbarn wall as if it had not been augmented at all.
A considerable amount of time was spent exploring physical display arrangements.
An initial design was considered consisting of a large screen placed next to the artwork to display augmentations.
However, after exploring a number of alternatives it was finally decided that the most effective presentation would be to directly project onto the Merzbarn wall, with no projected content being visible when no marker was visible in the ARTECT camera view.
Interaction involved moving markers on the table and thus augmenting the Merzbarn wall with projected 3D models.
The experience was configured so that these sounds were played whenever the two objects were displayed close to each other.
The final hybrid exhibit developed by the team allowed participants to explore the spatial relationships between digital 3D models  and the physical objects embedded in the real Merzbarn wall.
This also allowed the free exploration of partially hidden objects.
Beyond this, participants were able to create their own unique visual and auditory collages, reflecting the way that Schwitters himself originally worked.
Figure 11 shows a screenshot of the configuration linking the seven markers to different 3D models that realized this experience.
The configurations created and their corresponding representations  are quite complex although the level of complexity was not problematic in practice during workshop sessions.
However, these arrangements may not be as easily understood by others and could prove to be difficult to maintain and update at a later time.
In future, this might be addressed adopting strategies from visual programming and diagramming such as composite  components and automatic layout.
Their focus was on the hybrid exhibit to be created rather than on the minutiae of interaction between software components.
This had a significant impact on the nature of our emerging tool, shaping its focus and the representations used.
The mixed skill sets of the teams involved in the assembly of hybrid artefacts also suggested the need for multiple levels of access to the underlying infrastructure.
In terms of designing the overall hybrid experience the software arrangement covered by our tool represented only a fraction of the overall solution.
The broad strategy adopted in the workshop was to reason from the physical interactive arrangements towards the supporting software infrastructure.
Once the physical arrangement of a hybrid exhibit was determined the configuration of markers and resources needed to realise it was explored and established through the ARTECT tool.
The technical nature of the underlying components to be used did not feature in how the museum staff reasoned about the design or explored alternative possibilities.
They tended to reason at the level of the best presentation of the artefacts to be augmented, the physical nature of the devices used for augmentation and the associated digital resources to be presented to users.
Using ARTECT, experimenting in the physical setting with new ideas was quick and easy, and working hybrid systems were quickly prototyped.
In fact, the ease of experimentation allowed workshop participants to go beyond the presentation of a single set of explanatory materials around a particular exhibit.
Both demonstrator exhibitions focused, in very different ways, on how digital augmentation fits into a physical environment and can support the layering of different types of information, which can then be selectively revealed by visitors.
On the other hand this same approach can encourage a more playful and imaginative engagement with exhibits and exploration of relationships between objects, as demonstrated in the Merzbarn example.
A rich set of digital resources to augment physical artefacts also became an important feature of prototyping.
The layering of digital information needed to create an engaging experience required a broad set of digital resources.
This included 3D models, sound files, images, HTML documents.
These also needed to be presented via a broad set of physical input and output devices.
Consequently the things that can be referred to in our tool and the relationships that can be created need to be extensive.
As it is impossible to foresee every combination that might be required in the future, our tool includes a framework for incorporating resources, relationships and representations that is flexible and relatively easy to adapt.
In this way the expressive range of the tool can be co-developed by endusers during a prototyping process as we have highlighted.
Our starting point was a component oriented editor.
It built upon the lessons learned from other development toolkits by providing a visual interface that offered maximum flexibility in terms of the technical components making up the system.
However, this technical complexity while showing promise to the professionals was inherently difficult for them to reason about.
Although motivated by the need to give users more control, offering maximum software flexibility if anything had the opposite effect.
The overhead of configuring components and learning the details of the various elements involved was viewed by the professionals as masking broader considerations of the experience and the initial tool was seen as too complex.
Promoting the creative use of the tool required us to remove the barrier associated with the complexity of software configuration.
We changed the level of reasoning of the tool to emphasise key elements involved in the installation.
This allowed authors to think in terms of the physical and digital "things" that comprise the experience .
A rulebase mapped the arrangement of "things" to underlying software components in real time.
The representation provided to users became a bridge between the physical arrangement and the underlying software infrastructure.
This shift in representation reduced the flexibility in our tool by only offering a simpler set of elements and a smaller set of connections.
Users were now free to focus on the overall effect of the installation and could build upon the simple and predictable arrangements established by our tool.
This allowed designers to get going very quickly with a fully functional prototype arrangement, without having to worry about the underlying technical complexities.
At the start of our design process we considered our user base as being relatively homogeneous.
In fact, people at our workshops had a very broad range of technical expertise.
He reported that working with the interface was easy and enjoyable but stressed that he would also like to author at a lower, more "technical" level, which would allow the authoring of more complex behaviours.
We would argue for the importance of designing tools that support the needs of groups with different skill sets.
One possibility for providing such support within ARTECT is to exploit the multiple levels of access we provide to the core infrastructure.
For example, our original ECT software component editor and the experience oriented interface both access and manipulate the same underlying software infrastructure each providing distinct views.
This means that a "general" user might manipulate the abstract representations of physical and digital "things" in the world model, and might never be exposed to the underlying software entities.
Alternatively an "expert" user might drill down to the underlying software components, using the original ECT editor, effectively providing a multiperspective view on the same authoring problem.
Authoring: Generic Context from Programmer to Designer, Proc.
Reflective physical prototyping through integrated design, test, and analysis.
Hull, R. Clayton, B., Melamad, T. Rapid Authoring of Mediascapes, Proc.
Kato, H. and Billinghurst, M. Marker Tracking and HMD Calibration for a video-based Augmented Reality Conferencing System.
DART: A Toolkit for Rapid Design Exploration of Augmented Reality Experiences.
McKenzie, J. and Darnell, D. The eyeMagic Book.
New Zealand Centre for Childrens Literature and Christchurch College of Education, 2004.
Naismith, L., Ting, J. and Sharples, M. CAERUS: A context aware educational resource system for outdoor sites, Proc.
Person-oriented guided visits in a physical museum.
Engaging augmented reality in public places.
Semper, R., Spasojevic, M. The Electronic Guidebook: Using Portable Devices and a Wireless Web-based Network to Extend the Museum Experience.
Schnadelbach, H., Koleva, B. et al.
Schmalstieg, D., Fuhrmann, A. et al.
The Studierstube Augmented Reality Project, Presence: Teleoperators and Virtual Environments, 11, MIT Press , 32-54 22.
Schmalstieg, D. and Wagner D. A Handheld Augmented Reality Museum Guide, Proc.
IADIS International Conference on Mobile Learning 2005.
VRPN: A DeviceIndependent, Network-Transparent VR Peripheral System, Proc.
ARCHEOGUIDE: First results of an Augmented Reality, Mobile Computing System in Cultural Heritage Sites, Proc.
Wojciechowski, R., Walczak, K. et al.
Building Virtual and Augmented Reality Museum Exhibitions.
The work reported in this paper has presented a tool and an approach to promote easy experimentation and prototyping of hybrid exhibits.
It was developed using a user-centred development process with three iterations, in partnership with museum staff.
The evolution of the tool highlights the advantages of adopting an experience oriented rather than a software development perspective when creating hybrid artefacts.
This approach has allowed museum professionals to easily prototype two distinctive museum exhibits and in doing so to explore locally appropriate uses of hybrid artefacts to enhance the museum experience.
