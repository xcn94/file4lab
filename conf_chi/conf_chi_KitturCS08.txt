User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies.
However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs.
Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs.
Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks.
Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.
Thus it is often not possible to acquire user input that is both low-cost and timely enough to impact development.
The high costs of sampling additional users lead practitioners to trade off the number of participants with monetary and time costs .
Collecting input from only a small set of participants is problematic in many design situations.
In usability testing, many issues and errors  are not easily caught with a small number of participants .
In both prototyping and system validation, small samples often lead to a lack of statistical reliability, making it difficult to determine whether one approach is more effective than another.
The lack of statistical rigor associated with small sample sizes is also problematic for both experimental and observational research.
These factors have led to new ways for practitioners to collect input from users on the Web, including tools for user surveys , online experiments , and remote usability testing .
Such tools expand the potential user pool to anyone connected to the internet.
However, many of these approaches still either rely on the practitioner to actually recruit participants, or have a limited pool of users to draw on.
In this article we investigate a different paradigm for collecting user input: the micro-task market.
We define a micro-task market as a system in which small tasks  are entered into a common system in which users can select and complete them for some reward which can be monetary or non-monetary .
Micro-task markets offer the practitioner a way to quickly access a large user pool, collect data, and compensate users with micro-payments.
Here we examine the utility of a general purpose micro-task market, Amazon's Mechanical Turk , as a way to rapidly collect user input at low cost.
User studies are vital to the success of virtually any design endeavor.
Early user input can substantially improve the interaction design, and input after development can provide important feedback for continued improvement.
User evaluations may include methods such as surveys, usability tests, rapid prototyping, cognitive walkthroughs, quantitative ratings, and performance measures.
An important factor in planning user evaluation is the economics of collecting user input.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Amazon's Mechanical Turk is a market in which anyone can post tasks to be completed and specify prices paid for completing them.
The inspiration of the system was to have human users complete simple tasks that would otherwise be extremely difficult  for computers to perform.
A number of businesses use Mechanical Turk to source thousands of micro-tasks that require human intelligence, for example to identify objects in images, find relevant information, or to do natural language processing.
Tasks typically require little time and effort, and users are paid a very small amount upon completion .
Adapting this system for use as a research and design platform presents serious challenges.
First, an important driver of the success of the system appears to be the low participation costs for accepting and completing simple, short tasks .
In contrast, paradigms for user evaluation traditionally employ far fewer users with more complex tasks, which incur higher participation costs.
Second, Mechanical Turk is best suited for tasks in which there is a bona fide answer, as otherwise users would be able to "game" the system and provide nonsense answers in order to decrease their time spent and thus increase their rate of pay.
However, when collecting user ratings and opinions there is often no single definite answer, making it difficult to identify answers provided by malicious users.
Third, the diversity and unknown nature of the Mechanical Turk user base is both a benefit and a drawback.
Since many users are sampled with a pool drawn from all over the globe, results found using the Mechanical Turk population have the potential to generalize to a varied population more than the small user samples and limited geographic diversity typical of more traditional recruiting methods.
On the other hand, the lack of demographic information, unknown expertise, and limited experimenter contact with the Mechanical Turk population raise the question of whether useful data can be collected using micro-task markets.
Articles were originally chosen for a different purpose , but included a range of expert-rated quality ratings.
Old versions of the articles  were used so that turkers would see the same content as the admins.
Examples of articles include "Germany", "Noam Chomsky", "Hinduism", and "KaDee Strickland", amongst others.
In the first experiment we attempted to mirror the task given to admins as closely as possible.
Thus, similar to the original admin task, we had users rate articles on a 7-point Likert-scale according to a set of factors including how well written, factually accurate, neutral, well structured, and overall high quality the article was.
These questions were taken from the Wikipedia "Featured article criteria" page as guidelines for vetting high-quality articles .
Brief descriptions of what was meant by each question were presented as part of the question, again summarized from the Wikipedia featured article criteria.
In addition, users were required to fill out a free-form text box describing what improvements they thought the article needed.
This was done primarily to provide a check on whether users had in fact attended to the article or had just provided random ratings.
Turkers were paid 5 cents for each task completed.
We conducted two experiments to test the utility of Mechanical Turk as a user study platform.
We used tasks that collected quantitative user ratings as well as qualitative feedback regarding the quality of Wikipedia articles.
Wikipedia is an online encyclopedia which allows any user to contribute and change content, with those changes immediately visible to visiting users.
Assessing the quality of an article in Wikipedia has been the subject of much effort both from researchers  and the Wikipedia community itself .
A rapid, robust, and cost-effective method for assessing the quality of content could be useful for many other systems as well in which content is contributed or changed by users.
We conducted an experiment in which we had Mechanical Turk users rate a set of 14 Wikipedia articles, and then compared their ratings to an expert group of Wikipedia administrators from a previous experiment .
User response was extremely fast, with 93 of the ratings received in the first 24 hours after the task was posted, and the remaining 117 received in the next 24 hours.
Many tasks were completed within minutes of entry into the system, attesting to the rapid speed of user testing capable with Mechanical Turk.
However, the correlation between Mechanical Turk user ratings and Wikipedia admin ratings was only marginally significant , providing only very weak support for the utility of Mechanical Turk ratings mirroring expert ratings.
Furthermore, a closer look at user responses suggested widespread "gaming" of the system.
An examination of the time taken to complete each rating also suggested gaming, with 64 ratings completed in less than 1 minute .
123  ratings were flagged as potentially invalid based either on their comments or duration.
The remaining responses were too sparse to conduct a robust statistical analysis.
However, many of the invalid responses were due to a small minority of users.
Only 8 users gave 5 or more responses flagged as potentially invalid based on either comments or time; yet these same users accounted for 73%  of all flagged responses.
Thus it appeared that, rather than widespread gaming, a small group of users were trying to take advantage of the system multiple times.
The positive correlation between Mechanical Turk and Wikipedia administrator ratings was also higher than Experiment 1, and was statistically significant .
In addition to the improved match to expert ratings, there were dramatically fewer responses that appeared invalid.
Only 7 responses had meaningless, incorrect, or copy-and-paste summaries, versus 102 in Experiment 1.
The results from Experiment 1 provided only weak support for the utility of Mechanical Turk as a user measurement tool.
Furthermore, they demonstrated the susceptibility of the system to malicious user behavior.
Even though these users were not rewarded , they consumed experimenter resources in finding, removing, and rejecting their responses.
In experiment 2, we tried a different method of collecting user responses in order to see whether the match to expert user responses could be improved and the number of invalid responses reduced.
The new design was intended to make creating believable invalid responses as effortful as completing the task in good faith.
The task was also designed such that completing the known and verifiable portions of the questionnaire would likely give the user sufficient familiarity with the content to accurately complete the subjective portion .
All procedures were identical to Experiment 1, except that the rating task was altered.
In the new rating task, users were required to complete four questions that had verifiable, quantitative answers before rating the quality of the article.
Questions were selected to remain quantitative and verifiable yet require users to attend to similar criteria as the Wikipedia featured article criteria, and as what Wikipedia administrators claimed they used when rating an article.
These questions required users to input how many references, images, and sections the article had.
In addition, users were required to provide 4-6 keywords that would give someone a good summary of the contents of the article.
This question was added to require users to process the content of the article as well as simply counting various features, while being more objective and verifiable than the request for constructive feedback in Experiment 1.
Users were then asked to provide a rating of the overall quality of the article, described as "By quality we mean that it is well written, factually comprehensive and accurate, fair and without bias,
In Experiment 1 we found only a marginal correlation of turkers' quality ratings with expert admins, and also encountered a high proportion of suspect ratings.
However, a simple redesign of the task in Experiment 2 resulted in a better match to expert ratings, a dramatic decrease in suspect responses, and an increase in time-on-task.
The match to expert ratings is somewhat remarkable given the major differences between the turkers and the admins.
Since the turker population is drawn from a wide range of users, they represent a more novice perspective and likely weight different criteria in making quality judgments than the highly expert admin population.
The correlation between the two populations supports the utility of using crowds to approximate expert judgments in this setting.
For some applications in which collecting many varied data points is important, such as prototype testing or user measurements, judgments from a varied crowd population may be even more useful than a limited pool of experts.
The strong difference between the two experiments points to design recommendations for practitioners looking to harness the capabilities of micro-task markets: First, it is extremely important to have explicitly verifiable questions as part of the task.
In Experiment 2 the first four questions users answered could be concretely verified.
Another important role of verifiable questions is in signaling to users that their answers will be scrutinized, which may play a role in both reducing invalid responses and increasing time-on-task.
Second, it is advantageous to design the task such that completing it accurately and in good faith requires as much or less effort than non-obvious random or malicious completion.
Part of the reason that user ratings in Experiment 2 matched up with expert ratings more closely is likely due to the task mirroring some of the evaluations that experts make, such as examining references and article structure.
These tasks and the summarization activity of keyword tagging raise the cost of generating non-obvious malicious responses to at least as high as producing good-faith responses.
Third, it is useful to have multiple ways to detect suspect responses.
Even for highly subjective responses there are certain patterns that in combination can indicate a response is suspect.
For example, extremely short task durations and comments that are repeated verbatim across multiple tasks are indicators of suspect edits.
In this case the restrictions on participant assignment are removed as all the work is done on the experimenters' side; however, this also requires significantly more programming and setup resources to execute.
Further work is needed to understand the kinds of experiments that are well-suited to user testing via micro-task markets and determining effective techniques for promoting useful user participation.
For example, one research question is whether participants might police each other  in micro-task markets.
Also, tasks requiring significant interaction between users  might be less suitable for using a micro-task market than independent tasks.
Given the many advantages of micro-task markets, understanding the types of tasks they are effective for is an important area for future research.
In this study we examined a single user task using Mechanical Turk, finding that even for a subjective task the use of task-relevant, verifiable questions led to consistent answers that matched expert judgments.
These results suggest that micro-task markets may be useful for other types of user study tasks that combine objective and subjective information gathering.
For example, Mechanical Turk could be used for rapid iterative prototyping by asking users a number of verifiable questions regarding the content and design of a prototype followed by a subjective rating; or for surveying users by asking them to fill out common-knowledge questions before asking for their opinion; or for online experiments by collecting objective measurements prior to subjective responses.
However, Mechanical Turk also has a number of limitations.
Some of these are common to online experimentation: for example, ecological validity cannot be guaranteed, since there is no easy way for experimenters to fully control the experimental setting, leading to potential issues such as different browser experiences or distractions in the physical environment.
Moreover, Mechanical Turk does not have robust support for participant assignment, making even simple between-subject designs difficult to execute.
However, there is support for qualifying users by using automated pre-tests, or for including or excluding users from future tasks based on their responses to past tasks.
It is possible to simply use Mechanical Turk as a recruitment device and to host the user study oneself using a simple API to send and receive participant information from Amazon.
Micro-task markets such as Amazon's Mechanical Turk are promising platforms for conducting a variety of user study tasks, ranging from surveys to rapid prototyping to quantitative performance measures.
Hundreds of users can be recruited for highly interactive tasks for marginal costs within a timeframe of days or even minutes.
However, special care must be taken in the design of the task, especially for user measurements that are subjective or qualitative.
