We examine the shape of hand and forearm occlusion on a multi-touch table for different touch contact types and tasks.
Individuals have characteristic occlusion shapes, but with commonalities across tasks, postures, and handedness.
Based on this, we create templates for designers to justify occlusion-related decisions and we propose geometric models capturing the shape of occlusion.
A model using diffused illumination captures performed well when augmented with a forearm rectangle, as did a modified circle and rectangle model with ellipse "fingers" suitable when only X-Y contact positions are available.
Finally, we describe the corpus of detailed multi-touch input data we generated which is available to the community.
We studied 18 conditions covering typical combinations of 9 different contact types with 3 tasks: tapping, dragging, and transforming.
By examining the mean occlusion shapes, we find that individuals use consistent hand postures, and although there are differences between individuals, there is enough commonality to use overall mean shapes to inform interface design.
For this purpose, we create calibrated occlusion template shapes to guide designers with interface layouts which reduce occlusion.
We conducted our experiment on a diffused illumination  table top.
The raw infrared  image blob near the contact points should be a good estimate of the occluded area, and we propose a geometric model of occlusion combining the IR blob with a "forearm" rectangle.
However, input technologies like capacitance only sense XY contact positions, so we also created an alternative model.
We add ellipses for extended fingers to Vogel et al.
In a test fit of these models to occlusion silhouettes, the DI model achieves a F1 score of 0.80, while the multi-touch geometric model slightly outperforms it at 0.82.
This suggests that real-time prediction of the occluded area, even with only X-Y contact positions, is possible -- enabling occlusion-aware interaction techniques  on any multi-touch table regardless of hardware technology.
In the course of this project, we generated a large corpus of images synchronized with logged multi-touch data for common tasks.
We feel that this is also a contribution, and make it available for related research.
Operating a computer by directly touching the display surface has many benefits, and in tabletop computing, multi-touch is arguably the most natural form of input.
However, with any form of direct input, where the input device and the output display are coincident, the hand and arm cover -- or occlude -- part of the display.
This can be a problem, because compared to manipulating objects on a real tabletop, a tabletop computer is dynamic and can display relevant information, sequential widgets, and system messages in occluded areas.
Researchers are aware of occlusion: they suggest it impedes performance  and use it to motivate the design of interaction techniques .
Yet, there has not been a systematic study of hand occlusion with multi-touch tabletops.
We adapt their methods for video capture, augmented reality marker tracking, and image processing to a multi-touch tabletop .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Vogel and Balakrishnan  list hand occlusion as one of five direct pen input problems.
They observed hidden status messages, missed previews, inefficient movements, and occlusion contortion.
Pen input work by Hancock and Booth reached similar conclusions , but in practice it is difficult to strictly control for occlusion without resorting to different input paradigms like indirect pointing.
With touch screens, finger occlusion has long been known to be problematic .
Recent work has argued that finger occlusion is not directly responsible for errors , but it is undeniable that one cannot see what is beneath their finger.
With larger multi-touch tables and tablets, this is compounded as multiple fingers, hands, and forearms cover more of the display.
So it makes sense that reducing occlusion is an important aspect of direct input interface design.
It is impossible to study multi-touch occlusion without considering the shape of the hand.
Past work has looked at what postures people use for various types of multi-touch interactions in controlled experiments , elicitation studies , and in the field .
This has provided insights such as: people use different fingers for the same contact type ; people use any number of fingers for operations like dragging ; and people use different open- and closed-hand postures for single finger contacts .
Rather than looking at what postures are used, we examine the literal posture shape from a person's point-of-view.
While our focus is different, our methodology and the corpus of data we generated can be applied to investigations of other posture characteristics like the studies above.
Several touch techniques address occlusion directly.
For example, expanding feedback beyond occluding fingers , shifting a copy of the hidden area out from under the finger , and creating methods to manipulate objects remotely to avoid occluding them .
More radical solutions like moving touch to the back  or underside  also work, but they reduce the directness of touch.
Other work uses occlusion as a motivation for the spatial layout of interface designs.
For example, FingerGlass  and ShadowGuides  use spatial offsets to reduce the chance of occlusion, but it is unclear how the exact offset was determined, or if it is optimal.
Eden  describes multiple design decisions to reduce occlusion and argues for occlusion-awareness in multi-touch applications.
The goal of our experiment is to study the shape of the occluded area for canonical multi-touch table postures and interaction tasks.
We focus on a multi-touch table because smaller tablet form factors use fewer gestures due to their size and capacitive sensing makes some postures impractical.
Limitations are discussed in our conclusion.
We adapt the methodology introduced by Vogel et al.
As participants perform common multi-touch gestures, we record a video of their hands using a head-mounted camera.
Then we extract key frames and isolate occlusion silhouettes to create a rectified view of the hands from their vantage point.
We are not interested in performance time.
When making such occlusion-motivated design decisions, there is an implied reference to the shape of a "typically occluded" area.
However, this is typically determined in an ad hoc manner.
With direct pen input, empirically based occlusion-aware layout decisions have been used, such as Hancock and Booth's  context menu placement by radial selection time and Brandl et al.
Although encouraging, in neither of these cases is the shape of occlusion analyzed directly as in Vogel et al.
In a controlled experiment, they capture images of the occluded area using a head-mounted camera.
These occlusion silhouette images are used to visualize mean shapes and develop a simple geometric model.
This work led directly to Vogel & Balakrishnan's  design for an individually configurable, real-time occlusion model to realize occlusion-aware interfaces and interaction techniques.
Our methodology is based closely on this work, but we introduce new experiment tasks and refined geometric models tailored for multi-touch input.
3 participants were left-handed .
7 participants said they had experience with a multi-touch table and 21 said they had used a multi-touch phone or tablet.
We recorded the height of all participants and found a mean of 176.3 cm .
This measurement is to suggest a reasonable sampling of person size, not to search for correlations between anatomical size and occlusion shape since Vogel et al.
We conducted the experiment on a Microsoft Surface 1.0 multi-touch table.
The table-like case is 560 mm high with a 690 x 1080 mm top, approximating a small coffee table.
Participants sat in a fixed chair centred along one of the long sides and we asked them to refrain from leaning from side-to-side.
We did not observe anyone having difficulty reaching distant target locations.
The small head-mounted video camera for the participant's point-of-view recorded the experiment at 960 x 720 px resolution and 15 FPS.
It was positioned as closely as possible to the centre of the eyes, without interfering with the participants' line of sight.
Since the camera is mounted above the centre of the eyes, it does not capture the exact point-of-view.
Since we have a greater distance from the eye to the hand, our error would be reduced further.
To enable us to track and rectify the Surface display in the camera image, we displayed a 4 x 5 grid of 59 mm fiducial markers.
We could not put the markers on the bezel due to the size of the display and the field-of-view of the camera.
We were initially hesitant to show this pattern under our experimental stimulus, but found that participants effectively ignored it within the first few practice trials.
The experiment code is in C# using the Surface SDK.
During task activity, the position, ellipse size, and ellipse orientation of all touch contacts were logged at more than 60 Hz along with 15 FPS of 768 x 576 px  raw IR captures.
In addition to the head-mounted camera, we recorded everything with a stationary camera above the Surface, but did not use this in the present analysis.
The gesture movements define three tasks: Tap, Drag, and Transform.
Each task has a main circular target with diameter set according to posture .
The smallest diameter is three times the minimum recommended touch target size , the largest based on anthropomorphic palm size , and intermediate sizes selected to easily accommodate postures.
This balances ease-of-selection with location constraints.
In the same spirit of Kin et al.
For example, a 2 digit posture could be a thumb and index finger, or an index and middle finger.
Overly suggestive targets or terms like those for teaching specific gestures  would prevent natural posture strategies, leading to different shapes.
For most postures, our code prevents interaction unless the correct number of contacts are on the target.
This worked well for 1 to 5 digits, but had to be relaxed for palm, fist, and side since the Surface detects an irregular number of contacts in these cases.
In the spirit of allowing participants to adopt posture strategies, we do not control for the number of contacts with the two-handed condition.
We wanted to see if people used two index fingers or some other combination of fingers across hands.
To complete the Tap task, the participant touches a circular target using the required posture for 333 ms.
This short delay reduces motion blur and increases the tolerance for event log synchronization for the point-of-view frame captures, addressing problems reported by Vogel et al.
We cover a range of extreme positions by centering the grid in the display and spacing columns and rows at 192 mm and 112 mm respectively.
To complete the Drag task, the participant uses the required posture to drag a circular target from the centre of the display to one of 8 circular dock locations on the same 3 x 3 grid.
Compared to Vogel et al.
We focus on single-handed gestures since many two-handed gestures may be factored into two separate gestures for the purpose of shape analysis.
We identified three main types of interaction movements: tapping, dragging, and object transformation.
A fourth choice would have been "flicks," but these resemble a short, high speed drag.
To avoid redundancy between dragging and transforming, we restrict transformations to simultaneous rotation and scale only.
We identified eight common types of postures: using 1, 2, 3, 4, or 5 digits ; a flat palm, the side of the fist, and the side of the hand.
All 8 of these postures are paired with tapping, but only the first 6 with dragging since dragging with the fist or the side of the hand are less common.
We paired transforming with 2 digit and 5 digit poses only since a 2 digit posture captures the common pinch gesture and the 5 digit posture is also common for transformations .
Like the Tap task, the target must be held within the dock using the correct posture for 333 ms. Transform Task.
To complete the Transform task, the participant rotates and scales a circular target until a pin aligns with a 30 mm rotation tolerance "key" and the target border fits within a 15 mm outer ring tolerance.
This position must also be held for 333 ms to complete the task.
All Transform tasks are at the center grid position, but with 4 rotation and scale conditions: clock-wise  and counter-clockwise  60 rotation and scaling up or down by 45 mm.
The initial target angle is -60 for CW and 0 for CCW to minimize key occlusion.
To transform the point-of-view video into occlusion silhouettes, we use the same steps as Vogel et al.
After synchronizing the video and the data log using visual time markers, we capture one video frame at the end of all tasks and one frame when the participant first touched the target in Transform and Drag.
We wrote custom software using the ARToolkitPlus augmented reality library  to track the fiducial markers.
After tracking the image-space marker positions, we use OpenCV to calculate the homography matrix and rectify the image of the hand against the display.
Our software application enables us to manually track markers when automatic tracking fails .
About 5% of the frames had to be partially tracked manually.
To isolate the binary occlusion silhouette images for analysis, we use similar image processing steps as Vogel et al.
Since the fiducial markers are in the display space, we add a median background subtraction step to remove them.
This works reasonably well, but we realized that colouring the fiducial markers saturated blue instead of black would have greatly simplified this task.
The main experiment had 3 Blocks, with each block consisting of 3 Task Sections.
Each Task Section contained all permutations of Posture and Task Condition for a Task, grouped by posture: the Tap Task had 8 posture groups, each at 9 grid position Conditions; the Drag task had 6 Posture Groups, each at 8 grid position Conditions; and the Transform task had 3 Posture Groups, each with 4 rotation and scale Conditions.
The Postures were presented in approximate order of increasing difficulty .
Within each Posture Group, the order of Conditions were randomized.
All blocks had the same Task ordering, but this order was counter-balanced across participants.
In summary: 3 Blocks x 8 Postures x 9 Conditions  + 6 Postures x 8 Conditions  + 3 Postures x 4 Conditions  = 540 data points per participant Before beginning the main experiment blocks, participants completed 26 practice trials: 1 centre Tap trial for each Posture; 1 Drag trial for each Posture to a random outer grid position; and all permutations of Transform trials.
After the main experiment, participants also completed 72 trials with their non-dominant hand covering a subset of Conditions for all Tasks and Postures: 5 Tap trials for each Posture at all grid positions except corners; 4 Drag trials for each Posture to all grid positions except corners; and 8 Transform trials covering all Conditions except the twohanded posture.
Since our goal is to study the shape of the occluded area, we begin with an examination of the overall mean occlusion silhouette shapes shown in Figure 8 .
These are created by registering all silhouettes by target grid position , then finding average pixel values across all participants and conditions for each Task and Posture.
Using the usual experimental assumption that our 24 participants provide a reasonable population sample, the darker areas are more likely to be occluded.
For Tap and Drag, the darker areas show most Postures clearly, suggesting homogeneity across participants and grid positions, but heterogeneity between Postures.
Differences between Fist and Side are subtle, but consistent.
Medium grey areas suggest different Posture strategies.
For example, the ghost-like shape of other fingers for 1-Digit and 2-Digits suggest a mixture of open and closed hand postures.
There is also surprising similarity between Tap and Drag tasks.
For most postures, the differences are unperceivable; the largest differences are with the Palm.
For Transform, the darker areas are less defined.
This is partly due to high positional variance in the different conditions, but also indicates a greater variety of Posture strategies.
A general thumb and index finger pinch shape can be seen for 2-Digits, perhaps because most participants reported experience with multi-touch devices where this strategy is standard.
For Two Hands, the hands can be discerned, but it is unclear what digits are used.
The posture for 5 Digits is particularly heterogeneous.
While 2-Digit Transform has similarities to 2-Digit Tap and Drag, 5-Digit and Two Hand similarities to Tap and Drag are less so.
Mean areas by Posture for Tap and Drag also support visual observations.
The trend continues with Palm's large task discrepancy and area, 152 cm2  for Tap and 141 cm2  for Drag.
With Transform, we observed visual differences for common Postures with Tap and Drag, but quantitatively the difference in area is within 10 cm2 with measurements of 89 cm2  for 2-Digits and 106 cm2  for 5-Digits.
The mean area for Two Hands is 98 cm2  which is closer to 3-Digit Tap and Drag.
We also examined all mean silhouettes for Participants by Task and Posture.
Due to space, our discussion here focuses on Tap and Transform for 8 participants chosen for diversity, which are reproduced in Figure 9 .
Individual differences in posture strategy, hand shape, and hand size are apparent, but there are common shape characteristics overall.
This is most clear in Tap where the palm resembles a circle with one or more fingers extended, and the forearm typically projects down to the lower-right.
This is more difficult to see with Transform where start and end positions are merged together.
Open- and closed-hand strategies are clearly seen for 1Digit, 2-Digits and 3-Digits.
For example, participants 8, 3, 21, and 24 extend their touch fingers from a closed fist, whereas others extend all fingers regardless.
Although we observed some variation in digit used , each participant generally used the same digit for a given Posture.
For Palm posture, most participants spread their fingers  and differences between the fist and side are subtle.
Across postures and tasks, forearm angle appears consistent.
With 2- and 5-Digit Transform, there is more wrist variability.
Mean breadth for Tap and Drag across common Postures is similar at 170  and 181 , but with higher variance.
Differences between the same Posture for Tap and Drag are less than 4, except Palm at 59 which supports our observations.
Overall, the spread monotonically increases from 119 to 225 for 1-Digit to 5Digits.
For Transform, 2 Digit spread is greater than 2-Digit Tap and Drag reflecting the variability in the pinch gesture.
Arm angle is remarkably consistent for common Postures between Tap and Drag.
The overall means are 58  and 57  respectively, with individual variations less than 3 .
We can compare Transform and Tap at the centre location only.
Here the values diverge, with means of 45  and 58  respectively, likely due to the variability of the Transform Task.
As the hand reaches target locations, the forearm angle changes according to kinematics.
While arm angle varies according to contact location, these angles are remarkably consistent between Drag and Tap and across participants.
Descriptive statistics for hand occlusion:  occluded area in cm2 within a 100 mm radius of the target center;  angular hand posture breadth within a 50 to 100 mm ring;  arm angle from horizontal, calculated from the target center to the centre of mass beyond a 150 mm circle.
These results are of theoretical interest to researchers and provide causal evidence for the potential impact of occlusion during interaction.
But, how can it be used?
Next, we show how this occlusion shape information is made accessible to tabletop interface designers, and describe a simple model to capture the essence of the occluded area setting the stage for multi-touch occlusion-aware interfaces.
Figure 5 illustrates two examples of how designers can use these templates to guide occlusion related decisions: x When specifying the position of an information bubble opened with a two finger tap, a designer can use the 2 digit tap template to position the bubble at an offset and angle least likely to be occluded, but also minimizing distance for ideal Gestalt association .
While labels should avoid the "often occluded area," the designer can utilize the "possibly occluded" band to place lower priority labels.
The template shape enables further refinements, such placing highest priority labels at `1', since they are least likely to be occluded.
Using the overall mean silhouettes, we created design-time "occlusion-awareness" templates for designers .
Each template is a dimension-calibrated image showing areas which may be occluded relative to the expected contact centroid.
Two bands of occlusion severity are illustrated, calculated from pixel density thresholds in the associated mean image: the possibly occluded area  and the often occluded area .
These templates are available for download1 as a layer-separated PDF and can be imported into common design applications like Adobe Photoshop or Illustrator.
After scaling the template to match the real world units of an interface design, the designer can use it as an overlay to make occlusion-aware layout decisions.
We created templates for all postures across t and Transform task .
Other scenarios include contextual menu placement , pie menus , and visualizations .
The designers of FingerGlass  and ShadowGuides  could use these templates to justify their choice for spatial offsets and there are multiple opportunities to refine layouts to minimize occlusion in a complex application like Eden .
Although helpful, these are design-time decisions using mean shapes.
Knowledge of the currently occluded area at any given moment would be even better.
In this section, we develop and test different ways to model the occluded area suitable for high fidelity technologies like DI and more limited hardware like capacitance.
Our aim is to show that applying a single model to a wide range of postures is possible, and establish an upper bound on potential performance.
This sets the stage for a configurable real time model of occlusion as future work.
With minor rotation and offset transformations relative to the contact centroid, this should match a portion of the occluded area and forms a key part of our first geometric model.
The problem is that the whole forearm is not usually captured due to its height above the diffuser, so we add a rectangle with a constant offset of 100 mm from the same centroid .
This DI model has five parameters: a distance and angle to describe the offset of the DI image, an angle for rotation of the DI image, and a rotation angle and width for the rectangle.
Note that we are not learning model parameters, but rather estimating an upper bound for model capability.
The reader can consult the prior art for methodology details noting these changes: we use a single fitting stage; we only used pattern search; we use the posture contact centroid; and our objective function simply maximizes the F2 score.
F2 favours recall over precision so more of the occluded area is covered creating more false positives, but fewer false negatives -- a quality Vogel et al.
To remain consistent with past work, we compare fidelity with the equally weighted F1 score.
Since the models are one-handed, we remove two-handed transform cases.
Fitting each model to the 9209 test cases took more than 12 hours with a 2.66 GHz quad processor.
We also tested the "fingerless" Vogel et al.
Since our model is based closely on it, a similar score is expected, but it is encouraging to see the finger ellipses improve fidelity without additional parameters.
Typically, FTIR only provides the shape and size of the contacts , and capacitive only X-Y contact positions.
To cover a wider range of devices, we extended Vogel at al.
To represent extended fingers, we add an ellipse for each contact and position it relative to the circle.
Specifically, the major axis is aligned with the vector from the contact point to the circle centre and its length is such that the minor axis forms a chord on the circle.
We set the minor axis to 15 mm and scale the major axis such that the tip extends 10 mm beyond the contact.
Since these are constants, no additional model parameters are introduced for the ellipses.
When faced with modeling extended finger postures, the ellipses should increase accuracy compared to only Vogel et al.
A limitation is that we only have actual X-Y contacts, so a single finger contact with an open hand  would only have a single contact.
In this case, the model can increase r and decrease q to remove the ellipse and cover the entire hand with the circle.
Our geometric model is primarily useful for non-DI devices where only individual finger contacts are sensed, not postures like palm, fist, and side.
Thus, comparing mean F1 scores using only 1 to 5 digit contacts is more relevant.
In this test, the DI model achieves a similar F1 score of 0.802  and a precision-recall plot illustrates a precision bias .
Our multi-touch circle and rectangle model improves with 0.819  and the plot suggests very high recall and good precision .
Generating the occlusion design-time templates and testing different occlusion models leverages the large corpus of images and metadata we created in our experiment.
This includes 16,320 sets of images synchronized with contact positions, sizes, and orientations.
We adapted an established image-based methodology  to study the shape of occlusion on a multi-touch tabletop.
By examining the shapes of mean occlusion silhouette visualizations and calculating descriptive statistics, we found common characteristics across people, postures, and tasks.
Based on this, we created occlusion awareness templates to guide interface layout decisions, and tested different geometric models suitable for high- and lowfidelity multi-touch input technologies.
This latter contribution is a necessary step towards a real-time, configurable model to enable multi-touch occlusion-aware techniques such as those created for direct pen input .
Using a tabletop allowed us to test a wider assortment of postures and test DI captures as a potential model, but we had to accept potential limitations.
Although the Microsoft Surface is popular and has spawned other similarly-sized tables, larger and taller tables could influence body posture and resulting occlusion.
More broadly, an obvious question is how well our results generalize to other multi-touch phones, tablets, tables, inclined desks, and vertical walls.
We argue that at least for near horizontal cases, the relative relationship and viewing angle of operator to device is similar.
The biggest shape change was due to forearm angle when reaching, which does not apply to small devices.
However, body postures may contort more drastically when reaching targets at extreme edges of a large surface.
A second limiting factor is that our results are relative to body location.
For larger surfaces, we assume that the location of people around the table can be determined using sensors , or perhaps using the angle or shape of the hand in the spirit of Dang et al.
Finally, we are eager to see how the community might use the large corpus of data we created.
