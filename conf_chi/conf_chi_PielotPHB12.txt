In this paper, we report about a large-scale in-situ study of tactile feedback for pedestrian navigation systems.
Recent advances in smartphone technology have enabled a number of interaction techniques for smartphone that use tactile feedback to deliver navigation information.
The aim is to enable eyes-free usage and avoid distracting the user from the environment.
Field studies where participants had to fulfill given navigation tasks, have found these techniques to be efficient and beneficial in terms of distraction.
But it is not yet clear whether these findings will replicate in in-situ usage.
We, therefore, developed a Google Maps-like navigation application that incorporates interaction techniques proposed in previous work.
The application was published for free on the Android Market and so people were able to use it as a navigation system in their everyday life.
The data collected through anonymous monitoring suggests that tactile feedback is successfully adopted in one third of all trips and has positive effects on the user's level of distraction.
More and more vendors consider pedestrian navigation as a growing market and promote pedestrian support as a key feature.
However, a significant body of research on interaction with handheld devices shows that visual and, to some extent, auditory user interfaces may not always be suitable for typical usage scenarios.
Users may suffer from so-called situation induced impairments , i.e.
For example, in a PewInternet study  one of six  cell-owning adults reported to have physically bumped into another person while being busy with their mobile phone.
Using earplugs, frees the eyes but can lead to iPod Zombie Trance, which refers to the loss of situational awareness from listening to loud audio content.
According to the Sydney Morning Herald, authorities in Australia are speculating that listening to loud audio content is one of the main contributing factors to the still increasing pedestrian fatalities.
As a solution for these challenges, several research groups have investigated using the sense of touch to convey the navigation information in a non-distractive manner.
Examples are vibrating vests or belts , vibration patterns , or using the mobile phone as a pointing device that vibrates when facing the direction to go .
These studies provide evidence that tactile feedback can significantly reduce the navigator's distraction.
So far, these techniques have been studied in controlled field and lab studies.
The navigation tasks were given by the researchers and limited in terms of usage time and geography.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In this paper, we report on the first large-scale in-situ study of tactile feedback for pedestrian navigation systems.
We, therefore, developed a map-based pedestrian navigation application for Android phones.
Based on tried and tested previous work, we included tactile feedback which guides the user along the route without the need to look at the display.
The application was distributed for free via the Android Market and downloaded over 17,000 times.
By monitoring how the application is used, we collected data from in-situ usage.
In this paper, we report on findings of over 9,400 hours covering a period of eleven months.
Left: Magic wand metaphor - the user scans for spatial information; the presence of an entity, such as a waypoint, in the pointing direction is indicated by vibration.
Right: The sixth sense metaphor - the direction of a spatial entity, such as a waypoint, is encoded in vibration feedback itself, e.g.
Tscheligi and Sefelin  argue that considering the context of use appropriately is one of the main prerequisites for the success of pedestrian navigation systems.
One of the main challenges is the distraction that emerges from the use of mobile devices on the move .
Pedestrians might lose their situation awareness, which may be dangerous when walking through lively, traffic-heavy areas .
A solution to this problem can be derived from Wicken's Multiple Resource Theory .
Each sensory channel has its own pool of attentional resources.
When conveying two bits of information by different sensory channels it is more likely that both are perceived and processed.
For example, when a user has to watch the traffic, it is easier to listen to navigation instructions than reading them from a small display.
Since the sense of touch is hardly used to monitor the environment for dangerous traffic situations, it is best suited to communicate navigation information and avoiding dangerous distraction at the same time.
In 1997 Tan and Pentland  proposed a 3x3 array of tactile actuators worn on the back for conveying navigation information.
E.g., a series of pulses moving from the left to the right of the display could be used to indicate "turn right" or "right-hand side".
A more intuitive form of presenting directional information , by creating vibro-tactile stimuli on the torso that "point" in the direction to move, was proposed by van Erp et al.
An early example by Tsukada and Yasumura  is the ActiveBelt, which is a waist belt equipped with eight vibro-tactile actuators.
It allows creating tactile stimuli around the wearer's waist to "point" into a horizontal direction.
By pointing into the direction the user has to go, such tactile displays can guide pedestrians along a route.
This form of waypoint navigation has found to be effective and beneficial for the user's distraction .
Tactile displays, such as tactile belts, might become common in the near future, but yet they might not always be available when the user is traveling.
The magic wand metaphor  refers to pointing at a distant object with a handheld device to learn about its presence or access information about it.
Technically, this has become possible since smartphones are increasingly more often equipped with digital compasses that allow obtaining a device's pointing direction.
Recent implementations provide feedback when the user roughly points at a relevant spatial entity, such as the travel destination .
In the context of navigation support, users can stay aware of the general direction of their travel destination by actively scanning the environment for it.
It has been shown that this technique is intuitive and allows users to effectively reach a given destination .
However, the intuitiveness is traded with the drawback that the techniques requires active pointing gestures, which might become cumbersome over time .
The sixth sense metaphor describes interaction techniques that use multimodal feedback to alert the user about changes in the environment, such as the location of a travel destination in relation to the traveler's location and orientation.
This has been applied by issuing turning instructions in vibration patterns  as well as cueing the direction of spatial entities in vibration patterns  by the means of Tactons .
The advantage this approach is that users are not required to search for spatial entities via pointing gestures.
However, it requires users to learn the meaning of the feedback, e.g.
Further, it has shown that both approaches go well together .
These groups have successfully tested conveying the direction of a spatial entity relative to the devices pointing direction using vibration patterns.
The combination of both approaches has the advantage that the design has the intuitiveness of the magic wand approach, but can also be used without doing pointing gestures.
In terms of situation awareness, previous field studies  have provided evidence that tactile feedback for navigation systems can significantly reduce the traveler's distraction.
Thus, there are a number of imminent limitations.
Using a navigation system was not a real need of the participants.
All studies took place at one single location and with rather homogeneous user samples.
These controlled setting ensure the internal validity of the results, but consequently they lack external validity.
This means, that it is not clear, whether the intuitiveness and the benefits of tactile feedback in terms of distraction will translate to in-situ, real world usage .
The question remains, whether users will benefit from tactile feedback in all different kinds of situations, where navigation support is a real necessity of the user.
The presented study aims at bringing the research on navigation systems from the lab into everyday life.
The goal is to study how travelers use the tactile feedback and if there is a positive effect on the level of distraction.
Furthermore, we aim at collecting data from as many usage scenarios as possible in order to provide evidence that the positive findings from previous studies translate to typical navigation situations.
So far, only few in-situ studies with tactile feedback have been conducted.
Hoggan and Brewster  conducted an in-situ study where nine participants were asked to use a handheld device with a multimodally enhanced keyboard for eight days.
These studies were conducted with only a few participants because the implementation of long-term studies is resource intensive.
In order to tackle that problem, we applied the emerging approach of doing "research in the large" .
By using mobile application distribution channels, such as Apple's App Store or Google's Android Market, we bring our research apparatus "further into the wild" .
We, therefore, developed a navigation system for pedestrians with added tactile feedback.
By providing this application for free via the Android Market, we hoped to encourage as many Android users as possible to use the application on their trips.
To be able to collect evidence from a large amount of users with our given resources, and to avoid being obtrusive to our users, we used automated context sensing and logging to obtain our data.
Besides calculating and showing a route on a map, most of today's navigation systems can also provide navigation instructions.
In cars, navigation systems typically give turning instructions, such as "turn right in fifty meters".
For pedestrians, this has a few disadvantages.
Cars usually travel in confined lanes of a road.
Pedestrians, however, can move freely and change direction at any time.
Also, pedestrians move much slower, so the typical distances  are too big.
For pedestrians, distances would have to be reduced to a few meters only.
This, however, is difficult to achieve, since GPS is hardly accurate enough.
Previous work has suggested the concept of waypoint navigation as an alternative form of guiding pedestrians .
Routes are divided into sets of waypoints.
Once this waypoint has been reached the system switches to the subsequent waypoint.
In our application, the arrow in the lower left corner of the screen  constantly points at the next waypoint.
By walking into the indicated direction, users are guided along the route until reaching the destination.
As apparatus for the experiment we used the PocketNavigator , a map-based navigation system developed by our research group that is similar to Google Maps.
The PocketNavigator, as shown in Figure 3, uses OpenStreetMap data which has highly detailed data on pedestrian paths in many countries.
Further, the application provides all the essential navigation functionalities available as with Google Maps: an icon drawn onto the map indicates the user's position and orientation.
The map can be set to automatically rotate and align itself with the environment, so the "up" direction on the screen corresponds to the device's orientation.
Users can search for addresses or just select their destination via the map.
Since our goal is to study tactile feedback, we added a tactile user interface to indicate the direction of the next waypoint.
In order to build on a proven technique we combined previously proposed instances of the Magic Wand and the Sixth Sense metaphor.
As instance of the Sixth Sense metaphor, we used the Tactile Compass design described in our previous work .
The patterns used in our prototype are illustrated in Figure 4.
For example, when a user walks towards the next waypoint, two short vibration buzzes indicate "ahead".
When the next waypoint is to the left hand side, a long buzz is followed by a short buzz.
These directions are given in relation to the user's walking direction.
The walking direction is obtained from the GPS signal.
This has the advantage that the device can be left in the pocket and requires no potentially fatiguing gestures.
The disadvantages are that the patterns have to be learned and that no walking direction can be determined when the user does not move.
The pilot tests of our previous experiment  showed that a simple implementation of waypoint navigation which strictly requires reaching the waypoints in the given order will not work in the real world.
Users may always skip waypoints, e.g.
If the system does not react to such situations, users may be forced to go back to a missed waypoint.
Our application, therefore, monitors the distance to several waypoints, and skips to a closer waypoint when applicable.
If the user has left the route for too long, a new route is calculated.
The success of waypoint navigation also depends on how close the user needs to get to a waypoint until the system switches to the subsequent waypoint.
Switching too late causes the user to reach a decision point without knowing where to turn.
Switching to the next waypoint too early can result into direction information that may be hard to interpret, e.g.
In a series of pilot studies, we optimized the switching algorithm to provide the new directional information in the most suitable moment.
One of the tweaks we used was to switch to the next waypoint earlier, the faster the user walks and the less accurate the GPS signal becomes.
To help users to learn the vibration feedback, a tutorial was included that automatically popped up when starting the application.
In a series of pictures it showed different routing situations and the according vibration patterns.
Further, we added visualizations of the vibration patterns to the visual direction arrow .
Thus, users had sufficient hints to learn and understand the vibration patterns.
To compensate for these disadvantages, we added a Magic Wand design which closely follows the ones that were successfully tested in previous work .
When the device is held parallel to the ground it becomes a pointing device.
This allows users to scan for the direction to go.
For example, when the device points at the next waypoint, users receive the "ahead" signal, as in the Sixth Sense metaphor.
Here, the application uses the magnetometer to obtain in which direction the device is pointing.
In both interaction designs, the vibration feedback is repeated every 3-4 seconds.
In a previous experiment  we found that while some people appreciate the constant feedback, others were afraid to become annoyed over time.
In order to not having to turn off the tactile feedback manually, we introduced two features that apply to the Sixth Sense mode.
One feature is that the feedback mutes when the user stands still.
Thus, when typically no information is needed the device remains silent.
The other one is that we introduced a "silent corridor".
As long as the user walks towards the next waypoint, the system remains silent.
In the Magic Wand mode, we did not silence the vibration feedback since we believed that users would use this mode to actively sweep for directions and hence desire the information.
A further effect that played in our hands was that the vibration of typical devices is usually too weak to be perceived well in the pocket on the move.
We found that users then just touch the pocket to feel the vibration with the fingers.
This has the advantage that it allows users to receive tactile feedback ondemand by simply touching the device.
Since our aim was to study the effect of tactile feedback on the level of distraction, the presence of the tactile feedback is considered as the independent variable.
It has two levels: being turned on corresponds to the experimental condition, being turned off to the control condition.
We first envisioned a between-group experiment design where the condition would be randomly assigned to the user.
For half of the users the tactile feedback would have been always turned on, for the other half it would always been turned off.
However, we discarded this approach, since we heavily advertised the application's vibration guidance feature.
We were afraid to confuse or annoy users if they were not receiving the expected vibration feedback.
Further, in our pilot studies we learned that turning off the tactile feedback is sometimes necessary, e.g.
Thus, we allowed the user to turn the tactile feedback on and off.
Consequently, the experiment is not a true experiment, but a quasi-experiment.
To monitor the usage of the device, our goal was to recognize important events via the available sensors.
Since important events, such as looking at the display, may be very short, our primary strategy was to continuously monitor the device's state.
For practical reasons, the application takes snapshots of the current context once every second.
To limit the use of bandwidth, we only stored and transmitted snapshots where a route was present.
8,187 different routes were found.
The average characteristics of these routes suggest that the typical usage of the application was to browse and explore routes rather than using the application as navigation system.
Users moved in only 13.0 % of all collected snapshots.
Thus, most of the recorded routes do not reflect usage on the move by a pedestrian.
Since the aim of this work is analyzing usage on the move we applied a set of filters.
The goal was to only keep those routes, which were part of a trip, i.e.
We therefore kept only these routes that lasted longer than 30 seconds, were shorter than 40 km.
Further, the user had to have moved at least 25 % of the time with a mean travel speed being below 8 km/h and a the maximum travel speed being below 20 km/h.
By using these constraints we filtered out all routes that are not likely to be traveled by foot.
To avoid ethical issues we never stored any personal data, i.e.
To be able to match data to a certain phone, we calculated an MD-5 hash code from the phone's device ID and used this as an identifier.
Hash codes have the advantage that they are not reversible but can be used as a unique identifier.
Further, we never stored any GPS locations, nor the names or addresses of the travel destinations.
The PocketNavigator was uploaded to the Android Market in April 2010.
It was advertised in blogs and social networks.
The logging framework used for this study was added February 2011.
The data we report reflects usage between this date and December 2011.
In the time between April 2010 and February 2011 the PocketNavigator has been subject to numerous pilot tests and a published field experiment .
Thus, during the study period the application had already been thoroughly tried and tested.
Therefore, bugs or salient usability issues do not bias our findings.
During the study period the application was started on 3,338 different devices.
As shown in Figure 5 the locales reported by the devices lets us suggest that our sample mostly contains users from the so called Western Countries, i.e.
More than 58 % of the devices named English  as system language.
Table 1 shows the average characteristics of a trip.
Most of the trips  represent distances below 1 km, which are easy to travel on foot in 6 to 12 minutes .
For the calculation of the effective travel speed we included only the context snapshots where the user was moving, so the result does not get biased when the user is standing.
For most of the time  users followed the calculated route.
This means they stayed close to the shortest path between the previous and the next waypoint.
Since the destination was reached on 15 % of all trips only, we updated logging framework by adding the distance to the destination as a parameter to the context snapshots.
Judging from 130 trips containing this added parameter, the mean distance between the user and the travel destination was 874.7 m  when the routing was discontinued.
Thus, users did not interact much with the device during their trips.
Yet, the tactile feedback had a significant effect on the amount of interaction .
29.9 % of the users used the tactile feedback, i.e.
It was turned on in about one forth of all context snapshots .
In terms of navigation performance, not statistically significant differences could be found between the conditions.
To understand how the tactile feedback was used we analyzed how often and how long users scanned for the next waypoint when the tactile feedback was turned on.
From our previous studies we can confirm that this is the typical posture when people scan for the next waypoints.
Related work argues that travelers are often distracted, since they spent a lot of time looking at maps and screens on the move .
Thus, we wanted to understand how often the users held the device in the hand so that they could see the screen.
We call these in hand events.
From our pilot tests we knew that users tend to hold the device nearly parallel to the ground with a slight tilt towards the face when looking at the display.
Hence, whenever the screen was turned on and the device was held roughly as e.g.
As shown in Table 2 scanning events occurred frequently but were rather short most of the time.
The difference between the mean duration  and the median duration  of a scan shows that there were quite different usage strategies.
One seems to involve frequent, but short scans, while the other means sustain scanning for the next waypoint.
Users used the scanning feature during one sixth to one third of the route .
Scanning occurred more often when the tactile feedback was turned on , but the difference is not statistically significant .
As shown in Table 4 users held the device in hand for more than half of the trip time in average.
The tactile feedback had a significant effect on the time the device was held in hand  The device was held significantly less in hand when the tactile feedback was turned on  compared to when it was turned off .
Further, we identified those events where distraction is highly unlikely, since the screen was turned off.
Whether the screen is turned on or of can be determined from the lifecycle status of an Android application.
Only when the status is started the screen is on.
To see in what way the user interacted with the device we analyzed the touch events generated by the touch screen.
Whenever a touch event was recorded during a snapshot we counted an interaction event for the second that the snapshot represents.
Table 3 shows the detailed analysis of the interaction events.
The median value shows that during many trips users touched the screen only once and briefly.
In total, the screen was kept off for nearly half of the trip time.
The difference between mean and median duration and the high standard deviation of 351.3 s show that few users kept the screen turned off for several minutes, while the majority of the users turned it off for a short time  only.
The tactile feedback had a significant effect on the time the screen was turned off .
With the tactile feedback the screen was turned off significantly longer  than without tactile feedback .
The notable difference between the number of calculated routes and those that were actually navigated by the user indicates that the application was mainly used to explore environments with the help of the map.
Browsing a map, therefore, seems to be the more frequent use case than navigation.
This could be explained by previous findings suggesting that people appreciate the fact that maps provide them with an overview .
Further, the destination was only reached in one sixth of the trips.
In average the distance was about 850 m away when the application was turned off.
At the same time, users followed the route most of the time .
One explanation might be that the application was mainly used to guide the users along parts of a route only.
From the initial release to the end of this study we received about 35 comments via the Android Market's rating facility.
Six of them addressed the vibration guidance: * Good applet but consumes too much battery  * Works fine on Legend.
Nice if you don't want to stare at your mobile all the time to find the way.
Needs a bit practice to interpret the vibration.
Not sure how long my hero's battery will last with GPS on and my phone vibrating every second to indicate if on right track!?!
Too annoying and so I didn't try it for navigation yet.
No rerouting, and sends me back to points.
We found three main take-aways:  the users' main concern is degrading the battery life by the vibration,  training is needed to make use of the tactile feedback, and  the idea is received well.
We addressed the concern about the battery life by enabling the user to reduce the amount of vibration by turning on the silent corridor.
The desire for more training was addressed by adding the tutorial demonstrating the vibration patterns.
Both of these additions were in place when the data logging began.
Beyond that the user comments were not at all helpful with respect to our research questions.
We believe that researchers should never rely on user comments alone when studying user interfaces via the Android Market or similar distribution channels.
The tactile feedback was used in roughly one third of the travel time and in one forth of all trips.
Given the fact that the users had to learn the vibration patterns from the tutorial and the indications on the visual compass icon  only, we consider this as a positive finding.
On the trips where tactile feedback was used, the Magic Wand technique, i. e. situations where users were sweeping the device for the direction of the next waypoint, was used more than one third of the travel time in average.
About one third of the users scanned often but only for a few seconds, another third of the users scanned only a few times but for a longer time.
However, these findings have to be handled with care.
The application cannot decide whether the user actually intended to scan for the next waypoint, or maybe just checked the map and accidentally held the device parallel to the ground.
All dependent measures related to the level of distraction show a positive effect of the tactile feedback and, therefore, confirm findings from previously reported field studies.
Previous results  indicate that navigators spent a significant portion of their attention on reading the map and navigation instructions.
In , participants looked every 5.8 seconds on the map or the display of the navigation system.
In , the participants looked at the map for about 25 % of the total trip in average -- some participants for even half of the travel time.
Our results suggest that users looked at the display for 53.5 % of the trip time, which shows that the results from previous work are not necessarily caused by the context of the user study.
Given that the average trip was 7.8 minutes and during the averate trip user switched 6.5 times from a posture where s/he could look at the display to a posture where this was not possible, we infer that users checked the map at least every 72 seconds at the device.
However, unlike Rukzio et al., we are not able to detect quick context switches between checking the display and checking the environment.
Thus, there might have been many more context switches, which, however, did not cause the user to change the posture of the device.
We collected data from 8,187 routes and more than 9,400 hours of usage.
Most of the time a route was calculated, the application was presumably not used as a navigation aid as the user did not move.
Filtering our data according to the characteristics of a pedestrian traveler, we found 301 trips where pedestrians navigated along the calculated route.
The tactile feedback was used in 29.9 % of the trips with no effect on the navigation performance.
However, we found evidence that suggests statistically significant effects on the level of distraction: users interacted less with the touch screen, looked less often at the display, and turned off the screen more often.
These findings from previous work  also suggest that navigators consult their navigation system far more often than necessary.
Instead of just checking the map or navigation instructions when approaching a decision point, users keep looking at the display throughout the trip.
Hence, motivating travelers to turn off the display of the navigation system or to stow it away more often will make it less likely that users glance at it and, therefore, become distracted from the environment.
Our results here show that the tactile feedback caused users to interact less with the touch screen, turn off the display more often, and hold the device less often in a way that users typically do in order to read the display's content.
Thus, we conclude that adding the tactile feedback caused the users to be less distracted from the environment.
This confirms findings from previous field studies which have shown that replacing visual navigation cues by vibro-tactile navigation cues lower the travelers' level of distraction  or that conveying tactile navigation cues while navigating with a map can also reduce the level of distraction .
These results are in line with findings from previous field studies.
Furthermore, since we obtained these results from an application from the Android Market, this study is the first which provides results from in-situ usage and numerous different users and usage contexts.
Using this novel study methodology, allows us the conclusion that users would accept tactile feedback in their handheld devices and are able to make sense of it with no help but a tutorial.
These findings encourage incorporating tactile feedback into applications for handheld devices that are designed to be used on the move.
Future work will have to investigate how to assess and increase the internal validity of studies where the experimenter is not present.
With the right kind of measurement tools and data collection strategies, we may be able to study novel interfaces in much broader contexts than we do today.
With our study design of conducting a study on the Android market, we focus on the external validity of the findings.
We gave up control over how our participants use the application and traded internal validity for external validity.
Our results show that people use applications in other than the indented forms, such as exploring a route without going on the actual trip.
This makes it harder to rule out confounding variables.
However, our very strict set of filters ensures that we only analyzed situations, were travelers were navigating along the route on foot for a minimum of half a minute.
On the other hand, our findings reflect in-situ usage, and cover a wide range of users, usage environments and contexts.
The controlled studies in previous work, in contrast, only reflect a limited set of usage situations and always required the user to fulfill a given navigation task.
We believe that previous work and the presented study in combination cancel out each other's weaknesses.
With this study we provide the missing evidence that allows reinforcing the external validity of these previous studies.
Taken together, previous work and the study presented provide evidence that tactile feedback is usable and accepted by a good share of pedestrians, and has a positive effect on the level of distraction.
In this paper, we report on the first large-scale in-situ evaluation of tactile feedback for pedestrian navigation systems.
We have enhanced a map-based navigation system with vibration patterns in order to deliver navigation cues in an eyes-free way.
The system was published for free on the Android Market.
The collected data provides evidence that tactile feedback was used in nearly one third of the usage time.
When the tactile feedback was enabled, users interacted less with the touch screen, checked the display less, and turned off the screen more often.
