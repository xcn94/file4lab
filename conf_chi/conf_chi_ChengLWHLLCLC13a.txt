Automatic screen rotation improves viewing experience and usability of mobile devices, but current gravity-based approaches do not support postures such as lying on one side, and manual rotation switches require explicit user input.
Our insight is that users' grasps are consistent for each orientation, but significantly differ between different orientations.
Our prototype embeds a total of 32 light sensors along the four sides and the back of an iPod Touch, and uses support vector machine  to recognize grasps at 25Hz.
Results show that our grasp-based approach is promising, and our iRotate Grasp prototype could correctly rotate the screen 90.5% of the time when training and testing on different users.
91% of the respondents have experienced incorrect auto rotation, with 42% of the respondents encountering the problem several times a week.
Our previous work, iRotate , used face detection to track a user's intended viewing orientation, rotating the screen accordingly.
However, in many cases a user's face may not be clearly visible.
Another common solution to the screen rotation problem is providing an auto-rotation lock to fix the current screen orientation.
Several gestures have been proposed to temporarily override the auto-rotation setting .
However, the techniques still require explicit user input and require the user to learn new gestures as well.
To overcome the challenges of existing automatic screen rotation systems, we present iRotate Grasp, a system that automatically rotates a mobile device's screen based on a user's grasps.
We were inspired by several grasp-based user interface researches .
Our insight is that a user's grasp is consistent for a given viewing orientation, but is significantly different between screen orientations.
Figure 1 shows examples of grasps in different postures for both portrait and landscape modes.
Modern mobile devices, such as the iPhone, iPad, Android phones, and tablets, all support automatic screen rotation in order to improve the viewing experience and usability.
Current gravity-based approaches assume that users are standing or sitting upright while using the devices, which causes the screen to rotate incorrectly when users are in near horizontal postures, such as when lying down on one side.
Butler, A., Izadi, S., and Hodges, S. SideSight: multitouch interaction around small devices.
Proceedings of the 21st annual ACM symposium on User interface software and technology, ACM , 201-204.
LIBSVM: a library for support vector machines.
ACM Transactions on Intelligent Systems and Technology  2, 3 .
Forstall, S. and Blumenberg, C. Portrait-Landscape Rotation Heuristics for a Portable Multifunction Device.
Hinckley, K. and Song, H. Sensor synaesthesia: touch in motion, and motion in touch.
Hand grip pattern recognition for mobile user interfaces.
Screen Rotation Gestures on a Portable Multifunction Device.
Graspables: grasprecognition as a user interface.
Proceedings of the 27th international conference on Human factors in computing systems, ACM , 917-926.
Wimmer, R. and Boring, S. HandSense: discriminating different ways of grasping and holding a tangible user interface.
Proceedings of the 3rd International Conference on Tangible and Embedded Interaction, ACM , 359-362.
Wimmer, R. Grasp sensing for human-computer interaction.
Proceedings of the fifth international conference on Tangible, embedded, and embodied interaction, , 221-228.
In light of SideSight  that used light sensors to detect the presence and position of fingers around the device, we implemented a grasp sensing prototype by embedding 32 light sensors and an iPod Touch 4 inside an iPhone 4S case in order to explore how well grasp can be used to infer the correct screen orientation.
The light sensors are connected to the iPod Touch via an Arduino Pro Mini 328 that connects to iPod Touch's serial port.
The prototype, as shown in Figure 2, is similar in size to iPhone 4S and its weight is 150g, 10g heavier than iPhone 4S.
In order to train our system to recognize the mapping between grasps and orientations, we recruited 6 participants  and asked them to perform the following 54 conditions twice: * * * * grasping the device using left, right, and both hands  x and scroll, pinch-to-zoom and type  x while sitting and lying down on one side  x in portrait, landscape-left, and landscape-right orientations .
We sampled the light sensors at 30Hz for a total of 194400  grasp recordings, and we use LIBSVM , a support vector machines library, for the grasp orientation recognition.
The results of 6-fold, subject-independent cross validation, in which we train with 5 users' data and test on the 6th user, shows 90.5% accuracy in average .
