The touch screen interaction paradigm, currently dominant in mobile devices, begins to fail when very small systems are considered.
Specifically, "fat fingers", a term referring to the fact that users' extremities physically obstruct their view of screen content and feedback, become particularly problematic.
This paper presents a novel solution for this issue based on sensing touches to the perpendicular edges of a device featuring a front-mounted screen.
The use of such offset contact points ensures that both a user's fingers and the device screen remain clearly in view throughout a targeting operation.
The configuration also supports a range of novel interaction scenarios based on the touch, grip and grasp patterns it affords.
To explore the viability of this concept, this paper describes EdgeTouch, a small  hardware prototype instantiating this multi-touch functionality.
User studies characterizing targeting performance, typical user grasps and exploring input affordances are presented.
The results show that targets of 7.5-22.5 degrees in angular size are acquired in 1.25-1.75 seconds and with accuracy rates of 3%-18%, promising results considering the small form factor of the device.
Furthermore, grasps made with between two and five fingers are robustly identifiable.
Finally, we characterize the types of input users envisage performing with EdgeTouch, and report occurrence rates for key interactions such as taps, holds, strokes and multi-touch and compound input.
The paper concludes with a discussion of the interaction scenarios enabled by offset sensing.
Four views of the EdgeTouch prototype, showing:  the top mounted OLED screen;  the ring of 24 metallic capacitive sensors around the perpendicular edge of the device;  a three finger multi-touch grip with contact points highlighted with red cursors  and;  a target selection task with the target shown as a hollow red polygon and a green cursor marking the position of the right index finger.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
These include systems that support unobtrusive health monitoring , tangible gaming  or serve as fully-fledged mobile media or communication devices  or remotes that interface with such tools .
The benefits of miniaturization are reported to be substantial and include wearability , comfort, portability and aesthetics.
However, interaction with small devices presents novel challenges.
Although sophisticated display technology allows such systems to provide high-resolution and expressive output despite their diminutive dimensions, standard touch-screen interaction techniques do not scale-down so well .
A lack of screen real estate also means that effective solutions to this problem on larger devices, such as providing offset graphical cues  are ineffective on very small systems.
As it is fully offset, at 90 to the screen, touches to such a sensor system inherently avoid obscuring graphical content.
The remainder of this paper assesses the novelty and viability of this idea by positioning it against related work, by describing EdgeTouch , a prototype that realizes this functionality and by presenting a series of studies on this device and concept.
These studies assess basic targeting performance, capture the kinds of grips and grasps used to hold the device and explore how users conceive of interacting with this kind of functionality.
The paper closes with a discussion of the contributions and limitations of this work.
The problems of interaction with miniature or wearable computational devices have attracted considerable research attention.
Much work has focused on supporting target selection tasks without obscuring graphical contents.
The concept has been extended by a range of other authors to encompass the design of more sophisticated interaction techniques, such as those based on pinches , and to domains such as authentication .
Another approach has been to explore techniques that enable pointing input that is fully decoupled from the device itself.
Examples include Butler et al.
A final strand of work has looked at extending input from the device screen to an adjacent area using physical controllers that transmit and route a user's touches to particular optimized zones on the display .
This paper tackles the same problem space as this prior work, but aims to derive a solution capable of operating in a greater variety of situations.
These include typical use cases such as when a device is worn, as in the watch scenario shown in Figure 2 .
In such scenarios the back of the device is inaccessible and it is undesirable to rely on additional handheld equipment such as styli - arguably, the very point of such systems is that they do not require a user to hold equipment in their hands in order to interact.
A second motivating scenario comes in the form of small computers that can be fully held in the hand.
The form factor of such devices might resemble a coin, or a loosely attached wearable object such as a pendant.
We argue that touches to the edge of such devices, as shown in Figure 2 , represent a natural, confortable way to interact without occluding content on a front-mounted screen.
Other work on small or wearable devices has looked at how physical structures around the edges of screens can support gestural and targeting tasks.
For example, both Blasko and Feiner  and Ashbrook et al.
These studies differ from the current investigation in that they focus on how raised ridges can support interaction with a touchscreen, whereas the current study looks at the unexplored potential of offset touches to the sides, rather than the front-facing extremities, of a device.
Edge based interaction has also been recently explored in the context of pressure based input.
For example, Spelmezan et al.
The work in this paper complements this promising input modality by investigating a relatively high-resolution position sensor and general pointing operations rather than pressure input.
Scholars have also explored the use of touch sensors mounted around a mobile device to support context-sensing interactions.
More recently, this notion has being extended, refined and applied to current mobile device form factors.
Finally, in a radically different approach to the capacitive sensing paradigm, Sato et al.
The work in this paper differs from this literature in that it uses of an array of capacitive sensors to detect highly localized touches in directed targeting tasks and other intentional  interaction scenarios.
In summary, much research attention has been directed to developing novel interaction techniques for small devices.
One key theme has been on extending touch-sensing capabilities to regions of the device other than the screen.
This paper builds on this existing body of work by examining a novel configuration: a high-resolution position sensor situated around the edge of a device featuring a front mounted screen.
It argues this arrangement is highly suitable for very small handheld or wearable devices and sets out to explore the validity of the claim by describing a prototype that realizes this functionality and a series of user studies that characterize how users interact with it.
They took the form of simple eight mm M3 bolts  secured to the plastic shell by nuts and wired directly to the jumper points on the two MPR121 boards.
The bolts were screwed as flush as possible to the surface of the disc and, in order to provide a smoother texture, a layer of electrical tape was wound around the rim of the device.
The MPR121 sensor boards communicate using the I2C protocol.
Figure 1  shows the internals of this hardware setup; the ultimate dimensions of the device were selected to minimize size while robustly enclosing all elements of the sensing and display hardware.
A remotely situated Arduino Mega 1280 interfaced with the prototype and communicated, via a second RS232 serial link, to a host PC as and when required.
In order to minimize latency, most computation took place on the Arduino and the link to the PC was used primarily to log data and issue high-level commands.
The Arduino polled the sensor boards 100 times a second and distributed commands to screen sporadically and in response to feedback requirements and application logic.
The EdgeTouch prototype is shown in Figure 1.
It is a 3D printed hollow disc with a removable lid, 1.8 cm in height, 6 cm in diameter and with a resultant circumference of 18.85 cm.
A 2.7 cm square full color OLED screen with a resolution of 128 by 128 pixels  was secured to the center of the lid using M3 bolts through integrated fixtures; it sat proud from the surface by 7 mm.
The screen features an on-board graphics processor that can be controlled remotely via commands delivered over an RS232 serial link and is capable of rendering limited amounts of text and simple graphical primitives to the screen in real time.
Although the screen is square, a disc shaped housing was chosen to ensure an unambiguous oneto-one mapping between touches to the edge of the unit and positions on a circular region around the center of the screen.
This ensured the relationship between touches to the edge of the device and the corresponding position on the front-mounted screen was as clear as possible.
Capacitive sensing was implemented with two Sparkfun breakout boards featuring the MPR121 capacitive sensing microprocessor from Freescale.
These were fully enclosed within the prototype.
Data from EdgeTouch's 24 individual sensors were interpolated to create 48 uniquely touchable locations - if two adjacent sensors were simultaneously active, a touch was recorded at the mid-point, leading to a uniquely identifiable location every 7.5 degrees .
Essentially, small blocks of adjacently selected sensor locations  were resolved to a single central touch while larger blocks  were treated as two touches and still larger blocks simply ignored.
All detected touches were marked as small round brightly colored cursors drawn on the edge of a 2.7 cm diameter circle centered at the midpoint of the screen.
The rim of this circle was always 1.65 cm distant from touches made on the rim of the EdgeTouch device; see Figure 1  and  for examples of this feedback.
Finally, in order to provide a more consistent experience during targeting operations that involved movement on the device surface, changing patterns of sensor activation were processed and cursors that animated smoothly to match such dynamic, persistent touches were presented to users.
This simple tracking process stored current contact points as cursor locations and, in subsequent sensor readings, associated adjacent sensor activations with these cursors and smoothly animated their on-screen representations towards these new positions.
They were two stages and goals to this study.
In the first stage the objective was to investigate targeting performance - whether touches to the side of the EdgeTouch device would allow users to rapidly and reliably select targets displayed on the front mounted screen.
In the second stage, conducted immediately after the completion of the first, the goal was to characterize the basic set of grips and grasps used to hold the device.
In contrast to the single-touch input in the first stage, the second stage of the study was primarily descriptive and looked at multiple simultaneous contact points.
These studies are described below.
The same twelve participants  completed both stages of the study.
All were students or staff at an affiliated research institute and none were compensated.
Two were left-handed and, on ratings out of 5, all reported they were experienced with computers , smartphones  and touch-screens .
None had prior exposure to the EdgeTouch prototype.
This investigated targeting performance using the EdgeTouch prototype.
To achieve this aim meaningfully, two aspects of the targets were systematically varied: their size and polar position.
Three size conditions were considered: small, medium and large respectively spanning one, two and three sensor locations .
These were presented in a fully balanced repeated measures design - all participants completed all conditions in one of six possible orders.
Furthermore, for each condition targets centered on every one of the 48 uniquely detectable locations was presented in a random order.
There were three runs through this set of locations for each size condition, the first of which was discarded as practice.
This led to an experimental data set composed of 288 selections  for each participant.
Completing this stage of the study typically took 45 minutes.
Figure 1  shows a trial in progress - a participant has correctly touched the device over a target with their right index finger and simply needs release the touch to successfully complete the trial.
All instructions in this stage of the study were shown on the EdgeTouch device.
In order to minimize confounding behaviors that might emerge from different postures and grips, participants were instructed to start each trial with the device  in both hands and held between their thumb and middle fingers, as shown in Figure 1 .
Each trial then began by requesting that the participant tap the side of the device.
After releasing this touch, a fixation spot was displayed for 500ms, followed by a target in the form of a 10 pixel deep red polygon occupying an appropriate position and portion of the polar space.
Participants then had to select this target.
This was achieved by touching the edge of the device, at which point a cursor was displayed, either green  or red .
Movements across the surface of the device adjusted the cursor position and a selection event was recorded only when the finger was released.
The targeting experiment had multiple objectives.
First and foremost it sought to capture data expressing the viability of touching the rim of a handheld device for target selection tasks.
Secondly, it aimed to explore this input space in more depth by recording variations in performance caused by changes to target size.
A third goal was to explore the impact of the polar position of targets.
Consequently, measures used in the study included touch time, the point from the presentation of a target until an initial touch of the sensors and hold time, the time until the sensors were released.
These time data were calculated only for correct trials.
The mean timing and outcome data from the three target size conditions are presented in Figures 3 and 4.
In order to reduce the data from the 48 target locations to a manageable size for analysis, it was clustered into eight angular segments, each aggregating trials from six targets centered on a 45-degree region aligned along one of the eight cardinal or ordinal directions.
This summary data for both time measures and trial outcomes is shown in the five radar diagrams in Figure 5.
Data for taps are not shown, as these are simply the complement of the slide-on data.
All data were analyzed using two-way repeated measures ANOVAs followed by post-hoc t-tests incorporating Bonferroni confidence interval corrections.
In cases when the data violated the sphericity assumption, GreenhouseGeisser corrections were used.
Effect sizes are reported in the form of partial eta squared , a figure that represents the proportion of otherwise unaccounted for variance that can be attributed to the effects of each variable.
The error data for the angular position variable was too sparse and irregularly distributed to support formal analysis - several conditions resulted in zero errors, while others exhibited high error rates indicative of outlier performance such as, in one condition, a single participant recording eight errors on the same target .
Consequently, the angular position variable was collapsed for the miss and slip-off error measures and these data were analyzed with one-way repeated measures ANOVAs.
In terms of the time data, no significant interactions were uncovered in either metric .
The pair-wise tests revealed all differences to be significant for hold time , but that only the small condition differed from the medium and large conditions in terms of touch time .
However, the post-hoc tests showed few differences between angular locations in terms of hold time - only the top-right segment  showed faster performance than the top and left segments .
Table 1 shows the significant pairwise comparisons from the touch time data.
These findings can be summarized with the observation that targets towards the top of the device, close to participants' index fingers, led to more rapid targeting performance.
Total task completion times and accuracy rates in the large condition, which enables 16 non-overlapping polar targets each sized at 22.5, were fast  and accurate .
These figures speak for themselves and also compare favorably to data reporting in prior studies of targeting on small devices.
For example, in Ashbrook et al.
These rates are substantially worse than those observed in the large condition with EdgeTouch - a minimum of three times greater.
Furthermore, as Ashbrook et al.
With shallow targets situated only around the rim of the device, performance dropped dramatically - the smallest targets considered, approximately three mm in width, led to projected error rates of 62%.
EdgeTouch, by relying on input around its edge rather than its front surface, avoids this problem and performance does not depend on the depth of on-screen targets.
Indeed, the targets used in the current study were only 2.1mm  deep, freeing up much of the limited device screen space for other content.
This evidence suggests that that the offset sensing paradigm used in EdgeTouch offers advantages over radial targets on the front of a touch screen: it improves considerably on accuracy and frees up screen real estate.
It is also useful to contrast performance in the current study with Baudisch and Chu's  work examining item selection via touches to the back of a 2.4-inch  device featuring 12 targets.
In this work, target selection times and error rates are reported to be between one and three seconds and five and 30 percent, depending on input style and target size.
Looking at the optimal conditions from these data, we can conclude these are broadly comparable to those from EdgeTouch.
This is an encouraging result that suggests that both back and edge make equally viable surfaces for pointing input on small-screen devices.
Furthermore the edge of a device is available in situations, such as those involving a worn object like a watch, when the back is not.
Finally, Harrison and Hudson  describe polar targeting performance in free space in a system that senses the angle between a screen and a magnet mounted on a user's finger.
With 16 targets, task completion time is approximately two seconds, 60% greater than that recorded with EdgeTouch while error rates remain broadly comparable at five percent.
This suggests that the physical contact required to select radial targets in EdgeTouch may facilitate more rapid use than the free floating near-device interaction space investigated by Harrison and Hudson.
Top left chart shows mean hold times for 48 different sensor locations in three conditions in the user study.
Top right chart shows mean hold times in the small condition by angular segment and parity - whether each location is addressed by contact with one or two sensors.
Bottom images show typical fingers touches to side and top of device, with red highlighting differences in contact area  that can explain this interaction.
Moving beyond these comparisons, it is unsurprising to note that as target sizes decreased, performance dropped.
In the medium condition targets of 15 led to modest decreases in performance, suggesting that arrangements of 24 unique targets may well be viable.
In the small condition, with 48 separate 7.5 targets, performance lowered further to 1.75 seconds and 18% errors, figures that still compare favorably to those in the literature - both Ashbrook et al.
This suggests that EdgeTouch's offset pointing technique scales better than these prior approaches.
Performance also varied significantly according to polar position - given the hand pose participants were asked to adopt, areas within easy reach of the index fingers and thumbs tended to result in the fastest and most error free performance.
This highlights a close relationship between hand posture and performance that needs to be taken into account during the physical design of any system implementing the kind of offset targeting functionality discussed in this paper.
One simple design strategy would be to adjust the angular size of targets according to expected hand pose - basically to deploy small targets in easy to reach locations and use larger targets elsewhere.
Finally, in an exploratory analysis to better understand the rise in hold times between conditions, we examined this data on a target-by-target basis .
It shows a prominent zig-zag in the small condition.
These include that high accuracy can be maintained with small visual targets, that edge interaction is available in scenarios involving worn devices and finally that the physical contact implied by touching the device may lead to higher performance that in near-device interaction scenarios.
We also determined that limitations of the sensor arrangement in the current hardware prototype affected the results.
However, these helped highlight important aspects of the physical act of touching a curved edge surface and mainly affected the smallest targets studied.
The main conclusions of this study are drawn from the larger targets that were immune to such issues and maintain their validity - showing that edge interaction is a rapid, accurate and effective way to conduct targeting tasks.
Grasp Study: Experimental Design and Procedure Figure 7.
Grasps of the EdgeTouch prototype with two , three , four  and five  digits of one hand.
Fingers are shown on the outside of the circles.
Positions are plotted from mean angular intervals between finger touches .
These results indicate that it was quicker to select targets involving a single sensor in the left and right regions of the device while more rapid selections of targets in the top and bottom regions occurred when contact with two interpolated sensors was required.
A candidate explanation for this effect is based on the size of contact area involved in typical touches to the device: left and right touches likely involved the relatively small finger tip while top and bottom touches involved the larger finger or thumb pad, as illustrated in Figure 6 .
The different sizes of these contact areas facilitated activation of different numbers of EdgeTouch sensors - with a small finger tip contact region, selecting an interpolated twosensor target was challenging, while the inverse was true for larger finger pad touches.
While this effect is largely due to the resolution limitations of the current sensor system, these results also demonstrate the importance of considering finger and hand posture in the design of curved touch input surfaces.
Different positions result in widely different reach and contact profiles, variations that need be considered and incorporated into the design of interfaces.
Overall, the results of the study endorse the idea that offset input on the sides of a device is well suited to target selection tasks on very small computers.
The goal of this stage of the study was to explore the how the EdgeTouch prototype is grasped and the viability of detecting such grasps.
This activity was descriptive rather than inferential - it sought to capture data regarding the grips and grasps participants performed rather than compare between any particular conditions.
Accordingly, it was simply structured and composed of three repeating sets of four trials, the first set of which was considered practice and not analyzed.
In each set, participants picked up the EdgeTouch prototype by its rim with two, three, four and finally all five digits on their dominant hand.
When it was held comfortably, they pressed a key on an adjacent laptop with their non-dominant hand.
This logged the grasp information and they then put the device back down.
Within these constraints, they were instructed to hold the device in any way they liked - including grasps that obscured the device screen.
To facilitate this process, all instructions for this stage of the study were presented on a laptop computer in front of the participants.
In total 96 grasps were captured, eight from each participant.
Participants were allowed to rest before starting this stage of the study.
EdgeTouch records relative touch positions around its rim.
This data was aggregated as follows.
Firstly, eight trials  in which the requested number of fingers were not detected were discarded.
These cases fell into two error categories.
Either the user made contact with a large number of adjacent sensors, making it impossible to determine the number and placement of a user's digits  or there was one too few digits recorded .
This latter category is likely due to the discrepancy between the physical depth of the EdgeTouch device  compared with the depth of the sensor electrodes  - basically one finger failed to reach the sensors.
As these errors all occurred in trials when participants needed to grip the device with all five digits, the limited reach of users' baby  fingers is the likely cause of this effect.
These data are plotted in Figure 7, arranged such that the largest two distances originate at the base of each diagram.
This figure suggests that the noticeably isolated bottommost touch represents the thumb and detecting this digit would be trivial.
Indeed, one-way ANOVAs and posthoc t-tests on the distances generated in the three, four and five digit conditions show the largest two inter-touch distances are always greater than the others.
In sum, the results of this experiment suggest that the EdgeTouch is suitable for interaction techniques based on detecting grips - study participants were able to produce simple, highly distinctive grasps with a high level of accuracy.
It was also possible to extract higher-level features, such as determining which contact point represents the thumb with a trivial analysis.
These represent valuable interaction primitives by themselves and, in the future, we suggest that combining grip detection with inertial sensing could disambiguate further information about hand pose.
For example, specific device angles with respect to gravity could be combined with the current system to infer which hand is holding a device, or to identify index fingers in a multi-finger grasp, increasing the richness of interaction space available for devices featuring offset sensing.
Ten participants  completed this experiment.
All were either students or recent graduates and they were compensated with approx.
On average, they indicated they had been smartphone users for more than 3 years and had experience with  four small digital devices such as mp3 players.
None had any prior exposure to the EdgeTouch prototype.
To provide a context for the tasks performed in this experiment, an interactive image-based prototype of an interface to a personal media player was implemented.
It featured three types of menu interface  as well as volume, radio and setting control screens.
Example on-device screen shots can be seen in Figure 8.
Commands were based on those in existing media players and, in total, 14 tasks were modeled, encompassing menu navigation , media playback , radio use  and toggling settings .
In line with similar studies in prior work , participants' task was to report on two types of input they would make to achieve the operations naturally.
To create a richer data set they were asked to consider both one-handed and two-handed operation of the device.
After defining gestures they were also rated their preference for each of their gestures on a five-point scale.
In total each participant defined 112 gestures  in an hour-long session.
Encouraged by these results, we ran a follow-up study to elicit the kinds of gestures and interactions that users naively perform on the EdgeTouch hardware.
In order to achieve this objective, we deployed a variation on Lee et al's  methodology to capture and describe interactions with imaginary future devices.
Essentially, participants were asked to perform a range of activities, such as menu navigation or setting adjustment, on a simulated media player interface shown on the EdgeTouch prototype.
Their task was to devise interactions they would use to achieve these activities naturally and effectively.
At the start of the study participants completed a brief demographics questionnaire, then watched a 60 second video showing basic EdgeTouch gestures like tapping, double tapping or holding a touch and swiping  over the device's edge.
Movements of the device such as traditional tilting  and spinning clockwise or anti-clockwise were also shown.
Participants were free to ask questions and the experimenter also gave a brief inperson introduction to the device and its features.
This took the form of a wizard of Oz simulation in which the experimenter remotely manipulated on-device content in accordance with the input the users stated they were making.
Users were given freedom to select and define input and gestures composed of one or more touches and movements on the device edge and any sequence of device rotations.
There were no instructions or restrictions regarding grasp posture and participants verbally reported the input techniques they were devising.
Finally, overall, participants reported they were satisfied with their gestures - ratings of 3.8 out of five.
In terms of the different interaction tasks presented to participants, these can be broadly categorized as requiring translational movements , rotations  or issuing commands .
Where possible, participants focused on generating spatial mappings between the tasks and their inputs - they made taps to appropriate edges to move in particular directions or issued strokes in the desired direction of movement.
Device motions, such as tilting, were also used to achieve these operations and rotating the device around its screen was the most popular mechanism to navigate in the pie menu.
To issue commands, when there was a clear correspondence between displayed items and the device edge , participants directly touched areas relating to desired targets.
Synthesizing this data in order to inform design, we conclude that users relied heavily on relating edge-based touches to spatial aspects of desired on-screen transitions.
They particularly focused on taps and touches with individual fingers and grips and grasps with multiple fingers.
In terms of such holds, the number of digits  was frequently intended to signify input - e.g.
Finally, participants often combined inputs, such as holding and stroking.
This analysis provides concrete recommendations for the design of offset sensing interaction techniques.
Interaction techniques and scenarios for offset sensing.
Top: a roll gesture, in which a device is spun clockwise between the fingers .
Bottom left: a menu and sub-menu accessed by touches to either side of a device.
Bottom right: offset sensing in a camera-based scenario with sensors mounted around the rim of the lens.
In total 1120 interaction techniques were recorded over the entire study.
This descriptive study provided a detailed summary of the diverse ways participants sought to interact with EdgeTouch.
Physical contact with the device edge was integral to the vast majority of inputs - only 18% of inputs involved no contact and were based solely on device movements such as tilting.
In contrast, 55% of gestures involved a single finger, 17% two fingers and, respectively, 5%, 3% and 2% were made with three, four and five fingers - a total of 27% of inputs were multi-touch.
Participants also favored their dominant hands and use of their thumb and index fingers.
Specifically, the right index finger accounted for 43% of inputs while the right thumb  and the left index finger  were also frequently employed and the right middle  and left thumb  made up much of the remaining usage.
The input types selected were diverse - the most common input types were taps  and strokes  with a specific finger followed by taps at a specific device location  and one finger holds  and double-taps .
The remaining 45% of inputs were widely distributed over other action types.
In sum, the results of the studies described in this paper demonstrate that offset sensing enables rapid, accurate targeting performance.
It also offers a range of qualitative advantages over prior work: it frees up screen real estate, does not require additional equipment to support pointing and is accessible in situations when a device is worn against the skin.
In addition, compared to existing physical controls  it offers the advantages of spatial multi-touch input - users can touch input areas that clearly and unambiguously map to dynamic screen content.
An examination of device grasp poses also suggests key properties of this modality, such as basic digit identification, can be readily determined.
Finally, a userelicitation study provided insights into the ways naive users conceive of interacting with offset sensing systems.
Drawing together these results, we argue that offset sensing has the potential to enable a wide range of novel interaction techniques and be applicable to a wide range of devices and scenarios.
To support this point, we provide examples inspired by the study results at three levels: interaction technique, interface and application scenario.
As it is not reliant on inertial sensing, this technique may be relatively high precision and immune to noise from other bodily activities and movements .
Finally, in terms of application, Figure 9  highlights how edge sensing functionality could be integrated into larger and more advanced devices such as a camera.
Inspired by the Lytro camera , a small device with a capacitive slider built into the edge just above the LCD viewfinder that controls zoom level, this example highlights the potential of offset touches to support interaction with a wide range of digital devices and tools.
Moving beyond such scenarios to practical limitations and avenues for future work, one key weakness of the current system is its resolution.
Although cheap and effective, the results of targeting study suggest that a higher resolution system would improve performance.
Further avenues for development include adding pressure-sensors to combine the interaction techniques discussed in this paper with those of Spelmezan et al.
Additional studies to characterize gesturing and stroking performance on edgemounted sensors would also complement the data reported in this paper.
Another key limitation relates to the preliminary nature of our current efforts to design meaningful interfaces.
To address this issue, we plan to develop and extend the UI concepts from this paper into prototypes combining context inference  with explicit interaction.
Ultimately, we believe that the offset sensing proposed in this paper will help enable rich expressive interactions on very small mobile and wearable computers.
