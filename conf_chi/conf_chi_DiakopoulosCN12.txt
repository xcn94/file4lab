The contributions of this work fall along two lines:  the development of novel filters and aggregate cues driven by journalism practice and information production in events, and  the evaluation of those cues with domain experts in a way that allows us to reason about current practices and the potential to augment them.
In particular, to enable finding and assessing information sources in social media, we develop a number of filtering and information cues.
A high precision eyewitness detector is developed using a principled dictionary approach.
A method to aggregate location information from a potential source's friend network is used as a cue towards location knowledge.
Finally, network connectivity and aggregate content cues are computed in order to assist journalists trying to assess the verity of sources.
Social media is already a fixture for reporting for many journalists, especially around breaking news events where non-professionals may already be on the scene to share an eyewitness report, photo, or video of the event.
At the same time, the huge amount of content posted in conjunction with such events serves as a challenge to finding interesting and trustworthy sources in the din of the stream.
In this paper we develop and investigate new methods for filtering and assessing the verity of sources found through social media by journalists.
We take a human centered design approach to developing a system, SRSR , informed by journalistic practices and knowledge of information production in events.
We then used the system, together with a realistic reporting scenario, to evaluate the filtering and visual cue features that we developed.
Our evaluation offers insights into social media information sourcing practices and challenges, and highlights the role technology can play in the solution.
Social media contains a wealth of information and value for journalists and has already proven to have a large impact on news reporting.
Whether it's terrorist attacks in Mumbai , a plane crash landing on the Hudson River , or first-hand reports of a demonstration in Cairo, the use of social media in news reporting is no longer uncommon.
Social media offers an opportunity for journalists to reach beyond their typical source networks of elite or otherwise affiliated sources , as well as to build a personal brand and following and to disseminate information to their network .
In this work we address the opportunity social media offers for journalists to find and assess information sources.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We discuss relevant research on journalistic tools, issues of source verification, and variable sourcing needs such as authorities or eyewitnesses in different reporting contexts.
Journalism studies scholars have suggested a need for better tools to help tame the cacophony of social media .
Indeed, efforts in the visual analytics  and HCI communities  have already made forays into social media tools with journalistic applications.
For instance, the Vox Civitas system  organized and visualized tweets around broadcast news events and showed that journalists were able to find interesting story angles based on both individual tweets as well as aggregate cues of volume or sentiment response.
Results of the Vox Civitas study also suggested considering ways in which sources in social media might be better characterized to help journalists assess their verity, which we address in this work.
Indeed veracity has been identified in prior work as a key concern for gathering information on social media .
Rumors and misinformation can be rapidly disseminated when corroboration or cues for trust are missing.
Recent computational work has considered automatically detecting misinformation  and assessing information credibility , as well as more deeply understanding credibility cues on Twitter .
In this work we focus on supplying a range of informational cues that can enable a professional journalist to come to their own determination of information quality.
Identifying credible sources to help tell a story in news reporting can depend on different factors such as time demands , type of reporting and news event , and proximity .
For instance, some stories may call for identifying experts who can speak authoritatively to a topic or issue.
Such sources are known as cognitive authorities , and in some cases may include the journalist themselves if they have a particular credential or have built an expertise on a particular topic over time .
There is a growing body of research and products that seek to address the need for finding topical, cognitive authorities by identifying such authorities in social media.
For instance, Weng , Pal and Counts , as well as online tools such as the Klout  and Sulia services  all seek to find or rank authoritative authors on various topics.
Identifying such sources is not a focus of our work here, although we do integrate the Klout score to provide a basic informational cue about the magnitude and topics of authority for sources presented in our interface.
Eyewitnesses do not posses any special authority aside from a claim to have witnessed some event first hand and an ability to report on an event using their own perceptions of the world.
As Zelizer points out, news organizations often use eyewitnesses to add credibility to reports by virtue of the correspondent's on-site proximity to the event .
Witnessing and reporting on what the journalist had witnessed have long been seen as quintessential acts of journalism .
Social media provides a platform where once passive witnesses  can become active and share their eyewitness testimony with the world, including with journalists who may choose to amplify their report.
To date, we are not aware of research that helps to automatically identify eyewitnesses in social media.
In this work, our classifiers and filters are designed, in part, to help professional journalists more effectively find eyewitnesses from social media content.
As designers, we first considered the related work and divergently explored potential cues based on our intuition and common practices in journalism.
In the process, we interviewed four professional reporters to understand how they were using social media in their daily practices.
We pursued a scenario based design approach  to consider the variety of contexts in which the tool might be used as well as to shape the feature set.
Using this feedback we iterated on the design and implemented the core features in a fully-functional prototype.
Early in the design process we arranged interviews with four reporters, fluent with social media, from an online news outlet, newspaper, and radio station.
The interviews were informal  and uncompensated.
We approached the interviews with several questions in mind, aiming to understand how journalists use social media, including what tools they were using to find or assess sources, why they follow or contact sources on social media, and what kinds of events they use social media to cover.
The interviews helped solidify the idea that identifying eyewitnesses to events was a primary use-case for journalists using social media.
Journalists often search for personal observations of people with real first-hand information, or individuals who may be directly affected by events.
Journalists look for people with a story relating to the event, or that are in some way involved with or impacted by the event.
Other use cases of social media included finding people to "humanize" or exemplify some trend that is being reported, or for finding expert sources.
Additionally, the interviewees mentioned a number of useful informational features about social media sources, including measures of retweets, conversational engagement, and linking behavior.
The importance of source location was mentioned several times, especially in the context of breaking news.
These dimensions inform our development of scenarios and also provide a context for our evaluation and results.
The dimensions include the type of news content, the time profile of reporting on an event, and the proximity and geographic extent of the event.
Reporting around hard "enterprise" stories such as investigative projects or in-depth reports has been shown to exhibit different patterns of sourcing such as more source diversity .
Reporting on soft news, or longer-term enterprise stories often has less time pressure and allows the journalist more time in identifying the best sources of information for a story.
We thus expect that the time-pressure associated with a reporting scenario will affect the way sources are found and evaluated with a tool such as SRSR.
Other factors that may also affect the time pressure of reporting include: if it is a breaking news story , if there is a time lag to the event , if the report is retrospective , or if reporting an ongoing story that unfolds over a longer time-frame .
Research in journalism also distinguishes two additional dimensions that are important to the design of SRSR.
These are whether the news coverage is routine or non-routine, and whether the event is proximate or non-proximate to the reporter.
Routine reports, such as around news releases or public meetings tend to have higher proportions of affiliated sources than non-routine reports.
For less proximate events  journalists typically have a less diverse pool of sources developed, thus relying more on official or affiliated sources .
Another dimension of proximity is the geographically relevant extent of the event: such as if the event is purely local, or has national or international interest.
An international story may also have language or cultural challenges associated with obtaining sources.
Based on these design dimensions we wrote several detailed scenarios that explored different combinations of event and reporting characteristics as a way to expose additional design requirements .
For instance, we considered combinations such as , , and .
Though we could not explore all combinations of characteristics, this exercise did expose the need to consider flexibility in the design of SRSR so as to meet diverse information needs of journalists in these different situations.
The ideal tool would be flexible along the appropriate dimensions so as to allow it to be used for reporting on various types of events with various information needs.
This section describes the interface and interaction design of SRSR, including informational cues we developed based on the background interviews and analysis of event types and scenarios described in the previous section.
The SRSR interface  combines abilities to find  and assess users that can serve as potential sources of information.
Finding sources could be a significant problem given the magnitude of  content posted for each event.
Assessment and verification of information and sources is also important to journalists, and could be a time consuming process.
We thus strove to develop and include visual and informational cues that could help journalists both find sources, as well as  assess their credibility.
The interface is presented as a scrolling list of Twitter users who are potential sources for a given event.
Each row captures information for a single user, with the detailed SRSR profile on the left, and the user's content  on the right.
The SRSR profile includes information about the user that is retrieved, aggregated, or computed from Twitter as detailed next.
The SRSR profile incorporates many of the standard elements retrieved from Twitter such as the full user name, screen name, user image, age of the account, number of followers and followees, as well as the user-provided description and URL.
In addition to the basic Twitter information, the SRSR profile includes information about the user aggregated or derived from Twitter data.
These items include the standardized location of the user , which is prominently displayed as a cue .
The profile also includes aggregate information such as the number of times any of the user's event-related tweets were retweeted.
A small network "sketch" shows other users active in the event that are following this user , a potential cue to how involved the user is in the dissemination of information related to the event .
Several of the most important cues shown in the interface are computed through various methods such as automatic classifiers .
First, SRSR indicates, through an icon and a label, whether the source has been identified as a journalist/blogger, organization, or "ordinary" person .
Originally inspired by the categories of information brokers , the categories we use have been further refined to represent different types of information that may be available from different types of users .
Second, SRSR shows a set of pie charts indicating the top three locations where the user's Twitter contacts are located .
Finally, to help identify eyewitnesses, a red eyewitness icon is shown next to the username for profiles that SRSR identifies as probable eyewitnesses .
The evaluation examines the value of these different cues in finding and assessing sources.
In addition to presenting these multilple cues, SRSR allows journalists to filter, sort, and search the set of users in different ways to effectively find interesting sources of information.
A row of filter tabs is present above the source list .
One set of filters acts on the source, while another set is for filtering based on content from the source.
In the source column the user can filter on user type, or by whether the user has been identified as an eyewitness.
The content column allows filtering by excluding RTs , or by tweets that contain links to images or videos.
The sorting options in SRSR allow the user to re-rank sources based on various criteria including: latest content timestamp , number of times the user has been retweeted, the number of tweets posted by the user for the event, number of links shared, number of the user's followers who are also tweeting about the event, the eyewitness-ness score , and finally, the Klout "influence" score.
Our evaluation considers how these filtering and sorting options might play a role in finding sources.
Beyond the information shown in Figure 1, SRSR interactions can expose additional details about a source.
Hovering over the general profile area exposes three actions that the user can take on the source: favorite, follow, and contact .
In order to render many of the information cues in the interface we computed a number of features based on each source's profile data.
We provide a description of the various features developed for SRSR in three main parts.
The first part describes the classification of the sources into user archetypes and eyewitnesses.
In the second part we discuss the features characterizing the content shared by the sources.
Finally in the third part we discuss a location characterization of the social graph of a source.
The first classifier categorizes users into types .
The second classifier identifies users that are likely eyewitnesses to the event in question.
We developed and implemented a classifier for three user archetypes: organizations, journalists/bloggers, and ordinary people.
We chose these categories because they delineate different information sharing behaviors with respect to an event.
More specifically, the organizations category refers to Twitter accounts that are associated with an organization in the real world, catering to some social, political or business goal .
Journalists/bloggers includes individuals who are associated with a mass media enterprise/news organization or who maintain a blog that reflects professional interests.
Finally, ordinary individuals are other people on Twitter for a variety of reasons: posting updates on their day-to-day life, expanding professional opportunities, maintaining contacts with their friends, or to discover relevant/useful content relating to their interests.
The classification methodology and details about the performance of this classifier are discussed in .
The k-Nearest Neighbor based classifier performs quite well, with balanced accuracy of over 90% for the three main categories .
This binary classifier was geared towards associating "eyewitness"  labels to different posts on an event.
First, our system marks each post from an event as containing "eyewitness words".
A post is marked if it contains at least one eyewitness word from the categories above .
This mechanism yields a labeled set of event posts that contain eyewitness words.
Finally, a user is considered an eyewitness if they made at least one post that contains eyewitness words.
Eyewitnesses can also be ranked by their eyewitness-ness based on the number of posts they shared containing eyewitness words.
This simple classifier proved effective  on a test dataset of 1000 posts.
To test the classifier's performance, we created a ground truth dataset of 1000 posts collected from four diverse events .
We used Amazon Mechanical Turk  to label each post as "eyewitness" or not.
We only allowed "Master" workers  to complete our task.
We iteratively developed our coding instructions by comparing Turk codes for a subsample  of posts that were labeled by one of the researchers.
We collected labels from three independent workers for each post and took the majority vote label as the final label for the post.
An accuracy of 89% was reached for the Turk coders with respect to the researcher labeled posts.
We then used Turk to label the rest of the sample.
We evaluated the classifier output for individual posts against the ground truth labels generated by the Turkers.
The classifier precision was 0.89  while the recall was 0.32 .
In the context of SRSR, since a journalist is exploring the sources via a visual interface, the high precision yielded by our algorithm is a desirable finding as it means that few false positives will be presented in the interface.
While this simple classifier is only a first step toward the challenge of classifying eyewitnesses, it provided us with enough precision to demonstrate some utility in the user study.
This classifier aims to identify users that were eyewitnesses to the event.
These users can be valuable information sources in terms of gathering honest and first-hand information or experience.
Eyewitnesses often serve as the locus of information on an event, which may then be propagated by other social media users.
For the purposes of this research we define eyewitnesses as "people who see, hear, or know by personal experience and perception".
As a first attempt toward trying to identify eyewitnesses to an event we developed a simple eyewitness detector using a dictionary-based technique to analyze the content of the posts.
We use the text analysis dictionary LIWC .
LIWC provides a rich set of words categorized across 70 different language dimensions .
We hypothesize that categories that are likely to be reflective of a source's space/time or experiential authority of the event are indicative of eyewitness-ness.
These categories include the LIWC categories `percept', `see', `hear', `feel', reflective of a user's presence during the event in space/time.
We also include `insight' and `certainty' LIWC categories, reflective of the source's experience.
To provide cues on the user's range of interests, we extract named entities for every user based on their past posts on Twitter.
These named entities capture the interest domains and potential expertise of the source.
We derive the named entities using Open Calais , which analyzes the content of the posts, as well as any URLs present in them, to extract entities for people, places, and organizations  and their types .
To reflect the type of information shared by the user, SRSR aggregates user's linking behavior, including the number of links that they share , as well as the number of image and video links shared.
To this end, we label the URLs that appear in the content as "image", "video" or "article".
Then, we associate each URL with a label based on a hand-crafted dictionary of domain names and the likely media featured on those domains.
For example, in our dictionary, "youtube.com" was associated with "video", and `flickr.com" with "image".
Note that there could be domains that feature more than one type of media; e.g., nytimes.com content typically includes articles but can also include images and video; twitpic.com hosts primarily images, however, at times, videos too.
In such cases, we use the most frequent media type for the domain  as the corresponding label.
Using this dictionary we were able to label 89% of all the posts bearing a URL with the remaining 11% left unlabeled.
Generating place names from the location field text data involves a series of steps.
We compare each complete freetext location string against a Gazetteer constructed from Wikipedia1 and U.S. Census data2.
If there is a  match, we obtain the place name  representation for the user's location string.
In cases where the location string did not match any city, state, or country , we try to resolve the location string using the MapQuest Geocoding API, which responds with a place name match or an "unknown" response.
In order to compute the accuracy of the place name extraction process, we randomly sampled 200 users from each of three events, for the purpose of specificity, and another 200 users from the public timeline, for the purpose of generality.
We then manually compared the standardized location and the actual location string shared on Twitter, for each user in our sample.
We found that blank or non-geographically valid location strings are approximately 25% of our data, which concurs with prior work .
Some users  had their location fields automatically set to the latitude/longitude of their location .
Excluding the cases where blank or non-geographically valid location strings were shared by users, the accuracy of our place name process was in the range of 86-89%.
Errors are due to places that exist in more than one state or country .
Our process prioritizes locations in the U.S. over others.
Note that other ways to derive approximate location information for a Twitter user exist  and can augment our method.
However, we opted for a straightforward approach that would not require the system to collect content from the user and all their friends, and would still provide reasonable results for aggregation.
To judge the likelihood of users to post immediate content, and their potential proximity to the event, we provide a cue that captures whether the user is posting content from a mobile device.
To label posts as mobile or non-mobile, we compiled a dictionary of posting applications typically used by Twitter users.
In the dictionary each posting tool 
We use this dictionary to associate "mobile" or "non-mobile" labels to each post related to an event.
The 20% of the posts from "unknown" tools were assigned, as a default, the non-mobile label, in order to maintain high precision for the mobile label.
In order to expose a user's "ties" or other personal interest and knowledge about the location of the event, we represent the location of the user, as well as the dominant locations of their network contacts.
An understanding of the aggregate location distribution of a source's social contacts will reveal how a source relates to various geographies, as well as suggest the source's potential knowledge and connections to or interests in a geographically embedded event.
To perform this spatial characterization, the system focuses on a source's reciprocal contacts: users to which the source has both a following and follower relationship.
Henceforth we refer to these connections as "friends".
The goal of our spatial characterization is to identify the most prevalent locations of the user's friends.
The challenge is that the location field on Twitter is a free text field, containing data that may not even describe a geographic location, or describe one in an obscure or unspecific manner .
We thus need to undertake a process of standardizing user locations into recognized and established place names that can in turn allow for reliable aggregation.
We conducted an exploratory study of SRSR to gain an understanding of whether and how the system and its various elements were helpful to journalists searching for and evaluating sources in social media, and how they match journalists' needs and current practices.
We probed generally about their use of social media in journalism and more specifically investigated their use of SRSR to find and assess sources, noting how the SRSR features were used for their tasks.
In the course of our study, we observed their use of the tool with two different event datasets, and interviewed each participant before and after their use.
The SRSR interface was then demoed to the participant using a dataset for a small event.
All of the features of the interface were demonstrated and explained, such as how to search, filter, and sort and what data was used to construct the visual cues they were seeing.
The algorithms and expected accuracy for user-type and eyewitness classifiers were also explained.
The participant then used the application with content from two separate breaking news events.
With the first event, the participant was asked to get comfortable with the interface and explore the different features and options.
They were asked to think aloud as they were doing so.
After some time to explore, the researcher prompted the participant to try any features that they hadn't yet in order to make sure that they were fully aware of all of the options.
With the second event, the participant was given a concrete scenario to frame their use of the tool.
They were told to imagine that they were trying to cover a breaking news event that had just happened a few hours beforehand and that their news organization did not have any reporters on the scene yet.
They were asked to use the SRSR tool to "find sources, people, stories, or angles that you think could add to your coverage of the event."
As before, the participant was asked to think aloud as they used the interface.
After using the tool for approximately 20-25 minutes in the scenario we conducted a semi-structured interview with each participant.
We asked about their general satisfaction with the tool and about how each of the interface cues was useful or not in finding or assessing sources.
Interviews were audio recorded and transcribed for analysis.
We evaluated SRSR with working journalism professionals who have direct social media responsibilities as part of their position.
Such professionals have knowledge of the real issues and workflows associated with using social media in reporting.
Combined with the in situ setting in which the study took place, this approach gives us more ecological validity to reflect on how the tool might be useful in practice.
We recruited participants by soliciting social media editors for their participation in the study via email.
We continued to snowball sample, asking for referrals to other journalists working with social media content.
Participants were offered a $35 Amazon.com gift card for their time, though two participants declined the gift card.
In all we recruited seven participants  from local , national , and international  news outlets.
Our participants represent some of the best, most prominent, and leading-edge users of social media in journalism today.
We made use of three distinct events for our evaluation, one for demo purposes, and two for the journalists to interact with.
For each of these events, we collected a sample of Twitter posts using the Twitter streaming API.
We manually chose appropriate keywords  for each event, and then used them as search terms in our crawler.
The streaming API crawler gave us a sample of about 2% of the Twitter stream postings around each event.
From the tweet sample we then extracted the unique users for each event and collected a variety of data from each source in order to render the appropriate aggregate cues in the interface.
The first event, used to demonstrate the SRSR system, was a small conference-like local meeting in New York City, July 9th, 2011, containing 67 sources and 277 Twitter posts.
The second event, used for allowing the participant to get comfortable with the interface, was a set of 402 sources and 551 posts collected from the first evening of the Tottenham riots, which took place in a neighborhood in London, England on August 7th, 2011.
As part of the interviews with participants we asked broadly about their current use of social media in reporting and journalism.
For breaking news events the perceived value of social media for finding first-hand reports, monitoring events, and reporting under time duress were often emphasized.
Other aspects of sourcing such as building beat-lists were also mentioned.
Overwhelmingly, the journalists were concerned with the issue of verification, either of content or of sources.
Several participants mentioned that their role included debunking or confirming social media rumors , as well as assessing the credibility of sources .
One participant articulated the workflow challenge around a specific reporting incident: "We were tempted to say `there's a gunman in the Flatiron building' and 60 people are saying that it's so on Twitter ... learning how to apply all of the traditional skills of journalism, verifying, checking information and what it means... doing that quickly and accurately and with a team of people not always in the same place, that workflow has been a challenge" .
In the following sub-sections we go into more detail on the two high level tasks reflected above, finding and assessing verity, and discuss informational cues that journalists currently use or used in SRSR to accomplish these tasks.
Given our scenario of a breaking news event centered on a specific location many  of the participants in our study immediately oriented themselves towards finding eyewitness accounts including photos and videos.
The eyewitness filter was often the first thing participants clicked on when confronted with the Joplin event, "I gravitated to the eyewitness thing first to quickly find people... on the ground stuff" .
Oftentimes participants would first click to filter sources for eyewitnesses, and then further filter for only tweets containing images or videos.
Several participants also stressed the importance of location in conjunction with eyewitnesses, which the purely contentbased eyewitness detector that we prototyped did not take into account.
Almost all participants acknowledged the need to combine location search and filtering with eyewitness detection in order to find the true eyewitnesses.
But despite this underlying shortcoming, participants nonetheless found valuable sources via the eyewitness filter, "I am finding people in eyewitnesses who are there, so even though it's not perfect I'm finding useful information and people I probably wouldn't otherwise find" .
One participant sorted by eyewitness-ness on the Tottenham event and noted that it showed her someone she acknowledged had been a great source of information when she had actually been covering that event.
But still there were two participants that expressed some skepticism over the eyewitness algorithm, "I would need to be able to test it out in real time for something that I'm aware of who the eyewitnesses are, just to see if it matches up" .
We also explicitly asked participants in our study about their reactions to and use of the different user type filters.
The overall utility of the filters is perhaps best summarized by a participant who said, "I could see these categories being useful if I'm doing something in real-time or if I have a very long list that I want to quickly segment.
I would probably give each one a quick glance."
Of the different user-types it appears that filters for organizations and journalists/bloggers were the most interesting in the given scenario, with filters for ordinary people perceived as less useful, "I don't think I need ordinary people...
I think I'm just gonna stick to all, eyewitnesses, or organizations" ; "I don't need ordinary person."
There was more of an interest in finding organizations, "The local stations in Joplin would be very interesting to me, what aid organizations or local organizations."
And there was also an emphasis on finding the locally relevant organizations, "I would probably go through and eliminate national organizations and keep the ones that are local level or focused directly on the event."
When looking for more authoritative or fact-based reports, participants also made use of the capability to filter for journalists/bloggers, "I would look to other journalists/bloggers to compile a list of more authoritative sources" ; "I would tend to focus on the eyewitnesses and journalists/bloggers.
This notion of journalists/bloggers providing better information was echoed by others as well, "Let's say it's 20 minutes after it's happened, one of the first things I'd do is click journalists/bloggers because I'd want to see who the local voices are, who are more likely to know what they're talking about."
Searching was used to a lesser extent than filtering by most participants.
They wanted to find out if other content had been posted about those places, suggesting that search pivots might be built around locations mentioned in tweets.
Another strategy employed by journalists searching social media was to look for words or phrases that would indicate someone directly involved in the event, such as "I'm OK" or "I'm safe".
As one participant related, she will often imagine she is the person in the situation in order to figure out what someone might say .
A tension that arose for two participants was the interface's emphasis on sources over the raw content of the tweets.
As one participant said, "I think when we work, we're accustomed to the stream and content is primary and then you want to make decisions about what to include or not include around how credible the person is, but you don't care about their credibility unless the content is relevant" .
Another participant echoed this sentiment of wanting the interface "flipped" to put more emphasis on content rather than sources.
These comments suggest that future designs may consider ways in which to better support the direct finding of content, rather than the emphasis in our design on finding sources.
During the study participants related that source context including historical tweets, account age, website, interactions with others , network properties, and Klout score were all valuable cues that they routinely use to assess their trust of sources.
Below, we focus the discussion on the aggregate and computed cues included in SRSR, including friends' locations, the network sketches, and historical entities used.
Three participants noted that sources that had friends in the location of the event were more believable, indicating that showing friends' locations can be an indicator of credibility.
One participant related this to the authority of the source, "I think if it's someone without any friends in the region that they're tweeting about then that's not nearly as authoritative, whereas if I find somebody who has 50% of friends are in Joplin, I would immediately look at that."
Another participant noted that he might focus more on sources with dense friends' networks in the location of the event, but also alluded to how it impacts source assessment,
The friends' location cue was also discussed as a valuable hint about the source's location, suggesting where the source may have stronger connections geographically: "It gives me a better sense of where they maybe actually are" ; "It shows if they have connections in the places where you want to see them have connections" .
One participant noted how the locative cue provided by friends' locations would be even more valuable if it could be aggregated at different scales: "It would be great to toggle between city and state or country because there are times where I want to know that this person has the vast majority of followers are in Egypt but they're spread out between a dozen cities and cities don't mean anything to me."
Another cue that we provided in the SRSR interface was a network sketch of people tweeting in the event who also follow the source, designed to give a quick, glance-able view of the source's connectivity to others taking part in the event stream.
Three of our participants found this network information explicitly useful for assessing sources.
For instance, one participant noted that a source had much more credibility because it had the Red Cross as a follower.
Another participant noted that the more connected a source is, the more reliable he considers the source.
Participants noted that they could use the network information for finding sources also.
Other types of network connections were sought; participants mentioned benefit in seeing who the source is talking to , who they are friends with, and who they are retweeting.
These findings suggests two possible extensions for the network information features: one, providing a number of different network sketches , which could be useful, both for verity and for finding tasks; and two, devising representations that quickly confer credibility from known sources to less known sources, which would be valuable in a time-constrained reporting scenario.
Finally, SRSR displayed each user's top five most used named entities.
Participants were mixed on this information cue in terms of its value and utility given the breaking news scenario, "Unless it mentions the city where it takes place it's not very useful... theoretically it's important but I think it's going to be important on a case by case basis."
Three participants noted that, although they didn't really use the information in the breaking news scenario, that it could be useful in other scenarios, "In a different scenario, not a hard breaking news scenario but maybe I want to put together an interesting stream of thought leaders on the AfPak region... entities then might matter more to me."
Another participant echoed this sentiment that historical entities used would be "more useful for expert finding" .
The overall reaction to the constellation of cues provided in the SRSR interface was positive.
The information and context around each source aided in both finding and rapidly assessing sources by giving an overall view of the source.
As one participant put it, "This gives you context... you have the context for whether or not you think they're reputable or whether or not they're worth reaching out to."
Having this context visible and close at hand is what aids the verification of sources, "It's giving me a lot of context which is really useful when you're trying to verify if someone is reputable or not."
Most of the participants alluded to the importance of building lists from twitter sources, "One of the things I'd like to develop is in any given story here's a potential pool of people that knows what's going on."
Related to this notion of curation, three participants also wanted to be able to hide or eliminate sources from their view in order to reduce noise in their stream as they assessed sources.
Various other content filters were also suggested by participants and may be opportunities for future work and development.
For instance, two participants mentioned that a filter based on opinion might be beneficial to help hone in on sources that are less opinionated and more fact-based in their tweets.
This suggests an interesting opportunity for studying a filter based on sentiment analysis.
Automatically classifying tweets as subjective or objective could enable such a filter.
Yet another suggestion for filtering was to be able to filter content based on @-replies, both to separate content addressed to the journalists , or just to get a better sense of a source's behavior to see if they are "conversational".
Social media have altered our society's information and communication fabric and will continue to be increasingly integrated in various ways into journalistic practice.
As a consequence, it is important to continue to develop information tools for journalists, and other professional and non-professional actors, to find and evaluate information from social media sources.
In this paper we have developed and evaluated a tool for journalists to search for and assess sources in social media around breaking news events.
We presented a number of computational information cues that were found to be beneficial for this task and match their needs and practices.
In particular we developed a high precision eyewitness detector that was perceived to have value for journalists seeking immediate, on-the-ground reports around breaking news.
Visual cues about a source's network and their friend's locations were used as heuristics for credibility.
For instance, journalists used a source's friends' locations to estimate the authority of that person for information related to the event location.
As a prototype system, SRSR has advanced the state of the art, but many opportunities remain.
First, SRSR mimics a real-time scenario, but there are technical challenges in adapting the SRSR algorithms and methods to truly work in real-time.
Second, the eyewitness classification algorithm, though a simple and high-precision method, could incorporate more sophisticated machine learning and additional source features such as location or other language features.
Third, other network representations can be developed to evaluate a source or confer credibility.
Finally, estimates of location information of the user and their friends could improve based on language content or geo-coded posts.
Berkowitz, D. and Beach, D., W. News Sources and News Context: The Effect of Routine News, Conflict, and Proximity.
News at Work: Imitation in an Age of Information Abundance.
University of Chicago Press, 2010.
Carlson, M. Dueling, Dancing, or Dominating?
Making Use: Scenario-Based Design of Human-Computer Interactions.
Castillo, C., Mendoza, M. and Poblete, B., Information Credibility on Twitter.
Cheng, Z., Caverlee, J. and Lee, K., You are where you tweet: a content-based approach to geo-locating twitter users.
De Choudhury, M., Diakopoulos, N. and Naaman, M., Unfolding the Event Landscape on Twitter: Classification and Exploration of User Categories.
Diakopoulos, N., Naaman, M. and Kivran-Swaine, F., Diamonds in the Rough: Social Media Visual Analytics for Journalistic Inquiry.
Diakopoulos, N. and Shamma, D.A., Characterizing Debate Performance via Aggregated Twitter Sentiment.
Hansen, K. Source Diversity and Newspaper Enterprise Journalism.
