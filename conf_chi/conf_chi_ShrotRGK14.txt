Interruptions can have a significant impact on users working to complete a task.
When people are collaborating, either with other users or with systems, coordinating interruptions is an important factor in maintaining efficiency and preventing information overload.
Computer systems can observe user behavior, model it, and use this to optimize the interruptions to minimize disruption.
However, current techniques often require long training periods that make them unsuitable for online collaborative environments where new users frequently participate.
In this paper, we present a novel synthesis between Collaborative Filtering methods and machine learning classification algorithms to create a fast learning algorithm, CRISP.
CRISP exploits the similarities between users in order to apply data from known users to new users, therefore requiring less information on each person.
Results from user studies indicate the algorithm significantly improves users' performances in completing the task and their perception of how long it took to complete each task.
Real-time collaboration as part of the work process has become increasingly common, through using chat tools while editing, with collaborative-editing environments like Google Docs, and with other platforms.
However, users working in a distributed way may lack the ability to perceive the other's work process; thus, social cues about when to interrupt are lost.
In domains like collaborative document editing where users are communicating while editing, interruptions for communication about the task are common .
For example, two people may be working on a document together and sending messages as they write.
In person, one author would not interrupt the other if she could see he was in the middle of focused writing or editing.
Online, though, she can't see this, so may send a message that pops up and disrupts his workflow.
Similarly, system dialogues can be important for users to complete tasks.
However, a poorly timed message  can affect the user's focus level which may lead to averse effects on task performance and users' frustration levels .
A system that can hold interruptions until less disruptive points in a user's workflow may improve the speed and efficiency of the work.
Prior work has shown that interruptions can be extremely disruptive to users maintaining concentration on their tasks, and indeed that completely eliminating interruptions may be the best option .
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
However, while eliminating interruptions may be ideal for allowing users to stay on task, when working in an environment where interruptions are allowed or are a natural part of the process , an algorithmic approach to minimize disruption can be helpful.
Any solution requires estimating the cost of an interruption for a user.
Existing methods generally require hours of observations to build a model for each person .
For systems like Google Docs, where new users can easily be invited into the system, such a long training period can be impractical.
Our contribution is a collaborative filtering-inspired interruption management algorithm, designed to minimize the negative effects of interruptions.
By leveraging data from other similar system users, a novel approach, the algorithm is able to immediately begin effectively controlling interruptions for a new user, offering a significant advantage over existing techniques that require long training phases.
We demonstrate the positive effect this algorithm has on a user's speed for a collaborative editing task compared with interruptions that occur randomly or that are controlled by a rulebased algorithm.
Our algorithm models people's preferences for interruption timing and uses those insights to control interruptions.
Our new approach, called CRISP , leverages the underlying ideas of Collaborative Filtering algorithms, an environment dependent rule-based algorithm, and basic classification algorithms.
The advantage of CRISP over traditional learning methods is its significant reduction in the learning time needed to model a given user.
This allows us to quickly decide about the efficiency of an interruption with only limited data and can avoid pitfalls such as protracted learning periods and elicitation of private user data.
It makes the algorithm ideal for systems that want this control up and running for a user very quickly after they join.
This paper presents the details of our approach for modeling user preferences.
We validate this with two user studies while comparing it to random interruptions and a rule based algorithm.
We found that our algorithm allows users to work faster and improves their perception of speed as well.
However, during low workload periods, they deferred only 6% of the time.
This illustrates that users do have periods in which interruptions are easier to deal with.
Other work by Iqbal & Bailey  modeled users working on a task and identified the "best" and "worst" times to interrupt a user.
Their results showed the interruptions at the "best" time significantly improved the user experience.
Several previous works have student different aspects of the interruption management problem.
Arroyo & Selker  developed the Disruption Management Framework, a system that takes into consideration the users' motivation in how they handle interruptions.
The framework was developed to support interruption mediating in multitasking environments and is constructed of 3 layers.
The layers are built to take into consideration tasks' related information, users' related information and information related to the users' current behavioral pattern.
Interruption was also addressed in McCrickard et al.
In particular, they described a classification model based on 3 elements of awareness: interruption, reaction, and comprehension.
This model worked well, but required hours of video recording which limited the scope of the study.
In our work we focused on developing an automated method that needs less time to collect its data.
Applications of this idea have occurred in a number of domains.
This emphasizes the importance of detecting users' patterns of interaction and timing such that a system can automatically identify these opportune times when they occur.
Our work builds on this research by creating a new system that automatically models users' preferences for interruption timing and uses that to control messages communicated through the system.
In addition, we used the breakpoints identified by those researchers as baseline in order to validate the contribution of our system.
The importance of user attention and the impact of interruptions has been studied from a variety of perspectives in the HCI community.
Adamczyk & Bailey  studied the impact of interruptions on users' emotional and cognitive states when they were interrupted at different times in the course of their task.
They found that timing did have an impact and suggested that an "attention manager" could be an effective tool for minimizing the negative impact that interruptions have on users.
Analyzing users themselves provides insight into when interruptions are best handled.
One of the most important issues concerning the initiation of interruptions is the ability to accurately estimate the cost arising from the interruption.
Accurate estimation will enable interruption only when it will have a positive impact on the group's performance .
Interruptions have two ways to negatively affect users: a long term effect and a short term effect.
Both the long and short term effects must be taken into account when calculating the cost of the interruptions.
Previous research has investigated how to estimate the cost of interruptions .
Fleming & Cohen  were the first to build a user-specific model which generally takes the user's specific factors into account.
However, they assume that they have statistical data about the users' knowledge and utility values.
Our algorithm quickly builds a model with no individual background information.
Tan & Richardson  studied the cost of interruptions to facilitate delaying when an interruption will occur.
Our algorithm has similar features for deciding when to interrupt.
Bailey & Iqbal  studied interruption managing in text editing environment after long observation they developed identified breakpoints and subtasks during workload, which considered good timing to interrupt users.
The key difference between our research and previous works is that we study cost-estimations that can work in dynamic domains in which the environment's conditions rapidly change, actions occur quickly, and users' abilities change over time.
In contrast, previous work required long and expensive training sessions to gather the necessary information for the algorithm to function.
Rules are used to identify the natural breakpoints .
The k-nearest neighbors  algorithm  is a method for classifying objects based on closest training examples in the given labeled database.
This creates a dynamic, multi-model ad-hock clustering of the subjects.
Many previous works combine several machine learning algorithms in order to achieve better results than each algorithm archives individually.
We use a similar approach.
However, while most hybrid approaches use different algorithms in each phase, they use the same data in both phases.
Our approach differs as we collect different types of information - the user's task information and general domain information - and use different information in separate phases.
Rzeszotarski & Kittur  had a similar idea of using information gathered from past AMT worker in order to generate a labeled corpus that will assist in minimizing the time it takes the system to make a decision about a new user.
They used a corpus of past users' behaviors in order to quickly model and evaluate task fingerprinting.
They used past information gathered from AMT workers in order to capture crowdsourced behavior and make inferences about their task performance.
Our work builds on our past work by Shrot et al.
However, while our work was checked online with real people against other algorithms in real word conditions, Shrot et al.
Furthermore, they only checked data offline and thus could not decide if interruptions should be posed during task execution as this work does.
In addition, Shrot et al.
One of the ways our algorithm can quickly create a model for timing interruptions is by relying on data from other users as initial background.
This approach borrows insights from Collaborative Filtering.
Collaborative Filtering  is a method of making automatic predictions  about the preferences of a user by collecting data on the preferences of many users .
There are many examples of recommendation systems via Collaborative Filtering .
Collaborate Filtering models can be built based on users or items.
User Based collaborative filtering systems find other users that have displayed similar tastes to the active user and recommend the items similar users have preferred .
That is, if user u1 and user u2 shown similar taste then items that u1 likes will be offered to u2 and vise verses.
Users' similarity is calculated by comparing users' history and identifying similar ranks to the same items.
Item-based models recommend items that are most similar to the set of items the active user has rated .
That is, if user u liked item i1 , then items found to be similar to i1 will be offered to u.
The assumption within CF models is that similar users will always make similar decisions, thus de-emphasizing the role of individual preferences.
Hybrid approaches are also common.
Karypis  was the first to recommend an approach that combines the best of the Item-based and the User-based  algorithms, by first identifying a reasonably large neighborhood of similar users and then using this subset to derive the Item Based recommendation model.
Our proposed algorithm, CRISP,  is motivated by the Collaborative Filtering hybrid approach.
CRISP has three phases: a "rule based" algorithm phase , a "user" phase , and an "item" phase .
The first phase uses rules to identify natural breakpoints that are relatively unintrusive interruption points.
The second phase uses user-specific data to identify similar users in the database and to construct an environment of similar users.
The third phase of the algorithm decides if it is a good or bad time to interrupt.
This leverages data from "similar" users that were discovered in the second phase.
The algorithm's structure is inspired by elements of Collaborative Filtering.
Accept a new non labeled situation s = .
If ip is following the rule based algorithm RU LES Label the situation based on RU LES , else: 3.
Use h to create a user profile p for s. 4.
Use the profile p and the database db to build a neighborhood N GB of l situations that were found to be similar  to s. 5.
The second data type - the state specific data - is the interruption profile .
It is also important to note that h and ip usually refer to different attributes in the vectors.
A situation  is a pair of user's latest history h and the interruption profile ip that immediately follows it.
Therefore, a situation is constructed from two distinct data types, both user specific and state specific data, giving us a wider point of view about the interruption's influence.
A labeled situation is a pair  that matches a situation with the label of whether it is a "good" situation for interruption or not, namely, whether the group's gain from this interruption is higher than the cost of the interruption.
CRISP 's  input is a small database of labeled situations  and a non-labeled situation s, for which we wish to discover whether it is a "good timing" or not.
In the first phase the algorithm checks the given situation based on its initial rule based algorithm.
If the situation is matching one of the given rules it will label it according to the appropriate rule in the algorithm.
In the Experimental Setup section we detail exactly what rules were implemented.
If none of the rules apply, it will continue to the next phase of the algorithm.
However, Collaborative Filtering methods cannot feasibly be implemented in interruption management domains because the interruption managing system in this mixed agent-user environment does not have access to users' characteristics data nor does it have access to a specific user's votes on his past interruption.
Consequently, different methods must be found to model new users' and items' similarities.
CRISP  contains information from old and different situations that were already examined.
In these situations the outcome of the interruption is already known.
Therefore, it is possible to label these interruptions as either good or bad interruption timing.
Since our data can be labeled, our solution is to use traditional machine learning classification algorithms.
The classification algorithms are used to quickly compare the users  and items  without resorting to a shared database of all users' characteristics or voting history.
The basic element of information used is the user's state .
A user's state is a vector that contains numerical or other discrete values for different attributes about the user's work progress and user's state.
These attributes include information such as percentage of the task accomplished and time left to complete the task.
All values in the vectors' attributes are normalized to the same scale and all vectors in a given domain will have the same attributes.
A time element is an element t  {0...Tmax } that represents the time that has passed since the beginning of the task .
A timed state  is a pair  that represents the user's state at time t. As mentioned earlier, we use two types of data - user-specific data and state-specific data.
The user-specific data is the user's latest history data .
The length of these states was between 3 and 6 seconds gathered in 30 to 60 second intervals in our experiments.
These intervals were chosen based on previous work  as observed subtask lengths.
In the second phase  the algorithm builds a user's similarity model between the new given situation  and the given labeled situations in the database  .
This phase uses only the historical data .
The user similarity model is built according to the similarity between the "user's latest history" data in the situations.
Specifically, h is a set of k vectors that represents the user's behavior in the short period  sampled before the interruption .
For each situation, the algorithm calculates how each attribute's value changed  between the sampling in h .
The user's profile is the vector of averages changes.
Then, similarity between two user situations is measured as the distance between the two calculated profiles .
The assumption behind this approach is that users with similar profiles  undergo the same process and therefore most probably act in similar ways.
This model represents the user's similarity level between the new situation  and the given labeled situations.
Once the user's similarity model is completed, the algorithm uses the k-nearest neighbor  algorithm  to choose the l most similar situations as the new situation neighborhood.
This neighborhood is used in the algorithm's next stage.
For the experiments l was selected as a percentage of the entire database length  and in our experiments this was 60 of the 600 users in the database.
The next stage of the algorithm  uses only interruption profile .
Once the user's profile and neighborhood are constructed, the algorithm builds an interruption profiles similarity model between the situations that belong in the neighborhood.
A machine learning classification algorithm runs over the neighborhood's situations and returns the calculated classification.
The net result is that once a new situation arrives, the algorithm needs only a very short time to gather enough data in order to decide how to treat it.
This allows for a faster and more accurate classification than the base machine learning algorithms alone could provide.
Our experiments were designed to see if interruptions, as controlled by CRISP, offered users benefits over other interruption timing in terms of speed, efficiency, and user perception of task difficulty.
We were inspired by Bailey & Iqbal  to studied this problem in the context of a text editing task.
Our context was a text editing task where pop-up messages interrupted users with information they had to note down.
These simulated interruptions from a collaborator might occur when working together on a document.
The experiment was run on Amazon Mechanical Turk  and our AMT usage parallels previous work .
The user's task was to locate and correct all spelling and grammatical errors within the text.
For every mistake fixed, the subject earned a bonus .
The assignment had a time limit  and the participant was instructed to fix as many mistakes possible before the end of the assignment.
To simplify the logging process we disabled the subjects' ability to change the position within the text document by using the mouse.
This forced the subjects to only use the keyboard, something that was easier to log.
The experimental environment is shown in  with a message over the text being edited.
In order to avoid disruption from these events, subjects were instructed to save frequently and load the last saved document after it was erased.
Note that our algorithm does not try to control these interruptions, because they essentially represent system errors or accidental use of the system .
We have included them in the study since they represent events that affect user behavior.
The second type of interruptions are those caused by communication.
We simulated a situation where two users were collaboratively editing a document.
The subject was correcting typos and the other user, whom we simulated, sent requests for the subject to update the format of references in the document.
In the experiments, the agent that represented the other user made 10 interruptions as the subject edited the document.
The interruptions appear in a popup window .
Once the window opens, the focus moves to it and the user may not edit or move in the document.
The users are instructed to make a note of the change on paper for later editing.
After noting the change, the subject clicks "OK" and returns to editing.
User interruptions have been previously categorized as being either external or internal with external interruptions being initiated by other people .
While this work noted that both types of interruptions occur with nearly equal frequencies, we focus on external interruptions as the system potentially has more control over these events, as opposed to internal interruptions which people initiate for themselves.
Users were interrupted as they were editing the document.
Both of these interruptions can be classified as immediate interruptions as per McFarlance's taxonomy .
The first type of interruptions represent an unexpected and uncontrolled event that interferes with the user's regular course of action.
These are interruptions that we cannot control and they have nothing to do with the user's task or the agent.
They are usually rare and reflect a highly disturbing phenomenon.
We simulated those interruptions by random events that erase the user's document.
The experiment contains 3 documents.
Example A short, 2 paragraph  document with 3 mistakes.
Document A A long  document with 50 mistakes.
Document B A different long  document with 50 mistakes.
In order to check the efficacy of CRISP we compared the timing of its interruptions with either a random interruption algorithm  or a rule based - breakpoints identified algorithm .
All the algorithms have to time their interruptions while following the same basic pattern:
This time is used to gather information regarding the user behavior .
The length of this pattern was chosen based on  observation regarding subtask length.
The protocol for each subject was as follows: 1.
Learning stage: Running the "Example" document with random interruption.
Document Editing: Running "Document A" document with either CRISP-timed communication interruption or comparison  interruptions.
Document Editing: Running "Document B" document with either CRISP-timed communication interruption or comparison  interruptions.
All subjects completed the example document editing  first, as a training and task learning session.
The order in which they completed scenarios 2 and 4 varied.
50% of the users edited Document A before editing Document B, and 50% did it the other way around.
In addition, order of the interruption algorithms was randomized.
This was done to negate any effects from the order of the documents or the combination of document and algorithm.
In all but the first scenario , once the users finished a task, they were asked to complete a questionnaire.
The first question was a request to quote one of the citation changes provided by the system in one of the interrupting pop-ups.
This was a filtering question meant to discover the users that did not follow the task instructions.
In our experiments we found one such user that ignored the task of cooperating with the agent.
This user was removed from our analysis.
After answering that question, users complete the NASA-TLX  assessment .
This survey is used to measure a subjects' perception of the mental, physical, and temporal demands of a task in addition to their perceptions of their own performance, effort, and frustration.
We compared the speed with which users edited and read the document and their NASA-TLX scores among conditions to see how helpful each was.
This section describes how CRISP's Rule based, User and Item stages were implemented in a text editing environment.
The rule component of CRISP has only one rule - if the user just re-loaded her document from a saved version, it is a good time to initiate a communication.
This rule is based on the fact that loading the document moves the user to the beginning of the documents, which means she has already lost focus, and disturbing her right now will not cause a massive interruption.
The user stage of the algorithm is based on the behavioral information collected about the user.
In the text editing environment the attributes that were taken for the us profile were as follows.
All times are in milliseconds.
Last action This attribute is for the rule-based part.
It saves the last meaningful action taken by the user.
Profile length The length of time this data was gathered in.
Key number The number of times the user pressed any key during the gathering of information .
Alpha number The number of times the user pressed any alphabetic key.
Mouse move number The number of times the user moved the mouse in this h. Average time Mouse Move The average length of a single mouse move as record in this h. Location distance The percentage of the document that was scanned during this h, measured by the change in document position from the start of h to its end.
Max location distance The maximum movement  the user did in this document during h. Moving indication An indicator that compares the user's past moving behavior  to her current moving behavior  to see if she is accelerating, slowing down or maintaining the same rhythm of work.
Fix distance Same as Location distance, but tracking the percentage of mistakes fixed.
Max fix distance Same as Max location distance, but tracking the percentage of mistakes fixed.
Fixing indication Same as Moving indication, but tracking the percentage of mistakes fixed.
Save number The number of times the user saved the document during h. Load number The number of times the user loaded the document during h. Interruption number The number of document erasing interruptions in h. Average length of interruption The average time it took the user to copy the citation on page and close the pop-up window in all the communication interruption she experienced so far in this document.
Average return from interruption The average time it took the user to return and do some action in the document after she closed the pop-up window.
In the first experiment, 30 people participated.
The average age was 32.1 with a standard deviation of 9.5.
The education level varied: 9 subjects' degree was a high school diploma, 17 had either a BA or BSc degree, and 4 had a postgraduate degree.
In the second experiment we had 18 people, all US citizens.
The item stage of the algorithm uses current state parameters in order to try and find the most similar states in the user database and learn what decision to make based on the classification model built from them.
In the text editing environment the attributes that were taken for the us profile to be used in this stage were as follows.
Time The time that passed from the beginning of the task in milliseconds.
Location in document The current location of the user in the document .
Correction of document The current number of mistakes that have been corrected in the document .
Time from last key The time that passed from the last time the user pressed a key on the keyboard.
Time from last alpha Same as "Time from last key" only for alpha keys.
Time from last fix The time that passed from the last time the user fixed a mistake in the document.
Time from last save The time that passed from the last time the user saved the document.
Time from last load The time that passed from the last time the user loaded the document.
Time from last interruption The time that passed from the last interruption.
Total number of mouse move The total number of mouse moves the user has made so far.
Time from last mouse move The time that passed from the last time the user moved the mouse.
The agent collect all of these attributes  of the user's state every second.
This information is necessary to construct the user's state  either for user's history   or interruption profile  .
Next, we search Alice and Bob's data for situations most similar to the one Dan is currently in.
Similar situations are defined as being around the same location within the document that Dan is currently editing , with a similar number of mistakes fixed  and done at a similar work rate .
These values are combined as a weighed sum as described previously.
We search for the situation within the cluster with the highest similarity to Dan's current situation.
CRISP then checks if this situation was a good or bad interruption, and decides if it will interrupt Dan or not accordingly.
If it was a good time, the algorithm will interrupt.
If it was a bad time, it will not.
If Dan is editing a document and there is a message for him, the system wants to choose the optimal time to interrupt him.
The algorithm first checks if Dan just completed a task, something that would signify a breakpoint .
If yes, it is a good time to interrupt.
If not, CRISP searches for the k most similar usersin the database .
In order to find similar users we must first calculate Dan's current behavioral profile.
For example we will focus on 3 attributes that CRISP uses: Key number , Fix distance, Save number.
Let's assume Dan typed an average of 20.4 keystrokes per minute , fixed 4 percent of the mistakes and saved twice during his current session.
Our database is constructed of 3 users: Alice, Bob and Chuck, and they have their own values for those parameters.
Let's assume their values are as shown in table 1.
By comparing the distance between the vectors it is clear that Alice and Bob are much more similar to Dan than Chuck.
Chuck is a much more active user.
We bootstrapped CRISP with a small initial database, that contains a set of pre-labeled situations.
In order to create that initial database we ran an offline data collection phase.
In this offline data collection phase we had 20 subjects.
Thirteen were female  and 7 were male.
The average age was 33.5 with a standard deviation of 13.3.
The education level varied: 12 subjects' highest degree was a high school diploma, 6 had either a BA or BSc degree, and 2 had a post-graduate degree.
As previously stated, Amazon's Mechanical Turk was used to select participants.
In our experiment we only allow USA citizens  in order to disable noise due to English not being the subjects' main language.
Each subject open the experiment on her machine and the size of the window was not fixed.
Yet, we limit the maximal number of characters a subject can see in any given moment.
Interruptions were generated randomly each 30  60 seconds, and the data was gathered.
Each subject performed the entire experimental protocol as described above, but both scenario 2 and scenario 4 had random interruptions.
The subject's state and status were sampled every 10 seconds and also before and after each interruption.
At the end of the research protocol each communication interruption was labeled as either "good" or "bad" according to the effects it had on the users' final outcomes at the end of the experiment , their emotional state , and according to the magnitude of interruption and frustration it caused the user.
The data resulting from this offline data collection phase was used to bootstrap the live experiments.
In this indicator, as seen in Figure 4, using CRISP, users felt less rushed during the task.
On a 20 point scale, they rated the time pressure with CRISP at a 14.6, but gave a significantly higher difficulty score of 16.3 to the task with random interruptions .
In this experiment we wanted to compare our algorithm to some of the state of the art work done in this field.
One common and high-performing technique is to identify natural "breakpoints" in the user's workflow.
Points at which user's attention has broken from the current task are good times for interruption since there is minimal disruption that will occur.
Many researchers have done work on creating rules for identifying breakpoints and they have demonstrated their effectiveness .
The best and most obvious indicator of success is how much of the task was completed.
We measure this by how many of document's mistakes were fixed.
Users performed significantly better with CRISP .
They fixed an average of 50.2% of the errors when interruptions were made using CRISP compared with only 38.0% of errors with the random interruptions.
These results are statistically significant .
A second indicator of success is that with CRISP the users were able to scan significantly more of the document before the 10-minute deadline .
Since users were only using the keyboard and could not scroll with the mouse, we could measure the percentage of the document scanned by tracking the location of the cursor.
We measured how far into the document each user went.
With CRISP , users covered 88.7% of the document, compared with only 72.2% with the randomized interruptions.
Results are shown in Figure 3.
An interesting results is that, unlike in the performance indicators that are significantly better using CRISP, most of the NASA-TLX indicators show no significant difference between the random timing algorithm and CRISP.
In this second experiment we ran the same protocol as before, only this time instead of comparing CRISP to randomly timed interruptions, we compared it to a rule-based algorithm that timed its interruptions to known breakpoints in the workflow.
We used the breakpoints that were identified in prior work : finishing a sub-task, saving one's work, and extreme attention shifts.
Within our task finishing a sub-task was fixing a textual mistake, saving one's work was identical to their work, and extreme attention shifts were represented by delete interruptions and when the user loaded the document.
As can be seen in Figures 5 and 6, the results are similar to those of the first experiment.
As this paper demonstrates, CRISP excels in modeling user behavior with only limited data through identifying how similar users react to interruptions using collaborative filtering  models.
While CF models do generally compare similar users based on their individual differences, assuming much more information exists about a given user, the general comparisons between users made by CF models could potentially be further differentiated- something that could significantly aid in predicting how different users react to interruptions .
In order to address such individual differences directly and differently from a CF approach, cognitive models of users must be created through either extensively learning users in a given task, or through observing external user traits such as working memory capacity .
Creating hybrid approaches combining CF approaches with individual cognitive models is an interesting challenge that we leave for future work.
Our algorithm, CRISP, has been presented and tested in the context of document editing, but the general technique could be applied in other contexts.
As long as data from other users' interactions is available, our collaborative-filtering inspired approach can be effective.
This makes CRISP particularly applicable in collaborative online applications.
Applications that could benefit from this technique are common and growing in popularity.
Collaborative editing in systems like Google Docs or real-time collaborative code editing are examples of places that users can interrupt and unintentionally disrupt one another's work flow.
A system control to hold messages until a less disruptive time  could make these experiences smoother.
Similarly, system messages can interrupt users as well.
These may be notifications of updates or other system messages in online editing environments, but even the common pop-up messages requesting subscriptions or offering help on websites could be better controlled.
Users may be more likely to quickly dismiss these windows if they interrupt their workflow, but they may be more likely to read them if they come at a less disruptive point in a user's interaction with a site.
Our contribution described here is this new approach to interruption management that uses a collaborative filteringinspired algorithm which we have demonstrated is effective in exemplar environments.
Our goal was not to show this is the best possible interruption management algorithm, but rather to demonstrate that it works well.
The benefit of a quick startup time compared to other algorithms makes the technique an important addition to the domain of interruption management algorithms.
For future work, the techniques we presented may be combined or integrated with other algorithms with a goal of achieving top performance.
In addition, it will be interesting to investigate other domains and problems.
For example, tasks with higher cognitive demand than the one used here may show greater impact from interruption.
Yet, there is one interesting exception: the "Performance" indicator.
In this experiment we found a significant difference between CRISP and the rule-based algorithm.
In reality  the reverse is true.
Despite this slight difference in users' perception, we demonstrated in this section that CRISP helped users perform significantly better in their text correction task compared to two different sets of users using the Random and a Rule-Based algorithm , and the CRISP users performed significantly better in the temporal category in the NASA-TLX temporal category.
There are several limitations to our study.
First in order to simplify the calculation we limit the editing environment in two ways:  We disabled the ability to move in the document using the mouse.
As these changes are unnatural and not evident in actual text editing tasks, the results of this study may be limited.
An additional limitation is that the system only supported two participants: The user and the interruption management system.
In many real-word systems, such as Google Docs, the environment will often contain more participants.
Nonetheless, we posit that different users can be models through CRISP.
Last, the experiments were conducted using AMT, which may or may not accurately model users in all settings.
We hope to further study these potential limitations in the future.
This paper introduces a novel approach to limit the disruptive impact that interruptions have on users when they are working in a system.
We introduced an algorithm, CRISP, that models user behavior and uses this to control the timing of messages sent by the system or other users.
In our experiments, we found that when compared to randomly timed interruptions and those controlled by a standard rule-based approach, CRISP -controlled interruptions allowed users to work more quickly.
Karypis, G. Evaluation of item-based top-n recommendation algorithms.
In Proceedings of the tenth international conference on Information and knowledge management , 247-254.
Linden, G., Smith, B., and York, J. Amazon.com recommendations: Item-to-item collaborative filtering.
A model for notification systems evaluationassessing user goals for multitasking activity.
McFarlane, D. Comparison of four primary methods for coordinating the interruption of people in human-computer interaction.
Middleton, S. E., Shadbolt, N. R., and De Roure, D. C. Ontological user profiling in recommender systems.
Rzeszotarski, J. M., and Kittur, A. Instrumenting the crowd: using implicit behavioral measures to predict task performance.
In Proceedings of the 24th annual ACM symposium on User interface software and technology , 13-22.
Salvucci, D. D., and Bogunovich, P. Multitasking and monotasking: The effects of mental workload on deferred task interruptions.
Shneiderman, B., and Bederson, B.
Maintaining concentration to achieve task completion.
In Proceedings of the 2005 conference on Designing for User eXperience , 9.
Shrot, T., Rosenfeld, A., and Kraus, S. Leveraging users for efficient interruption management in agent-user systems.
Tambe, M. Electric elves: What went wrong and why.
Please do not disturb: Managing interruptions and task complexity.
Vozalis, M., and Margaritis, K. G. On the combination of collaborative and item-based filtering.
In 3rd Hellenic Conference on Artificial Intelligence  .
In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, vol.
The development and prospect of personalized tv program recommendation systems.
In Proceedings Fourth International Symposium on Multimedia Software Engineering , 82-89.
