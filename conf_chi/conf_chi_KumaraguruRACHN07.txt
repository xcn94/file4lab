Phishing attacks, in which criminals lure Internet users to websites that impersonate legitimate sites, are occurring with increasing frequency and are causing considerable harm to victims.
In this paper we describe the design and evaluation of an embedded training email system that teaches people about phishing during their normal use of email.
We conducted lab experiments contrasting the effectiveness of standard security notices about phishing with two embedded training designs we developed.
We found that embedded training works better than the current practice of sending security notices.
We also derived sound design principles for embedded training systems.
These increasingly sophisticated attacks not only spoof email and websites, but can also spoof parts of a user's web browser, for example to hide warnings and URL information .
User studies have shown that a large number of people fall for these phishing attacks, even when the participants are made aware that their ability to identify phishing attacks is being tested .
Phishing attacks are initiated through several vectors, the most popular of which is currently email .
Phishing emails deploy a variety of tactics to trick people into giving up personal information: for instance, urging people to verify their account information, or asking people to take part in a survey in which they must provide their bank account number to be compensated.
The increasing sophistication of these attacks makes them hard to distinguish from legitimate emails, and reduces the trust users afford to genuine websites .
Previous anti-phishing research has focused either on algorithms for detecting phishing attacks in web browsers  or on evaluating the user interfaces of anti-phishing web browser toolbars .
However, there has been little work on preventing users from falling for phishing email messages .
Our work focuses on teaching people about the risks of phishing and training them to identify and avoid phishing attacks in email.
Towards this goal, we are developing an embedded training approach that teaches people how to protect themselves from phishing during their regular use of email.
Our approach consists of periodically sending users fake phishing emails that are actually from our system rather than from a scammer.
If a person falls for our fake email and clicks on a link, we display an intervention that provides immediate feedback about what happened and what simple actionable steps users could take to protect themselves.
A semantic attack is a computer-based attack that exploits human vulnerabilities.
Rather than taking advantage of system vulnerabilities, semantic attacks take advantage of the way humans interact with computers or interpret messages , exploiting the difference between the system model and the users' mental model .
Recently we have seen a dramatic increase in semantic attacks known as "phishing," in which victims get conned by spoofed emails and fraudulent websites.
We also present the results of a user study that compares the effectiveness of typical email security notices sent out by e-commerce companies to alert their customers about phishing to the effectiveness of our two designs.
Our evaluation suggests that typical email security notices are ineffective, while our embedded training designs are effective.
Based on our results, we outline some design principles for embedded training email notices that can be implemented easily today.
There are many approaches to training and educating users about phishing.
The most basic approach is to post articles about phishing on websites, as has been done by government organizations , non-profits  and businesses .
A more interactive approach is to provide web-based tests that allow users assess their own knowledge of phishing.
For example, Mail Frontier  has set up a website containing screenshots of potential phishing emails.
Users are scored based on how well they can identify which emails are legitimate and which are not.
Phishing education can also be conducted in a classroom setting, as has been done by Robila and Ragucci .
The idea of sending fake phishing emails to test users' vulnerability has been explored by several groups.
Typically, at the end of such studies, all users are given additional materials to teach them about phishing attacks.
This approach has been used with Indiana University students  and West Point cadets , as well as with employees at a New York state office .
Both the West Point and the New York state researchers conducted the study in two phases.
In the first phase, participants did not have any prior preparation or training about phishing before being tested for their ability to detect phishing attacks.
In the second phase, participants were given training materials and lectures about phishing before being tested again.
Both studies showed an improvement in the participants' ability to identify phishing emails.1 Our work differs in that we are focused on the design and evaluation of email interventions to understand what kinds of designs are more effective in teaching people about phishing and actually protecting them in practice.
For example, our studies suggest that the standard practice of sending out security notices is not an effective intervention.
Furthermore, our work evaluates how well people can generalize what we teach them to other kinds of related attacks.
The previous studies either tested participants only once  or tested participants on a single kind of attack on their intranet .
Our work aims to teach people what cues to look for to make better decisions in more general cases.
For example, rather than just teaching people not to fall for PayPal phishing attacks, we want people to learn how to identify phishing attacks in general.
The strategy of silently eliminating the threat provides protection without requiring any awareness or action on the part of users.
This includes finding phishing sites and shutting them down, as well as detecting and deleting phishing emails automatically .
If phishing threats could be completely eliminated using these methods, there would be no need for other protection strategies.
However, existing tools are unable to detect phishing emails with one hundred percent accuracy, and phishing websites stay online long enough to snare unsuspecting victims.
According to the Anti-Phishing Working Group , phishing sites stay online on average for 4.8 days .
A number of tools have been developed to warn users that the website they are visiting is likely fraudulent, either by providing explicit warnings or by providing interfaces that help people notice that they may be on a phishing website.
Ye and Sean  and Dhamija and Tygar  have developed prototype "trusted paths" for the Mozilla web browser that are designed to assist users in verifying that their browser has made a secure connection to a trusted site.
More common are web browser toolbars that provide extra cues--such as a red or green light indicating overall safety--to inform users that they may be at risk .
However, there are three weaknesses with this approach.
First, it requires people to install special software .
Second, user studies have shown that users often do not understand or act on the cues provided by toolbars .
Third, a recent study shows that some anti-phishing toolbars are not very accurate, and even the best toolbars may miss over 20% of phishing websites .
Although questions have been raised about the ethics of such deceptive approaches to educating users and studying the effectiveness of phishing attacks, the general consensus among the phishing research community seems to be that such studies are ethical when conducted with the approval of the appropriate institutional review boards .
This issue has been discussed at research conferences, for example, at a SOUPS 2005 panel "When User Studies Attack: Evaluating Security By Intentionally Attacking Users."
We are also developing a game-based approach to training people to identify phishing websites.
The email-based approach presented in this paper and the game-based training were designed to be complementary.
Our email approach is designed to provide training in the course of normal email usage.
If users are interested in learning more, they can then play our game to gain a more thorough understanding of phishing attacks and ways of identifying phishing websites.
Figure 1: Early iteration of an intervention.
When people clicked on a link in one of our training emails, it would bring up a screenshot  of the web browser showing that the URL they thought they clicked on was not the same as the URL that the web browser would go to .
In this section we describe our rationale for email intervention, the evolution of the design of our embedded training system, the results of an early version of that design, some design goals we derived from evaluating the early design and from related work, and the design of our current interventions.
Our embedded training system works roughly as follows.
People are periodically sent training emails, perhaps from their system administrator or from a training company.
These training emails look just like phishing emails, urging people to go to some website and log in.
If people fall for the training email and click on a link in that email, we provide an intervention that explains that they are at risk for phishing attacks and gives some tips for protecting themselves.
To gain insight into the design space we created and evaluated several prototypes of our embedded training system.
One early design consideration was whether to show interventions immediately after a person had clicked on a training email or after they had tried to log into the website.
Our paper prototypes strongly suggested that showing an intervention after a person had clicked on a link was better, since people who were shown interventions after logging in were confused as to why they were seeing warning messages about the risks of clicking on email links.
We believe this is due to a gap between cause  and effect .
To get a better feel for how well our ideas would work in practice, we created an HTML mockup in Squirrel Mail , a web-based email service.
People who used our system encountered our training emails interspersed with regular email messages.
If they clicked on a link in one of our training emails, they were taken to a separate web page and shown one of two interventions.
The first intervention  showed a screenshot of the email within the web browser itself, pointing out that the link they clicked on was not the same as the link they would actually go to as shown in the status bar.
The second intervention was similar, but told people more directly that the link they clicked on did not take them to the website they intended by showing the brand name itself .
Both interventions also provided text at the top of the image describing why the participants were seeing such a page and informing them that they were at risk of falling for phishing attacks.
We did a pilot evaluation of our design with ten participants, using a variation of the protocol developed by Downs et al .
We asked our participants to role play as an employee at a company and to handle the email in the employee's mailbox the way they normally would.
The employee's mailbox contained nineteen email messages, including a few phishing emails and two training emails.
Nine out of ten participants clicked on our first training message  and saw the information that we presented about phishing.
There are two primary intervention points for an antiphishing training system: email and web.
We chose to focus on email for three reasons.
First, email is the main vector for delivering phishing messages to users.
If we can prevent people from trusting phishing emails, it is likely they will not reach the vast majority of phishing websites.
Second, anti-phishing websites  require end-users to proactively visit them, limiting the number of people who will actually see these websites.
In contrast, our approach brings information to end users and teaches them over time to differentiate between legitimate and illegitimate emails.
Third, end users must already have some knowledge about phishing or other kinds of scams to seek out educational websites.
In contrast, our approach  works for experts as well as novices who are unaware of phishing, by educating end-users immediately after they have made a mistake.
They did not understand why they were sent this email.
Furthermore, most of the participants who viewed the training message did not understand what it was trying to convey.
A common response to the first intervention  was, "I don't know what it is trying to tell me."
Some users understood the training message but were uncertain how to respond as the message did not suggest any specific actions to take.
In debriefing sessions, participants reported that the second intervention was more useful than the first, since they could understand that the website they were visiting was not part of eBay.
Another problem was that people were sometimes confused by the screenshot of the web browser.
Many participants failed to notice the text at the top describing why they were seeing the warning, mostly because the browser screenshot was so large and visually dominating.
A third problem was that people had to scroll to see the entire warning.
Nine users fell for our first phishing email , and seven users fell for the final phishing email , suggesting that this early design was not effective.
Nearly all of the participants that clicked on a phishing link actually tried logging in, suggesting again that it would be better to intervene immediately after a person clicks on a link  rather than after they try to log in.
To develop these two interventions we analyzed 25 online anti-phishing tutorials and selected guidelines that were frequently mentioned, simple enough for people to do, and effective.
For example, some tutorials suggest using networking tools to analyze the age and owner of the domain.
While effective, this is not an easy strategy for the large majority of people.
Rather than attempting to teach people a complicated set of rules for differentiating between safe and unsafe links, we opted to teach them a simple rule, expecting that users would eventually work out their own adaptation of the rule.
The rationale for "Initiate contact" is that it is much safer for people to type in a web address into a web browser on their own or to use a bookmark, rather than trusting a link in an email.
For "Call customer service," the rationale is that many phishing attacks rely on scaring people into logging in to an account.
Calling customer service is a fairly reliable way of determining if there really are any problems with one's account .
We also believe that increasing the number of customer service calls will provide an incentive to companies to take stronger action against phishing, since such calls cost companies money.
Although this seems like an extreme measure, it is also worth noting that no person in our studies actually called customer service.
We argue that this is still a useful piece of advice given that it reminds people that there are offline ways to contact companies.
For "Never give out personal information", the rationale is that companies rarely ask for such information, and the large majority of such requests are phishing attacks.
However, learning science suggests that simply telling people to follow advice is insufficient.
The literature indicates that it is better to present abstract information using concrete examples .
In the text and graphics intervention, we chose to tie our advice to the email that led participants to the warning, by showing a small screenshot of that email and by showing a small screenshot of the web browser address bar.
In the comic strip intervention, we chose to tie our advice to a short story explaining how scammers work and how the reader could do simple things to avoid phishing attacks.
Learning science also suggests that situated learning , where instructions are provided while people are solving a problem, is an effective teaching strategy.
Informed by our early designs, we created two new interventions: a text and graphics intervention and a comic strip intervention.
The text and graphics intervention, shown in Figure 2, describes the risks of phishing, shows a small screenshot of the training email, points out cues that it is a phishing email, and outlines simple actions that users can take to protect themselves.
The comic strip intervention, shown in Figure 3, conveys roughly the same information as the text and graphics intervention, but in a comic strip format.
Our rationale here was that the first intervention had a great deal of text, which might cause people to just close the window without reading it.
Comic strip stories are a highly approachable medium , so we decided to test the effectiveness of a comic strip approach to anti-phishing training.
In the comic strip intervention, we take an alternative approach by situating people in a comic strip story that explains how scammers send phishing emails, how the reader can identify phishing cues, and what they can do if they suspect an email might be fraudulent.
We decided to show the interventions immediately after a person clicks on a link in a training email.
However, rather than taking people to a separate web page, we gray out our training email and display a floating window on top.
Our goal is to reduce confusion and let people know that they are still in the same place.
Both interventions include prominent titles and a cartoon image of a thief to help convey that participants are potentially at risk.
We designed the interventions to be read without requiring any scrolling or clicking on additional links within the interventions.
To view the latest designs please visit http://cups.cs.cmu.edu/trust/et_design.php.
As this research is focused on educating novice users about phishing attacks, we recruited participants with little technical knowledge.
We posted fliers around our university and local neighborhoods, and then screened users through an online survey.
We recruited users who said they had done no more than one of the following: changed preferences or settings in their web browser, created a web page, and helped someone fix a computer problem.
This approach has served as a good filter to recruit non-experts in other studies .
Each participant was randomly placed in one of three groups.
The "notices" group was shown typical security notices, the "text/graphics" group was shown the text and graphics intervention displayed in Figure 2.
The "comic" group was shown the comic strip intervention displayed in Figure 3.
Participants were told that the study investigated "how people effectively manage and use emails."
They were told that they should interact with their email the way they would normally do in their real life.
If a participant was not familiar with Squirrel mail, we gave that participant a quick tutorial describing how to perform simple actions.
We also mentioned that we would be able to answer questions about using Squirrel mail during the study, but we would not be able to help them make any decisions.
We asked participants a few pre-study questions about their use of email to reinforce the idea that this was a study about use of email systems.
We recorded the audio and screen interactions using Camtasia.
We gave participants an information sheet describing the scenario and asked them to read it aloud and ask clarification questions.
The information sheet included the usernames and passwords for Bobby Smith's email account and accounts at Amazon, American Express, Citibank, eBay and PayPal.
We also provided username and password information in a physical wallet that participants could use throughout the study.
Each participant was shown 19 email messages, arranged in a predefined order.
Nine messages were legitimate email messages that Bobby Smith received from co-workers at Cognix, friends and family.
These emails expected Bobby Smith to perform simple tasks such as replying.
Two messages were simulated legitimate emails from organizations with which Bobby Smith had an account.
The mailbox also contained two spam emails, four phishing emails, and two training emails .
Table 2 shows the email distribution shown to the users.
Of the four phishing emails only two of the emails were from organizations where Bobby Smith had an account.
One of these phishing emails was placed before the first training email and the other was placed after the second training email.
We used a 1.40GHz Compaq laptop running Microsoft Windows XP home edition to conduct the user studies.
The participants used Internet Explorer 6.0 for accessing emails through Squirrel mail .
One outlier in the notices group received 300 emails daily, but did not perform particularly better or worse than others in this group.
We found no significant relationship between propensity to fall for phishing attacks before the intervention and any of the demographic information we collected.
Other studies have also found no correlation between these demographics and susceptibility to phishing .
All the phishing, spam, and security notice emails that we used for this study were based on actual emails we had collected.
We created exact replicas of the phishing websites on our local machine by running Apache and modifying the host files in Windows so that IE would display the URL of the actual phishing websites.
All replicated phishing websites were completely functional and allowed people to submit information.
We used a completely functional Squirrel mail implementation for users to access Bobby Smith's email.
We wrote a Perl script to push emails into the Squirrel mail server; and used this script to change the training emails for each group.
After participants finished going through Bobby Smith's emails, we asked them some post-study questions and we debriefed them.
During the debriefing we asked them questions about their choices during the study.
We also showed training messages belonging to a different group than the one they had been placed in for the study.
For example, participants who viewed Figure 2 in their study were shown Figure 3 after the study and vice versa.
They were then asked about their views of both designs.
In this section we present the results of our user study.
In this paper we consider someone to have fallen for a phishing attack if they click on a link in a phishing email, regardless of whether they go on to provide personal information.
Although not everyone who clicks on a phishing link will go on to provide personal information to a website, in our study people who clicked on phishing links provided information 93% of the time.
In addition, clicking on phishing links can be dangerous even if someone does not actually provide personal information to the site because some phishing sites can transmit malware to a user's computer.
When asked why he had done so, the user said, "just because it  was there and I wanted to check what they show."
Most participants liked the way the information was presented; a common comment was: "Having the image and the text with callouts was helpful."
One user told us: "Giving the steps to follow to protect from phishing was helpful."
Another said, "This is definitely useful and good stuff and will remember that ."
There was no difference between the number of participants clicking on links in phishing emails before and after the two security notice messages.
The first security notice users saw was a security message that eBay/PayPal sends to customers.
The email was linked to a real website .
Only five  users in this group clicked on the first security notice link in the email to learn more about phishing attacks.
Among these five participants only two  actually read through the content in the web pages, whereas the other three  skimmed through the content and closed the window.
Nine  participants clicked on the second security notice; this security notice was sent from the system administrator of Cognix.
During the poststudy debriefing we asked whether the notices had been helpful.
The participants who had seen the security notices said the information took too long to read and they were not sure what the messages were trying to convey.
Nine participants  fell for the phishing email before the security notice email and nine participants  fell for the final phishing email.
The mean percentage of participants falling for the three phishing emails presented after the security notices was 63%.
Our results indicate that our comic strip intervention was the most effective in educating people about phishing attacks.
All the participants in this group fell for the first phishing email and also clicked on the training message.
Six participants  clicked on the second training message and only three participants  fell for the final phishing email.
The mean percentage of participants falling for the three phishing emails presented after the interventions was 23%.
Some participants said they preferred the comic to the text/graphics intervention because it engaged them with a story.
However, other participants felt that the text/graphics version was more serious and professional.
One user said, "The comic version is good for children but I would prefer text with the image."
We can see a significant difference in the ability to recognize phishing emails between the notices group and the comic group.
We also compared the effectiveness of security notices against the effectiveness of the text and graphic intervention.
The number of participants falling for phishing attacks before and after training in the notices group was nine , while the number of participants falling for phishing attacks in the text/graphics group was eight  before training and seven  after training.
The difference between these two groups was not as significant  as the difference between the notices and comic groups.
There was significant difference in effectiveness of the two embedded training interventions .
The mean scores across the three phishing emails after intervention was lowest for the comic group.
Figure 4 presents a comparison of the three training methodologies for all the emails that had links in them.
In our post-study questions we asked participants in the comic and text/graphics groups: "Which one  would you prefer and why would you prefer it?"
Nine  of the twenty participants preferred the comic version of the information representation and eleven  preferred the text with graphics version.
In this group eight participants  fell for the first phishing email while all participants clicked on the training message link in the training email.
Seven participants  clicked on the second training message and seven participants  fell for the final phishing email.
The mean percentage of participants falling for the three phishing emails presented after the interventions was 30%.
Many participants checked for whether they had an account with the financial institution before clicking on the link after going through the training message.
Participants seem to identify the Nigerian scam email  easily.
Only two of the thirty participants  clicked on the link in this email.
Only nine participants  actually clicked on the link in the second phishing email, which was ostensibly from a company they did not have an account with.
Among these nine participants, four  realized that they did not have an account with the service once they clicked on the link, and so they closed the window immediately.
Twenty-four  of all the participants were not familiar with the mouse-over technique to see the actual URL before clicking on the link.
Most participants appreciated being taught a technique for identifying the actual link in the emails.
One user said, "I did not know to look for links before , I will do it now."
One user in the text/graphics group did not click on any links in the emails because of his personal experience where he had been a victim of identity theft.
This user stated, "I was a victim of online credit card fraud, so from then on I decided not to click on links in the emails."
No user in the study actually entered random information to test the phishing site's reaction.
Two participants used search engines to help their decision about how to react to an email.
One user Googled the phrase "Bank of Africa" from the Nigerian scam.
Another user said, "I will ask one of my friends to help me make a decision here, she knows about these things better than me."
We plan to further investigate the idea of training users to seek help from external and reliable sources to help them make better decisions.
Among the participants who did not understand the training messages we saw similar behavior as discussed by Dhamija et al.
Novice users use misleading signals  to make their decisions.
For example, one of the participants used the privacy report icon on the phishing website that we created to decide that the website was legitimate.
When asked why he did that, he said: "I do that often to find whether the website is legitimate."
Another participant mentioned that "the logo  is real so the site must be legitimate."
Another participant said, "I visited this website  some days back.
It looks the same as before, so it must be legitimate."
A few other participants were satisfied that the website must be legitimate because it showed updated account information after they entered their personal information.
The repetitive training in a short time span was helpful for some participants.
Some participants did not understand what was going on the first time the training information was presented, but read it carefully the second time.
During the post-study session, we asked specific questions about the training methodology and about the awareness these methods raised about phishing.
One of the questions was: "Did the method create awareness about phishing attacks?"
Only two  participants said the security notices method created awareness about phishing attacks, while in both the other groups all participants  said the method created awareness about phishing attacks.
We also asked participants: "Do you think this method will help you learn techniques to identify false websites and email?"
None of the participants said the security notices would help them, while all of the participants in the other groups thought the embedded training messages would help them.
We also compared data for the individual performance of the participants before and after training.
We observed that 9 out of 10 participants  in the notices group clicked the first phishing email and out of these 8 participants  clicked on the final phishing email.
In the text/graphics group, 8 participants  clicked on the first phishing email out of which 5  clicked on the final phishing email.
In the comic group, 10 participants  clicked on the first phishing email out of which 3 participants  clicked on the final phishing email.
We found that individual performance of participants is significantly different between the notices group and comic group .
Also there was significant difference between the performance of participants in the text/graphics group and the comic group .
There was no significant difference between the performance of participants in the notices group and the text/graphics group.
During the post-study session we also asked the participants: "On a scale of 1 to 7, where 1 is not at all confident and 7 is most confident, how confident were you while making decisions on clicking links and replying to emails?"
As observed in other studies, we saw that novice users use misleading signals to make decisions.
We believe that properly designed training messages and interventions can help novice users to detect and use meaningful signals.
Our results strongly suggest that security notices are not very effective in teaching people about phishing attacks.
We believe this is because people are unclear as to why they are receiving such emails, and because it is difficult for them to relate to an abstract problem that they may not believe is likely to occur.
In addition, some participants claimed that they knew about phishing and knew how to protect themselves, but fell for the phishing scams regardless.
This also suggests that people may be overconfident about what they know, especially if they have seen such security notices in the past, and thus disregard them.
Our results also indicate that our comic strip intervention was most effective.
The primary differences between our two interventions is that the comic strip format has significantly less text and more graphics, and tells a story to convey its message.
We believe that it is worth investigating further to tease out which of these factors are most important, and if other media--such as a short video of a story--might be even more effective.
Based on the results of our low-fidelity prototypes and user studies with our embedded training system, we present some design principles that can be applied to the design of training messages and anti-phishing interventions.
One reason the security notices did not work well was too much text.
Our results suggest that the current practice of sending out security notices is ineffective.
Our results also indicate that both of our embedded training interventions helped teach people about phishing and to avoid phishing attacks, and that our comic strip format was the most effective intervention.
Based on the results of our early prototypes and user studies, we also presented some design principles for teaching people about phishing.
Our results can be put into immediate practice, as they can be implemented easily using current technologies.
We are currently designing a more interactive training system that can adapt to the skill level of participants.
We also plan to deploy and evaluate our system with a wider audience.
This work was supported in part by National Science Foundation under grant CCF-0524189, and by the Army Research Office grant number DAAD19-02-1-0389.
The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the National Science Foundation or the U.S. government.
The authors would like to thank all members of the Supporting Trust Decisions project for their feedback.
In this paper we have presented the design and evaluation of embedded training methods that teach people about phishing during their normal use of email.
From a series of low-fidelity prototypes we drew design criteria that guided the designs of two interventions .
