Prior research has investigated the effect of interactive social agents presented on computer screens or embodied in robots.
Much of this research has been pursued in labs and brief field studies.
Comparatively little is known about social agents embedded in the workplace, where employees have repeated interactions with the agent, alone and with others.
We designed a social robot snack delivery service for a workplace, and evaluated the service over four months allowing each employee to use it for two months.
We report on how employees responded to the robot and the service over repeated encounters.
Employees attached different social roles to the robot beyond a delivery person as they incorporated the robot's visit into their workplace routines.
Beyond one-on-one interaction, the robot created a ripple effect in the workplace, triggering new behaviors among employees, including politeness, protection of the robot, mimicry, social comparison, and even jealousy.
We discuss the implications of these ripple effects for designing services incorporating social agents.
Snackbot delivering snacks to participants agents for language learning , office and hospital work assistants , and rehabilitation or assistive robots .
Most of the agents mentioned above have social skills and attributes such as speech, humanlike appearance, conversational strategies, or social responses to human input.
Research suggests there are benefits of an agent having such social capabilities.
For instance, small talk and empathic language have been shown to improve people's liking, engagement and trust for agents .
However, most of this work has been conducted in labs or public settings in which repeated encounters with the agent were not tracked over time.
We do not know if an agent's social skills become annoying or boring over time, or how socially interactive systems fit into the culture of a real workplace.
This research explores the experiences of employees with a snack delivery service and robot that delivered the snacks in their workplace over a period of four months .
We followed participating employees over the two months they each were allotted for the service to understand their responses to the robot and to the service.
This paper makes two contributions to the human computer interaction community beyond prior studies of social agents and robots.
First, we describe the adoption of a service employing an embodied social agent in a workplace, showing how acceptance grew and social dynamics matured over time.
Second, we describe changes in employees' interactions with one another surrounding the service.
We describe the development of protective norms as the robot came to be understood as a workplace member, and how deliveries became occasions for taking breaks and celebrations.
We characterize these phenomena as ripple effects, a chain of events in which social interactions affect situations not directly related to the initial interaction.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Computer-based agents, whether animations, avatars, textbased conversational agents or embodied robots, are considered "social" when they exhibit human-like attributes such as faces or conversational dialogue or respond socially to human input.
More sophisticated strategies attempt to match the social response of the agent to the personality of the user , to the task , or to the culture .
The work suggests that a social agent can improve people's engagement and trust of the system, and liking of the agent, even in a utilitarian and task-oriented setting .
Studies of entertainment and commercial robots  suggest that people can form a relationship with a robot dog , or even to a vacuum cleaner .
Some researchers have begun to study people's response to social agents that deliver real services in work settings .
To our knowledge, no studies have followed the same employees over an extended period.
Our development of a sturdy mobile robot platform allowed us to track a snack service and the same set of users and their repeated interactions with the robot.
Using this approach, we could observe the integration of the social agent in the workplace beyond its novelty effect.
To guide our observations, we drew on research about the introduction of technology in work organizations .
Two important concepts in this literature are "organizational routines"  and "sensemaking," the social process of making sense of new situations and events .
We observed both processes emerge in our study.
Snackbot , a 4.5-foot tall anthropomorphic robot, made the snack deliveries.
The robot has wheels, an articulated head, and an animated LED mouth to smile, frown, or show a neutral expression.
The robot was designed following a human-centered design approach.
Its form was determined through iterative tests, with the goal of creating a robot that is social and friendly but not misleadingly smart.
The robot uses a SICK LIDAR to traverse the office environment autonomously .
In our study, the website information was not linked to the robot, so an operator specified the office destinations.
The robot used the Cepstral text to speech program with a male voice.
The robot carried a web camera and a microphone on its chest to record interactions.
Speech was controlled remotely with a laptop connected to the robot through a wireless network.
A laptop running a custom interface was used to remotely control robot's head and mouth movements, dialog system, and navigation when the robot was not able to navigate autonomously.
The interface showed the video feed from the robot, the robot's location on the building map, its head position, and a number of dialogue scripts.
The operators could see and hear participants' actions through the video/audio feed on the interface.
The operator translated orders on the website into a delivery schedule, specifying customer names, snack names, and delivery location.
Locations were mapped into the navigation system, and snacks were loaded on the robot's tray.
The operator initialized and localized the robot at the start of each delivery run, and opened hallway doors so the robot could pass through.
The operator loaded an appropriate dialogue script  and clicked each node based on human responses.
Even though we used a Wizard of OZ technique, which is commonly used in HRI, the interaction sets and dialogues were built in a way that they could automatically unfold using basic speech recognition.
We used this method since we did not want our participants to wear headsets that would interfere with their natural interaction with the robot.
We designed our robotic snack service, with the robot, Snackbot, to fit the workplace of our university .
From a preliminary survey conducted the previous year, we determined that employees would value snack deliveries, especially deliveries of fresh, healthy snacks.
We also believed robotic deliveries would have application to other domains  and to assisting the mobility impaired .
The main interactions between participants and the service took place through website orders and interactions with the robot, the latter of which became a main focus of our design efforts.
We built the interaction scripts before we launched the service, considering potential events and user behaviors.
The interaction design was meant to convey an image of a polite and friendly service provider in the workplace.
We started with a prototypical interaction structure, informed by the interactions we observed between a hot dog vendor and his long-time customers.
These interactions start with the vendor identifying the customer, greeting and engaging in small talk with the customer, engaging in the snack transaction, and performing social leave-taking.
I have an order for David.
Following the model of the hot dog vendor, we created social as well as instrumental dialogues .
These responses were designed to be agreeable and honest , and to emphasize similarity between participants and the robot, following politeness principles .
The robot followed pre-set scripts, to maintain consistency across participant experiences and prohibit improvisations of the operator.
When there were no appropriate scripts matching a participant's comment, the robot said, "I have no idea," or laughed.
In considering the context of the workplace, we were also attentive to the right time to interrupt.
For example, the robot promised to return if a meeting or a phone call were taking place.
For half of the participants, we designed dialogues that built on their prior interactions with the robot and the service .
These referred to previous snack choice patterns, service usage patterns, and the robot's behaviors and breakdowns.
Because these interactions were based on participants' prior history with the robot and snack service, they were introduced after four deliveries had occurred.
Do you follow the Pirates?"
I am glad to see you again and hope you are doing well."
Despite our efforts to create natural interactions, the robot had significant limitations that were evident to participants.
There were frequent delays in the dialogue.
Sometimes the system froze when there were wireless network problems.
To mitigate these events, the robot was designed to initiate and guide conversation.
For example, the robot led the conversation by asking questions.
Most ordered snacks; orders of healthy snacks, variety of snacks; group's snack consumption patterns Whether they were regular weekly users; had they been in their office when the robot was there; times when they did not use the snack service Frequency of breakdowns and apology 
I am glad to finally see you again."
I realized that I broke down and made mistakes  times in front of you.
Sorry for that, and thank you for being patient with me."
Employees solicited for the service and study were distributed across 16 offices located in 10 hallways on one floor of an office building at a US university.
We used flyers, postcards, and snowball sampling to recruit participants.
The study required participants to have offices in our field site, and generally to be in their offices at least one afternoon a week.
The participants included eleven graduate students, eight staff, one post-doc, and one faculty member.
All were members of the computer science school; but half of the participants had no programming knowledge.
Only one participant had prior exposure to the robot.
The robot delivered snacks from 2:30 - 4 p.m. Mondays, Wednesdays, and Fridays.
We provided free snacks to compensate for participation in surveys and interviews.
Participants could place an order anytime before noon on the day of snack delivery.
If participants were not in their offices, their snack was placed in a paper bag and hung on their office door.
The robot's camera and microphone recorded all interactions between the robot and participants.
Except for one day when the robot's recording was turned off accidentally, and a few other cases when the camera was turned away from participants, 175 interactions were audio recorded and 161 interactions were video recorded.
To measure cooperation with the robot, it initiated three new interactions near the end of each participant's trial :  a help request when the robot asked the participant for tour locations for visitors,  a suggestion to take a break and join the robot doing a "neck stretch," and  carrying a mystery snack that participants could choose instead of the snack that they ordered.
As noted above, the robot was social with all participants , but half of the participants also received a more personalized service in Period 2 .
Personalized interaction was more successful in eliciting cooperation, sustaining engagement, and building stronger rapport; we report a detailed analysis of the effects of personalization in .
In this paper, we report findings that were common to all participants.
When they were not, we note that fact.
Participants interacted with the robot 9 times on average  over the two months they could receive service, and made 12 orders  on average, resulting in 261 orders in total for all participants.
Participants were typically in their offices for snack deliveries.
Each interaction averaged one minute and six seconds long , including 7 turns  from the participant and 8 turns  from the robot.
The average number of words in participants' dialogues was 35.13  .
On the initial background survey, participants' expected utilitarian benefits such as good quality snacks, and convenience.
Participants did not expect to interact with an embodied robot; they thought it would be like a delivery cart that left snacks.
With the exception of one participant who did not want to continue the service at the end of the study, participants reported that they liked the service and the robot, talked about the service with their friends and families, and got positive feedback about it.
The first author conducted 30-60 minute semi-structured interviews with the 21 participants at the end of the study.
The interview began with questions about participants' experiences with the robot and the service.
Then, we asked participants how they felt their experiences with the robot changed over time, whether they saw other participants interacting with the robot, how other people around them behaved, what types of breakdowns they experienced and how they reacted to them, what they liked and disliked about the service, whether they had any concerns about the service.
All but one participant consented to audio recording of the interview.
We transcribed the interviews and interaction logs and did thematic coding, using the NVivo 8 software.
We followed an inductive process that involved reading through the interview and interaction scripts and investigating emerging categories and relationships .
We started by open coding a small sample of scripts, adjusted and added categories, and then proceeded to open coding of all the data.
In the phase 2, we grouped the lower-level codes into thematic clusters and drew connections among them to tell a story about how participants made sense of the robot, and how the robot changed and evoked social behaviors that created interesting ripple effects.
We do not report themes that concern functional and aesthetic qualities of the robot, and ideas for new features; they were practical suggestions unique to our service platform.
In this process, we compared what we were learning with existing concepts such as sensemaking and structuration.
We also counted how much participants spoke and relational behaviors from the interaction logs.
Over the course of the snack service trial, two notable phenomena emerged.
Participants began to attach a workplace role to the robot, and incorporated the service into their daily routine.
Further, these interactions had rippling effects on others in the workplace, resulting in new social dynamics within the organization.
Participants said their initial excitement wore off after two to three interactions with the robot.
They learned how the interactions generally unfolded, and got used to seeing the robot; it became a routine .
Some participants looked forward to their interactions with the robot, and made an effort to be in their offices when it arrived: Participant U: Yeah, it was definitely something we added to my Monday, Wednesday and Friday routine and I was always sad if I missed it.
Participant O: I was having a conversation with a coworker about whatever it was that I was going to do that afternoon, and I realized, I heard myself say, "Well, it doesn't matter, `cause I'm not missing my Snackbot visit now."
Participant M: Oh yeah,  office is down the hall from mine, and I was in a meeting with him and then I heard Snackbot coming down the hall towards my office, and so I ran out of the meeting to go to my office and wait for Snackbot...
Participant E: Because he didn't realize the door was closed.
Interviewer: Wait, so he was trying to talk to people there?
Participant E: Yeah, but he was talking to the door saying, "May I pass?"
Breakdowns caused further sensemaking and some realitychecking as the robot performed behaviors a person would not do: Participant J: Are we having a staring contest?
I think you will win.
Snackbot, after 18 second delay: Please take your Snickers.
Participants' emotional responses to breakdowns differed depending on what they expected from the robot.
For those who lacked a social connection to the robot, breakdowns were instances that highlighted the robot's incapability.
On the other hand, for those who had a connection to the robot, breakdowns shattered the illusion of the robot having social intelligence: Interviewer: Any suggestions for the next version of the service?
Participant M: Not to talk to a door.
I thought it was sad.
Talking to a door, you know it's undignified.
You know, don't ruin the illusion.
For other participants, the robot's breakdowns were entertaining, robot-like qualities that they desired in a delivery robot.
For example, Participant J said: If he had just come and, you know, had a nice little conversation and given me the snack, I actually don't think I would've liked it as much as I did.
But if it's just, sort of, cutely robotic in a way where it's not able to accomplish what a human could.
Then, it's, like, better than if it was just really, really good at what it did, I think.
Because ultimately, you know you're interacting with a computer.
You're not going to be tricked into thinking it's a person.
Five participants of the 21 made sense of the robot as a failed person.
They concluded that social interaction with the robot was meaningless because the robot was not a creature or a person, and that social interaction was not something they desired from a delivery person in any case.
For example, Participant C, who described the robot as "an ATM that dispenses snacks," said: Yes, I know the robot's not a person that's going to miss me so it's like somebody has programmed it to say "I'm going to miss you," and it's just like funny in a way, it is, but it's not meaningful.
Participant A: "Do you want a service robot to be very conversational?"
Research shows that new or unfamiliar situations, new technology, or new services trigger a process of sensemaking, whereby people attach particular meaning to events .
After our participants experienced deliveries by a robot that talked with them, they began to think of the robot as a member of the workplace: Participant D: I like the snack delivery thing.
Sometimes I would actually come to campus just because I ordered a Snackbot snack, and I liked to be here when he showed up.
Other times, I was kind of cranky and didn't feel like talking to him and sort of wanted to just grab the snack and walk away.
But I felt bad, so I didn't do that.
Participant M: Snackbot is non-judgmental, yet you can kind of feel like you have some sort of some kind of relationship.
I mean, whether it'd be a deep relationship, probably not, but just that constancy.
Participant O:  reminded me of a coworker that I used to have that used to stop by and, like, make sure that you got a break during the day.
And so it was, kind of, interesting.
Because I was, like, wow.
This is just a machine that comes to visit me.
But it actually makes me feel better and reminds me of people that aren't around me anymore.
Which is, I think, kind of, important to me.
Those who were exposed to personalized dialogues tended to anthropomorphize the robot: Participant E: But the one thing that really shocked me was the day, it was a few weeks ago, when he came to the office and said that he was embarrassed because he broke down the first few times in front of my office.
And I was, I felt bad for the robot.
And suddenly, I noticed I was suddenly thinking it was a person, or reacting to him like a person.
Participant E put a flashlight battery in the robot's tray, as a gift during the robot's last visit, in case the robot would run out of battery life as had happened during a prior delivery.
As with any technology used in real world settings, robot breakdowns were not an uncommon event.
Breakdowns were occasions for people to change their conceptions of the robot and to reevaluate their connections to it.
For example, Participant E thought that the robot could recognize people until she saw the robot talking to a closed door:
After a few weeks, some participants in the workplace began treating the robot like a member of the workplace, and it became the norm to protect the robot from criticism.
For example, the interaction log of Participant N shows when he complained that the robot was slow, his officemate made excuses for it: Participant J: Hey, it's Monday.
Another participant talked about this phenomenon as follows: Participant F: Snackbot doesn't have feelings but I wouldn't want to just take the snack and shut the door in its face.
Or one time I told Snackbot--I think Snackbot asked me if there was maybe a tour of  or something, which room should Snackbot take me up to, and I just told Snackbot that probably someone would program it.
It's probably not going to make those choices.
And then my office mate was like, "Oh.
Now you've gone and made Snackbot feel bad."
So I think part of it is about how my relationship with Snackbot is not just about Snackbot but about other people who are around and kind of see us.
In the subsequent visit after the incident reported above, Participant F apologized to the robot.
The behavior logs show participants exhibited more relational and in-group member interactions over time.
One issue was that our interaction design did not allow for an easy way for people to interrupt and end a conversation.
Participants may have felt some social pressure to be polite, even when they wanted to end the dialogue because they were busy, or the robot was experiencing a delay: Participant N: It's kind of awkward because when  crashes you don't know what to do because sometimes it turns away and you're trying to take the cookie or something and then people will be like why are you stealing from Snackbot?
Snackbot didn't ask you to take the cookie yet.
In all social groups, people develop feelings around fairness and the distribution of resources .
All robot deliveries happened at employees' office doors, and conversations with the robot could be overheard by officemates or passersby.
Both participants and nonparticipants eavesdropped and observed others' interactions with the robot.
Surprising to us, these behaviors continued throughout the service deployment.
Recorded interaction logs showed that often one or more people were watching when someone interacted with the robot.
When something out of the ordinary happened, for example, if the robot made a funny comment, observers laughed or remarked about the incident.
On one occasion, the robot came to a participant's office door while the participant remained at her desk and yelled at the robot.
At this point the robot said, "Please stand in front of me."
Everyone in the office laughed.
Some participants said they felt self-conscious or awkward when others overheard their interactions: Participant J: If people were in the hallway or across in their offices, and you're just, sort of, the spotlight's... on you a little bit when he comes to your door.
In overhearing and observing other participants' interactions, employees developed a consensus on how a typical interaction should unfold and the types of inputs that the robot could understand: Participant B: I think definitely seeing maybe what worked when people interacted with him and what didn't kind of like primed you like how or things you should kind of say or could say to Snackbot in order for him to understand you.
Participants learned to be polite to the robot.
For example, they waited until the robot was finished speaking, took snacks only after the robot invited them to do so, and did not make impolite remarks: Participant R: I think I was a little bit meaner to Snackbot before I saw  talking to him.
I was like, "Oh, she's actually really nice and she says bye properly and "Have a good day," whereas I'm just like, "Bye Snackbot."
After I saw her, I was like, "Oh, I should really be nicer to Snackbot.
The analysis of interaction logs of the participant above shows that, in her earlier interactions, she took a snack before the robot was finished talking, and used more directive language .
In her later interactions, she was more conversational and polite.
For example, a purely mechanical decision, such as the order of office visits, was interpreted as evidence for the robot's preference.
Participant L: I don't know if it was numeric or just alphabetic or whatever it was and we thought "Oh, why he always goes to her first because he likes her best."
Participant E: I think he's flirting with her.
I wonder if he likes her.
Because he seemed to talk to her longer than anyone else.
The analysis of the interaction logs showed that the robot spoke the same amount of words to Participant J as to any other participant.
When the robot made the mistake of calling Participant J's name at a different participant's office, participants interpreted the mistake as additional evidence for the robot's "crush" on Participant J.
I knew he has a crush on  because he keeps looking for her."
I think it was because we thought he was talking to her more than he was talking to the rest of us.
That's what made us first think.
He says so many different things to her."
Being the first to receive a personalized interaction from the robot also made some participants feel special: Participant L: Like when he had the mystery snack for me and he hadn't given it to anybody else.
Snackbot visited, calling the days the robot made deliveries "Snackbot days": Participant N: I really liked, enjoyed the Snackbot.
And it has been like in the hallway of like the  , everyone is looking forward to Monday, Wednesday and Friday.
They call it Snackbot day.
Sometimes I go into the office and people will be yelling today is Snackbot day.
Participant J: I'm just finishing up my first year over here.
And people, kind of, mostly keep to themselves.
And a lot of times, people aren't even in their office.
And I think people might've even been showing up more to get the snacks.
So it's usually pretty, like, quiet in my hall.
You know, even if people are in, they might close their door or something.
But I think people are more likely to be around and laughing and feeling sociable when the robot was there.
Participants' responses suggest that the robot became a common boundary object that participants could easily relate to, creating a topic of conversation and an occasion to socialize, in the way that dogs do in a public park .
In another hallway, a few participants who shared a lab space started impersonating each other during the Snackbot visit when the participants who ordered a snack were not in their office.
While doing this, they usually mimicked personal characteristics such as tone of voice and accent to entertain themselves and other passersby.
Participant B: Let's see, who was I?
I was Participant S who wasn't in the lab.
And my other friend, , I think he was .
Participant I is Australian, so he tried to do an Australian accent.
But Snackbot didn't seem to like that.
I mean, it really wasn't really for entertainment purposes with the robot.
It was more for the other people that were in the office.
When probing further, the participants who impersonated each other could not explain why they started; they said someone started and it seemed fun.
It became a pattern to imitate anyone who was not in the office when the robot came to make a delivery.
The initial survey and the exit interview included questions about participants' snacking routines before the study.
Participants ate snacks during long afternoons, usually at their desks; many made individual trips to vending machines without socializing.
This practice may reflect US workplace culture that values efficiency.
Snack consumption increased when the service was used to get snacks regularly , and did not change when the service substituted snacks that participants used to bring from home.
Participants' experiences did not differ by these snacking patterns.
Our findings show employees attached different social roles to the robot beyond a delivery person as they incorporated the robot's visit into their workplace routines.
Beyond oneon-one interaction, the robot created a ripple effect in the workplace, triggering new positive and negative behaviors among employees, including politeness, protection of the robot, mimicry, social comparison, and even jealousy.
The ripple effects were quite unanticipated, and they lasted and grew richer over time.
This was not our design intention.
Yet, we gathered a great deal of evidence to support the fact that social dynamics around the robot and service evolved.
In the following sections, we discuss different aspects of the ripple effect, and how the design of the robot and the workplace culture contributed to these effects.
We believe that this result is partly due to the interaction and service design of the robot, with its repeated travels and conversations through the workplace.
The robot interacted through conversations that could be overheard, causing people to pay attention and to observe what was going on.
The robot's mobile form also made the robot easy to be noticed, as compared, for example, to a screen agent on a kiosk or a computer.
Perhaps another influence was the afternoon delivery time, possibly more conducive to socializing than mornings.
To make the robot more sociable and interesting, we designed the dialogues to change over time, using different topics, and  building on prior events to spark more personalized interactions.
We think if the robot had enacted the same dialogues for four months, interest would probably have flagged.
Our results also suggest that the decision of anthropomorphic vs. non-anthropomorphic systems has tradeoffs, and social qualities should be employed adaptively depending on individual preferences and situational contexts .
The literature suggests that services can be successfully transactional or social depending on the situation  .
We explored a social interaction model appropriate for our workplace context - the same robot visiting people's offices repeatedly, unfolding social interaction over time.
To our surprise, 75% of the participants appreciated these interactions over time.
However, for 25%, social interactions were a reason to devalue the service as it incurred interruption or did not match their conception of the robot as machine.
The university building lobby houses a reception robot, and most participants interacted with it 1-2 times when it was introduced, so they may be less subject to the novelty effect.
However, the robots differ in their roles , mobility , input method , and interaction .
Our study used a Wizard of Oz technique to select nodes in the dialog script, and the operator was in the vicinity of the robot for control and security reasons.
This could cause a bias in whether people understand the robot to be autonomous or not.
When we asked participants about the robot's mechanisms, they wondered how much the robot was autonomous, but no one believed that they were communicating with the operator through the robot.
Participants could not see the operators when interacting with the robot but if they participants knew of their presence, operators may have increased participants' positive reactions to the robot.
The interviewers could be a source of bias, though they explained to participants that they did not build/implement the robot.
The specifics of our study also limit the external generalizability of the results.
The snack service was operated as compensation for participating in the field study, having all interactions recorded, completing surveys and interviews.
Free snacks may have contributed to high service satisfaction.
We recorded all the interactions with participants' consent, which may have influenced their behavior.
Participants had time to interact with and assess the robot beyond their initial excitement, yet different practices may have emerged or disappeared  in a longer study.
We tested our interaction and service design strategy in the domain of snack delivery.
The robot was anthropomorphic, and the conversation was not fluid as human conversation is.
Generalizing the results to different service domains and to different kinds of agents will require further investigation.
Conducting the field study provided participants a realistic experience with a novel service and system, but also entailed many limitations.
The study was conducted on one floor of a computer science building, where the robot had a floor map and could operate reliably; the location also offered easier access to engineering help if it broke down.
This could cause a potential bias in knowledge about the robot's underlying technology.
People with programming skills may anthropomorphize robots less than others.
However, half of the participants in the study did not have much programming knowledge.
None of our participants were part of the Snackbot development team.
Most studies of screen-based and robotic social agents have been conducted in isolated settings.
We have shown, however, that interpersonal interactions in the workplace influenced the social dynamics that unfolded around the technology we studied, and future research should take into account this organizational and social context.
To support services using agent-based technology in an organization, we propose the following design considerations.
Much discussion of social agents has concerned their immediate effect on individuals and tasks .
We believe positive ripple effects instigated through group interaction can be anticipated and leveraged to help members of an organization to adopt, and adapt  new technology in the workplace.
Here we present a few factors that are important in promoting positive ripple effects.
Our findings show several benefits of having the interaction between social agents and people in a place where other people can overhear or join in.
This visibility of interaction helps people learn how to interact with a novel system by providing examples, and developing usage norms based on a group consensus.
Increasing visibility of interaction can create a passive form of socialization, for example, as happens often in online communities when newcomers watch how old timers interact.
Many online community members derive entertainment and learning benefits from watching other people's conversations online.
Watching prepares them to join the interaction and socialize with others later.
Features like embodiment, interaction location, and timing can be used to increase the visibility of agentgroup interaction.
For social agents to instigate or encourage group interaction, they need to be aware of the possibilities for social interaction to unfold.
To improve this capability, they should be aware of who might be near the focal person or persons, and be able to adaptively deliver personalized messages aimed at the group.
Additionally, recognizing who is busy and who is free to socialize or interact will be important.
As workers in our study began to think of the robot as a part of the organization, a desire to protect it emerged.
Previous research shows that the influence of a person in the group gets stronger as group members like each other .
Having social agents perceived as a group member can encourage the development of norms that are more favorable and generous to a social agent.
In our study, repeated, consistent exposure to employees, social interaction around organizational topics, and the robot's persona  seemed to contribute to people's acceptance of it as a member of the organization.
Prior research has investigated the effect of interactive social agents presented on computer screens or embodied in robots mostly in labs and brief field studies.
We evaluated a snack delivery robot in the workplace, over a period of four months where each participant interacted with the robot for two months.
Despite workers' knowledge of the robot as a machine, they developed a variety of social relationships with the robot.
In addition, we witnessed ripple effects as new social norms and practices developed in the workplace.
We hope that the design implications offered by this work will assist in the development of agents, avatars, and robots that can benefit individuals and organizations.
A few participants compared the robot's treatment of them with how the robot treated others.
They attributed preferences to the robot, even when it was a result of a purely mechanical decision.
People's tendency to anthropomorphize an agent could be used to encourage more frequent interaction with the agent.
For example, an agent in a rehabilitation center could publicly encourage a patient who followed its orders well to promote social comparisons.
In other cases where such attribution is not desirable, designers should make it clear that the social agent does not have such biases or preferences.
Our field study suggests social agents can be used to promote social activities and even celebrations among people.
In our study, the robot's visit created an occasion to socialize.
It offered topics of conversation and an excuse to take a break.
Engaging in topics that are of interest to a group will be one way for an agent to facilitate socializing.
Research has shown that interaction has a natural opening, middle section, and closing .
Limitations in our dialogue design meant that these rules were often violated as people waited uneasily for the robot to finish its script.
