However, typically, such help facilities d o not allow users to ask f o l l o w u p questions or request further elaborations when they are not satisfied with the systems' initial offering.
O n e approach to alleviating this p r o b l e m is to present the user with a m e n u of possible follow u p questions at every point.
Limiting f o l l o w u p information requests to choices in a m e n u has m a n y advantages, but there are also a number of issues that must be dealt with in designing such a system.
To dynamically generate useful embedded menus, the system must be able to, a m o n g other things, determine the context of the request, represent and reason about the explanations presented to the user, and limit the number of choices presented in the menu.
This paper discusses such issues in the context of a patient education system that generates a natural language description in which the text is directly manipulable - clicking on portions of the text causes the system to generate m e n u s that can be used to request elaborations and further information.
Keywords: hyper-media, natural language, intelligent systems, user interface c o m p o n e n t s , usability engineering Introduction H e l p facilities play an important role in user acceptance of m a n y systems.
A n important characteristic of a help facility is the ability to allow the user to f o l l o w up on previous requests by asking for f u r t h e r elaborations or posing related questions in the context of the original request.
This illustrates that initial explanations are seldom sufficient and unlikely to satisfy users in real life.
To copy otherwise, or to republish, requires a fee and/or specific permission.
T h e ability to follow u p on information requests becomes even more critical in applications such as medical informatics where misunderstanding could result in serious, unintended consequences.
Unfortunately, handling follow up questions in the f o r m of unrestricted natural language queries is not yet possible.
Using a restricted sub-language of English  as a query language is also problematic, because users find it difficult to r e m e m b e r a circumscribed set of words and phrases they can use f o r expressing their questions .
Interference f r o m other s y n o n y m o u s , natural ways of phrasing questions makes it difficult for users to recall the restricted set of allowable inputs , and thus users find restricted natural languages difficult to learn and frustrating to use .
Moreover, restricted query languages require that users be able to pose well-formulated f o l l o w u p questions.
However, in many cases, users can still pinpoint the source of the problem more precisely than this, while still being unable to formulate a question in the s y s t e m ' s query language.
O n e method of alleviating this problem is to present users with a direct manipulation interface that allows t h e m to point to the portions of system generated information that they do not understand or would like clarified.
In response to the user's pointing action, the system then provides a menu of questions that may be asked about the highlighted text.
By allowing users to point to the text they d o not understand, many of the difficult referential problems in understanding free natural language input can be avoided.
However, for such an interface to be feasible, the system must be able to understand what the user is pointing at, i.e., the system must understand its own explanations.
In this paper, w e describe h o w the use of a plan based model of text generation can support this type of interface.
Today you were seen by Dr. Banks and diagnosed as suffering from migraine.
The most common symptom of migraine is a moderate to severe headache.
IVIigraine patients also frequently experience visual symptoms, nausea, sensitivity to light and noise, as well as confusion.
Your head may feel tender and sore when you have a migraine headache.
Much of the pain In a migraine headache is thought to be due to the stretching of blood vessels In the scalp and the head.
Your symptoms included flashes, light spots, double vision and nausea, which are all consistent with typical cases of migraine.
Migraine is also strongly hereditary.
You report that your mother and sisters have had severe headaches that are similar to yours.
Your family history is a further indication that you suffer from migraine.
You were diagnosed as suffering from migraine only.
Your physical examination was normal, which indicates that more serious causes of headache such bleeding from strokes or tumors are very unlikely.
Because this condition is often related to the level of female hormones, women are more likely than men to develop it.
Migraine attacks become less sevei as you grow older.
However, women who take hormor pills after menopause ^estrogen replacement t h e r a ^ may not get this improvement.
A migraine attack can be triggered by such things as stress, a change in your sleeping habits, or somethinc you ate or drank.
Figure 1: A snapshot of the system with some of the dynamically generated menus.
W e discuss the issues in designing a system to dynamically generate e m b e d d e d m e n u s of follow u p questions, and illustrate t h e m in the context of our patient education system for migraine , T h e system generates descriptions about migraine headaches in English: these contain the d o c t o r ' s diagnosis, r e c o m m e n d a t i o n s and prescriptions, along with a description of migraine headaches, their causes and their implications f o r the patient.
T h e user  can query the system on any aspect of the explanation by pointing at the relevant portion of the text and clicking the mouse.
A s the user m o v e s the m o u s e over the text, the system highlights portions of the text that h a v e f u r t h e r information associated with them.
C l i c k i n g on a mouse-sensitive text span causes the system to generate a m e n u that can be used to request answers to f o l l o w u p queries and further information.
Figure I s h o w s a snapshot of the system.
T h e figure shows the cursor  being m o v e d over the first sentence in the third paragraph, which is highlighted by the system.
Two of the m e n u s generated by clicking on other utterances are also shown in the figure.
In designing our system, w e considered ethnographic analyses of doctor-patient interactions, as well as interviews with patients, to determine their information needs at different times and in different contexts.
Based on these analyses, lists of questions that the patients had asked, or would have liked to h a v e asked were developed.
T h e discourse structures for the explanations generated by the system were analyzed to determine the c o m m u n i c a t i v e goals which in ideal cases would have fulfilled the information needs expressed by the patients.
Based on these analyses, w e developed the heuristics used by the system to generate such questions dynamically.
It is important to keep in mind that although the interface described here bears a resemblance to a hypertext-style interface, the system is not a hypertext system in the traditional sense, i.e., it is not organized as a collection of canned pieces of text interconnected by typed links.
T h e explanations are dynamically generated by the system in response to a u s e r ' s question or the system's need to c o m m u n i c a t e with the user.
A hypertext system would have all of the text prepared a priori, and the user would have to b r o w s e through it to find the information required.
Furthermore, in a hypertext system, all of the things that can be pointed to must be worked out in advance.
It is easy to imagine that users m a y have questions about items in the texts that were not envisioned, and hence not provided for, by the hypertext designers.
Background: The Text Planner The system uses a text planner to generate coherent natural language descriptions: given a top level communicative goal ,' the system finds operators capable of achieving this goal.
Operators typically post further subgoals to be satisfied, and planning continues until primitive speech acts - i.e., those directly realizable in English - are achieved.
In the discourse structure, goals are related to other goals by means of certain coherence relations.
This discourse structure is then passed to a grammar interface, which converts it into a set of inputs suitable for input to a natural language generation system, such as FUF .
Information about the relationships between goals is used by the grammar component to generate connectives and cue phrases, such as 'because,' 'however,' and so on in the final surface form.
Plan operators can be seen as small schemas  which describe how to achieve a goal; they are designed by studying natural language texts and transcripts.
Operators include conditions for their applicability, which can refer to resources such as the knowledge base , the user model, or the context .
A complete description of the generation system is beyond the scope of this paper - see  for more details.
Figure 2 shows a block diagram of the complete system.
Figure 3 shows part of the discourse structure generated by the system for the description in Figure 1.
Generating Follow Up Questions There are three main issues that must be addressed in designing a system to generate menus of follow up questions:  determining the types of text spans that can be queried by the user,  identifying the various sources of information that can be used to generate possible questions about the highlighted text, and  identifying factors that can be used to prune the set of candidate questions generated in  so that the resulting set of questions can be presented to the user as a menu.
This section discusses these three issues in the context of our migraine system.
Figure 2: A block diagram of the system.
Sources of follow up questions Since the system must highlight items that the user can ask questions about , it is necessary to determine a priori the types of text f r o m which the system can generate follow up questions.
Our system allows three types of text constituents to be highlighted, i.e., there are three sources of follow up questions  the entire clause,  the individual constituent clauses of a complex clause, or  the individual objects and entities that the clause refers to.
Selecting simple clauses: When the user highlights a simple clause, the follow up questions generated by the system contain the questions generated for each of the noun phrases in the clause, as well as those resulting f r o m the overall clause itself.
Since simple clauses arise f r o m single speech acts in the discourse structure, the questions about the overall clause depend upon the type of the underlying speech act: INFORM, ASK, RECOMMEND or COMMAND.
TTie system reasons about two types of questions that can be asked about each of these speech acts: why-questions and how-questions.
Since the interpretation of these questions depends upon both the type of the proposition, as well as the type of speech act, not all types of questions can be generated in all cases.
Figure 3: A skeletal view of the discourse structure generated by the text planner.
The speech act INFORIM: T h e INFORM speech act results in an assertion b e i n g generated in the surface form.
Both h o w - and w h y - q u e s t i o n s can be generated in some cases.
For example, if the user highlights the utterance "Ibuprofen is a drug," or "Ibuprofen has analgesic properties", the system can f o r m reasonable why-questions, i.e., "Why is ibuprofen a drug " or '"Why does ibuprofen have analgesic properties?
However, in cases w h e r e the proposition to the speech act involves an action, the system generates both how- and whyquestions.
A s their n a m e s indicate, these speech acts result in the generation of surface level f o r m s that recommend or command the user to u n d e r t a k e s o m e action.
In both of these cases, the user can ask both a w h y - and a how-question.
For instance, if the user happened to point at 'Take cafergot once every day', the system would generate the h o w - and why-questions as follows: Why should I take cafergot once every How should / take cafergot once every day?
The speech act ASK: The fourth type of speech act ASK - causes the surface realization c o m p o n e n t to phrase the proposition in the f o r m of a question to the user.
Questions can be highlighted with the m o u s e j u s t as any other clause the system produces.
The user may wish to understand w h y the system needs to k n o w the answer to its question in order to p e r f o r m its task, i.e., she may wish to ask " W h y are you asking m e this question?".
For instance, the user could point at the question 'Do you have migraines on weekends more frequently than on weekdays?
Note that it is not possible to f o r m a m e a n i n g f u l howquestion for text produced by an ASK speech act.
Generating Menu Entries for Complex Clauses In the most complex case, the user can highlight an entire complex clause, i.e., t w o  clauses related by a coherence relation.
C o m p l e x clauses are generated as a result of realizing multiple adjacent speech acts in the discourse structure.
Consider, for instance, the utterance:  Migraines often get milder as people age, since  h o r m o n e s play a role in m a k i n g the attacks more severe.
If the user highlights the complete utterance, the system generates f o l l o w u p questions f r o m both the simple clauses  and .
D e p e n d i n g u p o n the relation that holds between the text spans, the system can generate either h o w - or why-questions regarding the c o m p l e x clause.
In other words, the user is asking the system for justification for the choice of a particular method  to achieve the goal in question  as opposed to trying some other strategy.
Quite often, the questions generated f r o m a complex clause are the same ones generated f r o m either of its constituent clauses alone.
T h e system prunes duplicate questions f r o m its list while eliminating other irrelevant questions, as discussed later in this section.
Generating additional questions In addition to the heuristics discussed above, the system also generates other questions based on either the previous interaction with the user, or the information in the knowledge base .
For instance, if the system has already presented a description of t w o sibling concepts , and the user points to one of them, the system will generate  the candidate question: "What is the differ"Common side effects are stomach and discomfort"
Figure 4: Discourse structure for describing a drug.
If you c a n take food without vomiting, this may help you tolerate Motrin better.
Call Dr. Rivers if t h e s e effects persist, or if you experience black stools.
These two questions are generated by reasoning about the concept hierarchy in the KB: the goal being achieved is to list the common-side-effects  of Motrin .
Since there are two other sibling relations defined in the k n o w l e d g e base under side-effects - otherside-effects and serious-side-effects - the system generates questions about Motrin for those relations as well.
Pruning the set of carididate questions O n c e the system has generated candidate questions based on the type of speech act, the propositions in the speech act, as well as the discourse structure, the set of possible questions is pruned to remove questions that  have already been asked by the user,  have already been answered, or  may not be relevant based on the discourse context or the user model.
In addition to reasoning about the previous discourse, the system also reasons about the relationships in the KB to find other candidate questions that the user may wish to ask.
We use a structured inheritance network knowledge representation l a n g u a g e called L o o m .
Consider the f r a g m e n t of the discourse structure s h o w n in Figure 4 about the medications prescribed by the doctor: Motrin can help relieve the pain in most patients.
P r u n i n g previously asked questions is possible since the the discourse structure records all utterances, including the questions, in addition to the responses generated.
Thus questions such as "What is migraine?
In the second case, the system attempts to reason about complex clauses to p r u n e those questions generated by one of the constituent clauses that are answered by other constituent clause.
For instance, if the user were to highlight the utterance  'I am trying to reduce the frequency of your migraine attacks by  presoribing Elavil' the system reasons about the discourse structure underlying this utterance.
T h e t w o speech acts that give rise to  and  are related via the relation MEANS .
However, there are a n u m b e r of other issues that play a role in designing effective m e n u systems.
Issues such as the m e n u title, the number and ordering of menu items, their phrasing, and so on all impact on their effectiveness .
Since our efforts have been concentrated on generating the f o l l o w u p questions dynamically, some of these issues, such as the phrasing, and the n u m b e r of menu items, for instance, are not as central to this f r a m e w o r k as they are in the case of static, pre-determined menus.
In our f r a m e w o r k , the phrasing of the generated menu items is automatically consistent with the phrasing of the highlighted text, since they are both generated f r o m the same underlying representation.
These two factors result in m e n u s that are consistent with guidelines developed empirically, e.g., .
In the case of menu titles, it is less clear what w o u l d constitute an effective solution in our case.
T h e range of questions that the system can generate is quite large, and the entities being queried can range f r o m single w o r d s to entire sentences.
Based on the fact that the title is also used as a marker to indicate what the menu is about  , our system uses the highlighted text  being queried as the m e n u title.
Preliminary results f r o m our evaluation show that the users seem to find this acceptable.
Our f r a m e w o r k is well suited for dynamically determining the ordering and aggregation of menu items appropriately.
Studies on ordering menu items have shown that a fixed ordering scheme is better than one in which m e n u items are changed randomly, because people tend to learn with practice .
However, in our case, each m e n u generated is different f r o m the previous ones.
Thus, ordering of the menu items in our system is not constrained by previous presentations, but is determined dynamically, based on the utterance being queried, the previous questions asked, and the constituent parts of the utterance , in decreasing order of importance.
Bach utterance gives rise to s o m e questions that are m o r e closely related to the utterance than other questions that may be generated f r o m its constituents, e.g., the question "What is a symptom?"
However, the system does not attempt to p r u n e all questions w h o s e answers h a v e been been generated previously.
This is because the user m a y not have realized that an answer given in another context applies to the current situation as well.
For instance, if the answer to a question "Why should some drugs be taken after meals ?"
Even though the goal  appears in both the discourse plans, the user may not r e m e m b e r the reasoning in the previous case.
Therefore, in such oases, the system will generate a why-question for the r e c o m m e n d a t i o n 'Take it after f o o d ' in cases of other drugs as well.
In our system, a skeletal model of the user  is available to the system.
T h e information contained in this model is used to p r u n e the candidate set of questions as well.
T h e system orders the questions based on this notion of closeness.
T h e degree of closeness is computed by determining the distance of the different speech acts in the discourse structure  f r o m the speech act w h o s e utterance was highlighted by the user.
This helps ensure that the user can quickly identify the central question.
In general, the average amount of time spent on the system by each patient was 46 minutes, with the minimum being 14 minutes and the maximum being 160 minutes.
The average number of utterances cHcked on by the patients was 11, with the minimum being 1 and the maximum being 29.
The average number of questions asked was 16, ranging from a low of 3 to a high of 45.
Given that some of the patients in our evaluation had never used a mouse-based interface before, it was gratifying that none of them had any problems using this menu based interface.^ The follow up interview also revealed that the users were satisfied with the range of choices that were generated by the system.
They did not indicate any questions that they would have liked to have asked of the system, but were unable to do so.
The consensus was that the system was easy to use and the interface helped them find relevant information easily.
Twelve of the patients also stated that the questions in the menus had helped them "learn things that they would not have asked their doctor."
Preliminary results from our evaluation suggest that users favor this aggregation scheme as compared to aggregating based on the types of questions .
System Evaluation As mentioned previously, our current system is designed in the framework of a patient education system for migraine headaches.
It is implemented in Common Lisp and CUM and can generate a total of about 180 different questions in this domain.
The average number of menu items generated is 4.
In a typical description generated for a migraine patient , there are approximately 50 pieces of text that are mouse sensitive and result in menus being generated.
All the menus are one level deep.
Our system is currently undergoing normative evaluation.
We have had approximately 40 people use the dynamic menu facility to generate feedback on the ease of finding the desired follow up questions.
Based on these results, the heuristics for generating questions were further refined so that the system generated all possible questions in that context while suppressing questions perceived as irrelevant.
This evaluation also served to help us validate our models of titling the menus, as well as ordering and aggregating menu items.
The usability and utility of the migraine system has also been evaluated in two preliminary studies with actual patients suffering from migraine.
In the first study, three patients used the system in the context of an actual visit with a neurologist.
In the second study, thirteen persons with headache and one or more symptoms of migraine interacted with the interactive system without seeing the neurologist.
In both of these studies, the patients were observed using the system, and were also interviewed afterwards regarding their session with the system.
While we recognize that this is an evaluation of patients perceptions, and not a study of outcomes, we nevertheless believe that the results are helpful and encouraging.
Table 1 shows an excerpt of patients' assessment of the system:
It is clear that a more extensive and controlled evaluation is necessary before the actual benefits of such an interface can be determined.
We are in the process of designing such an evaluation.
In future, we plan to extend the range of application domains and evaluate its coverage and effectiveness in allowing the users to request further information or clarify ambiguities or misunderstandings.
Conclusions and Future Work This paper has described our approach to dynamically generating menus of follow up questions in explanatory or help systems.
The ability to handle follow up requests in context is essential in many applications, and can become crucial in situations such as the patient education system described here.
Our approach avoids the problem of natural language understanding and instead adopts a pragmatic  approach of generating choices for the user to select from.
Our initial evaluations of the system reveal that users seem comfortable with the interface as a means of asking follow up questions.
Our test subjects stated that they were satisfied with the range of questions generated by the system and that the interface helped them find relevant information.
Initial surveys suggest that such an interface is helpful to patients, not just in presenting queries to the system, but also in exploring additional, related information that the system possesses about the domain.
This is an important issue, and we are in the process of designing a more extensive and controlled evaluation of the interface.
Acknowiedgements This work was partially supported by grant number ROl LM05299 from the National Library of Medicine, National Institutes of Health.
The ethnographic analyses were carried out by Dr. D. E. Forsythe and Ms. M. Brostoff.
We would also like to acknowledge all the other members of the migraine project: G. Banks, N. Bee, B. Buchanan, G. Carenini, S. Margolis and S. Ohlsson.
