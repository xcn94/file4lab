Such digital artifacts could be a blog post, a self-description or other information distilled, reported or summarized in an online profile.
We use information from these digital artifacts to draw rapid inferences about personal characteristics and expected or anticipated behavior that may guide our future interaction .
A critical situation in which perceptions of digital information matter is when seeking expertise from others.
Technology mediated expertise search is largely about searching amongst strangers since most people will turn first to the people they know to get needed information  and only later use technological tools to seek out experts.
Research on expertise search is beginning to acknowledge that there is a second phase of evaluation over and above selecting the best expert, in which the user determines the likely responsiveness and social context of a short list of candidates .
This makes expertise search a good task for exploring issues of perceptions of information about strangers since there is a clear purpose to the interpretation.
Searching for experts often involves sifting through and making sense of massive amounts of information and making decisions under conditions of uncertainty.
This could be considered a form of sensemaking .
In that context, signaling theory may provide a useful framework regarding how certain information can be more reliable than others in contexts where deception is possible .
In this paper we describe a study that examined how people use the information gained from viewing online profiles to determine the most suitable candidate to contact for help on a topic.
The paper is organized as follows.
We first provide a brief overview of relevant research on systems that locate/recommend experts and technologies for selfpresentation.
We proceed by discussing sensemaking and how signaling theory may inform the sensemaking process.
We then present the results of our study and conclude with a discussion of our results in relation to signaling theory.
Contemporary work increasingly involves interacting with strangers in technology-mediated environments.
In this context, we come to rely on digital artifacts to infer characteristics of other people.
This paper reports the results of a study conducted in a global company that used expertise search as a vehicle for exploring how people interpret a range of information available in online profiles in evaluating whom to interact with for expertise.
Using signaling theory as a conceptual framework, we describe how certain `signals' in various social software are hard to fake, and are thus more reliable indicators of expertise.
Multi-level regression analysis revealed that participation in social software, social connection information, and selfdescribed expertise in the corporate directory were significantly helpful in the decision to contact someone for expertise.
Qualitative analysis provided further insights regarding the interpretations people form of others' expertise from digital artifacts.
We conclude with suggestions on differentiating various types of information available within online profiles and implications for the design of expertise locator/recommender systems.
Below we provide a non-exhaustive review of representative systems .
Implicit recommender systems allow individuals to first look for knowledge in documents, and provide pointers to individuals if contact is needed.
Answer Garden , the Designer Assistant  and PHOAKS  are examples of systems such as these.
They all present relevant information a user searched for, and an email address of the person responsible for the information in case further contact is needed.
On the other hand, social network based expertise recommender systems utilize both expertise information and social connections.
ReferralWeb uses co-authorship data to infer social relationships and presents a referral chain showing the path from the seeker to the expert.
Expertise Recommender mines software source control systems and technical support databases to associate specific individuals to specific software modules.
SmallBlue mines outgoing email and instant messaging transcripts and runs a Google PageRank-like algorithm to associate names with topics, as well as to infer social connections.
Our research aims to add to this body of work by unpacking user behavior related to searching for experts.
Many of these systems attempt to identify the individual that best possesses the expertise sought by a person.
However, we believe that in expertise search, there are other relational factors that need to be taken into consideration.
For example, simply identifying an individual that has the knowledge a person seeks is fruitless unless that person actually responds.
An individual's online activity may allow inferences of such responsiveness.
In this study, our goal is to gain a better understanding of how users weigh different pieces of online information to make inferences regarding the suitability of someone to contact.
We are however increasingly noticing information systems that mine content about us which we may not have any control over .
People search engines such as Spock  and Pipl  aggregate both self-authored and other-authored content and present it to anyone using their systems.
The content presented through these systems could be content we may not want presented.
Vizier & Gosling extend this to the digital world by demonstrating their existence in digital artifacts .
This is related to Erving Goffman's distinction between `expressions given' and `expressions given off' .
The former are the deliberately transmitted messages intending to show how one wants to be perceived, while the latter are much more unintentional.
With the proliferation of various social computing and search technologies and the ease of sharing information through them, a wide range of information can be available about a person that can be used to draw inferences about him.
For example, the impressions formed from looking at selfauthored content such as one's personal homepage may be different from other-authored content such as a blog post about that person.
Although digital artifacts provide unprecedented levels of information about an individual, making sense of such information is not easy.
How do people negotiate perceptions formed through self-authored and otherauthored or machine-authored content?
Do people put more trust in one information content over another?
In this study, we will investigate how people go about the process of making sense of others' based on a collection of digital artifacts.
Participating in social computing technologies afford individuals the ability to perform selective self-presentation and impression management .
Individuals can portray themselves through personal homepages and social networking profiles as they would like to be perceived.
While research on online profiles is clearly emerging, recent findings show that individuals quickly form impressions of personality traits of others from online profiles .
The impressions formed from these profiles also appear to be accurate.
Perceivers' personality trait ratings of Facebook profiles showed some correlation with users' own self ratings and friends' ratings .
However, recent research also shows that there is deception involved in online profiles, raising issues of the credibility of information found online .
Seeking to contact others for expertise using technology involves a set of interconnected cognitive activities, including generating a query, searching for relevant information, evaluating and making sense of information found, and coherently integrating different pieces of information into a coherent whole to arrive at a decision .
Although there is some confusion regarding what exactly constitutes sensemaking , we define it as the process of gathering complex, changing and potentially equivocal information, and comprehending it by connecting nuggets of information from many sources .
People appear to engage in this sensemaking process when looking for experts using technology.
Among the many models of sensemaking, two have been particularly influential in the HCI literature.
Dervin describes sensemaking as a cyclic activity of bridging the `knowledge gap' between the sensemaker's current knowledge and the knowledge needed to successfully accomplish a task .
Information that does not fit within the representation  accumulates and when the cost of ignoring the accumulated residue becomes too high to ignore, a new representation has to be formulated.
For example, an individual looking for an expert may form a certain `representation' of a person after looking at her self described expertise in her profile.
However, after browsing through her online forum posts, her blog posts, and her social tags and bookmarks, a change of representation might occur based on the lack of any mention of the skills she describes in her self reported expertise.
The `cost' of maintaining the original representation, to use Russell et al.
Thus the need arises to forego the old representation and develop a new representation.
The sensemaking claims suggested by Dervin and Russell et al.
We believe that the process through which individuals synthesize information about a person into a coherent whole is a form of `people sensemaking'.
An aim of this study is to demonstrate the use of signaling theory as a decision heuristic in the `people sensemaking' process.
Moreover, many of the sensemaking claims have yet to be tested empirically through field-based studies.
Therefore our research directly contributes to this body of literature.
Physical attributes can also serve as honest signals of quality.
For instance, the massive tail feathers of the male peacock are a signal of quality.
They make the peacock more vulnerable to recognition by predators.
But those males who are able to survive with these bright colors are higher in quality and more desirable .
Anthropologists have suggested that signaling theory as applied to animals can form the basis of systematic study of human signals .
Unfortunately, terminological confusion around what constitutes a signal has hampered the use of this theory.
We illustrate these through an example.
When courting a woman, suitors are advised to communicate qualities that are `costly to fake.'
Qualities such as being tall or owning a luxury car are honest signals since they cannot be easily imitated.
But why do those with less desirable qualities still continue their pursuit?
The `full disclosure' principle of signaling theory argues that competing suitors will pursue the woman, lest their silence be perceived as conceding they have less qualities than they actually do.
The costly to fake principle can be applied to people sensemaking, as we will demonstrate.
Handicap signals are costly to produce and are considered reliable because the quality they signal is `wasted' in the production of the signal, and the signal tends to be more expensive to produce for an individual with less of the quality.
An example of a handicap signal is active participation in online forums.
An employee with over 10,000 forum posts proves that she has enough time to be active in the forum, while still maintaining her job responsibilities.
She is signaling that she is competent enough to balance her job responsibilities and help others.
Index signals are directly related to the trait being advertised.
These are reliable since they require that the sender possesses the relevant trait.
For example, having a high number of positive ratings on the online auction site ebay is an index signal.
Being a good seller is a prerequisite to produce this signal.
This connection between signal and trait makes an index signal reliable.
Handicap and index signals are known together as assessment signals.
Assessment signals relate to the quality represented and one can assess the quality simply by observing the signal .
On the other hand, conventional signals are not correlated with a trait.
Signaling theory provides a useful framework in suggesting which pieces of information are more reliable when making sense of a person's expertise.
Reliable signals are pieces of information that are costly to fake.
Such information allows users to separate the wheat from the chaff by distinguishing between different types of information.
Signaling theory has its origins in both economics and biology.
In the economic view, Spence  describes signals as personal attributes, such as education, that are within the control of an individual.
Employers, lacking direct information about prospective employees' productivity, use signals to improve the chances of hiring productive employees.
Obtaining a degree with honors from an elite university is a signal.
All else being equal, employers are reasonably safe in assuming that a person who has such a degree is both smart and hard-working, since obtaining such a degree would be difficult without a combination of these traits .
In biology, signaling theory has been used to explain seemingly `wasteful'  and detrimental behaviors and ornaments in animals .
Among the frequently cited examples of a costly signal is stotting in gazelles .
When a gazelle notices a predator, it will stot, jumping high into the air on all four legs.
While this reveals the gazelle to the predator,
Because of this, conventional signals are less reliable and open to deception.
For example, it may be desirable to have an attractive picture of oneself on a social networking site such as MySpace.
In the absence of social connections that can vouch for the veracity of such a picture, an individual may choose to put up a deceptive picture.
If the use of such deceptive pictures becomes prevalent, the signal will loose its meaning as an indicator of attractiveness.
Conventional signals are thus unstable because excessive deception can cause a once meaningful signal to turn into noise .
Conventional signals, which are common online, have attracted the most research attention.
Donath looked at signaling in social networking sites such as Friendster and MySpace , where one might potentially artificially inflate the number of friends to appear popular or because of the social pressure to accept friend requests.
However, in the online world, assessment signals could be juxtaposed with conventional signals, albeit to a lesser degree.
Inferred social connection information, as opposed to self-reported social connection information that could potentially be deceptive, may act as an assessment signal of one's sociability.
A contribution of this paper is to look at how both assessment signals and conventional signals are perceived.
Figure 1 displays the plethora of information available in a profile.
Social connection information is displayed at the center of the profile.
This information could be considered an `assessment signal' since it is calculated based on actual communication.
This prevents artificial inflation of one's social network connections.
The system displays 15 paths in descending order of tie strength, with the top path considered as the `recommended path' and remaining ones `alternate paths.'
Basic corporate directory information is displayed on the top right hand side of the profile and includes a person's job title, job description, and geographic location.
This information is entered automatically for every employee.
Within the organization we studied, individuals could selfsubscribe to mailing lists of online communities they wanted to belong to.
Mailing list membership is displayed on the top left hand side of the profile.
The profile also contained pieces of information that are user-generated content and could be utilized for selfpresentation .
These include social tags and bookmarks, blog posts, forum posts, and self described expertise.
The bottom left hand side of Figure 1 shows the top 30 social bookmarking tags of a user, followed by the number of times the tag has been used.
This study was part of a larger study investigating how individuals use expertise locator systems to search for experts within a large geographically distributed organization.
Findings regarding how people make sense of signals in an initial search results page was reported in .
In this paper, we look at the second step of how signals are interpreted when looking at a more detailed profile page.
Understanding perceptions of signals is important since research has shown that perceptions of expertise is more influential than actual expertise in expertise seeking .
Our study was conducted at a global company specializing in information technology products and services.
We used the expertise locator system SmallBlue   in our study.
SmallBlue analyzes the content of outgoing email messages and instant messaging transcripts to infer social connections and expertise.
Users search for individuals with specific expertise by typing in a query term.
The system displays a list of people that it considers to be experts for the entered query term.
The timestamps provide an indication of the recent activity level of a person.
Below the social bookmarks is the `self described expertise' section where employees can describe their skills and the projects they've worked on.
The different pieces of information within a profile could be considered to represent a person's behavioral, social and personal characteristics.
It is worth mentioning that the data aggregated together by SmallBlue presents information "as is" from those sources.
There is no attempt to aggregate the different elements into any kind of metric or weight any one element differently from any other, nor is there any editing of the elements except to limit the number of entries in any one category to fit in the available space.
We should emphasize that this study is not an evaluation of SmallBlue.
It was used because it provided a convenient research platform that allowed us to look at how individuals make sense of various pieces of information within a profile in the context of searching to contact someone for expertise.
You decide to use SmallBlue to find an expert in AJAX to contact."
Due to the geographic spread of participants and to facilitate ease of setup, we conducted this study over the phone.
Conversations were recorded with the permission of participants.
We felt that telephone interviews were an acceptable research method given that it would not be possible to meet with all our participants face to face.
As the participant entered the search term, the researcher would do the same.
The way SmallBlue operates, typing in the same search term returns the same results for everyone.
Once the results appeared, participants were given time to review the set of names.
The researcher then asked which of the 10 experts the participant would like to find more information about.
There was no limit on the number of choices.
On average, a participant considered finding more information about 3 people.
After participants' informed the researcher whom they would like to find more information about, they were asked to go to the profile page of each person they were considering in turn.
After visiting a profile page, participants were told to look carefully over the different information displayed, paying special attention to how helpful the information is in helping him or her decide to hypothetically contact the person.
After a participant told the researcher that she was finished looking over all the information in the profile, the researcher would ask the participant to provide a rating on a scale of 1 to 9  about how helpful each of 7 pieces of information  were in helping her to decide whom to hypothetically contact.
When providing ratings most participants would spontaneously justify the reasons behind their ratings.
Occasionally the researcher would probe participants when they provided particularly high or low ratings.
Once the participant had a chance to look over the profile pages of all the experts she was considering contacting, the researcher would ask the participant to provide a rating on a scale of 1 to 9  of how likely the participant was to contact each expert.
The profiles were available if the participant needed to review them again.
The researcher would then ask the participant to state in her own words her reasons for hypothetically contacting someone as well as not contacting someone.
Finally, the researcher would ask about the number of people in the `recommended path' and `alternate path' since that information is personalized for each user.
The steps of the scenario are illustrated in Figure 2.
It took roughly half an hour to complete the scenario.
Email invitations were sent to 131 employees of the company that had performed at least 20 searches using SmallBlue.
In choosing whom to invite, consideration was given to the geographic location and business unit of invitees to ensure diversity in the participant pool.
At the end, 67 employees from 21 different countries and 9 business units participated, resulting in a response rate of 51.15%.
There were 48 males and 19 females.
Their average tenure at the company was 10.5 years.
A majority of them  were from the business services unit of the company.
Of the participants, majority  reported using the system at least once a month.
Participation in our study was not contingent on frequent use of the system.
We were interested in individuals that had a declared need for searching for people, as demonstrated through voluntarily performing over 20 searches using SmallBlue.
Using scenarios is a widely adopted method for investigating technology use by individuals .
Terveen & McDonald  suggest using scenarios that are specific to the participants' tasks and organizational settings.
Following their recommendations, we had our participants imagine themselves in the following scenario and asked them to try to act as if they are experiencing it in real life.
One of the committee members has remarked that the proposal is making inappropriate use of AJAX to implement a portion of the user interface.
AJAX is a web development technique that enables many of the Web 2.0 style interactions.
In order to determine the effect of participation in social software as a signal, we needed an expertise keyword that would be blogged about, talked about in forums, and bookmarked and tagged.
The AJAX keyword satisfies these criteria in most respects.
Although the data in SmallBlue updates and changes dynamically, the same set of 10 names appeared for all our participants.
The list of top ten experts provided us with an interesting dataset to understand the influence of various pieces of information such as social closeness and participation in social software.
Only nine  of our participants knew at least one expert directly.
The experts also had wide variability in their social software participation.
Figure 3 shows the number of social bookmarking tags, blog and forum posts of each expert.
As can be seen, there is considerable variation among the top ten experts.
In particular, experts in rank 3 and 5 have not participated in social software at all.
It should be noted that the expert rank algorithm does not take into account participation in these different forms of social software.
Ratings of social tags and bookmarks, blog posts, and forum posts were then combined to obtain an average rating for `social software'.
Other ratings were of `social connection info.
We used `AJAX familiarity' as a control variable since we expect people who were more familiar with AJAX would rate experts differently than those who were not.
This was obtained by asking participants to rate their familiarity on a scale of 1 to 5 where 1 = I have not heard of AJAX before, and 5 = I use it regularly.
The average rating was 3.81 with the majority of participants reporting that they had heard of AJAX but had no training in it.
In order to obtain a grounded appreciation of the people sensemaking process, we completely transcribed all audio interviews.
We then coded the reasons behind participants' ratings.
The authors categorized the set of responses independently.
Disagreements were resolved by discussion.
Representative quotes from the themes that emerged in relation to different information are included in the results section.
In order to triangulate our data, we collected both selfreported rating data as well as observed data.
In both cases, our dependent variable was a continuous variable on a scale of 1 to 9  measuring the likelihood of contacting each of the top ten experts that were considered by the participant.
The expert with the highest rating was considered to be the expert that a participant would hypothetically contact.
Observed data was countable raw data within profiles.
This was obtained by asking participants how many people were in between them and the expert in the recommended path on an expert's profile page.
For example, if the participant reported that there were two people in between her and the expert, this was coded as being 3 degrees away.
Since the system only displays connections up to six degrees, the lack of a connection path was coded as the expert being more than six degrees away.
Rating data was the responses each of our participants gave to the question how helpful on a scale of 1 to 9  each of the different pieces of information within the profile were in helping them to decide whom to hypothetically contact.
This was then reverse coded.
They contend that if goodwill is the substance of social capital, its effects flow from the information such goodwill makes available.
For instance, one participant responded: "Once I find somebody, I need to find out first of all what is, how competent are they.
And second of all how benevolent are they.
The act of them sharing gives them a lot of points in my book because it tells me they're willing to um help."
Interestingly, creating goodwill reflects findings of motivations behind participation in user generated content such as social software pretty well.
In a study of Wikipedia contributors, it was found that altruism and benefit to the community were primary motivations for contribution .
Our study lends support to the idea that the same perceptions of altruism might apply to people who actively participate in online forums, blogs, and social bookmarking systems.
In the organization we studied, employees are not paid to blog or participate in forums, and the opportunity cost of such participation leaves employees less time to focus on their primary task.
Yet through such participation, individuals may be signaling that they are more efficient with their time and have the greater good of the community in mind.
Essentially, their `wasteful' activity of participating in social software was a signal of their approachability.
Our participants felt that those who were already sharing their knowledge through social software participation are more likely to respond if contacted.
Out of all the information available in a profile, perhaps social network connection information could be considered the strongest `assessment signal' since it is calculated rather than self-reported.
These paths were honest signals of expertise since an expert would be linked to other experts within a connection chain, something fairly costly to fake.
They also served as instantly recognizable signals of social conduits that could be utilized to facilitate interaction: "...it wouldn't be too much of a cold call to say `hi, I understand you know my colleague so and so, I'm calling you about this other topic.'
I guess it would make me feel more comfortable knowing that I could sort of name drop."
For each point increase in helpfulness of `self described expertise' in the corporate directory, the likelihood of contacting that expert increases by 0.37 points .
The majority of the 10 AJAX experts were software developers, so their `corporate directory' basic job description did not provide much value  since it was the same.
Each of our participants selected three experts, on average, from the initial search result page, to gain further information before deciding whom to contact.
Thus each participant contributed multiple observations, which violates the key assumption of independence of observations in multiple regression.
To account for this, we ran a multi-level regression model with participant ID entered as a random effect.
Results of our analysis are summarized in Table 1.
For each point increase in the perceived helpfulness of `social software', likelihood of contact increased by 0.33 points .
Participants felt that participation in social software provided a signal regarding the likelihood of obtaining a response to a query.
I see that this person is quite open to contact.
I will feel free to just contact him directly."
Yet another participant said: "People who use  or forums are more likely to reach out to the community with their questions and their expertise and therefore I would think they would be more likely to assist in sharing their own expertise."
It appeared that individuals that participated in social software were perceived by others to be creating social capital by sharing their knowledge.
Adler & Kwon refer to social capital as the goodwill engendered by social relations Parameter Intercept Social software Social connection info.
The lack of an adequate expertise description led participants not to contact someone.
The lack of this information created perceptions that a person did not want to be contacted since they did not put any effort into describing their skills.
Finally, `mailing list membership' was not helpful in the decision to contact someone .
Since anyone can subscribe to any mailing list, this information is not costly to fake, and was not perceived as a reliable signal of expertise.
Participants rated experts higher when they were within a few degrees rather than further away.
Each degree increase in `social closeness' corresponds to a 0.29 point increase in likelihood of contact .
The difference of mean `social closeness' of experts contacted and those that were considered but not contacted was significant  = -3.08, p < 0.01.
This finding is consistent with prior field studies of expertise seeking behavior .
We triangulated our findings by running a multi-level regression model on observed data.
While inspecting the scatterplot of the `Social software participation' variable, we noticed it displayed a flattening out pattern.
So its quadratic form in addition to its linear form was entered in the model.
Results of our analysis are summarized in table 2.
It is noteworthy that the results reported in table 1 are purely based on rating data, whereas table 2 reports countable information from profile data.
Posting one more tag, blog, or forum post increased likelihood of contact by 0.01 points.
The range of this variable is 0 to 1100 and the co-efficient value is based on the addition of just one more tag, blog, or forum post.
Additionally, the quadratic form of this variable shows diminishing returns, indicating that after a certain point, participation will not increase likelihood of contact .
This implies that very high social software participation does not necessarily lead to a high rating of likelihood of contact.
This study attempted to introduce signaling theory as a decision heuristic for `people sensemaking.'
There is a lot of confusion around the term sensemaking.
We sought to focus on two models of sensemaking, that of Dervin  and Russell et al.
Dervin uses sensemaking as an activity that bridges a knowledge gap.
At its most fundamental level, signaling theory argues that information that is costly to fake is more reliable.
When seeking to bridge a knowledge gap or organize relevant information, focusing on information that is difficult to fake and consequently more credible, can help people in their sensemaking processes.
In this research, we used signaling theory to frame our thinking on the complex task of searching for experts.
Concepts borrowed from a theory originally developed in biology and economics were brought to bear on the `people sensemaking' process.
Specifically, we were interested in the idea that information that is costly to fake is more credible and should influence how people perceive and utilize different information.
Consistent with signaling theory, the participants in our study put more emphasis on signals that are costly to fake in deciding whom to contact for expertise.
When gauging the expertise of unknown others, the seeker is in a situation of imperfect information.
He or she is unsure of an expert's capabilities and responsiveness.
Our participants articulated their explanations behind relying on social software participation as a signal of approachability and social network data as a signal of accessibility and verifying expertise.
This research adds to the growing body of work on information search.
Instead, it approaches the information search problem from the vantage point of searching for people.
Factors such as familiarity with a person, accessibility, responsiveness, and the opportunity to have an interactive dialog where concerns can be addressed over multiple interactions need to be taken into account.
In document search, these factors do not come into play.
A user can judge whether a document is relevant or not by reading through it.
They need not worry about the relational factors mentioned above.
One of the principle findings in this study was the importance of social software participation.
A design implication that follows from this is to aggregate and display social software participation data in expertise locator systems.
We have not come across many expertise locators that include or perform any systematic analysis on such data.
Recent work has looked at how structural patterns within the social network of an online community can be used to identify `answer people' .
Similarly, systematic analysis of participation in various forms of social software could be used to identify experts that are more likely to respond.
This could be factored into search systems to create a `Page Rank' for experts.
Identifying people with the personality trait of sharing, as demonstrated by high social software participation, could be a useful way to augment expertise locator algorithms that focus on identifying the `best expert'.
A limitation of this study is the artificial scenario that was used.
We looked at a single expertise search keyword to negate any confounding effects of the nature of expertise.
Future work will involve systematically varying the nature of the expertise keyword and determining its effect on whom a person decides to contact.
A study following up on the response of an expert contacted, ensuing interaction and its quality would also be interesting.
With the increase in online activity it becomes more important for users to be able to accurately interpret the signals coming from the digital traces.
In this study we drew on signaling theory to begin to draw a distinction between digital information that is under the direct control of the user and information that is mined from sources that the user does not have as much direct control over.
This distinction, along with the way people interpret the data provides important insights regarding which information one should pay attention to when evaluating numerous pieces of information.
This research reveals the nuances of expertise search by illuminating how individuals successfully make decisions under uncertainty to accomplish the complex task of finding someone to contact.
A contribution of this study is the application of signaling theory to a new context of human communication.
