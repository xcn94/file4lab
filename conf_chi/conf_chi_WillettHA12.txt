Web-based social data analysis tools that rely on public discussion to produce hypotheses or explanations of patterns and trends in data rarely yield high-quality results in practice.
Crowdsourcing offers an alternative approach in which an analyst pays workers to generate such explanations.
Yet, asking workers with varying skills, backgrounds and motivations to simply "Explain why a chart is interesting" can result in irrelevant, unclear or speculative explanations of variable quality.
To address these problems, we contribute seven strategies for improving the quality and diversity of worker-generated explanations.
Our experiments show that using  feature-oriented prompts, providing  good examples, and including  reference gathering,  chart reading, and  annotation subtasks increases the quality of responses by 28% for US workers and 196% for nonUS workers.
Feature-oriented prompts improve explanation quality by 69% to 236% depending on the prompt.
We also show that  pre-annotating charts can focus workers' attention on relevant details, and demonstrate that  generating explanations iteratively increases explanation diversity without increasing worker attrition.
We used our techniques to generate 910 explanations for 16 datasets, and found that 63% were of high quality.
These results demonstrate that paid crowd workers can reliably generate diverse, high-quality explanations that support the analysis of specific datasets.
Outside the lab, in real-world web-based deployments, the vast majority of the visualizations in these social data analysis tools yield very little discussion.
Even fewer visualizations elicit high-quality analytical explanations that are clear, plausible, and relevant to a particular analysis question.
We recently surveyed the Many Eyes website and found that from 2006 to 2010, users published 162,282 datasets but generated only 77,984 visualizations and left just 15,464 comments.
We then randomly sampled 100 of the visualizations containing comments and found that just 11% of the comments included a plausible hypothesis or explanation for the data in the chart.
The low level of commenting may represent a shortage of viewers or may be due to lurking - a common web phenomenon in which visitors explore and read discussions, but do not contribute to them .
When comments do appear, they are often superficial or descriptive rather than explanatory .
Higher-quality analyses sometimes take place off-site  but tend to occur around limited  views of the data curated by a single author.
Ultimately, marshaling the analytic potential of crowds calls for a more systematic approach to social data analysis; one that explicitly encourages users to generate good hypotheses and explanations.
In this paper we show how paid crowd workers can be used to perform the key sensemaking task of generating explanations of data.
We develop an analysis workflow  in which an analyst first selects charts, then uses crowd workers to carry out analysis microtasks and rating microtasks to generate and rate possible explanations of outliers, trends and other features in the data.
Our approach makes it possible to quickly generate large numbers of good candidate explanations like the one in Figure 2c, in which a worker gives several specific policy changes as possible explanations for changes in Iran's oil output.
Such analytical explanations are extremely rare in existing social data analysis systems.
Making sense of large datasets is fundamentally a human process.
While automated data mining tools can find recurring patterns, outliers and other anomalies in data, only people can currently provide the explanations, hypotheses, and insights necessary to make sense of the data .
Social data analysis tools such as Sense.us , Pathfinder  and Many Eyes  address this problem by allowing groups of web-based volunteers to collaboratively explore visualizations, propose hypotheses, and seek out new insights.
Controlled experiments have shown that groups can use these tools to discover new, unexpected findings .
Yet, simply asking workers with varying skills, backgrounds, and motivations to "Explain why a chart is interesting" can result in irrelevant, unclear, or speculative explanations of variable quality.
We present a set of seven strategies that address these problems and improve the quality of workergenerated explanations of data.
Our seven strategies are to:  use feature-oriented prompts,  provide good examples,  include reference gathering subtasks,  include chart reading subtasks,  include annotation subtasks,  use pre-annotated charts, and  elicit explanations iteratively.
While some of these strategies have precedents in other crowdsourcing systems , the main contribution of this work is to demonstrate their impact in the context of collaborative data analysis.
We have applied these strategies to generate 910 explanations from 16 datasets, and found that 63% were of high quality.
We also conducted six experiments to test the strategies in depth.
We find that together our first five strategies  increase the quality ratings  of responses by 28% for US workers and 196% for non-US workers.
Featureoriented prompts  are particularly effective, increasing the number of workers who explain specific chart features by 60%-250% and improving quality by 69%-236% depending on the prompt.
Including chart annotation subtasks  or pre-annotating charts  also improves workers' attention to features.
Additionally, iterative rounds of explanation generation  can produce 71% new explanations without increasing worker attrition.
Finally we show how workers can help analysts identify the best unique explanations - providing quality ratings that correlate strongly with our own and identifying redundant explanations with 72% accuracy.
Our results show that by recruiting paid crowd workers we can reliably generate high-quality hypotheses and explanations, enabling detailed human analyses of large data sets.
Motivated users can visualize, share, and discuss datasets but, as we've noted, few of the visualizations exhibit high-quality analytical discussion.
In fact, many of the commercial websites no longer exist.
Heer and Agrawala  discuss a variety of issues in designing asynchronous social data analysis systems to improve sensemaking.
They suggest that these systems should facilitate division, allocation and integration of analysis work, support communication between workers and provide intrinsic and extrinsic incentives.
Building on these suggestions, Willett et al.
In this work, we further break the task of generating explanations into smaller microtasks in which paid workers explain features of the data and other workers rate those explanations.
With the rise of online labor marketplaces such as Amazon's Mechanical Turk , researchers have focused on the use of paid crowdsourcing to supplement purely computational approaches to problem solving and user testing .
In the context of visualization, recent work has used crowdsourced workers to perform graphical perception experiments on the effectiveness of charts and graphs .
We also pay crowd workers to make judgments about charts and graphs and to provide graphical annotations, but we focus on analytical sensemaking tasks.
Other work has examined how to incorporate human computation into larger workflows.
Soylent  uses paid workers to perform document editing tasks within a word processor, using a Find-Fix-Verify pattern to break editing tasks into smaller subtasks.
Similarly, our workflow helps an analyst break down complex data analysis operations into analysis microtasks that many workers can perform in parallel and rating microtasks that help the analyst consolidate the results of the parallel analyses.
We also take inspiration from CrowdForge , Jabberwocky , TurKit , and Turkomatic  which provide general-purpose programming models for leveraging crowds to perform complex tasks.
An example analysis microtask shows a single chart  along with chart-reading subtasks  an annotation subtask  and a feature-oriented explanation prompt designed to encourage workers to focus on the chart .
A request for outside URLs , encourages workers to check their facts and consider outside information.
Hypothesis  generation is a key step of Pirolli and Card's  sensemaking model and it requires human judgment.
Developing good hypotheses often involves generating a diverse set of candidate explanations based on understanding many different views of the data.
Our techniques allow an analyst to parallelize the sensemaking loop by dividing the work of generating and assessing hypotheses into smaller microtasks and efficiently distributing these microtasks across a large pool of workers.
We propose a four-stage workflow  for crowdsourcing data analysis.
An analyst first selects charts relevant to a specific question they have about the data.
Crowd workers then examine and explain these charts in analysis microtasks.
Optionally, an analyst can ask other workers to review these explanations in rating microtasks.
Finally, the analyst can view the results of the process, sorting and filtering the explanations based on workers' ratings.
The analyst may also choose to iterate the process and add additional rounds of analysis and rating to improve the quality and diversity of explanations.
An example rating microtask showing a single chart  along with explanations  from several workers.
The task contains a chartreading subtask  to help focus workers' attention on the charts and deter scammers, along with controls for rating individual responses , indicating redundant responses , and summarizing responses .
The analyst can then review these charts or post them directly to crowd workers to begin eliciting explanations.
We leave it to future work to build more sophisticated data mining algorithms for chart selection.
For each selected chart, our system creates an analysis microtask asking for a paid crowd worker to explain the visual features within it.
Each microtask contains a single chart and a series of prompts asking the worker to explain and/or annotate aspects of the chart .
The analyst can present each microtask to more than one worker to increase the diversity of responses.
Given a dataset, an analyst must initially select a set of charts for analysis.
The analyst may interactively peruse the data using a visual tool like Tableau  to find charts that raise questions or warrant further explanation.
Alternatively, the analyst may apply data mining techniques  to automatically identify subsets of the data that require further explanation.
In general, our workflow can work with any set of charts and is agnostic to their source.
In our experience, analysts often know a priori that they are interested in understanding specific features of the data such as outliers, strong peaks and valleys, or steep slopes.
If a large number of workers contribute explanations, the analyst may not have the time to read all of them and may instead wish to focus on just the clearest, most plausible or most unique explanations.
In the rating stage the analyst enlists crowd workers to aid in this sorting and filtering process.
Each rating microtask  includes a single chart along with a set of explanations authored by other workers.
Workers rate explanations by assigning each a binary  relevance score based on whether it explains the desired feature of the chart.
Workers also rate the clarity  and plausibility  of each response on a numerical  scale.
We combine these ratings into a numerical quality score  that mea-
Without sufficiently detailed instructions, workers may explain features irrelevant to the analyst.
For example, workers may comment on the visual design of the chart rather than the features of the data.
Refining the prompt to focus on the specific features the analyst is interested in increases the likelihood that workers will provide relevant explanations.
Consider the line charts in Figure 5.
An analyst may be interested in peaks and valleys or steep slopes and flat regions in the oil production chart because such features indicate significant events in the oil market.
Alternatively, the analyst may be interested in longer-term tendencies of the labor market as indicated by the overall trend of the census chart.
For other charts, analysts may be interested in more complex features such as clusters, repeating patterns, and correlations between dimensions.
Analysts can use these scores to quickly assess the quality of responses and quantitatively identify the best explanations.
Workers also mark each redundant response by indicating any other response in the set that provides a better version of the same explanation.
A feature-oriented prompt might ask workers to "explain the peaks and/or valleys in the chart ".
A specific prompt like this can increase the chance that workers will refer to peaks and valleys in their explanations, and also makes it easier for workers to note the absence of these features.
Such negative explanations can be just as informative as explanations of the features themselves.
Workers may not know what typical and atypical charts look like or what kinds of explanations they are expected to produce.
Similarly, they may not know how to identify specific features like peaks or slopes.
To introduce workers to a dataset or feature type, analysis microtasks can include example charts showing several representative views.
Similarly, including example responses may help to establish expectations and calibrate workers to the style and level of detail expected in their response .
In our implementation, analysts can generate examples by selecting a small set of charts  and performing the analysis microtask themselves.
We then package the example charts with the analyst's responses and present them to workers before they begin their first microtask.
To reduce the amount of work an analyst needs to do before launching a new dataset, the examples may come from different datasets analyzed earlier.
However, the data, chart type, and desired features should be similar to the new dataset.
Once workers have generated explanations, the analyst can view the responses and incorporate them into their own analyses.
If the explanations have been rated, the analyst can sort and filter them using the ratings and can hide redundant responses.
For example the analyst may examine only the most plausible, unique explanations.
Optionally, the analyst can examine and organize the results further using a collaborative visualization environment such as CommentSpace .
An analyst may also choose to have workers iterate on a task, generating additional unique explanations or explanations that improve on the best responses from a prior round.
Simply asking workers to look at a chart and explain why it is interesting may not produce good results.
We consider five types of problems that can reduce the quality of these explanations and discuss strategies  designed to mitigate these problems.
For illustration we focus our discussion of the strategies on two time series datasets ; historical data on world oil production by nation from 19652010, and US census counts of workers by profession from 1850-2000.
We consider other datasets later in the paper.
Explanations of data invariably depend on outside information not present in the data itself.
Often interpretations are speculative or based on assumptions from prior experience.
To encourage validation, an analysis microtask can require workers to provide references or links to corroborating information on the web .
Requiring such links may encourage workers to fact-check more speculative answers and may also uncover useful resources that the analyst can use later in the analysis process.
However, asking workers to gather outside references may increase the time and effort associated with a microtask, and may increase worker attrition.
In an effort to increase their payment, workers may proceed quickly through the microtask without thoughtfully considering the prompt.
They may also attempt to scam the task by entering junk responses.
Even well-intentioned workers may not attend to the chart features specified in the instructions.
Such questions force workers to familiarize themselves with the data and can draw attention to important aspects of a particular chart like missing data or a non-zero axis.
Additionally, because "gold standard" answers to such chart reading questions are known a priori, we can automatically check workers' answers and eliminate responses from spammers or workers who do not understand the instructions.
Including such gold standard questions is a well known technique for improving result quality in crowdsourcing tasks .
In our case these questions also help direct workers' attention to chart details.
Requiring workers to visually search for and mark features in the chart can further focus their attention on those details.
For example, the microtask may ask workers to first annotate relevant features of a chart and then explain those features .
Such annotations encourage attention to details and support deixis , allowing workers to ground their explanations by pointing directly to the features they are explaining.
In our implementation each annotation is labeled with a unique letter  so workers can refer to them in their text explanations.
The worker-drawn annotations are also amenable to further computation.
For example, when summarizing responses, a system could aggregate marks from multiple workers to highlight hot spots on a particular chart, or to calculate a collective "best guess" for the overall trend of a time series .
Alternatively, the analyst can pre-annotate visual features in the chart  so that workers pay attention to those details.
Such annotations help focus workers on specific chart details and also reduce irrelevant explanations .
Although preannotating charts greatly reduces the possibility that workers will attempt to explain the wrong feature, creating such annotations may require the analyst to perform additional data mining or manual annotation on the dataset.
We have deployed our crowdsourced data analysis workflow on Amazon's Mechanical Turk and used workers to generate 910 explanations for 64 different charts drawn from 16 different datasets.
Our deployment included the census and oil datasets described earlier, as well as data on world development , economics , and sports .
As a proof-of-concept, we generated a set of 2 to 5 charts for each dataset that exhibited a particular characteristic, such as sharp peaks, valleys or steep slopes.
In some cases we selected charts by hand, while in others we used our data-mining scripts to automatically select the charts.
We  examined and rated all 910 responses generated by workers and scored them using the quality metric described earlier in the Workflow section.
We assigned quality >= 3.5 to 276 of the 435 responses  that used our strategies but were not part of our experiments, indicating that most explanations were very good.
Throughout the deployment, we found that workers consistently generated high-quality explanations for all datasets and provided numerous explanations that we had not previously been aware of.
For example, one worker who examined the US debt dataset suggested that a large spike in British purchases of US debt might be due to Chinese purchases through British brokers.
In another case, five different workers examining a chart of baseball player John Mabry's batting average  independently attributed a prominent valley to a midseason trade that reduced his at-bats.
Other novel insights are shown in Figures 2, 4, and 6.
A full factorial experiment to evaluate all seven strategies would be prohibitively large.
Instead we evaluated the strategies as we developed them.
We first tested five initial strategies  together to gauge their overall impact.
We then examined the effects of S1, S2, and S5 in a factorial experiment.
Based on these results, we added three additional experiments to compare reference gathering , annotation strategies , and iteration .
Finally, we examined the results of our rating microtasks.
Multiple workers may generate similar explanations while leaving the larger space of possible explanations unexplored.
As with other human computation tasks , analysis microtasks can be run in multiple, sequential stages, in which workers see a chart along with the best explanations generated in prior iterations.
To evaluate the cumulative impact of the first five strategies  we had one pool of workers complete analysis microtasks that included all of them  while a second pool completed the same microtasks but without the strategies .
The microtask was preceded by instructions that included three example charts  with annotations and explanations.
The strategies condition also included a reference-gathering subtask  that required workers to provide the URL of a website that corroborated their explanation.
To help safeguard against scammers, we included chart-reading  subtasks in both conditions.
We also asked workers to fill out a demographic questionnaire.
We used both the oil production and US census datasets and selected five charts from each dataset with the largest variance.
All of the resulting charts exhibited a range of features including peaks, valleys, slopes, and large-scale trends.
We collected five explanations for each of the charts.
We also restricted each worker to a single condition  and allowed workers to explain each chart only once, for a maximum of 10 responses per worker.
We paid workers $0.05 per microtask during some early trials, but later increased the pay rate to $0.20 per microtask to reduce completion time.
We based these rates on prior studies  which have shown that while pay rate impacts completion time, it has little impact on response quality.
Over the course of the experiment, 104 different workers produced responses for the 200 microtasks.
To assess how well workers understood the tasks, we  calculated quality scores for each response .
We also analyzed the content of the responses, labeling each one as either an "explanation" if it explained the chart features or a "description" if it simply described the features.
We also examined whether or not each response referred to "peaks or valleys","steep slopes or flat regions", or an "overall trend".
We observed no significant difference in response quality, completion time, or length between the census and oil productions datasets in either worker population, indicating that producing explanations was of similar difficulty across both datasets.
Thus, we combine the results from both datasets in all subsequent analyses.
Response quality was higher for US workers  than for non-US workers   in part because 83% of responses from US workers contained relevant explanations, while only 42% of responses from nonUS workers did so.
Instead, 34% of non-US workers described the chart rather than explaining it, and 24% produced responses that were so poorly written we could not classify them.
The poor performance of non-US workers may reflect their lack of familiarity with the datasets as well as a language barrier.
In our demographic questionnaire, only 35% of non-US workers in the census conditions could accurately describe the US census, versus 100% of US workers.
Less than 20% of non-US workers reported English as their native language, versus 95% of US workers.
However, studies of Mechanical Turk have shown that workers from outside the United States exhibit poorer performance on content analysis  and labeling tasks .
We designed the experiment to determine if a similar performance gap exists for data analysis tasks and whether our strategies could improve results from these workers.
We hypothesized that:  Results from US workers would be of higher quality than results from non-US workers, but  employing strategies S1-S5 would increase the quality of explanations produced by workers in both groups.
Over the course of the experiment, we ran 200 analysis microtasks using Mechanical Turk.
We divided these microtasks into 8 experimental conditions: 2 strategy variants x 2 worker pools x 2 datasets = 8 The microtask in the no-strategies condition asked workers to "explain why any interesting sections of chart might have occurred".
However, the improvement in average quality of responses for non-US workers  was much larger than for US workers .
These results suggest that using strategies S1-S5 makes a bigger difference when workers are culturally unfamiliar with the task and/or dataset.
The introduction of strategies S1-S5 greatly increased workers' attention to peaks and valleys in the data.
Workers in the strategies condition, which included a featureoriented "peaks and valleys" prompt  along with examples  and annotation subtasks  that reinforced the prompt, referred to peaks and valleys very consistently .
Workers in the nostrategies condition, however, referenced very few of these features .
The nostrategies workers often referred to overall trends or slopes in the data or failed to provide an explanation at all.
Across both pools, workers took significantly longer to complete each microtask in the strategies condition  than they did in the no-strategies condition  .
We computed attrition as the percentage of participants who began a microtask but quit without completing it and found an attrition rate of 67% for workers in the strategies condition.
Attrition was 23% in the no-strategies condition.
These results suggest that workers are less willing to complete analysis microtasks that include additional subtasks like chart reading and reference gathering.
Because non-US workers generated such low quality explanations, we used only US workers in our subsequent experiments.
Also, because we saw similar results in Experiment 1 across both the oil production and US census datasets, we used only the census dataset in Experiments 2-5.
We hypothesized that:  Featureoriented prompts  would improve quality by increasing the proportion of responses that explained the specified feature;  Examples  would improve quality, especially when paired with a feature-oriented prompt, by familiarizing workers with the prompt and chart type as well as the expected length, style, and content of good responses;  Annotation subtasks  would encourage workers to mark the prompted feature and thereby improve quality by increasing the number of relevant responses.
In Experiment 2, we ran 160 explanation microtasks divided into 16 conditions: examples annotation  x  x  = 16 variants variants Our 4 prompts included three feature-oriented prompts  prompt-slopes, prompt-trend, and prompt-peaks, and one control prompt, prompt-control.
In the prompt-slopes conditions, we asked workers to "explain why any sharp slopes and/or flat regions in the chart might have occurred", while in the prompt-trend conditions we asked workers to "explain why the overall trend in the chart might have occurred".
The prompt-peaks and prompt-control conditions used the same prompts as the strategies and no-strategies conditions from Experiment 1, respectively.
To test the examples strategy , we included an examples condition that showed workers three examples of highquality explanations and a no-examples conditions that provided only short text instructions.
To test annotation subtasks , we included a worker-annotation condition that required workers to mark features in the charts and a noannotation condition that did not.
For consistency with Experiment 1, we included reference-gathering subtasks  and chart-reading subtasks  in all conditions.
Including a feature-oriented prompt  increased the percentage of responses that referred to that feature by between 60% and 250% compared to the control condition, depending on the feature .
Similarly, including prompts increased response quality by between 69% and 236% compared to prompt-control.
The increase for prompt-slopes  was not quite significant, probably because prompt-control workers were already more likely to explain slopes  than peaks or trends .
Additionally, the median completion time for no-gathering microtasks was only 2 minutes 36 seconds, significantly faster than the 3 minutes 45 second median for gathering tasks , suggesting that while reference gathering produces useful references, it does so at the cost of speed and quality.
Given the low number of trials and high variance, further study is necessary to fully understand this relationship.
Examples also improved the quality and consistency of annotations.
Workers in the workerannotation condition who saw examples of high-quality responses with annotated features, emulated the examples .
Workers who did not see such examples created annotations that were more difficult to interpret and often annotated more features than they explained.
In the worker-annotation condition, workers annotated chart features that were relevant to the prompt in 60 of the 80 trials.
Workers who received a feature-oriented prompt as well as an annotation subtask referred to the feature specified in their prompt more frequently  than workers who received a feature-oriented prompt without an annotation subtask , but the difference was not quite significant .
Many worker-annotation workers also referred to their annotations by letter in their responses, providing clear deictic references to features.
Neither the average time to complete the explanation microtask nor the attrition rate were significantly different between the worker-annotation and no-annotation conditions.
In Experiment 2, we asked workers in all 16 conditions to gather references from the web to support their responses.
Out of the 160 responses, 151 included valid URLs, of which 137 were unique.
We assigned each reference a quality score from 1-5 based on how well it supported the explanation.
In our first two experiments, we found that annotation subtasks  helped workers focus on chart features and facilitated deixis.
In some cases, however, the analyst may wish to pre-annotate charts  to focus workers' attention on specific features.
To compare the trade-offs between these two strategies, we conducted another study with 50 trials split between two conditions - worker-annotation, in which we asked workers to mark the prompted feature before they explained it, and pre-annotation, in which the feature was pre-marked.
We hypothesized that workers in the pre-annotation condition would generate more responses that explained the prompted feature than those in the workerannotation condition.
We found no significant differences between the two conditions.
However the number of response that explained the prompted feature  was high in both the pre-annotation  and worker-annotation  cases.
In 84% of the trials in the worker-annotation condition, workers marked the exact same peak or valley that we had highlighted in the pre-annotation condition, suggesting we shared a common notion of which peaks or valleys were important.
Based on results from Experiments 1 and 2, we hypothesized  that reference gathering  increased response quality, but  the effort required to gather references contributed to high attrition.
To test this hypothesis, we ran an experiment with 50 trials split between two conditions; the gathering condition was identical to the strategies condition in Experiment 1, while the no-gathering condition omitted the reference gathering subtask but was otherwise identical.
In our fifth experiment, we tested whether eliciting explanations iteratively  could improve the diversity of workers' explanations.
First, we asked one group of workers  to generate explanations for a dataset.
After a second group rated these explanations, we asked a third group of workers  to generate additional explanations that were different from the first set.
We hypothesized that  the iteration condition would produce mostly new explanations, but  would have a higher rate of attrition, since later workers might feel unable to author a response that differed from the initial explanations.
We conducted 25 trials in the initial round, producing five explanations each for the five US census charts.
In the iteration round, we conducted 25 more trials, in which we showed new workers the same five charts, along with the initial explanations.
We instructed iteration workers to generate new explanations that were "different from the explanations already shown".
Both conditions included pre-marked charts , but were otherwise identical to the strategies condition in Experiment 1.
The 25 trials in the initial condition produced 36 distinct explanations, while the 25 trials in the iteration condition produced 35 explanations.
Of the iteration explanations, 71% had not been proposed in the first round.
The attrition rate for the iteration condition  was also slightly lower than the attrition rate in the initial round , indicating that iteration can increase the diversity of explanations without increasing attrition.
In order for rating microtasks to provide an effective means of sorting explanations and identifying duplicates, workers must be able to generate consistent ratings.
To test this, we conducted a final experiment in which we asked workers to rate a subset of the explanations generated during our broader deployment.
We hypothesized that  quality ratings assigned by workers would be similar to our own quality ratings and that  workers would consistently detect and label redundant explanations.
Because not all workers' ratings are reliable, an analyst may wish to combine ratings from multiple workers to obtain a more accurate result.
To estimate the effect of using multiple raters, we took the set of responses that had been rated by at least ten raters and repeatedly sampled a subset of the ratings for each response.
For example, to estimate the effectiveness of using three raters, we randomly selected three worker's ratings for each response and used the median of them as the response's quality score.
We then computed the correlation between the median scores and our own quality scores for all responses.
To control for sampling error we randomly sampled and recomputed the correlation 10,000 times for each number of raters .
Using the median score from multiple workers produced results that correlated more strongly with our own - increasing steadily from a moderate correlation  to a strong one .
We tested workers' ability to detect redundant responses by examining the results from the 25 rating microtasks in which we seeded the set of responses with a known redundant explanation.
Across these 25 microtasks, workers connected the known redundant explanation to the explanation on which it was based 72% of the time.
Workers agreed strongly on 35% of the pairs, with all five raters indicating the same redundancy relationship.
We asked 243 Mechanical Turk workers to rate 192 different explanations across 37 charts.
Using the interface shown in Figure 4, workers rated each response according to the criteria  described in the Workflow section.
We compared these ratings against our own quality ratings for the same results.
Workers also indicated redundancy as follows: for each explanation, workers could mark at most one other response as providing a better version of the same explanation.
For each worker, we then formed a redundancy graph with the explanations as the nodes.
We linked two explanations with an undirected edge if the worker marked them as redundant.
To identify groups of redundant explanations we computed the transitive closure of this graph.
Each connected component then represented a unique explanation and all explanations within a component were redundant.
To reduce scamming in the rating microtask we also included one "gold standard" explanation on five of the charts.
We purposely based the content of each "gold standard" explanation on one of the worker-generated explanations, but modified the language to ensure that workers could not identify it as an exact duplicate.
We used these "gold standard" explanation with known redundancy to test whether or not workers could successfully detect redundant explanations.
In total, the workers produced 1,334 individual ratings for the 192 different explanations.
We compared these to our own quality ratings for the same responses.
A Pearson's chi-square test showed very strong agreement  between workers' relevance scores and our own, indicating that workers were good at identifying responses that did not explain the requested feature.
We have demonstrated that paid crowd workers can reliably produce high quality explanations and novel insights.
In fact, in our deployment we found that 63% of the responses we sampled contained good explanations - far more than in tools like Many Eyes.
Moreover, we found that using several basic strategies  can greatly improve explanation quality, particularly when users are unfamiliar with the data.
Because paid crowd workers are readily available and can provide good explanations, these results suggest that we may be able to conduct targeted social data analysis at a much larger scale than was possible in previous systems.
In practice, strategies may only be appropriate in certain circumstances.
For example, reference gathering  is useful if an analyst requires citations or references for their analy-
However, in our experience, reference gathering causes workers to take longer and can reduce the diversity of explanations, since workers cannot pose hypotheses for which they have no references.
Instead, it may be better to make references optional or provide bonuses for good references.
Similarly, while both annotation strategies we tested  improved workers' attention to prompted details, they are useful in different situations.
Annotation subtasks  are more useful when the specific features of interest are not yet known, while pre-annotated charts  are useful for directing workers' attention to more subtle features that are relevant to the analyst, but not obvious to workers.
Finally, our analysis of workers' performance on rating microtasks demonstrates that crowd workers can provide highquality ratings that correlate strongly with our own.
However, using multiple workers produces more accurate ratings.
Redundancy checking subtasks provide a reliable approach for identifying unique explanations, however, quality-control mechanisms such as "gold standard" questions with known responses may be necessary to make certain that workers understand the task.
While crowd workers generated good explanations for the wide range of public-interest datasets we tested, they may fare less well with domain-specific data.
In future work, we plan to apply similar strategies to elicit targeted analysis help from expert communities, volunteers, and enthusiasts.
Our work demonstrates how the key sensemaking task of generating explanations can be broken down and performed systematically by paid workers.
Relying on paid crowd workers rather than ad-hoc volunteers allows us to marshal the analytic power of hundreds of workers in a systematic way.
By packaging simple charts within analysis microtasks and dispatching them en-masse to the crowd, we can solicit large numbers of high-quality explanations much more predictably than we could using existing social data analysis platforms.
Moreover, we demonstrate that using a straightforward set of strategies, we can mitigate common problems such as irrelevant explanations, unclear and speculative worker expectations, and inattention to chart detail.
