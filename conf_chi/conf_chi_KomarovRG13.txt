Online labor markets, such as Amazon's Mechanical Turk , provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings.
However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive.
In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk.
We implemented three performance experiments to re-evaluate three previously well-studied user interface designs.
We conducted each experiment both in lab and online with participants recruited via MTurk.
The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar.
In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments.
These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.
Researchers are drawn to MTurk because the relative ease of recruitment affords larger-scale experimentation , a faster experimental revision cycle, and potentially greater diversity of participants compared to what is typical for lab-based experiments in an academic setting .
The downside of such remote experimentation is that the researchers give up the direct supervision of the participants' behavior and the control over the participants' environments.
In lab-based settings, the direct contact with the experimenter motivates participants to perform as instructed and allows the experimenter to detect and correct any behaviors that might compromise the validity of the data.
Because remote participants may lack the motivation to focus on the task, or may be more exposed to distraction than lab-based participants, concerns about the quality of the data collected in such settings are pervasive .
A variety of interventions and filtering methods have been explored to either motivate participants to perform as instructed or to assess the reliability of the data once it has been collected.
Missing from the literature and practice are methods for remotely conducting performance-based evaluations of user interface innovations -- such as evaluations of novel input methods, interaction techniques, or adaptive interfaces -- where accurate measurements of task completion times are the primary measure of interest.
Such experiments may be difficult to conduct in unsupervised settings for several reasons.
First, poor data quality may be hard to detect: For example, while major outliers caused by a participant taking a phone call in the middle of the experiment are easy to spot, problems such as a systematically slow performance due to a participant watching TV while completing the experimental tasks may not be as easily identifiable.
Second, the most popular quality control mechanisms used in paid crowdsourcing, such as gold standard tasks , verifiable tasks , or checking for output agreement , do not have obvious equivalents in this setting.
In this paper, we investigate the feasibility of conducting remote performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We conducted each experiment both in lab with participants recruited from the local community and online with participants recruited via MTurk.
We focused our investigation on three questions:  Would the data collected in the two settings demonstrate the presence of the same statistically significant differences?
The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings.
All statistically significant differences detected between experiment conditions in lab were also present on MTurk and the effect sizes were very similar.
In addition, there were no significant differences in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments.
These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces.
In summary, in this paper we make the following contributions: 1.
We present results of three experiments conducted on MTurk and in lab.
By showing that the results are closely matched, we build a case for the feasibility of MTurk as a tool for user interface research.
We synthesize a number of practical considerations that had substantial impact on the quality of the data we collected.
These considerations included mechanisms for ensuring instruction comprehension, accounting for age- and device-related differences in performance, techniques for robust outlier detection, and implementation challenges.
In contrast, survey-based research and research collecting simple subjective judgements have been less successful in leveraging MTurk.
A number of researchers reported problems with persistently low-quality data or outright malicious participant behavior .
These results are frequently attributed to two properties of surveys and similar instruments: the truthfulness of responses is hard to verify, and the effort required to provide truthful and accurate responses is substantially higher than just selecting random responses.
This attracts "spammers," who often find that they can earn the payoff easily with little chance of being caught.
To mitigate the data quality problems on MTurk, researchers have investigated several classes of approaches.
Gold standard tasks , instructional manipulation check , the Bayesian Truth Serum , or certain behavioral measures  have all been used to detect low quality data and unreliable participants.
Excluding such data from analysis can substantially improve the reliability and the statistical power of the subsequent analyses .
An ideal solution, however, would prevent low quality data from being collected in the first place.
Data quality can be improved, for example, if participants are forced to see the question for a certain "waiting period" .
However, attempts to manipulate participants' intrinsic motivation have been generally unsuccessful in improving quality of the work .
Financial incentives generally help with faster recruitment and may incentivize participants to do more work, but generally do not impact the quality of the work either  .
So far, we are not aware of any systematic evaluation of MTurk as a platform for performance-based evaluations of user interface technologies.
There are good reasons to believe that such tasks will fare well on MTurk: Turkers' incentives to finish the task as quickly as possible align with the goals of the experimenters.
However, the unsupervised and uncontrolled nature of the participants' environments leaves open the possibility that social and environmental distractions will introduce unacceptable variance or a systematic bias into the data.
In behavioral economics, there is mounting evidence that MTurk participants perform just like lab-based participants.
Replications of a number of classic experiments including those related to judgement and decision-making , public goods , and others , have all shown no differences between results collected on MTurk and those collected in lab.
Similarly, Heer and Bostock  presented compelling evidence that experiments related to graphical perception can be reliably conducted on MTurk.
However, their work also identified several potential pitfalls for experimenters.
For example, variations in hardware and software had significant impact on the results -- keeping careful records of these factors and controlling for them in the analysis overcame the problem.
Also, many participants demonstrated signs of not having understood the instructions, presumably because they hurried through that part of the experiment.
Qualification tasks, which required Turkers to demonstrate proficiency with a concept or skill, proved to be an adequate mechanism for enforcing instruction comprehension.
Novel research has already been published in graphics and information visualization based primarily on data collected on MTurk .
We begin by investigating whether experiments comparing user performance on two or more user interface variants yield the same results both in lab and on MTurk.
Specifically, we ask two questions.
First, are the conclusions of the statistical comparisons of the primary performance measures  between experimental conditions the same in both lab and MTurk settings?
Second, are the effect sizes comparable?
To answer these questions, we conducted -- both in lab and on MTurk -- three experiments evaluating three previously investigated user interface designs.
These designs were the Bubble Cursor , the Split Menus , and the Split Interfaces .
Also, each of the three experiments we chose required a different level of attention from the participants.
The baseline condition in the Bubble Cursor experiment was comprised entirely of simple pointing tasks requiring minimal cognitive engagement.
In contrast, Split Menus and Split Interfaces are adaptive techniques that operate by copying useful functionality to a more convenient location.
To reap the performance benefits afforded by these adaptive mechanisms, the user has to consciously monitor the adaptive part of the user interface to determine if a useful adaptation has taken place.
In prior studies, participants reported that scanning for useful adaptations required conscious effort and most participants missed useful adaptations at least part of the time .
Because concerns about online participants' lack of attentiveness are common , we included the Split Menus and the Split Interfaces as probes of the differences in cognitive engagement between lab- and MTurk-based participants.
We implemented all three experiments for web-based delivery.
We used the same software and the same instructions in both the lab and MTurk settings.
For lab experiments, we recruited between 10 and 14 subjects each.
These numbers were similar to those used in the original studies and were sufficient to demonstrate the main significant differences.
For experiments conducted on MTurk we recruited approximately 100 participants for each experiment taking advantage of the ease of recruitment and the negligible marginal effort required to include additional participants.
Lastly, we sought to establish a clear criterion for identifying and excluding extreme outliers.
A common approach in HCI research is to exclude results that are more than two standard deviations from the mean.
This approach is not robust in the presence of very extreme outliers that are different from the rest of the data by orders of magnitude because such outliers introduce large errors to the estimates of the mean and the standard deviation.
Unfortunately, such outliers, while rare in general, can be expected on MTurk and did, indeed, occur in our data.
For example, one participant spent over 2 minutes on a trivial target selection task that he accomplished previously in 1.5 seconds -- presumably the participant got distracted by an external event in the middle of the experiment.
To guard against such extreme outliers, we chose an outlier removal procedure that relies on a more robust statistic, the inter-quartile range , which is defined as the difference between the third and first quartiles.
With this procedure, an extreme outlier is one that is more than 3  IQR higher than the third quartile, or one that is more than 3  IQR lower than the first quartile .
For normally distributed data, this procedure would remove less than 0.00023% of the data compared to 4.6% removed by the more typical mean  2 standard deviations approach.
Thus, it targets the most extreme outliers without reducing legitimate diversity of the data.
We performed outlier detection based on two measurements: log-transformed per-participant mean selection time and logtransformed per-participant maximum selection time.
The three most recently selected items are copied in the adaptive top part of the menu.
This proved to be both a robust and an acceptably conservative approach.
While 30% exclusion rates are typical for MTurk, and some researchers discarded data from up to half of their participants , our procedure resulted in the exclusion of between 0% and 6% of MTurk participants in our three experiments.
In Split Menus, a small number of menu items that are predicted to be immediately useful to the user are copied1 to a clearly designated adaptive area at the top of the menu .
The items copied to the adaptive area represent the system's best prediction for the user's next selection.
The choice of a predictive algorithm varies across implementations, but the use of the the most recently used  and the most frequently used  algorithms is common.
Whether the Split Menus improve performance depends on the accuracy of the predictive algorithm and on the willingness of the user to pay attention to the adaptive portion of the menu.
In our experiment we used a menu with three categories  each containing 16 items, ordered alphabetically.
The adaptive portion at the top of each category contained the three most recently used items, which were initialized randomly at the beginning of each block.
For each category we generated a random sequence of 60 selections, constrained such that for 79% of the selections the goal item was in the adaptive portion.
The three sequences of 60 selections were then randomly shuffled to obtain the final a sequence of 180 selections.
As the control condition, we used a typical static menu design, which differed from the split menu only in the lack of an adaptive portion at the top.
Figure 2: Task completion times and effect sizes in all three experiments.
Next, they performed four experimental blocks: two 90-selection split-menu blocks and two 90selection static-menu blocks, alternating .
At the beginning of each trial, participants had to click the "Next" button for the prompt to display the next target item.
The experimental UI is shown in Figure 1.
We used a within subjects design with one factor  with two levels .
We analyzed two measures: trial completion time , and number of errors per block.
We recorded an error if a participant clicked on an incorrect item in a menu or clicked on an incorrect category label.
However, the latter proved unnecessary as practically all errors were item errors.
We log-transformed the timing data to account for the skewed distributions found in such data.
We analyzed the timing data using repeated measures ANOVA and we used the Wilcoxon signed-rank test for the error analysis.
There were 6 participants on MTurk who reported having a medical condition or a faulty hardware device that might have interfered with their work, and we discarded their data without further inspection.
Among the remaining 90 participants, 1 was classified as an extreme outlier and was removed from analysis.
In lab, there were no extreme outliers.
The results are summarized in Table 1 and are visualized in Figure 2.
In both settings  we observed the main effect of the menu type  on task completion time.
In both settings, participants were significantly faster with the adaptive split menus than with the static baseline design.
All participants committed fewer errors in the split condition, however the differences were not significant.
There were no substantial or significant differences in the magnitude of the effect size between the two settings .
Split Interfaces are a generalization of Split Menus.
In Split Interfaces, the items predicted to be most immediately useful to the user are copied to specially designated part of the user interface .
Therefore, just as in Split Menus, when the system correctly places the desired item in the adaptive part of the interface the user has the choice to either use the adaptive copy or to access the item at its usual location in the static part of the interface.
Next, they completed a second practice block consisting of 60 selections across all three categories such that in 60% of the selections the adaptive toolbar had a copy of the target.
During this second practice block and in the main experiment, the use of the adaptive toolbar was left entirely to each participant's discretion.
The main experiment consisted of two blocks with 60 selections each.
One of the blocks had predictive accuracy of 70% of containing the target item  and the other had predictive accuracy of 50% .
The ordering of high and low accuracy conditions were counter-balanced across participants.
The same selection sequences were used across all participants.
The 6 most recently used items  are copied to the adaptive toolbar.
We reproduced an experiment, which demonstrated the impact of the accuracy of the predictive algorithm on users' performance .
That is, the experiment compared two otherwise identical variants of a Split Interface , which differed only in their behavior: in one condition the desired item was present in the adaptive part of the interface with a 50% probability and in the other with a 70% probability.
In our experiment, the adaptive part of the interface was placed at the opposite side of the screen from the static part of the interface used during the experiment.
Therefore, to take advantage of the adaptation, users had to consciously direct their gaze to the adaptive part of the interface.
We used a within-subjects design with one factor  with two levels .
We measured two variables: trial completion time , and number of errors per block.
There were two types of errors: when the participant selected a wrong category and when the participant selected a wrong item.
We used the sum of the two in the analysis.
We log-transformed the timing data to account for skewed distributions found in such data.
Timing data were analyzed with a repeated measures ANOVA.
Errors were analyzed with the Wilcoxon signed-rank test.
We used a design with three categories of items for users to select: browsers, file types, and computer devices.
Each category had 9 items arranged in a 3-by-3 square-grid formation.
Each category was represented as a button on a toolbar.
Clicking this button opened up a drop down menu containing all the items in that category.
The adaptive toolbar contained the two most recently selected items per category for a total of six items.
At the beginning of each trial, a copy of the target item, used as the prompt, was displayed in the middle of the screen.
Also located there was a "Next" button used to proceed to the next selection.
Thus, the typical selection trial consisted of clicking the "Next" button, looking at the prompt to learn the appearance of the target, and selecting the target from either the adaptive toolbar or from the drop down menu.
Each participant started with a practice block of 10 selections in which they had to use the adaptive toolbar at least once or otherwise were asked to repeat the block.
There were 6 participant on MTurk whose data was discarded because they reported a medical condition or an unreliable pointing device that might have affected their performance.
There were no extreme outliers on MTurk.
In lab one participant was removed as an extreme outlier, because he was substantially slower than the rest of the participants for a reason we could not determine.
As before, the results are summarized in Table 1 and visualized in Figure 2.
We observed a significant main effect of the predictive accuracy on task completion time both in lab and on MTurk.
In both settings participants were faster in the high accuracy condition, and there was no evidence of a substantial difference in effect size .
In addition, participants in both settings made fewer errors in the high accuracy condition.
On MTurk the difference was significant, while in lab it was not .
In that case, the size of the cursor is reduced and the primary target is decorated with an outline as shown in Figure 4.
The Bubble Cursor takes advantage of the sparsity of the targets to increase their effective sizes.
The more sparsely the targets are distributed, the greater the potential benefits conferred by this design compared to the traditional point cursor.
We reproduced a study comparing the Bubble Cursor  to the point cursor.
In all other blocks, the use of the "bubble" mechanism was used to each participant's discretion.
The practice was followed by three 80-selection blocks ordered randomly for each participant: , , and .
For convenience, we will refer to these conditions as point, b3, b1, respectively.
As in the original version of the experiment, the target was rendered as green disc, the distracters as grey discs, and the bubble cursor as a semi-transparent grey disc.
Both the active target and the distracters were filled with red when the cursor captured them.
We used a within subjects design with one factor  with three levels .
We measured two variables: trial completion time , and the number of errors per block .
We log-transformed the timing data to account for the skewed distributions found in such data.
We analyzed the timing data using repeated measures ANOVA.
For the error data, we used the Friedman test.
Figure 4: Point and Bubble Cursors.
This figure based on .
There were five independent variables in the original Bubble Cursor experiment.
Cursor type CT = {P oint, Bubble}; amplitude A ; target width W ; the ration of effective width to width EW/W , where effective width is the width of the target W plus all surrounding pixels that are closer to it than to any other target ; and distracter density D, which, as also illustrated in Figure 4, is the density of the targets in the 20 degree slice between the starting point and the target.
In our replication of the experiment, we varied only the cursor type CT and the EW/W ratio, while fixing the other variables: W =12px, D=0.5, A=Uniform.
Each participant completed a practice block  consisting of 30 target selections.
Participants were required to use the "bubble" capability of the Bubble Cursor at least once during the practice block  or else they were forced to repeat the practice block.
There were 9 MTurk participants who reported having a medical condition or an unreliable input device that might have interfered with their work, and we discarded their data without further inspection.
From the remaining 114 participants 6 were excluded from the analysis as extreme outliers.
In lab, one participant was excluded because he mistakenly used a touchpad instead of the provided mouse.
As before, the results are summarized in Table 1 and visualized in Figure 2.
We observed the main effect of cursor type on task completion time in both the lab setting and on MTurk, and the pairwise comparison  showed significant difference  between point and b3, and b1 and b3 for both lab and MTurk participants .
In neither setting was the difference between point and b1 significant.
There was no significant difference in effect size between lab and MTurk .
All participants committed the most errors in the point condition, and the fewest in the b3 condition.
The effect of cursor type on error rate was significant in both the lab and the MTurk settings.
Is either population systematically faster, more accurate, or more consistent?
We compare the populations of the lab and MTurk participants in terms of speed, error rates, consistency, and utilization of the novel interactive mechanism presented in each experiment.
We also characterize the demographics of the two populations.
Finally, we investigate the reproducibility of the MTurk-based results.
To account for different distributions of the background variables related to the participants' demographics and environment we used two redundant, parallel methods of analysis, both of which led to the same conclusions.
In the first method, we included gender, input device, computer performance , and age as factors or covariates in the general linear model associated with the ANOVA.
The advantage of this approach is that it leveraged all available data, but as a linear regression model it made two assumptions:  participant performance was linear with age and computer performance and  gender and device had a constant additive effect.
In our second method we removed the need of the first assumption by matching the data in terms of age and computer performance using cutoff values.
Results from both methods are shown in Table 2, in which all reported means  are least-squares adjusted means computed based on the estimated parameters of the linear regression.
To compare the lab and MTurk populations, we captured the following measures: * Mean task completion times.
We used the per-participant standard deviation in task completion times as a measure of how consistently they performed throughout the experiment.
Each of our experiments included a novel interaction mechanism .
In all experiments, the use of these novel interaction mechanisms was designed to improve performance, but was optional and required a small amount of cognitive effort to use.
We define utilization as the fraction of times when the user used the novel interaction mechanisms when one was available.
This measure captures the fraction of participants who were excluded from analysis because they were classified as extreme outliers.
As before, we log-transformed the timing data before analysis.
We used ANOVA to analyze timing, consistency, and utilization data.
We used the  Wilcoxon rank-sum test to analyze error data.
The Bubble Cursor experiment posed additional challenges for the analysis, because computer performance had a significant effect on participant performance.
The experiment was implemented using the HTML5 canvas element, whose performance depends on browser type, browser version, operating system, graphic acceleration, and CPU speed.
Aware of these differences, we implemented an automatic performance check: Turkers whose browsers did not appear capable of redrawing the canvas in 30 ms or less were not allowed to proceed with the experiment.
We determined this threshold as the slowest drawing speed at which the performance of Bubble Cursor appeared smooth and did not register any perceptible lag.
For reference, the computer used in the lab study performed at 6 ms per frame .
The automatic performance check provided only an estimate of actual performance so we also logged the actual drawing performance during the experiment.
In reality, the Turkers' computers performed in the range of 4 to 50 MSPF.
Both analyses indicated that Turkers were slightly faster than lab participants, had lower individual standard deviations, had higher utilization rate, but made more errors.
However, the differences were not significant.
These differences were also relatively small: the differences in task completion times between the two populations were 3.5% in the analysis that included all data and 2.8% in the analysis with matched data.
In comparison, the performance differences between the two experimental conditions  were larger than 8.4%  in the previous section.
In both analyses, Turkers were slower than lab participants, had higher standard deviations, made more errors, and had lower utilization rates.
As before, the differences were not significant.
The differences in task completion times between the two populations were 5.9% in the analysis that included all data and 6.3% in the analysis with matched data.
In comparison, the performance differences between the two experimental conditions  were larger than 13.1%.
In both analyses, Turkers were slower, had higher individual standard deviations, made more errors, and had lower utilization rates, but none of the differences were significant.
The differences in task completion times between the two populations were 3.0% in the analysis that included all data and 4.9% in the analysis with matched data.
In comparison, the performance differences between the two experimental conditions  were larger than 16.4%.
Table 2: Task completion times, individual standard deviations, utilization, and number of errors in all experiments.
Except for errors, all reported values are least-squares means adjusted for device and gender.
Values in italics are additionally adjusted for age and computer performance.
None of the differences are statistically significant.
We repeated the ANOVA analyses using the combined data from all experiments.
The advantage of combining the data from all experiments is the increase in power of the test, making it possible to show statistically significant results for even smaller differences.
For the lab studies we analyzed data from 35 participants aged 18-53, M=26.
Table 3 summarizes the demographics of the two populations.
In lab, the age group 45-65 was underrepresented.
We run all the previously reported MTurk experiments on weekdays in the evenings at 7pm EDT .
The make up of the Turker population is known to change over time.
To investigate if it would have impact on the reproducibility of the results, we rerun the Split Interface experiment on a weekday morning at 11am EDT .
There were, indeed, substantial differences between the populations that participated in the two instances of the experiment: the morning population was 74% female, with median age of 43, and with 63% of the participants using a mouse and 27% using a touchpad.
In contrast, the evening population was only 57% female, with median age of 32, and with 86% of the participants using a mouse and 14% using a touchpad.
These differences had substantial impact on the average performance, but these performance differences became negligible and nonsignificant once age and device were introduced as factors into the analyses.
In all three experiments we have conducted, the data collected in lab and the data collected remotely with the participants recruited via MTurk both resulted in the same conclusions of the statistical comparisons and showed very similar effect sizes.
We also did not observe any significant differences in the absolute measurements of task completion times, error rates, consistency of performance, or the rates of utilization of novel interface mechanisms between the lab and MTurk populations.
Further, the results of the Split Menus and Split Interface experiments show that the Turkers did as well as lab-based participants on tasks where efficient operation of the user interface required some amount of cognitive engagement.
We also found MTurk to be reliable in that when we repeated the same experiment at two different times of the day, we got nearly identical results after correcting for different age and input device distributions.
We also detected very few outliers in our data: only 7 out of 290 participants  had to be removed from the analysis across the three experiments.
This is in contrast to other researchers who ended up discarding up to half of their MTurk-based data .
Although we designed our experiments as within-subjects comparisons, the close match in the absolute values of the measurements between the lab-based and online populations, as well as the lack of significant differences when the same measurements were repeated multiple times online, suggest that between-subjects designs could also be conducted robustly on MTurk.
Table 3: Demographics and pointing device of the final sample of participants, after excluding those classified as extreme outliers and those who reported a medical condition or a faulty device.
These positive results can be explained by the fact that participants were naturally motivated to complete tasks as quickly as possible, which aligned with the goals of the experiments.
However, we cannot discount the possible effects of the novelty factor: a number of the MTurk participants commented spontaneously that our experiments were more enjoyable than typical tasks available on MTurk.
Our results suggest that remote performance evaluations of user interfaces can usefully be conducted with participants recruited via MTurk.
However, because it is a novel methodology that hasn't been broadly validated, a small-scale labbased validation may be a prudent choice for any new experiment.
Because the make up of the MTurk work force changes on an hour-by-hour basis, running all conditions simultaneously is a common design choice that further contributes to the reliability of the results.
Some of the outliers we observed were more than an order of magnitude different from the typical performance.
Such extreme outliers substantially impact the estimates of mean and standard deviation.
Thus, the popular method of excluding values more than 2 standard deviations away from the mean may not be reliable.
Instead, we have used a method based on the inter-quartile ranges, which is much more robust to very extreme outliers .
Some experiments, like the Bubble Cursor evaluation presented in this paper, will be sensitive to the hardware and software performance on the participants' computers.
We both included automatic checks for detecting slow performing hardware/software configurations and we logged the actual time taken to render critical interface updates.
Further, different participants will have different network connectivity.
Standard techniques, such as preloading media, helped ensure that network performance was not a factor during the actual experiments.
Twenty-one Turkers who completed our experiments reported having either a disability or a technical difficulty that might have impacted their performance during the experiment.
We encouraged honest reporting of such problems by assuring participants that it would not affect their eligibility to receive compensation for participating in the study.
While we didn't analyze the data from these participants, it is conceivable that not including them in the analysis contributed positively to the overall high quality of the data and the low outlier rates.
Throughout this research, a number of practical considerations emerged that proved to be important for successful remote experimentation with Turkers.
Although a number of these considerations have already been mentioned in earlier sections, we synthesize them all below.
During our preliminary studies, we saw ample evidence that a fraction of the Turkers did not understand the novel capabilities afforded by the user interface mechanisms we were testing.
For example, some participants always brought the center of the Bubble Cursor over the target, while others never took advantage of the adaptive toolbar in Split Interfaces.
To ensure that all participants understood what each interface afforded, we required that the participants used the novel capability of the UI being tested at least once during the practice block.
If they didn't, they were shown the instructions again and were forced to repeat the practice block.
We made it clear to participants that they were free to use their own judgement whether to take advantage of the novel UI capabilities during the actual experimental blocks.
The comparable utilization rates between lab- and MTurk-based populations suggest that these interventions were effective.
Both age and the input device impact performance on input tasks such as pointing and text entry.
We have performed three distinct experiments both in a controlled laboratory setting and remotely with unsupervised online participants recruited via Amazon's Mechanical Turk .
Repeated measurements performed on MTurk at different times of the day showed changes in the demographics of the MTurk population, but yielded similar results after correcting for different age and input device distributions observed at different times.
These results provide evidence that MTurk can be a useful platform for conducting experiments that require participants to perform largely mechanical user interface tasks and where the primary measures of interest are task completion times and error rates.
We have also highlighted a number of practical challenges -- and our solutions to them -- related to instruction comprehension, accounting for age- and device-related differences in performance, techniques for robust outlier detection, as well as the effects of software, hardware, and network performance on participants' experience.
Compared to lab-based methods, conducting online experiments with participants recruited through micro task labor markets such as Amazon's Mechanical Turk can enable larger-scale experimentation, access to a more diverse subject pool, and a faster experimental revision cycle.
Although this is a novel methodology that has not been broadly validated, we believe it can provide a valuable complement to the existing approaches.
Grossman, T., and Balakrishnan, R. The bubble cursor: enhancing target acquisition by dynamic resizing of the cursor's activation area.
Heer, J., and Bostock, M. Crowdsourcing graphical perception: using mechanical turk to assess visualization design.
The online laboratory: Conducting experiments in a real labor market.
Kapelner, A., and Chandler, D. Preventing Satisficing in Online Surveys: A "Kapcha" to Ensure Higher Quality Data.
Kittur, A., Chi, E. H., and Suh, B. Crowdsourcing user studies with mechanical turk.
In Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, CHI '08, ACM , 453-456.
Kong, N., Heer, J., and Agrawala, M. Perceptual Guidelines for Creating Rectangular Treemaps.
Little, G., Chilton, L., Goldman, M., and Miller, R. Turkit: human computation algorithms on mechanical turk.
In Proceedings of the 23nd annual ACM symposium on User interface software and technology, ACM , 57-66.
Mao, A., Chen, Y., Gajos, K., Parkes, D., Procaccia, A., and Zhang, H. Turkserver: Enabling synchronous and longitudinal online experiments.
Mason, W., and Suri, S. Conducting behavioral research on amazon's mechanical turk.
Mason, W., and Watts, D. Financial incentives and the "performance of crowds".
HCOMP '09: Proceedings of the ACM SIGKDD Workshop on Human Computation .
Noronha, J., Hysen, E., Zhang, H., and Gajos, K. Z. Platemate: Crowdsourcing nutrition analysis from food photographs.
Oleson, D., Sorokin, A., Laughlin, G., Hester, V., Le, J., and Biewald, L. Programmatic gold: Targeted and scalable quality assurance in crowdsourcing.
Oppenheimer, D., Meyvis, T., and Davidenko, N. Instructional manipulation checks: Detecting satisficing to increase statistical power.
Paolacci, G., Chandler, J., and Ipeirotis, P. Running experiments on amazon mechanical turk.
Prelec, D. A Bayesian Truth Serum for Subjective Data.
Rand, D. G. The promise of Mechanical Turk: How online labor markets can help theorists run behavioral experiments.
Rzeszotarski, J. M., and Kittur, A. Instrumenting the crowd: Using implicit behavioral measures to predict task performance.
In Proceedings of the 24th annual ACM symposium on User interface software and technology, UIST '11, ACM .
Schmidt, L. Crowdsourcing for human subjects research.
Sears, A., and Shneiderman, B.
Split menus: effectively using selection frequency to organize menus.
Shaw, A. D., Horton, J. J., and Chen, D. L. Designing incentives for inexpert human raters.
Suri, S., and Watts, D. Cooperation and contagion in networked public goods experiments.
Steven Komarov was supported by the National Science Foundation under fellowship number DGE-1144152.
Katharina Reinecke was supported by the Swiss National Science Foundation under fellowship number PBZHP2-135922.
We thank Mike Terry and the CHI reviewers for helpful feedback on earlier versions of this manuscript.
Callison-Burch, C. Fast, cheap, and creative: evaluating translation quality using amazon's mechanical turk.
In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP '09, Association for Computational Linguistics , 286-295.
Chandler, D., and Kapelner, A.
Breaking monotony with meaning: Motivation in crowdsourcing markets.
Cole, F., Sanik, K., DeCarlo, D., Finkelstein, A., Funkhouser, T., Rusinkiewicz, S., and Singh, M. How well do line drawings depict shape?
Devore, J. Probability and Statistics for Engineering and the Sciences, seventh ed.
Gajos, K. Z., Czerwinski, M., Tan, D. S., and Weld, D. S. Exploring the design space for adaptive graphical user interfaces.
Gajos, K. Z., Everitt, K., Tan, D. S., Czerwinski, M., and Weld, D. S. Predictability and accuracy in adaptive user interfaces.
