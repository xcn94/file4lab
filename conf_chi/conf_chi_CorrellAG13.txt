A valuable task in text visualization is to have viewers make judgments about text that has been annotated .
In this work we look at the ability of viewers to make judgments about the relative quantities of tags in annotated text , and examine design choices that can improve performance at extracting statistical information from these texts.
We find that viewers can efficiently and accurately estimate the proportions of tag levels over a range of situations; however accuracy can be improved through color choice and area adjustments.
The ability to connect between aggregate patterns and specific words is desirable in a number of applications.
For example, a sentiment analysis application may analyze a collection of articles to identify positive and negative remarks about a product.
While one could display the final aggregate totals of this analysis, linking directly to the original text preserves content, context, allows detection of noise and outliers, and affords analysis at multiple scales.
A visualization using tagged text can show what words contribute to the sentiments and allow an analyst to identify where in the texts sentiment inducing remarks occur.
However, these details are only useful if an analyst can still determine the aggregate pattern: are there more positive than negative features?
This paper considers the specific task of estimating the proportion of words that are tagged with a particular class of tag.
We consider the common case where each word is associated with at most one tag, and there are few enough tag categories that each can be associated with a color.
In this case the task would be to estimate the approximate number  of a particular color of tag with respect to all the other colors of tag.
Our experiments seek to determine whether these numerosity judgments can be done efficiently and accurately.
The ability of the visual system to efficiently estimate aggregate properties has been shown repeatedly by the perception literature.
Reports of the ability of people to make approximate judgments of numerosity suggest that tagged text displays may be efficiently and accurately interpreted.
Therefore, we need to understand the performance of viewers at estimating the aggregate proportions of tagged text, and the potential biases that can affect accuracy.
Only with such understanding can we design effective tagged text displays.
In this paper we provide evidence for the ability of viewers to make accurate judgments about numerosity in tagged text, and offer design choices which further improve this ability.
Our work conducted a series of five experiments  that confirm that viewers can make efficient estimations using displays of tagged text, expose biases in those displays, and validate designs that address those biases.
Our experiments show that viewers are able to make numerosity judgments efficiently and accurately for a wide range of stimuli.
However, they also indicate there are certain properties of stimuli that can create biases in estimation.
Text analysis determines properties of collections of texts using techniques ranging from statistical processing to manual annotation.
Some text visualization tools attempt to convey patterns and trends across the entire  corpus.
Showing specific words can inform the viewer as to what textual details contribute to the overall pattern and can help them localize patterns in the larger text.
However, for such tagged text visualizations to be useful, the viewer must still be able to infer the larger trends from the lower level details in specific words.
In this paper, we empirically evaluate this ability of viewers to determine aggregate properties from displays using tagged text, both confirming the viewer's capability to estimate efficiently but also presenting and validating design ideas that address sources of inaccuracy.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Figure 1: A summary of the results of the five experiments reported in this paper.
Together, they suggest that tagged text displays can be a useful presentation of data that accurately conveys the overall proportions of tags while allowing the reader to see the individual words, providing some design guidelines are followed.
By avoiding color schemes with known perceptual issues and using small manipulations in inter-character spacing  we can mitigate these biases and promote good viewer performance even for a wide range of text stimuli.
In this paper we present the results of a series of experiments using crowd-sourced participants to make judgments about tagged text.
The experiments lead to the following conclusions : * Participants were able to make accurate  and efficient  judgments about the relative numbers of tagged words in paragraphs of text for a wide variety of stimuli.
Our work is part of a general trend of translating results from perceptual psychology to specific problems in information visualization.
Therefore we draw upon prior work in both the specific visualization task we consider  as well as the general perceptual research into perception of numerosity and general aggregation.
Large text corpora are often represented abstractly by highdimensional vectors which are then visualized in network graphs or in lower-dimensional spaces via dimensionality reduction .
The dimensions of these vectors are often specific words or tokens in the corpus, creating a one to one link between individual words and high level statistical patterns .
Other text visualization work, especially work dealing with streaming text data, has also incorporated annotated raw text or annotated word clouds into their visualizations .
Since the analyses behind these visualizations operate at the level of individual tokens, tagging can be used to connect higher level properties to specific passages of text.
Our recent research has highlighted the need for high level abstractions to be connected with the original text .
This connection to specific texts becomes even more important when dealing with applications for the humanities, where patterns of word usage and rhetorical style  are the subjects of analysis .
Aggregate statistics can identify different patterns of word usage , but they cannot help explain these differences: this requires close analysis of the texts.
In this paper we present results in the relatively unexplored domain of text annotation .
We present empirical validation of several factors related to performance at numerosity estimation tasks for tagged text, and further validate design choices that improve this ability.
Psychophysical experiments have confirmed that people are capable of efficiently perceiving the counts of small numbers of objects .
Even when the number of items is large there is still evidence for an approximate number system capable of estimating a range of numbers  which is distinct from verbal numerical associations .
The accuracy, precision, and extent of this system is still a matter of conjecture, but it is known that there are several confounding variables that can hurt the performance of people asked to estimate numerosity.
Multiple  models of the effects of these other factors on the approximate number system have been proposed , but there is still some ambiguity as to the relative importance of different visual information on resulting comparative or estimation numerosity tasks.
Stimuli for these studies of numerosity estimation are intentionally simplistic or artificial, to better control for certain channels of visual information .
Recent work extends these results to more complex stimuli for applications in information visualization .
This paper further extends this line of work to the more natural case of text visualization while at the same time identifying factors that influence task performance to inform the design of visual displays.
Figure 2: An example of a visualization that combines direct display of specific text tags with other aggregate displays.
This visualizations is used to connect patterns of usage to high level concepts.
Words are tagged based on rhetorical function and then used to classify different types of texts .
The right-hand side shows some overall statistical information, but only by considering particular passages of text in context were users able to craft arguments about the meaning of these overall patterns.
These tasks may not require exact calculations, but comparative estimates of relative tag counts.
Our collaborators have successfully used these tools to identify specific passages that exemplified larger patterns .
Figure 2 shows an example of a tagged text visualization currently in use: in all cases the high level patterns are important, but require grounding in low level word usage.
Only presenting aggregate statistics in a complementary display is not sufficient to examine the effects of high level patterns at the level of individual text.
The aggregate statistics of word tags can be computed easily and presented visually.
For example a wordle could be computed for each paragraph, or a graph could plot occurrence density over the length of the text.
While such displays may scale to longer texts, they also do not serve all needs.
In practice, displays of aggregate statistics offer complementary advantages to direct display of tags, and are often used together with them .
Methods of presenting and annotating raw text for visualization applications is understudied, partially because the affordances of annotated text are not fully understood.
This work hopes to provide more insight in this area with the idea of motivating further investigation and applications.
We conducted a series of five experiments in which participants were exposed to paragraphs of text where certain words had been given a colored background .
We asked the participants to make judgments about the relative counts of tagged words.
The relative proportion of tagged words of a certain color as a percentage of total tagged words  was used as a proxy for a general class of aggregate judgments.
Using this metric afforded two classes of experimental task: either a forced binary choice task  or an estimation task .
This task is a common one in current tagged text applications, and is also a primitive task in a larger set of quantitative and statistical tasks .
In practice, precise quantification is not always necessary, but sufficiently good estimation is required to make comparative judgments.
The basic experimental design was similar across our five experiments.
After giving consent, participants were shown a brief tutorial explaining the experimental task and emphasizing that the judgments would be about the count of words rather than the length of words.
They were then presented with a set of stimuli in random order.
All the words in the stimuli were made up of random, lowercase letters in 12-point Times New Roman font.
We chose to use random text to both remove possible confounds based on the semantic meanings of words or text, and also to discourage participants from spending too long reading through each paragraph.
This choice of random text also afforded us more control over the exact visual and textual properties of the stimuli, at the expense of task realism.
Our use of random text reserves study of confounds with comprehension for future work.
However, we feel that this tradeoff is important in these initial experiments: the control allows us to explore a larger space to understand a variety of conditions, including distributions both simpler and more extreme than natural .
Tags were represented using filled-color boxes surrounding the words.
Stimuli were 400 pixels wide, leftjustified, and consisted of between 200-300 words depending on the experiment.
We considered two orthogonal dimensions of tagged text: the ratio of tagged words  to the number of words in the paragraph, which we call "tag density", and the ratio of tags of a particular color to the total number of tagged words, or the "mixture level" as discussed above.
For the orangeor-purple color schemes, mixture level is defined as the ratio of specifically orange tags to the number of tagged words in the paragraph.
Figure 3 shows four points in this parameter space.
An important feature of both of these two factors is that they are unaffected by word length, as they deal solely with the numerosity of the tagged words rather than their lengths.
This is not true of a third factor we considered, which we will call "tag area," which we define as the percentage of physical area  encompassed by the tag boxes that corresponds to a particular color of tag.
If words that are systematically longer than the average are grouped in a class their perceived area will be larger, while classes with shorter words will take up comparatively less space.
Figure 3: Example stimuli from our parameter space, varying tag density  and tag mixture .
In real world examples both of these variables are presumably independent of the display choice but an underlying property of the data.
All 210 participants for the five experiments in this study were recruited using Amazon's Mechanical Turk infrastructure, specifically those in North America with at least a 95% approval rating.
The demographics data roughly conforms to the general distribution of North American Turk users : age of participants ranged from 18-65 , with 101 male and 109 female participants.
We followed acknowledged best practices to improve the reliability of our experimental data, including validation questions, randomizing questions, requiring mandatory questions of different input types, and checking for "click-through" behavior .
Despite these measures, we expect responses from crowd-sourced participants to have higher variance than inperson results.
We take positive results from crowd-sourcing as indication that the relevant effects are robust.
Our first question was to measure the general task performance and to confirm that this holds over a range of situations.
Our hypothesis, based on previous experiments with the perception of aggregate statistics, is that people can accurately estimate the relative numerosity of tagged text.
For our initial experiments with robustness, we chose mixture level and tag density.
We had no hypotheses about the effect of mixture or tag density on performance, but we still wished to confirm the null hypothesis in the general case to afford grounded manipulations of text stimuli in later experiments.
To test these three hypotheses, we performed an experiment that measured performance over a range of mixture levels and tag densities.
Our stimuli were 200 random words consisting of five random characters each.
20 participants were each presented with 36 paragraphs of text at different levels of tag density .
Performance was measured by the absolute difference of the participant's guess from the true distribution.
These validation stimuli were used as engagement checks and to verify that participants had properly understood the instructions, and were not considered in further analysis.
No participants needed to be excluded based on the validation stimuli.
To assess our first hypothesis that people can accurately estimate the relative numerosity of tagged text, we consider the error level across all conditions.
Here, we take the absolute difference between the presented mixture level and the participant reported level.
To test our hypotheses about robustness across conditions, we performed a two-way analysis of variance  to test whether tag density and mixture level affected absolute error.
Our results found no significant effect of tag density on performance  = 1.40, p = 0.20 but did find an effect of mixture on performance  = 5.66, p < .0001: in particular a post hoc comparison of Tukey's Honest Significant Difference  found that the absolute error was significantly lower for mixtures of where one color was only 10% of the total number of tags, and where there were equal counts of both colors of tag.
For example, at 10% mixture, the average error was approximately 1%, whereas the error across all conditions was closer to 6%.
Some of this difference may be due to the natural bias that arise when scales with midpoints are used in human subjects research .
A closer analysis of the patterns of response using confidence intervals at the  = 0.05 level of significance shows that, when given a slider input to choose the appropriate mixture level, participants were statistically more likely to choose the extremes and the midpoints of the scale than other responses .
This effect of mixture was consistent across all experiments with slider inputs .
We generated stimuli at two density levels, 30% and 70% dense, as a check for possible interaction effects.
We again recruited 20 subjects and asked them to use a slider to estimate the mixture levels of 36 stimuli .
We used similar validation stimuli to those in the first experiment  and again did not need to exclude any participants.
We performed a two-way ANOVA to test whether word length condition and mixture level affected absolute error.
As with the previous experiment, post-hoc analysis with Tukey's HSD test shows that the significant outliers were the mixtures at the edge cases of 0.1 and 0.9.
This disconfirms our initial hypothesis, but provides evidence of the generalizability of the good performance seen in these two experiments when applied to real text and real world applications, where one would expect variable word lengths but not necessarily systematic per-tag class skews in word length .
Having seen evidence of robustness for two potential factors, our next experiment explored another potentially relevant factor for performance, namely variance in word length.
Our initial hypothesis was that in the case where there is a large variance in word length, the noisier patterns in stimuli might make judgments difficult.
In real corpora, the lengths of tagged words can vary wildly .
It was infeasible to test every possible distribution of word lengths in texts.
For this experiment we wanted simply to determine if variation per se in word length would have an effect on performance: in the case of a positive result follow-on experiments would investigate the situations where degradation would occur.
The stimuli for this experiment were 300-word paragraphs with three different conditions for word length.
In the first condition, all words were five letters long, as in experiment 1.
In the second condition, word lengths were uniformly distributed across lengths of four, five, and six letters.
We wanted to extend our results to multi-category situations.
We hypothesized that performance would be robust to small numbers of categories.
However, indicating multiple categories requires choosing sets of colors to indicate the various tag classes.
We hypothesized that if we followed best practices in choosing sets of colors, the specific colors used would not affect performance.
The need to examine multi-way comparisons lead us to a different experimental design.
We used a forced choice design where the participant was asked to choose the most commonly occurring color from five choices.
This design enabled a different measure of performance.
The difference between the value of the "winning" class and the next highest class provides a measure of task hardness.
A two-way ANOVA tested the effects of d and the winning color on accuracy : our results show that the parameter d was an effective proxy for task difficulty  = 30.97, p < .0001.
A post hoc Tukey HSD test confirmed that task performance rose monotonically with d from 71% accuracy when d=5% to 95% when d=15%.
The results show that viewers can make judgements across multiple classes, but the performance degrades as the stimuli become more ambiguous.
A post hoc analysis of color choice using confidence intervals at the  = 0.05 level of significance shows that, while we would expect participants to evenly guess colors , participants guess red  significantly more than green .
Figure 6 shows this effect with respect to the number of times each color was guessed.
Since we had adjusted the colors to be isoluminate, we attribute this effect to known perceptual artifacts present for particular choice of colors.
Color biases of these sort  have occurred in other information visualization settings where area is perceived as larger when certain choices of color encoding are made .
Since the effect of these illusions is usually to artificially inflate the perceived area of regions, this supports the hypothesis that area calculations play a role in numerosity judgments.
Figure 5: The d parameter for the multicolor experiments .
In both cases orange tags are 35% of the text, but on the left red accounts for 30% of the area, versus 20% on the right.
The larger the value of d , the better the performance.
For this experiment stimuli were tagged with five different colors again drawn from the Colorbrewer qualitative color sets.
Each stimulus had a "winner" color with the highest proportion of tags and a clear "runner-up" with the second highest proportion.
While the winner always accounted for 35% of the tags in a given stimuli, we varied the difference between the winner and the runner-up  across three different conditions: 5%, 10%, and 15%.
The remaining three colors were given proportions at least five percent less common than the runner-up.
Figure 5 shows the effect of d on a sample stimulus.
Word lengths were evenly distributed from three to seven letters across all colors.
Each of our 20 subjects was shown three stimuli for each combination of d and winning color for a total of 45 stimuli.
We again included validation stimuli in which a single color had 100% mixture level, and again did not need to exclude any participants.
Figure 7: An example of a stimulus with an extreme mismatch between area and mixture: although there are more purple words than orange words in the paragraph, since the orange words are longer they both account for a larger amount of visual area on screen.
Even in this extreme case , participants were still fooled .
The color bias exposed in Experiment 3 suggested that we should investigate other potential sources of bias.
Previous research into numerosity estimation indicates that area, density, and other gestalt groupings can bias or otherwise confound the approximate number sense.
Area is particularly relevant in text, as different tag classes and texts contain different words that may have different skews in their word length distributions.
To be effective, tagged text displays must be robust to skews in word length  relative to numerosity.
Long words should not count for more than short words.
Post-hoc analysis using Tukey's HSD test shows that while accuracy is generally high when area and mixture level are aligned, there is a significant drop in performance when they disagree .
For example, accuracy may drop to as low as 37.5% in an extreme  situation .
While performance in realistic settings is unlikely to suffer to this extreme degree, biases in less extreme cases will arise in practice.
Therefore, it is clear that this problem must be addressed in the design of tagged text displays.
Figure 8: Participant accuracy at the forced choice estimation task 
Performance is best in the upper left and lower right corners, where the mismatch between area and number is away from the decision boundary in the correct direction.
It was our hypothesis that large mismatches would systematically bias results in the direction of the area.
However, some perception results suggest that number could dominate area for some types of stimuli.
In this experiment we tested our subjects' ability to differentiate between the numerical proportions of tags  and the proportions of the physical space they take up .
The task was a forced choice decision between two tag classes  to determine which tag was more common in a paragraph of text.
Each of our 20 subjects was shown two stimuli from every condition of mixture level cross tag area, for a total of 40 stimuli.
Figure 7 shows an extreme example stimulus with a large area/count mismatch.
Since the distinction between tag area and mixture level was so crucial to this experiment, we included extra stimuli  to make sure participants understood these definitions.
We were not forced to exclude any participants based on performance on these validation stimuli.
We conducted a two-way ANOVA to determine the effects of area and mixture on accuracy.
We would expect that in some cases area mismatches would be beneficial , but harmful in others .
Figure 10: A per condition breakdown of the effect of area manipulations on response.
For each level of mixture  we manipulated the area either by making the orange words significantly shorter than the other tags  or significantly longer than the other tags .
Without the use of any measures to correct for this bias, participants would conflate the area manipulation with the tag mixture, allowing confusion between different levels of stimuli.
When extra area is added to words and the inter-word tracking is adjusted, these confusions are reduced.
Experiment 4 revealed that area was a significant confound for numerosity.
In order to translate this result into design guidelines, we decided to investigate this effect at a finer level of granularity, as well as analyze the design space for overcoming the bias it introduces.
One potential strategy is to artificially correct the tag areas to match mixture levels.
In such a strategy, whenever a given color's area was lower than its mixture level, we could add extra colored pixels  to the beginnings and endings of underrepresented words until the area and mixture were equal.
Pilot tests, documented in the supplemental appendix, suggested the hypothesis that these corrections would partially  mitigate the effects of the area bias.
We attempted to evaluate these corrections more thoroughly in our final experiment.
Figure 9: Three levels of our area manipulation factor.
60% of the words in the paragraph are orange, but systematic biases in word length have made 70% of the tagged area orange.
On the left the mismatch between area and word count is unaltered.
In the middle case extra padding is added to the purple words to compensate for the bias.
In the last case inter-word tracking is adjusted to fill the extra buffer space while still maintaining legibility.
Figure 11: The effect of our different area manipulations on accuracy at determining the mixture  in a paragraph of tagged text.
Significances are at the  = 0.05 level.
The gray line represents participant's average performance when there is no mismatch between numerosity and area.
When systematic biases to area are introduced this accuracy suffers.
By adding extra space to under-represented tags this error is reduced.
By altering the inter-word spacing  of under-represented tags the error is reduced, but not significantly more so than in the previous case.
For each mixture, we generated cases where the area matched the mixture, the area was 10% greater than the mixture, or the area was 10% less than the mixture.
We recruited 60 total subjects , each of whom saw three stimuli for each mixture level cross tag area difference for a total of 36 stimuli.
The validation stimuli were not area corrected.
We did not need to exclude any participants based on validation performance.
We performed a one-way ANOVA to test whether area correction method had an effect on subject accuracy.
While a post hoc Tukey HSD test confirmed that our two area correction method were not significantly different from each other, both significantly increased overall subject performance over the noncorrected stimuli .
A Student's t-test shows no statistically significant difference between corrected stimuli and stimuli where no area bias was present .
This confirms that our manipulations were able to mitigate area/mixture mismatches.
By tracking-adjusted, we mean that as opposed to effectively resizing underrepresented tags and centering words inside them, we instead adjusted the tracking  so that each word fully filled  the horizontal width of its tag.
While both of these techniques corrected the area/mixture discrepancy, it was our belief that the trackingadjusted method represented a more natural solution to the area problem that would preserve legibility.
Figure 9 shows example stimuli for each of these conditions.
We presented participants with an estimation task in which they were presented with a paragraph of text with two tag classes  and asked to estimate, with intervals of 5%, the percentage of tags that were orange rather than purple.
Our experiments have shown that viewers can make accurate estimations of numerosity in tagged text for a wide range of stimuli.
While there are some factors that introduce bias, these can be mitigated through design.
Figure 12 shows one such design: a presentation that accounts for specific biases in human perception of numerosity but takes into account concerns of legibility and known aesthetic principles for text display drawn from the HCI literature.
Our experimental models were focused more on the lower level psychophysical features of the task.
As such we did not present stimuli using real text  which would make it difficult to limit exposure time in such a way as to prevent participants from explicitly counting the numbers of tagged words.
We think this choice improves the generalizability of the results at the expense of artificiality of the task, although we feel our stimuli are closer to what might be seen in an information visualization than previous lower-level results in this area.
Our work examines only short  texts.
As the scale of the task increases , performance may degrade.
Our future work will examine larger scales that require aggregate or statistical judgments in order to analyze the impact of summarization tools and techniques for quickly juxtaposing multiple short sections of text .
Figure 12: An example of inter-word tracking changes on real text.
Since the green tagged words are on average shorter than the other colors, there is a mismatch between perceived area and perceived numerosity.
Modifying the inter-word tracking space attenuates this mismatch and produces better accuracy.
The example is an extreme case of length mismatch: in practice, the required spacing changes are more subtle.
Experiment five showed two different designs for addressing the area bias problem.
In terms of measured performance the two had no significant difference.
However, they may have different impacts on aesthetics and legibility.
Adding space, either between words or letters, does impact text appearance.
This may create possible concerns over aesthetics and legibility.
However, the literature suggests that increased tracking for individual words may actually improve legibility .
Therefore, we feel that adjusting area by tracking provides a plausible mechanism for countering area bias, but should be more extensively tested in real-world applications where legibility and aesthetics must be considered.
Similarly, the choice of color sets has impacts on estimation performance, aesthetics, and legibility.
Guidelines for text backgrounds in the literature are mainly concerned with contrast, as this is a key element in legibility .
However, other effects suggest that certain colors be avoided.
In Experiment 3, we observed that red and green may be problematic, as shown in prior studies.
Without a better understanding of the underlying mechanism, it is hard to make stronger design suggestions.
The more conflict between visual appearance and semantics, the more difficult the associated text-related or color-related tasks .
In this paper we have examined the ability of viewers to make judgments about estimated values in tagged text.
We have shown viewers can accurately and efficiently make these judgements across a large set of stimuli.
However, we have shown that certain factors such as relative area or choice of color can degrade performance.
We have proposed and empirically validated a design which accounts for these factors.
Our work has implications for the design of visualizations.
First, it shows that designers can use tagged text displays with some confidence that the aggregate statistics will be conveyed accurately.
Second, it shows that these designs are robust across a number of factors.
Third, it shows that while there are some potentially problematic biases, these may be mitigated by considering them in the display design.
