This paper describes the design and evaluation of a technique, Direct Pointer, that enables users to interact intuitively with large displays using cameras equipped on handheld devices, such as mobile phones and personal digital assistant .
In contrast to many existing interaction methods that attempt to address the same problem, ours offers direct manipulation of the pointer position with continuous visual feedback.
The primary advantage of this technique is that it only requires equipment that is readily available: an electronic display, a handheld digital camera, and a connection between the two.
No special visual markers in the display content are needed, nor are fixed cameras pointing at the display.
We evaluated the performance of Direct Pointer as an interaction product, showing that it performs as well as comparable techniques that require more sophisticated equipment.
Rather than present a new interaction technique that requires a high learning effort, we attempt to enable an intuitive pointing technique: directly pointing to the desired target.
The result, Direct Pointer, is a system that controls the cursor on the display by analyzing the view of the camera.
A description of this system is followed by a performance evaluation of this technique in a controlled environment using a standard test, showing that ours is comparable with other interaction methods including laser pointers.
This paper concludes with a discussion of some possible applications that emerge as result of this research.
Earlier works tried to extend the desktop interaction scheme to large displays using peripherals such as a remote mouse with an isometric joystick .
Though it does work, this method is not ideal for ergonomic reasons: it is difficult to use with only one thumb.
Sweep  is a rather natural interaction technique that lets users move a camera-phone along the desired direction of the cursor motion.
It mimics the operation of a desk mouse in midair.
However, by comparing consecutive frames of the camera, it offers only indirect control of the cursor position.
Further, depending on the depth of objects in the camera images, same-distance camera motions may generate different distances for the cursor to move, making control difficult.
Point & Shoot  deduces the user's desired object of selection by displaying visual codes.
By recognizing those visual codes in the camera image, the position of the cursor relative to those landmarks is calculated.
This technique allows pointing directly at desired targets.
However, the stated implementation does not support real-time visual feedback of the cursor, forcing users to concentrate their attention on the display of the camera  rather than the large display.
Further, the visual codes on the display, even if they appear only for a brief time during the selection process, do contaminate the display for other viewers, making it inappropriate for shared displays.
Large displays, from projection-screens used in meeting rooms to train station billboards, are effective at displaying information but difficult to interact with.
Displays that are protected from physical interaction  have the same problem.
In situations like these, some type of remote interaction technique is desired.
Interacting with, and not merely pointing to, the content on the display is the problem addressed in this paper.
Camera-phones are perhaps the best candidate for remote interaction devices in such situations.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Leveraging the intuitiveness of a laser pointer, some systems use a pre-calibrated, fixed camera to visually track the bright dot on the display, creating a remote pointer .
These systems have the advantages of natural interaction and immediate feedback, but a fixed camera must be installed for each display, which is not always feasible.
What's more, security is not guaranteed in usages where user authentication is needed because anonymous users could also point to the display.
Other works include C-Blink  and VisionWand .
CBlink uses a hue-difference-based method to track the position of the cell phone display and receives data from it.
However, due to current performance of TFT screens and cameras, its throughput in data transmission is much smaller than traditional wireless techniques.
VisionWand uses two cameras to build 3D position and direction of the wand by tracking its colorful endings.
This setup requires even more effort than the regular laser pointer tracker.
Missing from earlier research is a system that allows for all the advantages of a common laser pointer along with the advantages of modern mouse cursors.
When a cursor is displayed in sight of the handheld camera, it is viewed by the camera at the center of the frame.
If the cursor is identified at a different location in the frame, its position should be updated so that it will move back to the center of the camera frame.
The system runs in the following stages:  The display shows a cursor.
This position is sent back to the display.
According to the cursor offset from the center of the camera frame, a new corrected cursor position is calculated.
Direct Pointer allows direct manipulation of the cursor with continuous visual feedback, closely resembling the laser pointer.
This is possible with the use of common components such as a display and a handheld camera, and without a fixed camera as in the laser pointer system.
Our method uses the position of the cursor on the display as the sole source of input.
There is no need for special tags on the display or around it and, it works independent on the display content.
What's more, since the interaction is performed via the wireless channel of the camera, security of interaction is also guaranteed.
The present system is based on a closed-loop feedback between the handheld device and the display.
The loop is constructed by capturing a view of the screen with the handheld camera and sending the data via wireless channel to the server.
The server then updates the cursor position .
Cursor identification is calculated on board the handheld device by its processing unit.
Different features of the cursor can be used during the identification, such as color, shape, unique motion or a combination of those.
When using motion as a feature, we assume that the motion of the cursor is different from the rest part of the display.
Estimating the new position of cursor.
Using the translation, rotation and scale transformation between them, the center  of the camera view is mapped to the new cursor position  in the display.
The displayed cursor is shifted , by its offset from the center  in the camera view.
First, compensation for the motion of the background in the camera frame is needed.
To do so, we identify several interest points between consecutive frames, and estimate the affine transformation between them .
Then, after warping one frame to the other according to the transformation, the cursor can be easily detected, as an area of difference between the frames.
The camera grabs 30 frames with the resolution 320x240 per second.
We combined a Logitech Cordless Presenter with the webcam to perform the basic clicking actions .
For this experiment, the handheld processing, was actually done on the display computer.
A multi-directional tapping test based on ISO 9241-9  was used in our experiment.
The numbers marked in Figure 4 show the order of the targets.
A block ends when the target returns to 0, so each block consists of 19 trails.
Fourteen subjects , aged 19-40, participated in the experiment.
A red circle with the diameter 48 pixels was used as the cursor for detection.
Each subject tested 4 blocks for each index of difficulty, so this experiment was a  x  x  x  factorial design.
The total number of clicks was 5320.
Before the experiment started, all subjects were asked to practice with the Direct Pointer until they felt comfortable.
A subjective questionnaire based on  was given to the participants after the experiment was done to gather the qualitative feedback.
Earlier works use visual tags positioned at known points on the screen boundary  or in the display content  to generate a global mapping between the screen and the camera image.
Such methods rely on additional data that might either interfere with the display content or be hard to get .
The present system maintains an approximation of the mapping between the screen and the camera image at real time, using only the location of the cursor.
Special markers on the display or on the screen boundaries are not required .
Instead of estimating the exact projective transformation, we use the last two positions of the cursor to generate a translation, rotation and scale transformation between them .
By mapping the center of the camera frame to the screen, using this transformation, we find the new position of the cursor.
Although the method above is only an approximation, and not the full description of the transformation between the camera and the screen, when applying it to the close-loop feedback, the cursor would gradually converge towards the correct position.
When the motion between consecutive frames is very small, which means the distance between the last two positions of the cursor is very small, the estimation of that transformation may be very noisy.
To handle this problem, we simplify the mapping even further to translation only, and shift the displayed cursor according to its offset from the center in the camera view.
Figure 2 illustrates this method.
Since no other interaction methods were involved in this experiment, we are more concerned about the throughput of our Direct Pointer.
This throughput is compared with other interaction techniques in the literature we found , and those techniques were also evaluated based on ISO 9241-9.
The result of our first implementation of the Direct Pointer is promising.
Compared with other interaction systems evaluated with this standard test, we get similar performance .
This work shows the potential of a remote pointing device that requires no fixed camera or changes in the display to achieve cursor control with instant feedback and guaranteed security.
This technique opens up new application and interaction opportunities with public and private displays such as billboards, home theaters, presentations, and more.
Future work includes the improvement in constructing the transformation where fast camera motion is more considered to make the mapping more accurate.
A multiuser interaction scheme with Direct Pointer is also an interesting topic.
Comparison of throughput within devices.
In Figure 5, a decrease of throughput was observed when the distance between targets became wider.
Several participants also complained that they had to delicately hold the camera to "feel safe" when performing long-distance tasks.
This is perhaps due to the form-factor of our prototype because we also find that for long-distance targets, participants made a fast motion of the camera near the target followed by minor adjustments.
Subsequent work on this system should address this.
From the questionnaire, most questions about operation scored in the middle of the 1-to-7 scale.
A poor score in the "smoothness during operation" question  is the result of the occasional "fly away" bug of the cursor in our prototype.
This bug is caused by the incorrect calculation of mapping due to the inherent delay of the camera.
Using a camera with a higher frame rate and less delay would alleviate this problem and is perfectly feasible.
