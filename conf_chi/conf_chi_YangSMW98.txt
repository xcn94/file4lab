Based on the maximum likelihood method, the model parameters can be adaptedfor different peopleand different lighting conditions.
The feasibility of the model has been demonstrated by the development of a real-time face tracker.The systemhas achieveda rate of 30-t-frames/second using a low-end workstation with a framegrabber and a camera.We also presenta top-down approachfor tracking facial featuressuch as eyes,nostrils, and lip comers.These real-time visual tracking techniqueshave beensuccessfully applied to many applicationssuch as gazetracking, andlipreading.The face tracker hasbeen combinedwith a microphonearray for extracting speechsignal from a specificperson.
The gaze tracker has been combined with a speech recognizer in a multimodal interface for controlling a panoramic imageviewer.
For example, a system can locate a userby merging visual facetracking algorithms and acoustic sound sourcelocalization, identify who is talking to whom by extracting head orientation and eye gaze,and extract message content by visual and acoustic speechrecognition.
First, we discusstechniques of tracking human faces.A human face provides a variety of different communicativefunctions such as identification, the perception of emotional expressions,and lipreading.Many applicationsin human computer interaction require tracking a human face.
Human skin-colors can be usedas a major featurefor tracking human faces.An adaptive stochasticmodel hasbeendevelopedto characterizethe skin-color distributions.
Basedon the maximum likelihood method,the model parameters can be adaptedfor different people and different lighting conditions.
The feasibility of the model has beendemonstrated by the developmentof a real-time face tracker.
The system has achieved a rate of 30-t- frames/secondusing a low-end workstation  with a framegrabber and a camera.Once a face is located,it is much easierto locatethe facial featuressuchas eyes,nostrils, and lips.
This top-down approachworks very well for many applications such as gaze tracking, and lipreading.We describesomeapplicationsof the visual tracking techniquesto multimodal human computer interaction.
The face tracker has been combined with a microphone array for extracting speechsignal from a specific person.
The gaze tracker has been combined with a speechrecognizer in a multimodal interface to control a panoramic imageviewer.
While multimodal interfaces offer greater flexibility and robustnessthan traditional mouse/keyboard interfaces,they havebeenlargely pen/voice-based, user activated,and operated in settings where some constraining devices are required.
For truly effective and unobtrusive multimodal human-computer interaction, we envision systems that allow for freedom of movementin a possibly noisy room without the needfor intrusive devicessuch as headsets and close-talkingmicrophones.In order to makethis goal a reality, we require not only efficient ways to integratemultiple modalities but also a better model of the human user based on a mixture of verbal and non-verbal, acousticand visual cues.
Pemkion to make digitalhard fopies of all or part ofthis material for Personal Or chssroom use is granted without fee provided that the copies a~ not made or distributed for profit or mmmercial advantage.,the copy@ht notice, the title oftbe publication and its date appear, and notice is $`a that copyright is by permission of the ACM, Inc. To copy ot&w& torqublish, topost on sm'mor to rediibute to Iists, rq& specific permission and/or fee.
Locating and tracking humanfacesis a prerequisitefor face recognition and/or facial expressions analysis,although it is often assumed that a normalizedface image is available.
In order to locate a humanface,the systemneedsto capturean image using a camera and a framegrabber,process the image,searchfor important featuresin the image, and then use thesefeaturesto determinethe location of the face.
In order to track a human face, the system not only needsto locate a face, but also needs to find the same face in a sequence of images.
Facial features,such asthe eyes,noseand mouth, are natural candidatesfor locating human faces.
These features,however, may change from time to time.
These methods are computation expensive and hardly achieve real-time performance.Several systemsof locating the human face have been reported.Eigenfaces,obtainedby performing a principal componentanalysison a set of faces, have been used to identify faces .
Sung and Poggio  reported a face detection system based on clustering techniques.The systempassesa small window over all portions of the image, and determines whether a face exists in eachwindow- A similar systemwith better results has beenclaimed by Rowley et al.
Color has been long used for recognition and segmentation .
Using skincolor as a feature for tracking a face has several advantages.
Processingcolor is much faster than processingother facial features.Under certain lighting conditions, color is orientation invariant This property makesmotion estimationmuch easierbecauseonly a translation model is neededfor motion estimation.However, color is not a physical phenomenon.
It is a perceptual phenomenonthat is related to the spectral characteristics of electro-magneticradiation in the visible wavelen,&s striking the retina .
Thus, tracking human facesusing color as a featurehas several problems.
First, different cameras may generatedifferent colors even for the samepersonunder the samelighting condition.
Second, different people have different color appearances.
Finally, the color appearance of the sameperson may differ under different environmental conditions.
In order to use color as a feature for face tracking, we have to deal with theseproblems.
SMn Color hrlodeling Color is the perceptualresult of light in the visible region of the spectrum incident upon the retina.
Physical power is expressed in a spectralpower distribution.
Much researchhas been directed to understandingand making use of color information.
The human retina has three different types of color photoreceptor cone cells, which respond to incident radiation with somewhatdifferent spectralresponse curves.
Based on the human color perceptualsystem,three numerical components are necessary and sufficient to describe a color, provided that appropriatespectral weighting functions are used.
In order to use skin color as a feature,we first have to characterize skin colors.
Color can be characterizedby a nonparametricmodel suchas a color map, or a parametricmodel suchas a distribution model.
We are interestedin developing a distribution model for representinghumanskin color distributions.
In order to investigateall theseproblems,we needa large amount of data.
We have built up a databasewhich contains about 1000 face imagesdownloaded from the Internet and taken from our laboratory.
This databasecovers different races  and different lighting conditions.
Skin Color Cluster A color histogram is a distribution of colors in the color spaceand has long beenused by the computer vision community in image understanding.For example, analysis of color histogramshas been a key tool in applying physicsbasedmodels to computer vision.
In the mid-1980s, it was recognizedthat the color histogram for a single inhomogeneoussurfacewith highlights will have a planar distribution in color space.
The histogramsof humanskin color coincide with theseobservations.Figure 1 shows a face image and the skin-color occurrencesin the RGB color space.The skin-colors are clustered in a small areain the RGB color space,i.e., only a few of all possiblecolors actually occur in a human face.
It is well known that different people have different skincolor appearances.
Even for the sameperson, his/her skincolor appearancewill be different in a different environment.
In other words, many factors contribute to human skin-color appearance.
For human color perception, a 3D color spacesuch as an RGB space,is essential for describing a true color.
However, a 3D spaceis not necessarily essential for all other problems.In the problem of tracking human faces, brightnessis not important.
Therefore we can remove it from the original information by normalization.
Our experiments reveal that human color appearances differ more in brightness than in color itself.
If we can remove the brightness from the color representation,the difference among human skin-colors can be greatly reduced.In fact, a triple  in the RGB spacerepresentsnot only color but also brightness.If the correspondingelementsin two points,  and , areproportional, i.e.,
By closely investigating the-facecolor cluster,we have discoveredthat the distribution hasa regular shape.By comparingthe shape of skin-color distributions with a bivariate normal distribution, we concludethat it is possibleto usea bivariate normal distribution to characterize the skin-color distributions.
Unlike most of the methodsused in engineering statistics which assumea normal distribution of the measureddata, we have examinedwhether the measureddata of a sample do indeed have a normal distribution by goodness-of-fit techniques.
Goodness-of-fittechniques examinehow well a set of sampledatamatcheswith a given distribution as its population.
The methodsof performing a goodness-of-fittest can be an analytic or graphic approach.
In the graphic approach,the most common method is Q-Q plot.
Weusethis methodto test our skin-color distributions.
The basicidea of the Q-Q plot is to usethe cumulative probability of the sampling data againstthat of the testeddistribution.
A straight line indicates that we cannot reject the null hypothesis.We have testedmarginal distributions and bivariate distribution.
When we do marginal test, we test each variable separately against the normal distribution.
When we test the bivariate distribution, we test the transformed variable against Chi-square distribution.
We have built up a database which contains about 1000 face images down-loadedfrom the Internet and taken from our laboratory.
This database coversface imagesof people in different races , genders, and the lighting conditions.
Using this database, we tested the following NULL hypothesis:human skin-color is normally distributed in a normalized bivariate space.
Another advantageof the color normalization is, we found out, that the color variancecan be greatly reducedafter the normalization.
The same skin color cluster has a smaller variancein the normalizedcolor spacethan that in an RGB space.Skin-colors of different peopleare less variant in the normalizedcolor space.This result is significant because it provides evidence of the possibility of modeling human faces with different color appearances in the chromatic color space.
Table 1 showsmeanvaluesandvariancesof the sameskin color cluster in different color spaces.
Obviously, the variances are much smaller in the normalized color space.
Although under a certain environment the skin-color distribution of eachindividual is a multivariate normal distribution, the parameters of the distribution for different people and different lighting conditions are different.
A number of viewing factors, such as light sources,background colors, luminance levels, and media, impact greatly on the change in color appearance of an image.Most color-basedsystems are sensitive to changes in viewing environment.
Even under the samelighting conditions, backgroundcolors such as colored clothes may influence skin-color appearance.
Furthermore,if a personis moving, the apparentskin-colors changeas the person'sposition relative to cameraor light changes.
Therefore,the ability of handling lighting changes is the key to success for a skin-color model.
There are two schoolsof philosophy to handle environment changes: tolerating and adapting.
The most common approachfor tolerating lighting changesis Color constancy.
Color constancyrefers to the ability to identify a surfaceas having the samecolor under considerablydifferent viewing conditions.
Although human beings have such ability, the underlying mechanismis still unclear.
We have so far revealed that human skin-colors cluster in the color spaceand are less variant in the chromatic color space.We are further interestedin the representation of the skin-color distributions.
Sincewe areinvestigating the skincolor distributions in a bivariate normalized color space,it is convenient to examinethem graphically.
Figure 2 shows the skin color distribution of the imagein Figure 1.
We have found that the shapeof the skin-color distribution of a person remainssimilar although thereis a shift in the distribu-
One way to adapttheseparameters is to use a linear combination of the known parameters to predict the new parameters.
The underlying theory is that a linear combination of Gaussiandistributions is still a Gaussiandistribution.
Where $i and $ are updatedmean and covariance,m andS are the previous mean and covariance, O!
Based on the identification of the skin-color distribution at each sampling point, we can obtain its mean vector and covariancematrix.
Then the problem becomesan optimization problem.
We can use the maximum likelihood criterion to obtain the best set of coefficientsfor the prediction.
We have investigatedadapting the meanonly, and adaptingboth the meanand covariancematrix .
AdaptingMean In this case,the covariancematrix is assumedto be a constant and the meanvector p is assumed to be a linear combination of the previous meanvectors:
Tracking Human Face in Real-time A direct application of the skin-color model is to locate a face in an image.
A straightforward way to locate a face is to match the model with the input image to find the face color clusters.Each pixel of the original image is converted into the chromatic color spaceand then comparedwith the distribution of the skin-color model.
Since the skin colors occur in a small area of the chromatic color space, the matching processis very fast.
This is useful for real-time face tracking.
By combining the adaptive skin color model with the motion model and the camera model, we have developeda real-time face tracker .
The systemhas achieved a rate of 30+ frames/second with 305 x 229 input sequences of imageson both HP and Alpha workstations.The systemcantrack a person'sface while the personwalks, jumps, sits and rises.
The QuickTime movies of demo sequences in different situations and on different subjects can be found on the web site http:// www.is.cs.cmu.edu/ISL.multimodal.face.html.
Applicationto Tele-conference An immediate application of the face tracker is to use it to automatically follow the speakerin a tele-conference.We describea more interesting application in this subsection.
In a tele-conference,the quality of the conferencegreatly dependson image transmission.The bottle neck of the traffic is in the network.
People have been working very hard on datacompression techniquesto reducedatatransmission.
However, there is a limitation on compression.In such a case,if we want to continue the conference,we have to discard somedata.
One way to do this is to skip someframes, which may result in losing important information.
We want to keep the important information and discard relative unimportant data.To achieve this goal, we can add a selective function on the top of original codec to select the important information.
In a tele-conference,the speakeris the center,We would like to keep updating speaker'sinformation.
PAPERS speakerand selectthe region surroundingthe facial areaby a window.
The window size is adjustablebasedon network bandwidth.When network trafiic is good, the window is the entire image.
When the network bandwidth is not enough, the window size is shrunk, and even the imageis converted to grey scale.
We have developeda system by adding the face tracker on the top of vie, a public domain available tele-conference software.The systemcan provide several filtering schemessuch pseudo-cropping, slicing, and blurring.
Figure 3 shows how these filtering schemes work.
First, the approximatepositions of the lip corners are predicted, using the positions of the eyes, the face-modeland the assumption,that we have a near-frontal view.
A generously big areaaroundthosepoints is extractedand usedfor further search.
Finding the vertical position of the line betweenthe lips is done by using a horizontal integral projection PI, of the grey-scale-image in the search-region.Becausethe lip line is the darkesthorizontally extendedstructure in the search area, its vertical position can be located where Ph has its global minimum.
The horizontal boundariesof the lips can be found by applying a horizontal edge detector to the refined searcharea and regarding the vertical integral projection of this horizontal edge image.
The positions of the lip comers can be found by looking for the darkest pixel along the two columnsin the searcharealocated at the horizontal boundaries.
Different filtering schemes:  original;  pseudo-cropping;  slicing;  blurring SEARCHING AND TRACKINGFACIALFEATURES The facetrackercan also be usedasa basefor other applications.
Oncea faceis located,it is much easierto locateother features such as pupils, lips and nostrils.
This top-down approachworks very well for many applications.In this section, we show how to track thesefacial featuresin real-time and use them to estimate human gaze direction .
Locating Facial Features We first describemethodsto locate and track the pupils, the lip comersand the nostrils within a found face.
Similarly to searchingfor the eyes,the nostrils can be found by searchingfor two dark regions, that satisfy certain geometric constraints.Here the search-regionis restrictedto an area below the eyes and above the lips.
Again, iterative thresholdingis usedto find a pair of legal dark regions,that areconsideredas the nostrils.
Tracking Facial Features Once the facial featuresare located, the problems become tracking thosefeatures.
Assuming a frontal view of the face initially, we can search for the pupils by looking for two dark regions that satisfy certain geometricconstraintsand lie within a certain areaof the face.
For a given situation, these dark regions can be located by applying a fixed threshold to the gray-scale image.However,the thresholdvalue may changefor different people and lighting conditions.
To use the thresholding method under changing lighting conditions, we developed an iterative thresholding algorithm.
The algorithm iteratively thresholdsthe image until a pair of regions that satisfies the geometricconstraintscan be found.
Figure 4 showsthe iterative thresholdingof the searchwindow for the eyes with thresholdski- After three iterations, both pupils are found.
Our approachto track the lip-comers consistsof the following steps: 1.
Searchfor the darkestpixel in a search-region right of the predictedposition of the left comer and left of the predicted position of the right comer.The found points will lie on the line betweenthe lips.
Searchfor the darkestpath along the lip-line for a certain distanced to the left andright respectively,and choosepositions with maximum contrast along the search-pathas lipcomers.
Becausethe shadowbetweenthe upper and lower lip is the darkest region in the lip-area, the search for the darkest pixel in the searchwindows near the predicted lip corners ensuresthat even with a bad prediction of the lip corners,a point on the line betweenthe lips is found.
Fi=mre5 shows the two searchwindows for the points on the line between the lips.
The two white lines mark the searchpathsalong the darkestpaths,starting from wherethe datkest pixel in the searchwindows have beenfound.
The found comersare marked with small boxes.
But whereas we have to search for a relatively large area in the initial search, during tracking, the search-window can be positioned around the previous positions of the nostrils, and can be much smaller.
Furthermore, the initial threshold can be initialized with a value that is a little lower than the intensity of the nostrils in the previous frame.This limits the number of necessary iterations to a small value.
However, both nostrils are not always visible in the image.
For example,when the headis rotated strongly to the right, the right nostril will disappear,and only the left one will remain visible.
To deal with this problem,the searchfor two nostrils is done only for a certain number of iterations.
If no nostril-pair is found, then only one nostril is searched for by looking for the darkestpixel in the searchwindow for the nostrils.
To decidewhich of the two nostrils was found, we choose thenostril, that leads to the pose which implies smoother motion of the head comparedto the pose obtainedchoosing the other nostril.
Human-human communication takes advantage of many communication channels.
We use verbal and non-verbal channels.We speak,point, gesture,write, use facial expressions, headmotion, and eye contact.
However, most of current multimodal human computer interfaces have been focusedon integration of speech,handwriting and pen gestures.
In fact, visual information can play an important role in multimodal human computer interaction.
We present three examplesof multimodal interfacesthat include visual information as a modality in this section.
It is well known that hearing impaired listeners and listening in adverseacousticenvironmentsrely heavily on visual input to disambiguate among acoustically confusable speechelements.It hasbeendemonstrated that visual information can enhance the accuracy of speech recognition.
However, many current lip-reading systemsrequire usersto keep still or put special marks on their faces.
We have developed a lip-reading system based on the face tracker.
The system first locates the face and then extracts the lip regions asshown in Figure 7.
The visual and acousticTDNNs are trained separately,and visual and acoustic information are combined at the phonetic level.
The system has been applied to the task of speaker-dependent continuous spelling Germanletters.
Letter sequencesof arbitrary length and content are spelled without pauses.Wordsin our database are 8 letters long on average.The task is thus equivalent to continuous recognition with small but highly confusablevocabulary.
Through the first three layers  the acoustic and visual inputs are processedseparately.
The third layer producesactivations for 62 phoneme or 42 viseme states for acoustic and visual data, respectively.
A viseme,the rough visual correlateof a phoneme,is the smallest visually distinguishable unit of speech.
But it is more natural to control the panning and tilting with the gaze,and the zoom: ing with the voice.
We have developedan interface to control a panoramic image viewer by combining the gaze tracker with a speechrecognizer .
With such an interface, a user can fully control the panoramicimage viewer without using his/her hands.The usercan scroll through the panoramicimagesby looking to the left and right or up and down, and he can control the zoom by speakingthe commands"zoom in," "zoom in two times," "zoom out" "zoom out five times," etc.
Figure 8 showshow the systemworks.
The entropy quantities SA and Sv are computed for the acousticand visual activationsby normalizing theseto sum to one  and treating them asprobability massfunctions.
High entropy is found when activations are evenly spread over the units which indicates high ambiguity of the decision from that particular modality.
The bias b pre-skews the weight in favor of one of the modalities.
This bias is set dependingon the signal-to-noise ratio .
The quality of the speech data is generally well describedby the SNR.
The systemusesthe gray-scaleimagesof the lip region as inputs.
A one-dimensional microphonearray allows the speechsignal to be receivedin the half plane in front of the array.
If the array is steeredtowards a given spot the differencesof sound arrival time between the microphones are compensatedfor waves originating exactly from this location.
By summing these aligned signals one achieves an enhancement of the desiredsignal while sound coming from other locations is not in the samephaseand thus its audibility is deteriorated.On the other hand, if the system knows the speaker's location from visual tracking, it is possible to form a beamto select the desiredsound source to enhance the quality of speechsignal for speechrecognition.
We have demonstrated that a more accuratelocalization in spacecan be delivered visually than acoustically.Given a reliable fix, beamformingsubstantially improves recognition accuracy .Figure 9 showsthe setupof the system.
We have trained a speaker dependentrecognizer on 170 sequences of acoustic/visual data, and tested on 30 sequences.
For testing we also addedwhite noise to the test set, The results are shown in table 2, as performancemeasureword accuracyis used.
With our systemwe get an error reduction up to 50% comparedwith the acousticrecognition rate.
A panoramicimage provides a wide angle view of a scene.
In order to view a 360 degreepanoramicimage,we needto usea specialviewer.
Bnmelli, R, and Poggio, T , "Face recognition: features versus templates," IEEE Trans.
Pattern Analysis and Machine Intelligence, Vol.
E and Davis, Larry S. , `Model based object pose in 25 lines of code", Proceedingsof Second European Conference on Computer Vision, Santah&rgherita L&me, pp.
Forsyth, D. , "A novel algorithm for color constancy," International Journal of ComputerVision.
Twenty-Eight Asilomar Conferenceon Signals,Systems & Computers,Monterey, CA, USA Klinker, G1, Shafer,SA, and Kanade,X, `Using a color reflection model to separate highlights from object color," Proc.
145-150. hileier U., Hiirst W, and Duchnowski P , "Adaptive Biiodal Sensor Fusion for Automatic Speechreading" Proc.
Conferenceon Acoustics, Speechand Signal Processing,ICASSP 1996 hleier U., StiefelhagenR., Yang J.
Pentland, A, Moghaddam, B., and Starner, T. , "view-based and modular eigenspace for face recogni-
