Co-located work environments allow people to maintain awareness by observing others' actions , but the computerization of many tasks has dramatically reduced the observability of work actions.
The recent interest in gestural interaction techniques offers the possibility of recreating some of the noticeability of previous work actions, but little is known about the observability and identifiability of command gestures.
To investigate these basic issues, we carried out a study that asked people to observe and identify different sizes and morphologies of gestures from different locations, while carrying out an attention-demanding primary task.
We studied small , medium , and large  gestures.
Our study showed that although size did have significant effects, as expected, even small gestures were highly noticeable  and identifiable .
Our results provide empirical guidance about the ways that gesture size, morphology, and location affect observation, and show that gestural interaction has potential for improving group awareness in co-located environments.
The motion not only controls the landing gear, but just as important, it acts as a natural communication between the two pilots, letting both know that the action has been done.
However, consequential communication depends on large easilyobservable actions and controls, which are no longer common in most workplaces.
Instead, most tasks are now carried out on general-purpose computers with standard graphical user interfaces.
On these computers, activities that once had characteristic actions and artifacts  now all look very similar to an observer--that is, they all look like a person sitting at a computer monitor and moving a mouse.
Researchers in distributed groupware have looked at the problem of reduced observability , and have proposed visualization techniques to make others' actions in a shared workspace more obvious .
However, these enhancements often work only when people are observing the same part of the shared workspace, and the techniques do not provide a solution in situations where people are carrying out loosely coupled work in a co-located setting.
A recent development, and one that could potentially improve observability, is the rise of gestural interaction techniques.
Gestures are now common on touch-screen devices of all sizes, and larger gestures that involve full-body interaction have also been extensively studied .
Gestures and full-body interactions bring large easily-observable actions to general-purpose computers, and could thus be a solution to the problem of observability for collocated environments--they could be one way that designers help people maintain group awareness.
There is little information available, however, about whether gestural commands are in fact observable and interpretable, and what size of gesture is needed for an observer to notice the gesture while carrying out other tasks.
That is, how should gestures be designed to make possible the kind of group awareness that Norman described?
In 1993, Don Norman describes the usefulness of "big controls and big actions" for shared work: The critical thing about doing shared tasks is to keep everyone informed about the complete state of things  each pilot or member of the control team must be fully aware of the situation, of what has happened, what is planned.
And here is where those big controls come in handy.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Copyright is held by the owner/author.
Publication rights licensed to ACM.
We carried out an experiment to answer this question.
We measured how well participants were able to observe and identify gestures of various sizes and morphologies from different locations in a co-located environment, while carrying out an attention-demanding primary task.
We tested three gesture sizes: small gestures that took place on a tablet-sized device; medium gestures that occurred on a monitor-sized display; and large gestures that involved full-arm pointing to different locations in the room.
The study provided three main results:  Gesture size did significantly affect both observation and identification.
However, small gestures had surprisingly high rates of both observation  and identification .
Medium and large gestures showed no differences for observation , but large gestures were more accurately identified ;  Observation and identification were strongly affected by both location  and gesture morphology ;  Participants subjectively rated larger gestures as requiring significantly less effort than smaller gestures, and as being more observable and identifiable; people also strongly preferred the large gestures for the task of maintaining group awareness.
Our results suggest that gestural interaction techniques can provide the foundation for the consequential communication that underlies Norman's idea of `big actions'--and that although gesture size does matter, even small gestures on small devices are surprisingly visible.
We considered how smaller gestures could be so well observed; our analysis showed that even when command gestures are small, the preparatory and staging actions that precede them are often much more observable--for example, moving one's hand up to a mobile device is noticeable action that draws attention to the upcoming command gesture.
Our study provides empirical evidence about the benefits of gestural interfaces for collaborative activity, and is the first to analyze the specific effects of gesture size, morphology, and location on observability and identification.
Our work indicates that designers could use gestural interaction techniques as a way to improve the natural communication and awareness that occur in co-located work environments.
When people work in a group, they often engage in mixedfocus collaboration, i.e.
Coupling refers to the degree with which people have to interact to progress with their work .
When people are loosely coupled, they have to interact less with each other to complete their task as when they are tightly coupled .
However, even during loosely coupled work, people still need to be aware of others' activities .
Awareness is the perception and comprehension of the state of the environment .
Group awareness, that is an understanding of the activities of others, provides context for people's activities and is critical to successful collaboration .
Two factors determine the level of group awareness: the actor's nimbus , and the observer's focus  .
When nimbus and focus overlap, observers go through a three-phase process to gain group awareness: perception of an action, comprehension of the situation, and projection of the future status .
There are several methods for creating group awareness in collocated environments : direct communication, indirect productions, consequential communication, feedthrough, and environmental feedback.
Consequential communication occurs through visible or audible signs of interaction with a workspace .
The size of the actions necessary to operate controls makes actions public and creates situation awareness, which is important in many collaborative real-world tasks .
In HCI research, consequential communication is frequently mentioned as an awareness mechanism, and observational studies show that it is frequently used in real-world situations .
However, it is rarely explored in controlled studies and occasionally considered to be of little importance .
This is in contrast to other fields, which showed that consequential communication plays a crucial role throughout life, for example, as facilitator for learning through observation and imitation .
In HCI, the term gesture is used for a wide variety of concept .
A classification that fits best within the scope of this work might be a combination of 2D plane-based finger movements  and 3-D mid-air arm movements .
Initially, swipe gestures were used to move elements around a touch screen .
Researchers extended these simple gestures to include geometric forms, script, and multi-touch input .
Next, researchers created technologies that enables the use of gestures on different surfaces, such as the human arm  and virtual space in front of the user .
Two-D plane-based gestures have been investigated in great detail.
Researches have evaluated, for example, learnability , ergonomics , and social as-
Groups are sets of two to five people who carry out tasks in medium-sized workspaces .
Whenever people engage in collaborative activities, they have to split their attention between their working task and awareness maintenance.
Very few papers, however, have implicitly investigated gestures as a method for creating group awareness in co-located environments.
Morris et al., for example, used collaborative gestures to create awareness in a co-located work environment .
Full 3-D input using arm gestures was first mentioned by Bolt  and explored by Baudel and Beaudouin-Lafon .
Unfortunately, most papers in this area focus on technical aspects, and only two articles explore human factors of fullarm gestures in HCI: Virtual Shelves  and Air Pointing .
Neither of these works have investigated the consequential communication aspect of full-arm gestures.
To determine how factors such as gesture size, morphology, and location affect people's ability to see and interpret these gestures, we carried out a controlled experiment.
We picked these factors because they are of high importance for the observability of gestures .
We chose a dual-task study setup because it embodies the trade-off between primary working task and awareness-maintenance task.
In our simulated collaborative scenario, the study participants  carried out an attention-demanding choice reaction task  as primary task in a room where another person  executed various gestures.
The observer's job was to maintain awareness of the actor, but without reducing their performance on their primary task.
With our setup, the primary task required the majority of participants' attention.
The task required foveal vision focus on the touch screen, thus forcing participants to temporarily dedicate their full attention.
Furthermore, participants completed the primary task as often as the actor performed gestures.
We recruited participants from a local university, ages , female, male.
These participants were all experienced with traditional computer systems , and were all familiar with gestures on touch-based devices such as mobile phones and tablets.
The study was carried out in a large laboratory , in which we placed two moveable carts holding the study computers.
The actor's cart held a monitor and remained stationary during the study.
The observer's cart was moved to several different locations during the session .
It held a MiMo touch screen, on which the primary task was displayed, and on which the observer indicated their observations and identifications of the actor's gestures.
For each of the gesture sizes, we created 6 different gestures .
We chose a small gesture vocabulary in order to keep the recognition task simple, and to focus on our main interests of gesture observability and identifiability.
For small and medium gestures, we chose two gesture types that can be found on most touch screens  and one geometric gesture .
We based our large gestures on systems such as Virtual Shelves  or Air Pointing , which partition the space around the user into zones.
For our system, we used six zones that were arranged in front of the actor .
To gain quantifiable values for each of the gesture sizes, we measured magnitude and execution time of the actor's arm movement with an IR-based motion-tracking system.
The actor performed each gesture 10 times while we captured his shoulder, elbow, wrist, and index finger movement.
Participants observed the actor from seven different locations , comprising three positions arranged in a semicircle around the actor, and either two or three orientations at each position .
Figure 2 shows these locations.
The first two pairs, however, differ in a way that in L1 and L2 participants are behind the actor; in L6 and L7 they are in front.
The actor then started performing typical tasks at his station, which acted as distractor tasks in between gestures that the observer had to report.
The actor texted on the handheld tablet ; he typed using the on-screen keyboard on the horizontal screen ; and he fidgeted and moved objects around at the cart .
Within these typical activities, the actor performed a total of gestures  per location.
The actor's UI indicated when to perform the next gestures; the interval was randomly chosen .
When participants noticed a gesture, they could pause the primary working task, and specify the gesture they just observed from the UI.
In order to simulate a realistic work environment, we created an attention-demanding primary task for the observer to perform during the experiment.
The task involved repeatedly selecting one of four possible buttons indicated by a written message displayed on the observer's display .
Participants were given a short period to complete the selection ; if they did not finish their selection in time or made a wrong selection, the system would play a warning sound.
After each correct selection, the system would wait and then display another message.
The observer's system recorded all gesture observations and identifications, and tracked the participant's performance on the primary task.
After the experiment, participants filled out a basic demographic questionnaire, one NASA TLX form per gesture size, and one ranking questionnaire.
We formulated four hypotheses for our study: H1.
Larger gestures will be observed at a higher rate than smaller gestures.
Larger gestures will be identified more accurately than smaller gestures.
Larger gesture size will reduce the negative effects of occlusion H4.
Facing the actor will have higher observation and identification rates.
We performed a univariate ANOVA to investigate the effect of Gesture Size and Location on primary task performance measured as reaction rate.
To determine the effect of factors Gesture size and Location on observation and identification rates, we analyzed the trials in a repeatedmeasures ANOVA.
We carried out separate analyses of our dependent measures by gesture morphology  with a RMANOVA.
We evaluated the TLX data using a repeatedmeasures ANOVA, and we analyzed the rank data using a Friedman test for related samples.
All post-hoc tests used Bonferroni corrections.
When looking more closely at this finding, we saw that all affected participants performed significantly worse with the first gesture size they saw during the experiment.
We concluded that the training phase was too short for them to achieve their highest level of proficiency.
Since we counter-balanced the order of gesture sizes between participants and therefore controlled for this factor, we felt confident that primary task performance was independent from Gesture size and Location.
As a result, we omitted it from all further analyses.
Observation rate is the number of gesture observations made by a participant divided by the number of gestures performed by the actor.
Identification rate is the number of gestures correctly identified by a participant divided by the number of observations.
Sphericity was violated for observation rate by both Gesture size and Location , and for identification rate by Location .
For these analyses, we use Greenhouse-Geisser corrections.
Follow-up analyses showed that the observation rate at L1 was significantly worse than from all other locations , and L6 had a higher observation rate than its symmetric counterpart L2 .
As shown in Table 3 and Figure 7, participants had the highest identification rate from L4, followed by L6, and the worst observation rate from L1.
Identification rates from L1 and L2 were significantly worse than from all other locations .
As shown in Figure 4, small gestures were significantly better observed from L4 and L6  than from L1  and L2  .
Observation rate from L1 was significantly worse than from all other locations except L2 .
Medium gestures were best observed from L6  and L4  and worst observed from L1 .
Observation rates from L1 were significantly worse than from L3 through L6  and from L2 worse than from L6 and L4 .
Compared to small gestures, observation rate in L2 improved close to average .
As expected, mean differences became lower between symmetric locations L1-L7  and L2-L6  and stayed low between L3-L5  and L4-L6 .
Medium gestures were best identified from L4  and L6  and worst identified from L7 .
Identification rate from L7 was significantly worse than from L2, L4, and L6 .
As expected, mean differences became low between symmetric locations L1 - L7  and L2-L6  and stayed low between L3-L5  and L4-L6 .
Large gestures were best identified from L4 , L6  and L2  and worst identified from L1 .
There were no significant differences between locations.
At L1 and L2, participants showed significantly lower identification rates with small gestures than with medium and large gestures ; at L3 and L7, participants showed significantly higher identification rates with large gestures than with small and medium gestures .
Participants showed a significantly higher identification rate for the small gesture swipe: right than for the gestures swipe: left and tap: top right .
For medium gestures, participants identified swipe: top significantly less often than all gestures except tap: bottom right .
For large gestures, there were no significant differences.
We found a significant difference in mental demand between all three gesture sizes .
For physical demand and frustration, there were significant differences between large gestures and small and medium gestures .
Finally, participants rated small gestures as more effortful than medium and large ones .
We found that participants observed the small gesture circle: top significantly more often than the gestures tap: top left, tap: top right, and circle: bottom .
For medium gestures, participants observed tap: bottom right significantly less often than the gestures circle: left, circle: right, tap: top right, and swipe: top .
We also asked participants to rank the different gesture sizes in terms of perceived visibility, recognition accuracy, and their preference to work with .
A significant majority of participants ranked large gestures most visible  ,  and most recognizable  , .
In this discussion, we first explain how our results confirm our hypotheses and discuss some additional insights we gained from analyzing our results.
Then, we come back to our premise and lay out how our findings support Norman's idea of "big controls and big actions".
We describe some use cases, mention potential directions for future work, and address issues that come with the use of big gestures.
Finally, we list the limitations of our work.
Not surprisingly, the effect is smaller because there are other factors that affect identification rate.
Again, our regression analysis revealed, that one gesture  was a residual outlier.
For the curve fit, we removed this outlier ; we will come back to this particular case later in the discussion.
As predicted, participants showed significantly higher observation rates with large and medium gestures than with small gestures, and higher observation rates with large gestures than with medium gestures.
While this result is true on the  gesture-size scale , we also found a similar pattern when looking at the  gesture magnitude.
However, our regression analysis revealed that one large gesture  was a residual outlier.
For the curve fit, we removed this outlier ; we talk about this case later in the discussion.
We want to emphasize that the logarithmic relationship continues across different gesture sizes and morphologies .
This means we can generalize our findings to most gestures that fit our definition in the related work section.
In locations L1 and L2, gestures were occluded by the actor's body.
A comparison of symmetrical pairs L1-L7 and L2-L6 therefore shows how much occlusion affected participants' observation rate.
Our results showed that the mean differences in observation rate between L1 and L7 and between L2 and L6 decreased with increasing gesture size.
This implies that small gestures suffer significantly from occlusion and that this effect diminishes with increased gesture size.
With an unobstructed view to the actor, gesture size does not affect performance.
However, in multi-display environments where people move around freely, it is likely that occlusion will occur; in this case, larger gestures can enable higher group awareness.
Identification rates of all gestures were affected in similar ways by occlusion than observation rate.
For medium gestures, differences in identification rates between L1 and L7 and between L2 and L6 were smaller than these differences in observation rate.
We suspect that the location of the gestures on the 22" screen were responsible for this effect: five out of six gestures were performed close to the right edge of the screen, so observers were able to catch a glimpse of these gestures around the right side of the actor's body.
Participants showed significantly better performance with large gestures than with medium and significantly better performance with medium than with small gestures.
The overall identification rate of larger gestures is better than that of smaller gestures; even when observed, larger gestures are easier to identify than smaller gestures.
In locations L2, L4, and L6, participants were facing the actor, in locations L1, L3, L5, and L7, they were perpendicularly seated to the actor.
When pairwise comparing L1 - L2, L3-L4, L5-L4, and L6-L7, we found that participants performed on average better when facing the actor.
However, most of these comparisons showed no significant differ-
For small gestures, we found that "circle: top" was the easiest gesture to observe, significantly easier than both "taps" and "circle bottom" .
This was most likely because it had the longest execution time  and largest magnitude  among all small gestures.
For medium gestures, we found that "tap: bottom right" was significantly harder to observe than any other gesture except "swipe: right" .
Contributing factors were its low execution time  and its small magnitude .
For large gestures, we found that participants showed a significantly lower observation rate with "point: left, low" than with any other gesture .
As before, we assume that mostly execution time  and lack of magnitude  are responsible for this effect.
In addition, the gesture was performed very close to the body, which made it more difficult to spot than other large gestures, which were all performed away from the actor's body.
Norman's original idea was that big controls and big actions create awareness.
Our results showed that gestures, independently from their size, are indeed observable and can therefore improve group awareness: people know that something has happened.
When looking at identification rate, we can also give an initial estimation for the next step toward group awareness, knowing what exactly has happened.
Our results indicate that people can distinguish between at least six gestures.
We also showed that identification rate depends on more factors than observation rate.
A more thorough investigation of these factors could give us more insight about potential limitations, such as upper limits of an alphabet of discernible gesture, as well as guidelines for designing distinguishable gestures.
Another important issue is finding gesture sets with different levels of observability, so that interaction designers can select a gesture that matches an action's desired publicity.
There are many cases in which people would want to make their actions public.
Public gestures can be part of, for example, co-located multiplayer games where the group should be aware of certain actions.
Another example are Scrum-teams where the team should know about the completion of a single Scrum-task.
Likewise, there are many cases in which people want to keep actions private or do not want to distract others.
As said before, our findings show that people can control the publicity or privacy of their actions through gesture size.
There are, however, some disadvantages to large gestures.
For example, they require more physical effort, and there are some socio-cultural restrictions to the use of big gestures.
Again, we assume that large gestures will mostly be used in group environments, where each member accepts and understand large gestures in the context of their work.
For small and large gestures, we found no gesture that was consistently better or worse than the other ones.
For medium gestures, however, we found that participants performed significantly worse with "swipe: top" than with any other gesture except "tap: bottom right".
A detailed analysis showed that we can attribute more than half of the errors to confusing this gesture with the gesture "tap: top right".
These two seemingly different gestures share a similar post-stroke hold and retraction phase.
Apparently, participants oftentimes required the preparation and stroke phase of the actor's gesture to shift their attention from their primary working task to the perception phase of consequential communication.
To make gestures more distinguishable, we therefore recommend avoiding gestures that end with similar strokes and the same poststroke hold and retraction.
For example, the small-gesture swipes were rarely confused with the small-gesture taps.
There are a couple of limitations to our study.
While we selected our gesture sizes to reflect a broad variety of gestural interfaces, we only used a typical set of gestures within each size and not a broad variety of all possible gestures.
This allows us to only give an initial assessment and lower boundary about identification rates, leaving a more systematic approach to future work.
Common contextual and semantic knowledge, for example, can increase identification rates.
In addition, our study took place in a controlled laboratory environment.
We are confident but cannot guarantee how much our findings apply to a real-world scenario.
All directions in gesture descriptions were meant to be relative to the actor.
As a result, there was a danger that participants confused left and right and top and bottom when they were in front of the actor .
We analyzed all errors in conditions L6 and L7; no participant systematically confused any of these labels.
In this paper, we demonstrated that gestural interaction techniques can be used for creating visible device interaction, thus laying the groundwork for providing consequential communication to co-located collaborators.
We measured observation and identification rates of different gestures and showed that even small gestures are visible and could create consequential communication.
However, larger gestures are more easily observable, mainly due to a re-
In addition, increasing size makes gestures more easily identifiable.
Considering our findings, we encourage interaction designer to include gestural interfaces in their groupware applications, so that users can benefit from having consequential communication as an implicit method for gaining group awareness.
Baecker, R., Grudin, J., Buxton, W., and Greenberg, S. Touch, gesture, and marking.
Baudel, T. and Beaudouin-Lafon, M. Charade: remote control of objects using free-hand gestures.
Benford, S. and Fahlen, L. A spatial model of interaction in large virtual environments.
Bolt, R. Put-that-there: Voice and gesture at the graphics interface.
Cadoz, C. and Wanderley, M. Gesture: music.
In Trends in gestural control of music.
Air pointing: design and evaluation of spatial target acquisition with and without visual feedback.
Dourish, P. and Bellotti, V. Awareness and coordination in shared workspaces.
Endsley, M. Design and evaluation for situation awareness enhancement.
Endsley, M. Toward a theory of situation awareness in dynamic systems.
Goldberg, D, Richardson, C. Touch-typing with a stylus.
Gutwin, C. and Greenberg, S. Workspace awareness for groupware.
Gutwin, C. and Greenberg, S. Design for individuals, design for groups: tradeoffs between power and workspace awareness.
Gutwin, C. and Greenberg, S. A descriptive framework of workspace awareness for real-time groupware.
Hanna, E. and Meltzoff, A.
Peer imitation by toddlers in laboratory, home, and day-care contexts: implications for social learning and memory.
Harrison, C., Tan, D., and Morris, D. Skinput: appropriating the body as an input surface.
Speech and gestures for graphic image manipulation.
