Subtle changes in analysis system interfaces can be used purposely to alter users' analytic behaviors.
In a controlled study subjects completed three analyses at one-week intervals using an analysis support system.
Control subjects used one interface in all sessions.
Test subjects used modified versions in the last two sessions: a first set of changes aimed at increasing subjects' use of the system and their consideration of alternative hypotheses; a second set of changes aimed at increasing the amount of evidence collected.
Results show that in the second session test subjects used the interface 39% more and switched between hypotheses 19% more than in the first session.
They then collected 26% more evidence in the third than in the second session.
These increases differ significantly  from near constant control rates.
We hypothesize that this approach can be used in many real applications to guide analysts unobtrusively towards improved analytic strategies.
Two sets of non-functional changes were made to the analysis support interface before the second and third sessions.
These changes were designed to improve three hypothesized or observed analytic deficiencies: analysts' excessive reliance on memory, inability to consider hypotheses in parallel, and insufficient search for evidence.
Our quantitative results show that the interface changes succeeded in alleviating these deficiencies.
Compared to a control group, our test subjects used the support module more, they switched among hypotheses more often, and they collected more evidence per hypothesis.
Our data not merely show that changes in interfaces translate into different user behavior, but demonstrate that we can leverage interface design and cognitive principles in controlled ways to overcome known analytic deficiencies.
Our work was motivated by extensive cognitive science research showing that human thinking is subject to heuristics and biases that often lead to suboptimal decision making .
Recent visual analytics efforts  suggest that visualization and interfaces can offer support against such cognitive biases and heuristics, possibly by leveraging the expertise of the cognitive science and intelligence communities .
However, to the best of our knowledge, few concrete attempts have used visual analytics techniques to align descriptive analysis  to normative analysis .
Here, we evaluate a potential solution inspired by previous work in the fields of behavioral economics and human-computer interaction : libertarian paternalism  and persuasive technology  are similar concepts that advocate designing choice layouts and computer interfaces so that they nudge users towards decisions that are in their best interest.
Contributions: We hypothesize that subtle changes in visualization interfaces can be used in controlled ways to guide users towards more normative analysis and provide quantitative evidence that supports this hypothesis.
We also present qualitative observations on analytic strategies, biases, and heuristics that our subjects used in their tasks.
Roadmap: Next, a related work section summarizes existing research that we build on and extend.
We then describe a user-study that validates our hypothesis and summarize its results.
We end with a discussion and concluding remarks.
This paper provides experimental support for the hypothesis that we can use subtle changes in the interfaces of visual analysis systems to influence users' analytic behavior and thus unobtrusively guide them towards improved analytic strategies.
We posit that this approach may facilitate the use of visual analytics expertise to correct biases and heuristics documented in the cognitive science community.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
By making subtle, non-functional changes in the interface of an analysis support module  we generated statistically significant changes in users' analytic behavior in a visual problem-solving task.
A first set of changes nudged subjects to increase their use of the analysis module by 39%  in an attempt to support our subjects' working memory.
It also caused them to switch among hypotheses 19% more often , indicating more consideration of alternative hypotheses.
A second set of changes then led subjects to gather 26% more evidence per hypothesis .
These three increases compare to smaller or negative variations in a control group .
Thaler and Sunstein's work  in the field of behavioral economics popularized the term choice architecture -- how a set of choices is presented to a consumer -- and the concept of libertarian-paternalism -- designing choice architectures that "nudge" consumers towards decisions that are in their own interest  while unrestricting choice .
A similar concept was proposed in the HCI domain by Fogg  who defines persuasive technology as "interactive information technology designed for changing users' attitudes or behavior".
We build on these previous approaches and demonstrate empirically how the nudge paradigm can further the visual analytics agenda.
Sunstein and Thaler as well as Fogg motivate their approaches with two arguments, which they support with experimental evidence.
First, any choice architecture or computer interface necessarily influences decision-making behavior, whether intentionally or not.
Second, as already shown, research indicates that people's choices and behaviors are not always aligned with their goals.
From a visual analytics perspective, this means that even if an analyst's objective is to select the optimal course of action based on available data, cognitive biases and heuristics can steer him towards suboptimal results.
Both approaches have inspired scientific results that validated their feasibility.
In the technological realm, the enhanced speedometer  changes its appearance based on the current speed limit , encouraging users to stay within speed limits, while the smart sink  augments a normal sink with visual cues that make energy consumption apparent.
This works has provided inspiring design models for the analysis nudges presented here.
Our work complements current research by using a visual analytics methodology to create a link between observed analytic deficiencies and corrected behavior.
Perhaps closest to our work is that of Savikhin et al.
We extend this result by linking it to the more general nudging approach proposed by Sunstein, Thaler and Fogg, by exploiting interface design in general, and by providing an experimental validation for a high-level analytic task.
Our work is motivated by extensive cognitive science research demonstrating that people are prone to a range of analysis biases and heuristics that can lead to analysis errors .
A specific manifestation of such effects occurs in the context of hypothesis-driven analysis.
For example, satisficing  limits analysis to a hypothesis that is good enough, a confirmation bias conditions us to confirm hypotheses rather than disconfirm them , and people often fail to to consider alternative explanations .
Many such studies have been conducted with naive subjects, but research shows that biases and heuristics also occur in scientific and clinical settings, with similar error-inducing effects .
Several results suggest however, that experts, while not immune to such biases and heuristics, may be better equipped to overcome them .
Evidence suggests that users can be helped to overcome biases and heuristics and instead use normative analysis, a change that usually yields improved analytic performance.
For instance, subjects conditioned to pursue alternative hypotheses and disconfirming evidence reached solutions to a scientific puzzle more often .
Analogies and subsequent unexpected findings lead to consideration of multiple hypotheses and novel findings .
Medical students using hypothesis-driven analysis outperformed those using a dataimmersion approach .
These results support our goal of guiding users towards normative analysis practices.
We conducted a controlled user study to test the hypothesis that small changes in a visualization system's interface can be used to produce targeted modifications in users' analytic workflows.
This section presents the design of this study.
We start with an overview description of the methodology used and continue with an in-depth presentation of each aspect of the study.
Subjects completed an analysis task inspired by a real scientific problem using a visualization and an analysis-support interface .
Each subject performed three such analysis sessions at one-week intervals.
Each session lasted roughly one hour.
Thirty-six subjects, mostly undergraduate and graduate students, were divided into two groups: 21 test and 15 control subjects.
The control group solved all three tasks using the same analysis-support interface.
Conversely, test-group subjects were given slightly different versions of the analysissupport interface in each session.
Specifically, two sets of interface nudges were added to the analysis system before the second and third sessions.
We hypothesized that, while changes between sessions would be observed in both groups due to task-learning effects, the test group would exhibit additional effects due to the interface nudges.
The analysis task was inspired by the proteomic domain: finding causal paths in protein interaction networks to explain the interdependency of pairs of proteins that are not directly connected.
None of the subjects was familiar with the task or background material beforehand and all received a 20-minute tutorial at the beginning of the study.
Our test system was instrumented to log users' interactions automatically.
Subjects were also asked to distill their analysis in a written questionnaire at the end of each of the three analysis sessions.
We analyzed the datasets both quantitatively, to look for support for our nudging hypothesis, and qualitatively, to gain insight into how subjects approached their task.
The paper "Illuminating the path"  introduced the field of visual analytics  as "the science of analytical reasoning facilitated by interactive visual interfaces".
The work presented in this paper extends the VA research agenda, concentrating on designing interfaces and visualizations that support the aggregation of data insights into cohesive scientific theories.
Our work is tangential to VA results on understanding the sense-making process  and draws on position papers that argue for leveraging the expertise of cognitive science and intelligence communities .
In our evaluation we use a system inspired by a range of efforts to design analysis support interfaces that let users store, annotate, and browse analysis artifacts such as hypotheses or evidence .
Proteins are functional molecules within cells that interact with one another to form complex causal pathways that determine the response of cells to events.
Such protein interactions are the object of intense scientific research because understanding these cellular pathways would let researchers devise efficient drugs to influence a cell's behavior without causing unwanted side effects.
Proteomicists often use visualizations of interaction networks to understand changes in protein activation patterns measured in experiments.
A distinct class of experiments is knockout experiments: here researchers deactivate particular proteins and compare protein activation levels before and after the removal.
Our subjects were given network visualizations that were said to depict protein interactions documented in recent publications.
Figure 1 shows one of three distinct networks that subjects were asked to analyze.
The networks were manually created and laid out.
The familiar Google Maps user interface was used to display the network images and offer basic interaction.
Clicking on nodes or edges opened information bubbles referring to these particular elements.
Interactions were described by fictional, brief paper abstracts detailing the particulars of each interaction and the context in which it was discovered.
Subjects were told that a knockout experiment had been performed on a specific type of cell.
They were informed that a protein was removed from the cell and that researchers subsequently observed changes  in the levels of several proteins.
These changes were marked on the network with arrows.
Finally, subjects were asked to use the available information to determine network paths likely to have produced those changes and to rank them in order of plausibility.
This network task represents a visual, complex, and open-ended implementation of causal reasoning tasks that have been typical choices of cognitive studies .
Our networks used proteomic terminology but introduced fictitious proteins, interactions and interaction mechanisms.
Thus, the probability of a regulation chain was determined by the logical consistency of the evidence presented.
The key rules that subjects were expected to extract from the evidence and use in their analysis were: the probability of a depicted interaction is lower if it was documented in species and cells other than those investigated in the knockout experiment; a correlation between two proteins should be treated as an edge with uncertain directionality; interactions could describe direct or inverse regulation mechanisms; and the edges sequence in a solution path should justify the sign of the observed change.
These assumptions, along with a general description of protein signaling, were illustrated in a 20-minute tutorial  and were clarified on request.
Moreover, essential terms were highlighted in all evidence text and in-situ explanations were displayed upon mouse clicks .
The order in which the three networks were presented to users was alternated to minimize the chance of network dif-
The two modified analysis interfaces include three evaluated nudges: a box lists online users actively interacting with the analysis module , a color gradient  shows recently analyzed hypotheses , a redesigned, larger evidence box asks users to commit to the implications of a hypotheses lacking associated evidence .
In addition to the protein network viewer, an analysis support interface augmented the experimental environment .
As noted in Section 3.1, control subjects used the same base analysis interface in all three sessions.
Test subjects started with an identical interface but then used upgraded versions in the second and third sessions.
These versions, obtained by incrementally including two sets of evaluated nudges in the base interface, are shown in Figure 2.
The base analysis module contained three lists in which users could store their hypotheses, confirming, and disconfirming evidence.
Hypotheses were entered into the system as noncyclical network paths by clicking on sequences of connected nodes.
Evidence was inserted into the confirming or disconfirming category by typing free text in a pop-up box.
Selecting existing hypotheses would highlight their corresponding paths on the visualization and display their associated evidence, thus allowing subjects to revisit and compare hypotheses.
Subjects were familiarized with these features in the tutorial video at the beginning of the study.
Three nudges were designed to alleviate three analytic deficiencies.
First, we assumed that subjects would rely on their working memory rather than use the analysis system.
Second, based on cognitive science studies, we assumed that subjects would have trouble considering multiple hypotheses in parallel.
Third, we hypothesized that subjects would gather mostly confirming evidence for their hypotheses, and ignore the aspect of disconfirming evidence.
The results of our initial session one runs caused us to adjust our last assumption: subjects were gathering approximately equal amounts of confirming and disconfirming evidence but in overall small amounts.
We refined the design of our last nudge to target this issue better.
As already noted, the first evaluated nudge  aimed to increase users' reliance on the analysis module.
Our design rested on the assumption that if subjects knew other users were actively interacting with the module, they would do so as well.
To test this assumption, a section listing online users was added to the base analysis module.
As users interacted with the module, this was reflected in a publicly visible status message .
Fake user-bots were added to ensure that a nudge factor was present at all times.
This design was inspired by research on conformity effects and motivational factors for online contributions.
Specifically, humans change their behavior to match that of others  to gain social approval , or because they derive utility information from observing what others do .
In addition, visibility encourages users of social networks to increase their online contributions .
The second nudge was designed to encourage users to compare and contrast hypotheses in parallel rather than perform a sequential search in hypothesis space.
Initially we planned to evaluate this nudge by itself but ultimately merged it with the first one so as to make the length of the study manageable.
Recently active hypotheses were highlighted in the hypotheses list by using a color gradient based on the recency score.
Finally, thresholding the recency score allowed us to determine the number of a user's active hypotheses, display this information in the user status , and sort users based on how many hypotheses they were investigating.
This offered a visual and status reward.
While a user could trick the system by quickly switching between hypotheses, this was taken into account in data analysis  and we have observed just two intentional instances of it.
These first two nudges were integrated into the analysis interface before the second session.
Finally, the third nudge, deployed before the last session,
To that end we modified the evidence collection part of the interface .
First, the evidence-collection area was made more visually interesting and distinct from the rest of the interface.
Second, if no confirming or disconfirming evidence had been entered for a hypothesis, the evidence boxes would read "0 chances that hypothesis is false" or "hypothesis is unlikely".
This essentially required subjects to commit to extreme cases -- something that people are known to avoid .
Third, modification introduced unintentionally while implementing the design was that the evidence boxes in this nudge were larger than in the base interface.
We hypothesize that this nudge could be restricted to disconfirming evidence only, in which case it could potentially alleviate confirmation biases .
As noted, our subjects did not exhibit a confirmation bias in the early stages of the study, so that we resorted to testing the more general case of increasing the amount of total evidence.
Our study included a total of 36 subjects.
Six of them were young professionals, 18 were undergraduates, and 12 were graduate students.
Twenty-six of the subjects were active in sciences, while 10 were humanities students.
None of the subjects had previous experience with proteomic analysis.
Thus, all subjects relied solely on the tutorial provided at the beginning of the study.
Subjects were randomly distributed in control  and test  groups such that the two groups had similar distributions of gender and age .
Subjects were compensated for their participation.
Ease of hypothesis elicitation: A pilot run showed us that free-text specification of hypotheses would have produced considerable variability in what users entered as hypotheses.
To be able to compare results across subjects we limited hypotheses to paths of connected proteins.
This interaction mode, reinforced by the tutorial video, gave subjects an easy "recipe" for generating hypotheses: any network path was a valid hypothesis.
Lack of motivation: Our study did not involve monetary incentives to encourage subjects to provide valid solutions.
As a result, several subjects appeared not to devote significant effort in searching for clues beyond those immediately noticeable.
Unforeseen problem-solving strategies: A few of our early subjects copied the network on paper and annotated each interaction and protein.
This strategy is not scalable to real protein interaction networks and it does not capture the exploratory nature of analysis.
To avoid this, we instructed the rest of the subjects not to use such exhaustive analysis strategies.
Task misunderstanding: Instead of constructing short paths that linked the knockout protein to each changing protein, two subjects looked for long paths that linked the knockedout protein and all arrow proteins  together.
We retained these results because the subjects used this interpretation consistently in all three sessions.
Variation in analysis times: We urged users to spend approximately 60 minutes on each session.
Several subjects, however, insisted on finishing earlier.
Moreover, a few datasets showed prolonged intervals of inactivity and several users were observed to take web-browsing or texting breaks.
In our analysis we eliminated intervals with no activity and normalized all measurements by the time spent on the task.
Small number of subjects: Our sample size  was relatively low for the open-ended tasks our study involved.
However, we note that the trends in the data became apparent with as few as six users in each group and changed very little throughout the experiment.
Effect of change not captured: Our study does not capture the amount by which interface changes amplify the saliency of our nudges.
It may well be that nudges are less observable and effective if they are introduced into the first system release.
We found that by default most users did a comparative analysis of hypotheses at the very end.
Our nudge however was designed to encourage users constantly to consider alternatives.
In a second phase we also made a qualitative analysis of our subjects' workflows.
Our goals were to understand the dominant analytic strategies and behavioral patterns, and to verify the degree to which biases and heuristics were applied.
The premise of our experiment was that interface nudges would cause test subjects to change their behavior between sessions differently from how control subjects' behavior would evolve naturally as a consequence of learning or boredom.
Figures 3-5 demonstrate the validity of our premise by contrasting the relative changes in performance measures between consecutive sessions in both experimental groups.
As expected, change was negligible in control subjects , but was significant for test subjects when a nudge was present .
This suggests that subjects were responding not simply to interface changes but instead to nudges targeting particular performance measures.
Test subjects contributed 39% more hypotheses and evidence items to the analysis module in the second session than in the first.
This compares to an increase of only 15% in the control group .
Contributions remained close to constant between the second and third sessions in both the control and test group .
This conforms to the expected behavior since no nudge targeting contributions was added between these sessions.
The difference in switches between hypotheses was an increase of 18% in test subjects versus a decline of 17% in control subjects .
The first two nudges were both added before the second session.
Thus, we assign either of the observed changes not to any single nudge but to all interface changes made between the first two sessions.
The amount of evidence collected per hypothesis remained fairly constant between all consecutive sessions in the control group with a decrease of 2% .
Test subjects however, gathered on average 24% more evidence per hypothesis in the third condition than the second.
Thirty-two subjects completed all three sessions while four completed only the first two for a total of 28 x 3 + 4 x 2 = 104 datasets.
Four of the subjects, two from each group, solved the tasks on paper using exhaustive annotation of the networks.
Three additional users also switched to this approach in the final session.
All these data were discarded from the analyses leaving 104 - 4 x 3 - 3 x 1 = 89 datasets from 13 control subjects and 19 test subjects.
We measured and analyzed three quantitative indicators to support our nudging hypothesis.
First, we recorded the number of hypotheses and evidence entered into the system as a proxy for the degree to which subjects relied on the interface to trace their analysis.
This number was normalized by the time, in minutes, subjects spent on each session.
Second, we measured the number of times a subject switched between hypotheses and normalized it by the number of hypotheses, as an indicator of the degree to which hypotheses were analyzed in parallel during analysis.
Third, we recorded the number of evidence items collected and divided it by number of hypotheses.
In the case of hypotheses switches, we ignored hypotheses selections lasting less than 5 seconds because we observed that users sometimes cycled rapidly through hypotheses as a method of gauging progress.
Changes between the first two sessions  caused test subjects  to increase the number of hypotheses and evidence items entered into the analysis system by an additional 24% over the control subjects' relative increase.
The interface changes before the third session had no significant impact on this performance measure .
Observed workflows: More than half our subjects started with an initial exploration of the network.
This exploration was not hypothesis driven and typically lasted between three and six minutes.
Subjects then moved on to a hypothesisdriven analysis, trying to connect arrow proteins to the knockout protein .
We could discern two strategies for entering hypotheses.
Most subjects would pick a candidate path, do a pre-evaluation of its likelihood, enter it into the system if it was plausible, and then follow with a second pass to summarize and document evidence.
These users would often revisit hypotheses and compare them.
A few subjects added hypotheses without prior exploration and then summarized evidence in a following pass.
Generally, they did not reevaluate those hypotheses again until a final pass when they decided on a global likelihood ordering.
Observed biases and heuristics: An interesting finding was that confirmation bias was not dominant.
In fact, subjects gathered slightly more disconfirming evidence than confirming evidence.
A number of qualitative observations provide further support for this finding.
First, several users gathered almost exclusively disconfirming evidence, while others pruned paths that had strong negative evidence.
Second, one subject would copy entire sections from the information bubbles and enter it directly as confirming evidence, but would always carefully summarize negative evidence.
This suggests that she recognized the higher diagnostic value that the disconfirming evidence would have in her final ranking.
We noticed several cases in which subjects added complicated paths before shorter, more intuitive ones.
We also noticed an inability to operate with varying degrees of probability.
Our network setup was well suited to discovering conjunction fallacies, which occur when a specific condition is deemed more likely than a general one.
In our network task short paths should be more likely candidates for analysis than longer paths.
In general, our subjects seemed aware of this principle.
In fact most new hypotheses abided by this rule.
Additionally, several subjects added the short length of a path as positive evidence.
However, we noticed that subjects' analytic strategies tricked them into the conjunction fallacy in a significant number of cases.
We observed three main scenarios leading to this.
First, the favored method of expanding a set of hypotheses was to modify an existing one by rerouting part of its path.
Changes between the last two sessions  caused test subjects  to gather 24% more evidence for their hypotheses as opposed to a constant evidence/hypotheses ratio  between all consecutive control sessions .
Changes in the test group before the second session  produced non-significant changes in evidence collection as compared to the control group.
Most subjects avoided picking completely new routes, especially in network areas where they had already done some analysis.
Such small changes to initially short paths lead subjects to analyze increasingly longer paths.
Ultimately, subjects spent considerable time on long paths that were less likely than unexplored shorter options.
Second, subjects occasionally considered longer paths linking together multiple arrow proteins more likely than short paths from the knockout protein to each of those arrow proteins.
We hypothesize that users were looking for good unifying stories, a known cognitive tendency.
Interestingly, one of the subjects confessed that he was aware of the conjunction fallacy but that the "story was too good" to be irrelevant.
The third reason for multiple instances of conjunction fallacy was tied to the network layouts.
The way paths were visually displayed had an impact on which ones were chosen for analysis.
Most subjects preferred paths that described fairly continuous visual arches, or that were symmetric with ones they had already looked at.
Sharp-angled paths were usually selected last even if they were shorter than already analyzed hypotheses.
Another effect observable in several datasets was that symmetrical paths were more often compared to each other than to other hypotheses.
A few loose design guidelines, however, can be distilled from our work.
First, putting collaborative elements and conformity triggers in analysis systems can nudge users to change their behavior.
We hypothesize that artificial modelanalysts, such as used in our experiment, could nudge users towards conforming to a desired behavior.
Second, visual rewards, such as our recency score, will encourage users to consider options in parallel.
Third, messages in text areas, perhaps in conjunction with box size, may be used for boxes that should not be left empty.
Finally, from our qualitative analysis of our subjects' workflows we hypothesize that ways of automatically suggesting hypotheses may alleviate some of the observed conjunction fallacies and that subjects would benefit from support for multiple attribute analysis.
Both such mechanisms would need to be domain specific and are beyond the scope of the present work.
The data distributions may suggest that nudges, rather than uniformly targeting all subjects, tend to be particularly effective for a subgroup and less so for the rest.
As seen in Figures 3-5, measurements obtained from test subjects appear to form two clusters: one with values similar to those measured in the control group, and one with distinctively higher values.
These clusters do not correlate with the order in which networks were presented to users.
However, the data gathered as part of this study is insufficient to test this hypothesis.
The analytic biases and heuristics targeted in our study were chosen because they are amply documented in the cognitive science literature.
It is likely that one or more of these effects do not appear or are beneficial in some areas or settings.
In fact naturalistic decision making , a distinct research area, models situations  in which heuristics are an efficient analytic strategy.
The aim of this study was not to eliminate a specific set of biases and heuristics but to demonstrate that if such effects are identified we can use interface elements to reduce their occurrence.
Our study did not replicate several biases and heuristics documented in the cognitive science literature.
Most notably, people are thought to be unable to elicit many hypotheses and to be biased towards gathering predominantly confirming evidence.
Conversely, our subjects generated many hypotheses and showed no confirmation bias.
We see two possible explanations for this.
First, two of our study limitations may be responsible: the ease of generating hypotheses and subjects' lack of motivation led them to pursue multiple hypotheses and not develop attachments to favored ones.
An alternative explanation is that people can switch from a normal working mode to an analysis mode in which normative principles are more carefully observed.
Research by Dunbar  hints at this hypothesis.
This latter possibility supports our choice of analysis task.
Shorter and more focused tasks like the ones used in many cognitive experiments can be applied to large numbers of users and provide clean data.
Some of the findings reported in this paper may seem unsurprising.
That interface design can alter analytic workflows is evident, as is the fact that online visibility is correlated with increased online activity .
However, our study data shows more than that interface changes translate into different user behavior.
Our contribution lies in demonstrating that interface elements can be leveraged in controlled ways to unobtrusively correct users' strategies: our subjects' deficiency in supporting their hypotheses with evidence was observed in the first session and alleviated by a redesigned analysis-support module in the third session.
We believe this approach is valuable because it has the potential to correct and improve users' strategies without having to rely on coercive or obtrusive elements such as pop-up messages or help agents.
Our work was primarily aimed at providing experimental support for the nudge paradigm in the visual analytics domain rather than providing a set of design guidelines.
The nudge design space warrants more exhaustive exploration because it can either provide a tool for guiding users towards better analytic strategies or help us understand how our interfaces unintentionally shape users' exploratory and analytic patterns.
Our work actualized interesting questions about the degree to which tutorials, ways of entering and storing hypotheses, and even simple design choices such as text-area size and color can influence users' behavior.
As noted in the related work section, several studies indicate that there are observable differences between laboratory settings and real scientific or clinical situations.
Similarly, our study might have been more informative had we tested domain experts in their field of research rather than naive users on unfamiliar tasks.
It remains uncertain whether domain experts, who generally follow well established workflows, can be nudged as easily as our subjects.
Moreover, a high familiarity with an analysis system may also cause expert subjects to overlook new interface nudges.
Unfortunately, domain experts are scarce and the variability in the scientific problems they solve is high.
Thus, quantitative studies that faithfully replicate real-life scientific settings are scarce and likely to remain so.
Our choice of task and users implements a realistic approximation that provides insight into how to minimize the impact of biases and heuristics in scientific workflows.
This endeavor is important because, as remarked at the beginning of the paper, domain experts are not immune from cognitive biases and heuristics and often benefit from normative analysis strategies.
We presented results from a quantitative user study demonstrating that controlled changes in the interface of an analysis system can be employed to correct potential deficiencies in users' analytic behavior.
Specifically, we manipulated the design of a basic analysis tool over three analysis sessions to produce three changes in our subjects' analysis.
First, subjects were nudged to increase their reliance on the analysis-support module that accompanied the visualization.
Second, subjects were nudged to analyze hypotheses in parallel rather than sequentially.
Third, subjects were nudged to gather more evidence for their hypotheses.
The significance of our work is threefold.
First, we give an account of how even the simplest design decisions shape users' analytic behavior.
Second, we advance visual analytics efforts by introducing and validating an approach that leverages visualization environments to correct analytic biases and heuristics reported in the cognitive science literature.
Third, we provide a short overview of analysis workflows, and biases and heuristics that our subjects used on a scientifically inspired analysis task.
Ames, M., and Naaman, M. Why we tag: motivations for annotation in mobile and online media.
In Proceedings of the SIGCHI Conference on Human factors in Computing Systems, ACM , 980.
Arroyo, E., Bonanni, L., and Selker, T. Waterbot: exploring feedback and persuasive techniques at the sink.
In Proceedings of the SIGCHI Conference on Human factors in Computing Systems, ACM , 639.
Asch, S. Studies of independence and conformity: a minority of one against a unanimous majority.
Building and applying a human cognition model for visual analytics.
Hastie, R., and Dawes, R. Rational choice in an uncertain world.
Journal of the Indian Academy of Applied Psychology , 107.
Kenrick, D., Maner, J., Butner, J., Li, N., Becker, D., and Schaller, M. Dynamical evolutionary psychology: Mapping the domains of the new interactionist paradigm.
Confirmation, disconfirmation, and information in hypothesis testing.
Klein, G. A recognition-primed decision  model of rapid decision making.
Kumar, M., and Kim, T. Dynamic speedometer: dashboard redesign to discourage drivers from speeding.
Oinas-Kukkonen, H., and Harjumaa, M. Towards deeper understanding of persuasion in software and information systems.
In First International Conference on Advances in Computer-Human Interaction, IEEE , 200-205.
Pirolli, P., and Card, S. The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis.
In Proceedings of International Conference on Intelligence Analysis, vol.
The discard of study evidence by literature reviewers.
Savikhin, A., Maciejewski, R., and Ebert, D. Applied visual analytics for economic decision-making.
In IEEE Symposium on Visual Analytics Science and Technology, 2008.
Simon, H. Rationality as process and as product of thought.
Social influence and intergroup beliefs: The role of perceived social consensus.
Stasko, J., Gorg, C., and Liu, Z. Jigsaw: supporting investigative analysis through interactive visualization.
Sunstein, C., and Thaler, R. Libertarian paternalism is not an oxymoron.
Thaler, R., and Benartzi, S. Save More Tomorrow: using behavioral economics to increase employee saving.
Thaler, R., and Sunstein, C. Nudge: Improving decisions about health, wealth, and happiness.
Thomas, J., and Cook, K. Illuminating the path: The research and development agenda for visual analytics.
Wason, P. Reasoning about a rule.
The Sandbox for analysis: concepts and methods.
In Proceedings of the SIGCHI conference on Human Factors in computing systems, ACM New York, NY, USA , 801-810.
Yang, D., Rundensteiner, E., and Ward, M. Nugget discovery in visual exploration environments by query consolidation.
In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, ACM New York, NY, USA , 603-612.
