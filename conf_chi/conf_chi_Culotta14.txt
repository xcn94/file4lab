Understanding the relationships among environment, behavior, and health is a core concern of public health researchers.
While a number of recent studies have investigated the use of social media to track infectious diseases such as influenza, little work has been done to determine if other health concerns can be inferred.
In this paper, we present a large-scale study of 27 health-related statistics, including obesity, health insurance coverage, access to healthy foods, and teen birth rates.
We perform a linguistic analysis of the Twitter activity in the top 100 most populous counties in the U.S., and find a significant correlation with 6 of the 27 health statistics.
When compared to traditional models based on demographic variables alone, we find that augmenting models with Twitter-derived information improves predictive accuracy for 20 of 27 statistics, suggesting that this new methodology can complement existing approaches.
In this paper, we investigate the use of social media as a complementary data source to identify at-risk communities.
The popularity of websites like Twitter and Facebook continues to grow, making unprecedented amounts of information about attitudes and behaviors publicly available.
Given the research in economics , socio-linguistics , and psychiatry  indicating the relationship between language and health, we examine whether linguistic patterns in Twitter correlate with health-related statistics.
For each of the 100 most populous counties in the U.S., we collect 27 health-related statistics from the County Health Rankings & Roadmaps project, including health outcomes, behaviors, socio-economic status, and environmental factors.
We also collect over 1.4M user profiles and 4.3M posts from Twitter over a nine month span from the same 100 counties.
We then perform a statistical analysis to identify how accurately these health outcomes can be predicted from the Twitter data and which linguistic markers are most predictive of each statistic.
Our experiments2 investigate four research questions, the answers to which we summarize below: RQ1.
Is Twitter activity predictive of county-level health statistics?
We find a significant correlation on held-out data for 6 of 27 statistics, including obesity, diabetes, teen births, health insurance coverage, and access to healthy foods.
How does the linguistic representation affect accuracy?
We find that the LIWC lexicon  is more predictive than alternatives, and that normalizing linguistic vectors by the number of users in a county can greatly improve accuracy.
Does Twitter activity provide more information than common demographic covariates?
We find that models that augment demographic variables  with linguistic variables  are more accurate than models using demographic variables alone for 20 of the 27 health statistics we consider.
For two , the Twitter model in isolation is actually more accurate than the demographic variable model.
These results suggest that the two sources of information are complementary.
What are the linguistic indicators that are most predictive of each outcome?
Chronic diseases are the leading cause of death and disability in the U.S. and account for 75% of health care costs.1 Understanding the interaction among environment, behaviors, and health outcomes is critical to developing informed intervention strategies.
In response, the U.S. Centers for Disease Control and Prevention leads multiple community health data collection and intervention efforts such as the Behavioral Risk Factor Surveillance System, the National Health Interview Survey, and the Health Communities Program.
A major goal of these initiatives is to identify vulnerable populations in order to better target intervention strategies.
While these programs provide tremendous insight, they require considerable time and effort and are often limited in sample size, frequency, or geographic granularity.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Copyright is held by the owner/author.
Publication rights licensed to ACM.
For example, references to religion and certain pronouns  correlate with better socioemotional support; references to money and inhibition correlate with lower unemployment; and references to family and love correlate with higher rates of teen births.
While this new methodology requires further experimentation, we believe it can aid public health researchers by providing  a more nuanced alternative to demographic profiles for identifying at-risk populations;  a low-cost method to measure risk across different subpopulations;  a process to help formulate new hypotheses about the relationship between environment, behaviors, and health outcomes, which can then be tested in a more controlled setting.
The remainder of the paper is organized as follows: we first review related work, then we describe the data and its collection.
We next present our method for representing the linguistic activity of each county and the experimental framework for measuring accuracy and identifying significant linguistic variables.
Finally, we present the results and discuss their implications.
Most of these focus on detecting explicit mentions of a symptom of interest .
In contrast, the present work investigates more nuanced linguistic cues that correlate with the overall health of a population.
While some ancillary data is used for comparison , no correlation analysis is performed with obesity statistics.
Additionally, Paul & Dredze  use a topic model to discover obesity-related tweets, finding a .28 correlation with state obesity statistics.
Our methodology is most similar to that of Schwartz et al.
Here, we also use LIWC and PERMA lexicons as features in a regression model of county statistics.
In the context of this related work, the primary contributions of this paper are as follows:  we present the first large-scale social media analysis across a diverse set of 27 measures of community health;  we provide an empirical comparison of several important methodological decisions, such as linguistic lexicons, vector normalization, and source of linguistic content;  we provide a rigorous statistical treatment that identifies linguistic indicators from social media that are significant predictors of health outcomes even after controlling for demographic variables.
Language has long been investigated as an indicator of health.
For example, Gottschalk  performed a content analysis of patients to determine psychological state, such as anxiety, hostility, and alienation.
Pennebaker  provides an excellent review of research connecting linguistic patterns to demographics, personality, psychology, mental health.
While many studies support the connection between mental health and language, the connection between physical health and language is less well-established.
Some studies have reported correlations between "Type A" language and heart diseases and positive emotional language with longevity .
Given growing evidence supporting the link between emotional well-being and health , estimating psychological health may serve as a predictive surrogate for physical health.
The emerging study of the economics of language has also investigated how language relates to decision-making, which in turn can affect health.
For example, in a study of 76 countries, Chen  found that certain grammatical properties correlate with higher rates of savings and lower rates of smoking and obesity, concluding that some linguistic constructs may foster future-oriented behavior.
Chiswick  investigates how language proficiency of immigrants can impact employment and other socio-economic factors.
Using data from the U.S. Census' State-Based Counties Gazetteer,3 we collected the top 100 most populous counties in the U.S. along with their geographical coordinates.
Each county is assigned a Federal Information Processing Standards  code as a unique identifier.
The County Health Rankings & Roadmaps,4 a partnership between the Robert Wood Johnson Foundation and the University of Wisconsin Population Health Institute, aggregates county-level health factors from a wide range of sources, including the Behavioral Risk Factor Surveillance System, American Community Survey, and the National Center for Health Statistics, collected over the past three years.5 These publicly available data contain county statistics on 30 measures of mortality, morbidity, health behaviors, clinical care, socio-economic factors, and physical environment.
For each of the top 100 most populous counties, we collected 27 health statistics .
These are listed in Table 3.
As space precludes a precise definition of how each statistic was computed, we refer the reader to the County Health Rankings website for more information.
We describe some of these in more detail in the Discussion section.
We next constructed a set of 100 Twitter queries consisting of one geographical bounding box for each county, approximated by a 50 square mile area centered at the county coordinates obtained from the U.S. Census.6 We then submitted these queries continuously to Twitter's search API from December 5, 2012 to August 31, 2013 .
These queries return tweets that have been geolocated, typically tweets issued from a mobile device.
This resulted in 4.31M tweets from 1.46M unique users.
For each tweet, we retain the tweet content as well as the user description field, a short, user-provided summary .
Figure 1 shows distributions of tweets per county, users per county, and tweets per user.
While the demographic distributions of Twitter users are thought to skew young and urban , it is worth noting that these 1.46M users represent over 1% of the total population of these 100 counties .
As expected, Twitter usage varies significantly by county size.
On average, we collect 14.5K users per county, with 66 counties containing at least 10K users.
Hudson County  has the most with 52K users, Honolulu County the least with 845.
The tweets per user graph exhibits a typical long tail -- a few users tweet very often, but most tweet infrequently.
We note that this data collection methodology differs from that of Schwartz et al.
Given a collection of tweets categorized by county, we next must distill them into a set of variables to correlate with the health statistics.
Due to the small number of validation points  and the large number of potential variables , rather than considering words as variables, we instead consider word categories.
We build on prior work that considers two lexicons: * LIWC: The 2001 Linguistic Inquiry and Word Count lexicon  contains 74 categories and 2,300 word patterns .
Each word pattern may belong to multiple categories .
This lexicon was developed over a number of years to identify categories that capture emotional and cognitive cues of interest to health, sociology, and psychology.
The categories reflect the five dimensions of positive psychology  -- each category is either positive or negative.
For example, R+ indicates positive relationships and P- indicates negative emotions.
Only exact matches are considered, and each word belongs to exactly one category.
We select these lexicons based on their use in prior work  and the fact that they were designed to represent categories of relevance to health and personality.
For each county, then, we record the frequency with which each lexical category is used.
To do this, we use a simple tokenizer to process each tweet that removes punctuation and then splits by whitespace to return a list of tokens.
Additionally, we remove all mentions and URLs.
The remaining tokens are matched against the above lexicons, resulting in a vector of category frequencies for each county.
We distinguish between tokens appearing in the tweet text and tokens appearing in the user description, denoted by the prefixes  and .
We found that only 70 of the LIWC categories appear in our data, along with all 10 of the PERMA categories, yielding a total of 80 linguistic categories.
For each county, we create a vector of 160 values reflecting the frequency of each category .
Since the magnitude of these values will vary greatly based on the number of tweets collected from each county, we consider several normalization strategies to make the vectors comparable across counties: * None: No normalization used; each vector contains the raw frequency of each category.
This represents the relative prevalence of a category as compared to overall usage in that county.
Note that if one user tweets the same word category many times, this will only increase the numerator by one; the denominator is the total number of users from that county.
In addition to LIWC and PERMA, some experiments also include five demographic control variables: < 18: the proportion of people under the age of 18.
65 and over: the proportion of people at least 65 years old.
Female: the proportion of people who are female.
Afro-Hispanic: the proportion of people who are AfricanAmerican or Hispanic.
We select these variables because of they are used in prior Twitter work , they are prevalent in governmental data collection for health studies , and they have been linked to health outcomes in epidemiological studies .
We collect these variables from the County Health Rankings Roadmap data.
To address our four research questions from the Introduction, we perform regression to predict each of the 27 health-related statistics using the 180 linguistic variables described above.
Given the large number of independent variables  relative to the number of validation points , we use ridge regression to reduce overfitting.7 To estimate generalization accuracy, we use five fold crossvalidation -- each fold fits the model on 80 counties and predicts on the remaining 20.
The splits are created uniformly at random, except that we additionally ensure that counties from the same state do not appear in both the training and test split in one fold.
This is to confirm that the model is learning more than simply the state identity of each county.8 We report two measures of accuracy: * Pearson's r: We collect all the predicted values from the held-out data in each fold  and compute the correlation with the true values; r  ; larger is better.
This is a useful alternative to the more common mean-squared error as it can compare outcome variables that have different ranges.
Our first research question asks whether Twitter-derived linguistic variables are predictive of a county's health statistics.
Columns labeled T  in Table 3 display the results for our two evaluation metrics across 27 statistics for the model containing all 160 linguistic variables  with User normalization .
To compute statistical significance of each correlation value, we use a Bonferroni correction to adjust for multiple comparisons.
Additionally, we replace the traditional p-value calculation with the Clifford & Richardson correction , which computes an effective sample size based on spatial autocorrelation9 , as measured by Moran's I 
We find that for nine statistics the prediction of the linguistic model is significantly correlated with the health statistic.
The strongest correlations are for No Insurance  , Vehicle Mortality  , Limited Healthy Food  , teen birth rate  , Dentist Access  , and Obesity  .
Figure 2 shows scatter plots of the true and predicted values on held-out data for three of the significantly correlated predictions using the LIWC+PERMA model.
The largest errors generally appear at extreme values.
Table 2: Held-out correlation and SMAPE for the Controls+LIWC+PERMA model averaged across all 27 output variables using various normalization strategies.
We speculate that the superiority of User normalization here is mostly due to the inclusion of user description variables, which should only be counted once per user.
Table 1: Held-out correlation and SMAPE averaged across all 27 output variables using various combinations of input variables.
All models use User normalization.
Similarly, Miami-Dade County in Florida has a high uninsured rate of 36%, while the model predicts only 17%.
The outlier in the No Insurance plot  is Pima County, AZ -- this may in part be explained by the limited Twitter data from that county .
Our second research question asks how the choices of representation and normalization affects accuracy.
Table 1 displays the evaluation metrics averaged across all 27 outcomes for all combinations of lexicon choice and inclusion of the demographic control variables.
Somewhat surprisingly, the PERMA lexicon does not appear to add much value -- this may in part be due to the fact that it only contains 10 categories .
As it does not hurt performance, we retain it in other experiments -- we show below that for certain health statistics it does produce statistically significant predictors.
We delay discussion of the demographic control variables until the next section.
Table 2 evaluates the different normalization strategies using the Controls+LIWC+PERMA model.
It is clear that using no normalization at all leads to poor results.
This is not unexpected, since the variables will have very different ranges across different counties.
We do find it informative that user normalization outperforms the alternative, more common normalization strategies.
Our third research question asks what if any predictive value these linguistic variables provide beyond that of commonly used demographic covariates.
It is possible that the correlations found in the linguistic variables in RQ1 are simply surrogates for demographic variables.
Given the strong predictive accuracy of the control variables , it is important to quantify the additional value added by Twitter.
Table 1 provides a partial answer to this question -- averaged across all 27 health statistics, including linguistic variables leads to an absolute 3% improvement in held-out correlation and a .28% improvement in SMAPE.
Table 3 provides a more detailed answer for each health statistic.
By comparing values for C and T+C, we can see the change in held-out accuracy obtained by including linguistic variables in the model.
We see that higher correlation is obtained by including linguistic variables for 19 of the 27 statistics and lower SMAPE is obtained for 20.
Performing a Wilcoxon signed-rank test on the SMAPE values, we find three statistics that are significantly more accurately predicted  versus one that is significantly less accurately predicted .
Table 3: Held-out correlation and mean SMAPE  for each outcome under three models -- T: Twitter model using LIWC and PERMA lexicons; C: control variables ; T+C: Twitter and controls.
All models use User normalization.
The thresholds have been Bonferroni-corrected .
These results appear to support the hypothesis that Twitterderived variables complement demographic variables.
Figure 3 plots the top 10 most correlated variables for the top 12 statistics from Table 3.
Significance values are again computed using spatially-adjusted p-values.
To disentangle those linguistic variables that are acting as surrogates for demographic variables, we perform an additional analysis which controls for these factors.
For each linguistic variable, we perform regression in which the independent variables consist of one linguistic variable and the five demographic control variables and the dependent variable is one of the 27 health statistics.
We then compute the statistical significance of the coefficient estimated for each linguistic variable, again using a Bonferroni correction.
To the best of our knowledge, no previous work has explicitly controlled for these demographic variables when identifying significant linguistic categories.
This is important to determine which variables are simply recovering demographics, and which are providing additional information.
Because of the spatial autocorrelation inherent in this geographical data, rather than using ordinary least squares regression , we use two stage least squares spatial regres-
Our fourth research question seeks to identify linguistic categories that are significantly correlated with each health statistic.
In addition to providing an additional validation, this may help health researchers formulate new hypotheses about the connection between behavior, personality, and health.
We use a kernel weight matrix with the default values.
This analysis yielded 33 linguistic categories that were significant predictors of 6 different health statistics after controlling for demographics.
Table 4 displays the 15 categories that were found to be significant predictors of at least two different statistics.
For each, we display the top five most common words found in the category and the list of health statistics for which they are predictive.
Overall, we find that the user description is more predictive than the tweet itself -- 80% of the significant variables come from the user description.
Caution must be taken when interpreting these results -- the true context of word usage on Twitter often differs from intuition.
Below, we highlight a few significant categories, including examples of the most common phrases to provide missing context.
As this is a purely correlational analysis, we make no claims as to the causal mechanisms underlying these findings.
This category contains words such as "love", "happy", and "smile"; e.g.
These words often appear with ambition-oriented quotes people enter in their description field .
Such profiles tends to be correlated with limited access to healthy foods, lack of health insurance, and more vehicle mortalities.
This may in part be explained by increased church attendance rates for counties in the deep South, which tend to be ranked lower by many health outcomes.
Table 6: Comparison of rates of limited healthy food access, predictions, and linguistic variables for Los Angeles County, CA and Jefferson County, AL, along with the most important linguistic categories.
Augmenting the control variable model  with Twitter variables  improves accuracy.
We used the following process: For each health statistic, we compared the true value to the held-out value predicted by the controls only model  and to the Twitter plus controls model .
We identified the county whose prediction was most improved by T + C. We then chose a second county that had a similar value predicted by the controls model, but a different true value .
Finally, we compared the linguistic variables from each of the two counties and identified those that differed the most, weighted by importance .
In this way, we identified counties that appear similar when considering demographic variables, but exhibit different linguistic properties on Twitter.
We highlight two examples from this analysis.
For obesity, we compared Kings County, NY  and Wayne County, MI .
Both are highly urbanized counties in the northern United States with similar demographics.
Figure 3: For the top 12 outcomes in Table 3, we plot the 10 variables with the highest correlation 
The thresholds have been Bonferroni-corrected using the total number of variables  times the number of outcomes .
The prefix d= denotes lexical categories from the description field of a user's Twitter profile.
Otherwise, the categories are derived from the tweet text.
For comparison, the control variables are also included.
Outcomes Low Birth Weight:d+ No socio-emotional support:dNo socio-emotional support:d- Teen Births:d+ Low Birth Weight:t+/d+ No socio-emotional support:tNo socio-emotional support:t-/d- Teen Births:d+ Low Birth Weight:t+ No socio-emotional support:dMentally Unhealthy:d- No socio-emotional support:t-/d- Poor Health:dUnemployment:t-/dLow Birth Weight:d+ No socio-emotional support:dLow Birth Weight:d+ No socio-emotional support:dLow Birth Weight:d+ No socio-emotional support:dLow Birth Weight:d+ No socio-emotional support:t-/dNo socio-emotional support:t-/d- Teen Births:d+ Low Birth Weight:d+ No socio-emotional support:dLow Birth Weight:d+ No socio-emotional support:dLow Birth Weight:t+/d+ No socio-emotional support:dNo socio-emotional support:d+ Poor Health:d+
Table 4: A summary of 15 of the 80 lexical categories.
These were selected by collecting all categories that are significantly correlated with at least two outcomes after controlling for demographics variables .
We list the significantly correlated outcomes, the sign of correlation, and the field where the word was found: t for text and d for user description.
E.g., the second row indicates that the presence of a word from the Family category in a user description is positively correlated with teen birth rates.
In part because Wayne County has a higher proportion of people over 65 , and this correlates with lower obesity rates, the controls-only model erroneously predicts a lower rate of obesity for Wayne County .
Including the linguistic variables improves accuracy considerably for both counties.
We display the top five linguistic variables that influenced the score for T+C.
As we can see, user descriptions from Wayne County are more likely to contain references to school and sports -- the most common references are to football and basketball teams.
All of these lexical categories correlate with higher obesity rates.
In this case, the linguistic variables provide a more nuanced distinction between two highly urbanized areas in the northern U.S. Table 6 repeats this analysis for the Limited Healthy Food statistic .
Here, we compare Los Angeles County, CA and Jefferson County, AL .
Los Angeles has much greater access to healthy foods, as predicted by both models.
Income is a strong predictor of this statistic, so the smaller median income in Jefferson County has influenced the controls-only model.
However, adding the Twitter variables results in a much larger  difference between the two counties.
Jefferson County is more likely to have user descriptions containing religious and metaphysical words , family words , and less likely to contain TV references .
In this case, the linguistic categories appear to be distinguishing between two very different types of urban environments .
We find similar patterns with these linguistic categories for No Insurance, Vehicle Mortality, and Teen Birth Rate.
The main conclusion of our analysis is that Twitter activity provides a more fine-grained representation of a community's health than demographics alone -- for example, the health of counties with similar demographics can be distinguished by the prevalence of words indicating negative engagement , television habits , and religious observance .
The reason for this appears to come from the insights Twitter provides into personality, attitudes, and behavior, which in turn correlate with health outcomes.
We have provided a methodology to discover such predictive patterns from county-aligned Twitter data.
Given the large number of variables explored, we have used very conservative estimates of significance to reduce the chance of Type 1 errors and adjust for spatial autocorrelation.
In the future, we plan to consider automatically-learned word categories , as well as extralinguistic attributes .
