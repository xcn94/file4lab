Additionally, within a tweet, a specific twitter user can be mentioned by prefacing their username with an @ symbol.
This creates links between users and allows for threaded conversations between users.
When people tweet live about a media event they are in effect annotating.
When mined for their affective content, these annotations can identify parts of the video that gained interest or proved controversial.
In this work we wish to characterize a media event, a debate, according to how people are reacting to it.
We are however not interested in the automatic detection of a winner or loser.
To do this, we describe an analytic methodology for detecting affective patterns that could aid in the development of media analytical tools.
Such a tool could serve to help a journalist or public affairs person become aware of trends and patterns in public opinion around media events.
Television broadcasters are beginning to combine social micro-blogging systems such as Twitter with television to create social video experiences around events.
We looked at one such event, the first U.S. presidential debate in 2008, in conjunction with aggregated ratings of message sentiment from Twitter.
We begin to develop an analytical methodology and visual representations that could help a journalist or public affairs person better understand the temporal dynamics of sentiment in reaction to the debate video.
We demonstrate visuals and metrics that can be used to detect sentiment pulse, anomalies in that pulse, and indications of controversial topics that can be used to inform the design of visual analytic systems for social media events.
In the fall of 2008, Current TV ran a program called Hack the Debate where they called for people to microblog comments during a live event.
Using the popular Twitter service, these posts--called tweets--were displayed on TV underneath the live presidential debate between Barack Obama and John McCain.
The success of Current's program has lead to many broadcasters to call for tweets during live broadcasts.
While viewers can see opinions one by one when watching, the collection of tweets provides an opportunity to understand the overall sentiment of microbloggers during the event.
Twitter is a microblogging platform that limits each post to 140 characters, which is slightly less than an SMS/text message to a cell phone.
Similarly, it is just text and does not support other formats like pictures or videos; people add URLs to their posts when they wish to send rich media.
Each user updates their feed.
A user can also watch, or follow, another user's feed.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Several prior studies have examined the usage patterns and the social motives of Twitter, such as through metrics of reciprocity.
They found users of micro-blogging systems to engage in a higher social reciprocity as measured by a publish-to-subscribe ratio.
Honeycutt and Herring  measured the usage of the @ symbol to measure conversational engagement.
Our work builds on that of Shamma et al.
By examining conversation volume and activity over time, they were able to temporally segment a live news event and identify the key people in the event.
Explicit media annotation and sharing while watching TV has been studied in a variety of manners  however these systems often involve integrated set-top boxes to support collaboration.
Online video annotation and conversation has also been examined, such as the Videolyzer system which supports collaborative information quality annotation of video .
Other work has more broadly characterized temporal patterns of messaging behavior on social networks, though not in conjunction with sentiment or with anchor media .
More sophisticated bias correction schemes have also been developed , but we found this simple filter eliminated ratings from the most blatantly biased workers.
The overall worker quality filter works by discarding the remaining ratings from workers whose ratio of ratings retained to ratings already discarded is below 0.5.
This threshold guarantees that in aggregate the quality of the ratings will improve .
Intuitively, if someone has more than half of their ratings discarded from the filters they are likely a poor rater and we discard their remaining ratings.
Using these five filters we discarded 60% of all of the ratings collected from AMT.
To study the tweets about the debate, we crawled the Twitter search API for common related tweets by looking for related hashtags.
The mechanism for tagging posts on Twitter relies on the poster to prefix a term with the # symbol.
For the first presidential debate of 2008, we queried the Twitter Search API for #current, #debate08 and #tweetdebate.
This amounted to 1,820 tweets from 664 people during the 97-minute debate and 1,418 tweets from 762 people in the 53 minutes following the debate.
The top contributor made 42 tweets and 5.7% of users made 10 or more tweets indicating that there was a diverse distribution of user activity with most people chiming in only a single time.
In order to understand the valence of the sentiment during the debate we collected three independent sentiment ratings for each of the 3,238 tweets in our corpus.
Tweets were rated as belonging to one of four categories: negative, positive, mixed, and other.
Ratings were acquired using Amazon Mechanical Turk , a crowd-sourcing site where workers complete short tasks for small amounts of money.
AMT ratings have been shown in prior linguistic rating experiments to correlate well with and sometimes outperform expert raters .
Workers were compensated $0.05 for each batch of ten ratings that they submitted.
As there is oftentimes noise in AMT ratings , we applied a total of five filters to enhance the overall quality of the ratings and discard ratings from workers suspected of poor quality ratings.
The first filter was a time filter in which a batch of ten ratings was discarded if the amount of time it took the worker to submit those ratings was less than one standard deviation below the mean submission time for all workers.
Next we applied a sloppiness filter: if a worker did not submit a rating for any of the ten tweets in a batch, then all ten ratings were discarded; we might infer the worker was not being careful or thoughtful.
Each of the batches contained one from a set of control tweets that had an obvious and verified sentiment.
If a worker mislabeled a control tweet we discard all ten ratings in that batch.
We also included a simple worker bias and an overall worker quality filter.
The worker bias filter operates by measuring the distribution of ratings across the four categories for each rater.
Since our rating categories are not mutually exclusive a rating reliability measure such as Cohen's Kappa or Fleiss' Kappa is not appropriate.
We adopt a technique from  which computes the inter-annotator agreement  as the average Pearson correlation for each set of ratings with the aggregate rating.
The aggregate rating was produced for each tweet using a simple majority-voting rule over the three independent ratings.
Correlations were averaged across all possible ways to break ties in cases where there was no consensus.
Using this method we achieved an ITA of .655, indicating a good amount of agreement between ratings.
To verify that these ratings were accurate we had three experts  rate a subset of 200 randomly chosen tweets from the dataset.
The ITA for these expert ratings was 0.744, indicating that experts still agree with experts more often than non-experts agree with non-experts.
However, we believe our aggregated non-expert ratings are still adequate for drawing some conclusions about the sentiment response to the debate.
In this section we utilize the aggregated tweet ratings to characterize the debate in terms of the overall sentiment of the tweets, whether twitter users favored a particular candidate, and the temporal evolution and "pulse" of the sentiment observable in the tweets.
We were also interested in being able to detect anomalies in this pulse as well as understand the relationship of sentiment to the topicality and potential controversy of issues being discussed.
The overarching goal of this characterization was to understand what features would lend themselves toward a temporal media event analysis system as might be employed by a journalist or public affairs person.
The overall dominant negative response is consistent with theories of negativity in political evaluation formulation .
To understand whether tweet sentiment was favoring one candidate or another we used C-SPAN's  transcript and timing information as metadata for who was speaking during each minute of the debate--Barack Obama, John McCain, or the moderator Jim Lehrer.
For each minute we also define the aggregate valence of response as the number of positive tweets minus the number of negative tweets.
We excluded minutes from our analysis where both candidates spoke substantially since that would conflate response scores.
For minutes when only Obama spoke, the mean aggregate valence score was -2.09; for minutes when only McCain spoke the mean aggregate valence score was -5.64.
The sentiment of tweets suggests that tweeters favored Obama over McCain, with McCain's aggregate valence more than twice as negative as Obama's.
However an analyst interested in the performance of the debaters might also be interested in when the pulse was disrupted--anomalies when either candidate was underperforming or over performing as compared to their average aggregated valence response.
In some cases this would correspond to "flat" areas of Figure 1.
To help detect these anomalous areas we plot how much the aggregate valence score differs from the mean aggregate valence score for that candidate in Figure 2.
Looking at Figure 2 we can for instance quickly see that minute 17 was an exceptionally strong moment for Obama and that McCain had a strong point at minute 53.
We can also see weakness for Obama at minutes 56-57, and comparative strength for McCain at minutes 58-59 followed by an exceptionally weak point for McCain at minute 60.
The period between minutes 53-60 is a bit different in its signature so we looked to individual tweets to help explain the pattern at that time.
The candidates were addressing military issues--in particular troops in Afghanistan--at that time.
This seemed to bring out more positive reactions for McCain such as "You have to admit, McCain is VERY knowledgeable about foreign policy & what's happening in the middle east."
At minute 60 McCain tells an emotional war story for which the tweets are resoundingly critical.
The evolution of the valence of the tweets over the course of the 97-minute debate can be seen in Figure 1.
The aggregate valence of the debate fluctuated with who was speaker at that particular time and the overall valence declined and then fell steeply during the last 10 minutes.
Examination of individual tweets during this final period indicates that a combination of both the impending end of the event together with an inciting topic  led to a higher volume of activity.
To understand the pulse and periodicity of the aggregate valence shifts we took the discrete Fourier transform and found that the dominant frequency in the signal corresponds to a period of 5.19 minutes.
This is the amount of time it took for both candidates to take a complete turn and can for example be seen quite pronouncedly between minutes 12 and 18 in Figure 1.
Looking at the individual tweets during this period we confirmed that the peak valence response corresponds to when Obama was speaking and the trough response to when McCain was speaking.
The moderator broke this debate into distinct topics, which we collected from C-SPAN's website .
In order to give some indication of controversy we computed the Pearson correlation between the positive and negative responses for each topic.
Intuitively, a high correlation indicates that the given topic arouses interest on both sides of the issue according to some consistent pattern.
We did find significant correlations between positive and negative sentiment on the topics of financial recovery and terrorist threat.
For the first section on financial recovery this signature is also observable in Figure 1 as a relatively flatter curve between minutes 3-14.
The positive and negative sentiment was correlated in both segments, despite the uncorrelated 12-minute topic shift in-between.
Looking at the volume curve in Figure 1 we can also see that the second section on financial recovery as well as the section on terrorist threat are areas of high message volume.
We have demonstrated an analytical methodology including visual representations and metrics that aid in making sense of the sentiment of social media messages around a televised political debate.
We demonstrated that the overall sentiment of the debate was negative and that tweeters tended to favor Obama over McCain.
We also showed that interesting events can be detected by looking at anomalies in the pulse of the sentiment signal and that controversial topics can be identified by looking at correlated sentiment responses.
This analysis is highly dependent on the polarized structure of a political debate, however we wish to explore how other events, , could also be analyzed using sentiment.
We suggest that a system embedding such metrics and visuals as we have developed here could enable journalists to identify key sections of a debate performance, or could enable public affairs officials to optimize a candidate's performance.
Diakopoulos, N., Goldenberg, S. and Essa, I., Videolyzer: Quality Analysis of Online Informational Video for Bloggers and Journalists.
Golder, S., Wilkinson, D. and Huberman, B., Rhythms of social interaction: messaging within a massive online network.
Harboe, G., Metcalf, C.J., Bentley, F., Tullio, J., Massey, N. and Romano, G., Ambient social TV: drawing people into a shared experience.
Hunneycutt and Herring, Beyond Microblogging: Conversation and Collaboration via Twitter.
Java, A., Song, X., FInin, T. and Tseng, B., Why we twitter: understanding microblogging usage and communities.
Krishnamurthy, B., Gill, P. and Arlitt, M. A few chirps about twitter Workshop on online social networks , 2008.
Message Content in Social Awareness Streams.
Nakamura, S., Shimizu, M. and Tanaka, K., Can Social Annotation Support Users in Evaluating the Trustworthiness of Video Clips?
Pang, B. and Lee, L. Opinion Mining and Sentiment Analysis, 2008.
Shamma, D.A., Kennedy, L. and Churchill, E. Tweet the debates ACM Multimedia Workshop on Social Media , .
Sheng, V.S., Provost, F. and Ipeirotis, P.G., Get Another Label?
Improving Data Quality and Data Mining Using Multiple, Noisy Labelers.
Evaluating Non-Expert Annotations for Natural Language Tasks.
Williams, D., Ursu, M.F., Cesar, P., Bergstrom, K., Kegel, I. and Meenowa, J., An emergent role for TV in social communication.
One of the issues with this form of event annotation is that it infers a relationship between a media event and an affective response via a timestamp and a hashtag.
In reality, there were tweets during the debate which were evaluative, but which did not reference the event itself.
For instance, someone might be critical in response to another commenter or about something that is irrelevant to that particular time of the event.
In the future we intend to add more detailed textual analytics to help the analyst further disambiguate the Twitter response.
The debate tweets do not represent everyone who watched the debate, only those who had adopted Twitter and had chosen to respond.
Measuring population sentiment from a system like Twitter could not be substituted for a real poll.
As real-time social commenting around media events becomes more prevalent and the biases of users of these systems tend toward population biases, it will be helpful to have knowledge about the background of a user, such as political leaning or even just age, in order to better see the sentiment response of different slices of users.
While some of this will be explicitly available from user profiles, future work could also look at inferring background from sentiment response.
For example, can we predict a user's political leaning based on the history of their sentiments during either candidate's speaking minutes?
Since the response time for something like Mechanical Turk would be too long for a journalist trying to make sense of an event in near real-time, the analytic methodology that we have developed will require automatic methods for classifying tweets into positive and negative sentiment.
Using our annotated data as a training set we are confident that known automatic techniques for sentiment classification  can achieve viable results for such an application.
