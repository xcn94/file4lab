Substantial amount of research in Psychology has studied how people manipulate objects in the physical world.
This work has unveiled that people show strong signs of prospec tive motor planning, i.e., they choose initial grasps that avoid uncomfortable end postures and facilitate object manipula tion.
Interactive tabletops allow their users great flexibility in the manipulation of virtual objects but to our knowledge pre vious work has never examined whether prospective motor control takes place in this context.
To test this, we ran three experiments.
We systematically studied how users adapt their grasp when asked to translate and rotate virtual objects on a multitouch tabletop.
Our results demonstrate that target posi tion and orientation significantly affect the orientation of fin ger placement on the object.
We analyze our results in the light of the most recent model of planning for manipulating physical objects and identify their implications for the design of tabletop interfaces.
In particular, several experiments have shown that the ini tial grasp when acquiring an object is influenced by the sub sequent planned actions so as to optimize end-state com fort .
Research in Human-Computer Interaction has never validated or tested these results, which suggest that we could possibly anticipate people's intentions as soon as they grab an object and before its actual manipulation starts.
Given that multitouch interaction techniques  usu ally simulate object manipulation in the physical world, we hypothesize that movement planning also takes place when users directly manipulate virtual objects with their hands.
If this hypothesis is supported, we could possibly infer infor mation about users' prospective movement to improve user experience during the manipulation phase.
Interface design ers could, for example, develop techniques that adapt their graphical layout to improve visual feedback, avoid potential occlusion issues  or reduce interference  when mul tiple users interact in close proximity in collaborative settings.
We could also derive directions about how to design grips and visual guides to facilitate both the acquisition and the manip ulation of virtual objects.
We test this planning hypothesis by observing how people grasp objects prior to moving them to specific positions and orientations on a horizontal screen.
We present three experi mental studies that examine a simple two-dimensional dock ing task on the surface of a multitouch tabletop.
The first experiment tests translation-only tasks.
The second experi ment tests rotation-only tasks.
Finally, the third experiment examines tasks that combine both translational and rotational movements.
The results of all the three experiments confirm the planning hypothesis.
They show that the placement of the fingers at acquisition time is influenced by both the initial and the final state  of the virtual ob ject.
They also provide valuable information about how users grasp objects at different positions of a multitouch tabletop.
We analyze our results in the light of the Weighted Integration of Multiple Biases model , a very recent model in Psychol ogy research.
The model helps us to explain how the orien tation of a user's initial grasp is influenced by a combination of several factors or biases, where each bias pulls the grasp orientation towards a certain orientation.
We examine how our experimental results conform to this model.
Finally, we discuss the design implications of our findings and identify several future directions.
Our work focuses on multitouch tabletops but could serve as a framework for studying object manipulation in a larger range of user interfaces, including multitouch mobile devices and tangible interfaces.
The manipulation of virtual objects has a central role in in teraction with tabletops.
For example, users move and rotate documents and pictures around the surface to share them with other users.
Graphical designers manipulate information and graphical objects to create new content.
Multiple users work collaboratively to create schedules, make decisions, or solve complex problems.
In all these scenarios, users interact with their hands and their fingers; they grasp, translate, and rotate virtual documents as they would do with physical objects.
Literature in experimental Psychology contains a large body of work that studies the manipulation of physical objects.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Previous work has studied a range of multitouch gestures for manipulating objects on interactive surfaces.
Wu and Bal akrishnan  defined a set of gestures that make use of both hands and multiple fingers.
Among others, they demonstrated how to perform freeform rotations using the thumb and index finger.
Moscovich and Hughes  proposed multi-finger in teractions that allow users to control a larger number of de grees of freedom to translate, rotate, and deform an object in a single manipulation.
Studies reported in  have proposed sets of gestures defined by end-user elicitation methods and concluded that people pre fer conceptually and physically simpler gestures than the ones created by HCI researchers.
Finally, Hinrichs and Carpendale  examined how adults and children naturally interact with tabletops and observed significant variations among gestures of different users.
Discussing the properties of graspable user interfaces, Fitz maurice and Buxton  identify two main phases of inter action: acquisition and manipulation.
Although these two phases can be studied separately , previous results  indicate that manipulation performance may depend on proper acquisition.
Multitouch gestures are subject to the physical constraints imposed by the user's arm, wrist and fin ger joints.
As a result, they can result in joint stress and dis comfort.
They concluded that "multi touch interaction has impact on the entire hand shoulder sys tem and in some cases the impact can be at risk level".
They revealed that individuals favor initial hand placements that result in end positions that are either comfortable, i.e, opti mize end-state comfort , or yield the most control .
Short and Cauraught have corroborated these results .
This type of planning behavior is termed prospective movement control .
The above studies have mainly focused on discrete tasks where participants had to choose one of two grasps .
Choos ing one grasp yields an uncomfortable end position while the other one a comfortable, hence optimizes end-state comfort.
However manipulating physical or user interface objects usu ally involves more continuous tasks.
Other studies, reviewed by Herbort , have examined con tinuous tasks, in particular rotations of physical knobs for a range of angles.
Their results suggest that end-state com fort planning alone cannot sufficiently explain the observed grasp selection in such tasks.
Herbort  argues that it is un clear how precisely someone can anticipate a final posture of a movement and its associated costs, and therefore, optimal planning may not always be feasible.
To account for the var ious biases that determine a grasp selection, he proposes the Weighted Integration of Multiple Biases  model .
How people plan their acquisition and grasp to facilitate movement and optimize comfort has been the focus of a large body of work within the fields of Psychology and Motor Con trol.
This work can be expressed using the notion of orders of planning .
Within that system, the last task in a se quence that influences the behavior defines the planning or der.
First order planning occurs when a grasp is influenced by the immediate task, for example the objects shape.
Sec ond order, when the grasp is influenced by the subsequent task, e.g., grasping an object to rotate or translate it to a given position, and so on.
Research studying first order planning of grasp have revealed that the kinematics of the hand depend, for example, on the size, orientation, and shape  of the object of interest.
Several studies have considered second or higher order plan ning.
An anticipatory bias pulls the initial grasp toward a pronated or supinated angular po sition panti , depending on the intended direction of rotation.
The contributing weights wanti and wdef ault of the two biases can vary, for example, depending on the difficulty of the task or the required end precision.
The above model can be extended with additional bias terms, such as one that accounts for the effect of previ ous movements in a sequence of tasks that involve different rotation directions and angles .
To the best of our knowledge, HCI research has never vali dated or tested the above results.
The most relevant contri  llers et al.
They bution in this direction belongs to Mo tested the hypotheses of a predecessor and a successor  effect on the offset and angle of a touch point in a sequence of pointing tasks.
They observed that finger posture is influenced by the previous pointing action but not by the next pointing action.
This suggests that prospective control does not occur in this specific pointing case.
Our hypothesis is that movement planning plays a determi nant role in tabletop interaction as movements extend to a large space and object manipulation involves the coordination of multiple limbs, often in constrained positions and postures.
The WIMB model was based on results from pure rotation tasks with tangible objects.
Here, we examine trans lation in addition to rotation.
The task of all the three experiments consists of grasping and moving an object.
Each experiment, however, focuses on a different movement component.
In Experiment 1, we test a translation task where participants have to change the posi tion of the object while keeping its initial orientation.
Experi ment 2 involves rotations, requiring participants to change the objects' orientation but not their position.
In Experiment 3, we combine translations and rotations so participants need to both change the position and orientation of the object.
The experiments tested un constrained translation and rotation tasks on different loca tions of a multitouch surface.
As opposed to Hoggan et al.
This design configuration was driven from the observation that the orientation of a neutral hand posture changes in circular manner around the user.
Although the po lar coordinate system presented in Figure 2 is not an accurate representation of the user's biomechanical coordinate system, it offers a reasonable approximation and simplifies data anal ysis.
As we see later in this paper, our approach allows for better experimental control and a simpler interpretation of the observed grasp orientations.
Our studies are mostly inspired by the continuous-tasks ap proach  rather than discrete-tasks approach of Rosen baum et al.
The former is more generic and can describe situations with uncertainty about the final grasp orientation of a movement and the costs associated with a cer tain object acquisition strategy.
In such cases, optimal plan ning is difficult or even impossible.
The display was placed flat on a table in landscape orientation, re sulting in the multitouch surface to be at a height of 95 cm.
A digital video camera on a tripod above the display monitored the participant's hand and arm movements.
The experimental software was developed in Java 2D  and ran on a Macbook Pro 2.66 GHz Intel Core i7 with 4GB memory, running Mac OS X 10.6.8.
Touch noise was reduced with a complementary filter.
Figure 1 illustrates a typical scenario for our experimental tasks.
In all three experiments the touch display shows a cir cular start object, which can be moved and rotated, and a static circular target.
The start object is green and has a di ameter of 60 mm.
The target object is red and has a diameter of 70 mm.
To start a trial the user presses a touch button at the bottom half of the display.
The user has then to grab the start object with the thumb and the index of the right hand and manipulate it to make its position and orientation match the target.
The user can freely translate and rotate but not resize the object.
Translations follow displacements of the center of the segment connecting the touch points of the two fingers.
Rotations follow changes in the angular position of this segment.
The orientation of both object and target are indicated by a handle .
We hypothesize that init is determined by both the start and target object configurations.
We expect that planning will oc cur for both rotational and translational movements.
Since the orientation of ergonomic hand gestures changes along differ ent locations of the tabletop , we predict that users will plan appropriately in order to reduce the occurrence of un comfortable end-postures.
To complete a task the start object has to be held in the tar get for 600 ms.
The precision tolerance for placing the object into the target is 5 in angular direction and 5 mm in diam eter.
The angular positions  of objects, their radial distances  and their rotation angles  are specific to each experiment and will be detailed later.
The user interface provides visual feedback to indicate that the object was correctly placed into the target.
It also pro vides visual and audio feedback to inform the user about the completion of the task and errors, which occur when the user lifts a finger before task completion.
Prior to each experiment, participants had to wash their hands and dry carefully in order to minimize screen friction and fa cilitate object sliding.
Participants were positioned standing at the center of the long side of the display and were not al lowed to walk.
The operator asked them to only use the thumb and the index finger of the right hand to interact with the ob ject, while keeping their left hand down by their side.
Par ticipants were not explicitly encouraged to plan their grasps and were not aware of the experimental goals.
They were instructed not to rush and avoid errors.
We tested six screen positions for both the start and the tar get objects.
One was located close to the user, centered on the vertical axis of the display, 35 mm from the front edge.
We refer to it as the User position.
The other five positions were located around the User position with an angular posi tion start of -90 , -45 , 0 , 45 , and 90 , and a radial dis tance of r = 314 mm.
The start and the target objects could appear at the same position.
In this case, the user should hold the start object and keep it inside the target.
To test whether and to what extent planning occurs for trans lation tasks, there were two main conditions: Known Target.
The target appears with the start object at the beginning of the task.
Users are aware of the end position of their movements, and therefore, they can plan the orientation of their grasps.
This is a control condition.
The target is ini tially hidden.
It appears after the user acquires the start ob ject.
Thus, users cannot plan the orientation of their grasp.
We recorded detailed information about the position of the fingers on the multitouch screen and their movements.
Our two main dependent variables are: 1.
The initial grasp orientation init  , mea sured as the clockwise angle between the vertical axis and the vector from the thumb to the index finger .
Our 3M multitouch display could not differentiate be tween fingers.
We derived the correct grasp orientation from the range of attainable grasp orientations, measured at each screen position in a pre-study with 10 participants .
We also used detailed logs and recorded video to ensure that grasp orientation was derived correctly.
The default task-independent grasp orientation def ault   for each position of the display.
To measure it, we only consider trials where start and target configura tions are the same.
Start and target po sitions were randomized within each block.
Tasks were grouped by three but in a different way for each condition.
In the Known Target condition, groups contained the three repli cations of the same task, allowing participants to re-plan and possibly revise their grasp orientation.
In case of an error, the participant had to restart the task.
In the Hidden Target condi tion, groups contained a random selection of tasks.
When an error occurred, the task was not repeated immediately.
It was moved to the end of the block and was replaced by the follow ing in the list.
This design eliminates planning effects for this condition.
Experimental sessions lasted 50 to 60 minutes.
Second, the right arm is more constrained by the user's body when moving leftwards.
We be lieve that increased finger friction and movement constraints due to the anatomy can also explain this difference.
For error comparisons, we used the Wilcoxon signed-rank test.
For RT, we conducted a 5-way Repeated Measures  ANOVA with the complete set of factors.
Finally, for init , we split our data into three sets: 1.
P E R I P H E RY: The start and target objects are at the periph ery of the display.
O U T WA R D: The start object is close to the user.
I N WA R D: The target object is close to the user.
We conducted a 5-way RM ANOVA for the first set and 4 way RM ANOVAs for the second and third set, as the factors start and target , respectively, were not relevant for these sets.
We only report on main effects and two-factor inter actions that are meaningful and relevant to our hypotheses.
When possible, we use a 95% confidence interval   to report on the estimated difference between two means.
Figure 4 presents the estimated mean values.
The results suggest that planning only occurred for the first instance of each series of replicated tasks.
Figure 5 presents how def ault varied along different angular positions.
This seconds our re sults on response time for Known Target: participants planned their grasp for the first task in the group but did not refine it after.
As shown in Figure 6, init was mainly determined by the start position.
The target position contributed less, mainly for target positions at the left half of the display.
As shown in Figure 6, planning only occurred as a slight bias towards lower grasp angles for start positions at the right half of the display.
Figure 6 shows that planning occurred, but not as expected.
The orientation bias added by the target positions target =0 and target =45 has a direction opposite to the one suggested by their default orientations .
This means that participants chose a grasp away from both the start and endstate comfort position.
For the -45 target position, results are more unclear because different participants chose differ ent strategies.
Our interpretation is that comfort is not always determined by the start and end state of the movement.
As the arm and hand have multiple segments and joints that need to coordinate in order to accomplish a movement, transitions be tween intermediate states can play an important role.
In this particular case, we observed that participants adapted their grasp to optimize the flow of their movement.
Participants performed rotations in two directions dir  {clockwise, counterclockwise}.
As the task did not involve translations, the start and target positions overlapped.
We tested the same angular positions  as in Experiment 1 but added a closer ra dial distance r = 157 mm.
We discarded the U ser position, as rotational movements are uncomfortable when the hand is too close to the body.
Contrary to Experiment 1, the target object was always dis played.
Our pilot tests showed that completing the most diffi cult tasks  with no previous knowledge of the target was hard or impossible.
As shown in Figure 7, the effect of clockwise rotations was more pronounced.
This result is not surprising.
It can be explained by the fact that the right range of grasp orientations, which is used for the planning of counterclockwise rotations, is more constrained compared to the the left range of orientations .
This study involved the same participants as Experiment 2.
In addition to these positions that define the translational movement component, we tested three angles of rotation   {-90 , 0 , 90 }.
Overall, grip adaptation was more pronounced for distant positions .
Figure 9 illustrates these effects.
Results are consistent with the findings of Experiment 1.
Participants adapted their grasp orientation based on both the start and the target position of their movement.
Again, the bias of the start position was stronger than the bias of the target position.
As shown in Figure 9, results follow closely results of Experiment 2.
Participants antici pated how to adapt their initial grasp despite to the translation movement that occurred in parallel with the rotation task.
As in Experiment 2, we did not observe any learning effect.
This suggests that in some situations, different planning strategies can be appropriate for the same task.
We plan to further investigate this observation in future work.
As the studies reviewed by Herbort  considered only ro tational tasks, we can check if our results of rotations fit the same formal model.
Figure 10 presents the results of Ex periment 2 through WIMB's mathematical formulation  for r = 317 mm.
We have normalized the ini tial and default grasp orientations by setting pinit = init - def ault and pdef ault = 0, where the default orientations def ault are the values measured by Experiment 1.
Following Herbort's  approach, we examine clockwise and counter clockwise rotations separately.
Our results are consistent with previous results on the manipulation of physical objects, sum marized in his survey.
As WIMB predicts, we observe that users tend to compensate small angles proportionally more than large ones.
We also observe that the effect of the an ticipatory bias is stronger for clockwise rotations.
We hy pothesize that this is due to the fact that the range of motion is smaller in clockwise than counter-clockwise direction at most screen positions .
When a task involves a clockwise rotation, participants are required to do a larger  prepara tory rotation in the opposite direction to avoid uncomfortable or even impossible hand and arm positions.
This asymmetry in movement direction may also explain why we observe a longer planning time  for clockwise rota tions in Experiments 2 and 3.
Our results support our hypothesis, being in accordance with the general principles of Herbort's WIMB model for physi cal objects .
Users plan their grasp orientation in prepa ration for the manipulation of virtual objects.
Planning takes place under the influence of several biases that include at least a task-independent preferred bias and an anticipatory bias.
When planning is not possible, as in the Hidden target condi tion of experiment 1, participants adopt the strategy of using a "standard" initial grip for all target positions .
In all the three experiments, we found that the initial grasp orientation init is influenced by both the start and target configurations.
Experiment 1 showed that users adapt their init to account for the difference between the start and tar get value of def ault , which varies across distant angular po sitions .
Experiment 2 showed that users adapt their init in preparation for rotations so that they do not end up in uncomfortable positions.
Experiment 3 exam ined both translations and rotations and showed that both of the above effects occur in parallel, with planning for rotations having a stronger effect.
Finally, we observed that in special cases the start and target configurations are not the only fac tors to affect grasp orientation.
In Experiment 1, Outward tasks, participants used noticeably different planning strate gies for the -45 target position, demonstrated by the large confidence interval of init .
Using movement planning to improve feedback and prevent occlusion in an object-matching scenario.
The "alerts" box moves upwards, avoiding hand occlusion.
The system anticipates the planned rotation and indicates matches of it long edge.
It also moves the "alerts" box downwards to minimize occlusion during the manipulation of the object.
Finally, we found that clockwise rotations were more error prone than counterclockwise rotations.
These results are in agreement with the results of Hoggan et al.
The planning effect we observe in our experiment  llers et al.
However, looking closer at their task, we can see that comfort plays a minor role while start and target finger orientations are not constrained by each other.
We suspect that movement planning in this case adds cognitive overhead without necessarily aiding the task.
Our results could be also useful in collaborative scenarios where spatial interference and conflicts between the actions of collaborators are frequent .
We can foresee conflictresolution techniques that make use of information about prospective movement.
In addition, when users organize pieces of information collaboratively, the system could de tect potential relationships between objects located in differ ent personal workspaces and assist users with appropriate vi sual feedback.
For example, it could display handles around an object that suggest a grasp and thus a specific movement that would bring this object close to other related ones.
Finally, we are interested in studying the role of movement planning for other multitouch devices, such as tablets, espe cially in connection with how users grasp and hold them .
Future work also needs to explore its implications for tangible user interfaces, where grasping and acquisition are determi nant factors of user performance .
Our results open a new space for innovation with design im plications for several application scenarios.
First, they can in form the design of the form and affordances of virtual objects around a tabletop.
Different surface positions are associated with different ranges of motion and different default grasps.
Designers can make use of this information to appropriately position objects on the surface or design grips and interaction techniques that facilitate grasping .
Getting knowledge about the planned movement early enough when the user acquires an objet can be also valuable for improving user experience during its manipulation.
We are particularly interested in exploring the design of new occlusion-aware techniques .
Enhancing ex isting hand-occlusion models for multitouch  with a movement-planning model could possibly provide more reliable estimation about the occluded areas at acquisition time or during manipulation.
Such information could be useful for optimizing the display of feedback and visual content at visible locations of the screen.
It could be also useful for improving motor control, e.g., by avoiding object snapping around positions that are away from predicted targets.
We do not encourage designs that make blind use of such predictions, as this could be the source of user frustration in case of false predictions.
Figure 11 illustrates a simple scenario where movement planning is used to optimize visual feedback and reduce hand occlusion.
Translational and rotational tasks are manipulations com monly performed on multitouch tabletops.
We have inves tigated whether prospective planning is present when peo ple perform such manipulations.
We have shown that users choose a grip orientation that is influenced by three factors:  a preferred orientation defined by the start object position,  a preferred orientation defined by the target object posi tion, and  the anticipated object rotation.
We have exam ined these results in the view of the WIMB model, which has been recently introduced by Herbort  to explain planning for the manipulation of physical objects.
We have shown that our results are consistent with the WIMB model.
We have also shown that relative to the geometry of the table top, upwards, leftwards movements and clockwise rotations are more difficult for users to perform.
While the effects of planning on interaction with multitouch interfaces are not yet fully understood, our results provide a first look at a phe nomenon that should be taken into account when designing tabletop applications.
Fitzmaurice, G. W., and Buxton, W. An empirical evaluation of graspable user interfaces: towards specialized, space-multiplexed input.
Hancock, M. S., and Booth, K. S. Improving menu placement strategies for pen input.
Hancock, M. S., Carpendale, S., Vernier, F. D., Wigdor, D., and Shen, C. Rotation and translation mechanisms for tabletop interaction.
Herbort, O. Optimal versus heuristic planning of object manipulations: A review and a computational model of the continuous end-state comfort effect.
Herbort, O., and Butz, M. The continuous end-state comfort effect: weighted integration of multiple biases.
Hinrichs, U., and Carpendale, S. Gestures in the wild:
