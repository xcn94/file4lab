Auditory displays have been used in both human-machine and computer interfaces.
However, the use of non-speech audio in assistive communication for people with language disabilities, or in other applications that employ visual representations, is still under-investigated.
In this paper, we introduce SoundNet, a linguistic database that associates natural environmental sounds with words and concepts.
A sound labeling study was carried out to verify SoundNet associations and to investigate how well the sounds evoke concepts.
A second study was conducted using the verified SoundNet data to explore the power of environmental sounds to convey concepts in sentence contexts, compared with conventional icons and animations.
Our results show that sounds can effectively illustrate  concepts and can be applied to assistive interfaces.
However, little research has investigated the use of non-speech environmental sounds in assistive technologies to convey concepts as an extension for natural languages.
An auditory language could be useful in situations where spoken languages fail to communicate effectively, as for people with language disabilities or language barriers.
Although pictorial representations have been largely used in assistive communications in such situations , and many concepts can be evoked with images, others can be suggested more clearly and unambiguously with sounds.
Therefore, sounds may be a good complementary mode of non-verbal communication, and assistive devices might use them in conjunction with pictures.
We distinguish three advantages of sounds.
First, some concepts may simply not be imageable.
For example, the sound for "thunder" was easily identified, yet it is difficult to imagine a picture of thunder .
Second, fine-grained distinction in some cases is more easily made with sounds: the sound for "sneezing" and "coughing" can be easily discriminated, but pictorial representations cannot clearly distinguish the two related activities.
Third, events like "tuning " or "rewinding ," which unfold over time, are more difficult to represent in a static image.
To study the potential of sound in assistive technologies, we explored the use of natural audio to communicate familiar and frequently occurring concepts.
We built SoundNet, a lexical database enhanced with environmental sounds.
SoundNet could help people with language problems to receive and express information.
An example is a multimodal dictionary deployed on a mobile device.
One possible scenario is that of an aphasic individual suffering from a cold and trying to convey to a nurse or doctor symptoms like "sneezing" and "coughing" by means of the dictionary.
Conversely, a healthcare practitioner may create for the patient an association between a pill bottle on the table with a symptom evoked by the sound she plays from the dictionary.
In all cases, sounds supplement but do not fully replace visual or verbal communication.
We are fully aware of the limitations of sounds as a means of communication.
They include the fact that sounds, unlike images, require a specific sequence and longer display / processing time; many concepts are not audioable at all.
Non-verbal sounds, such as fire alarms and car horns, can be used to attract attention and deliver specific messages.
Currently, people have researched the use of audio in communication in two major areas.
First, in HumanComputer Interfaces , auditory icons  and earcons  use nonspeech audio  to convey computer events.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The first study collected a large number of human-generated semantic labels for the "soundnails"  in our database that were used to verify the concept-sound associations in SoundNet.
The second study explored how well our soundnails can convey concepts as verbal fillers in common phrases, compared with icons/animations  and a baseline condition .
Our results suggest that there are many concepts that soundnails can effectively evoke, in some cases better than icons/animations.
Thus, SoundNet has the potential to support communication in assistive systems for people with language disabilities and language barriers.
This suggests that for both language-impaired populations and for healthy speakers with compromised linguistic comprehension, environmental sounds have the potential of conveying concepts and assisting language comprehension.
Other work such as the Freesound Project  collected labels for recorded sounds from human volunteers.
Our work differs from previous research in that we evaluate the efficacy of audio to convey concepts from both linguistic and auditory perspectives.
Furthermore, through two large online studies , we collected human-generated semantic labels  and interpretations  of short soundnails, which were verified and can be used to extend SoundNet.
Audio studies using such large subject populations are novel in the field of Assistive Technologies.
Assistive technologies have traditionally used iconic stimuli to illustrate concepts for people with language disabilities .
Many existing communication devices continue this convention .
Research  has shown that other visual representations such as web images, animations, and videos can evoke clear concepts.
However, little work has been done on using auditory languages in such applications.
SoundNet is an environmental sound-enhanced lexical database.
All are frequently used English words.
Each data unit includes a synonym set, an audioability rating and, for audioable data, a soundnail; the data are interlinked via semantic relations from WordNet .
SonicFinder  is a computer interface exploring the use of auditory icons, which maps everyday sounds to computer events.
Sounds like bouncing and breaking were used to convey computer events analogous to concrete events.
Auditory icons restrict to conveying computer events.
Although we use natural sounds like auditory icons, our work differs in that we extend the auditory vocabulary to concepts from daily life, and we are targeting a potential user population with language problems.
Earcons are nonverbal structured audio patterns intended to provide information about objects, operations, status, and interactions in computer interface elements such as menus and alerts .
However, earcons are not sounds that people are familiar with outside the specific computer environment, and thus they require learning and memorization.
Earcons are less natural and accessible than auditory icons .
The original source of the SoundNet vocabulary is the glossary of Lingraphica , a commercial communication device developed by the Lingraphicare Company for people with aphasia.
Lingraphica includes common words from different parts of speech and phrases for constructing sentences for everyday communication.
After eliminating symbols and duplicates, and stemming, 1376 words were extracted from the Lingraphica vocabulary.
However, we could not assume that each word on the list could be represented by a sound, a property we call audioable.
To better establish the sound-concept correspondence, we included the sound clip labels from BBC Sound Effects Library , which constitute the majority of the environmental sounds used in SoundNet.
A list of 1368 words  was generated from the BBC sound captions.
The overlap between the Lingraphica and BBC word collections became the core vocabulary of SoundNet.
Each word in the core vocabulary was assigned to its most frequent sense and part of speech as reflected in WordNet.
For words that are audioable , each of the raters wrote a scene script that could be used to evoke the intended concept.
Two additional judges joined the discussion to finalize the audioability ratings and scripts.
Overall, 184 out of 322 words were voted to be audioable.
The scripts guided us in selecting associated sounds.
Over two thirds of the 184 audioable words had a representative sound in the BBC library that aligned with the rater scripts.
Two other sources of environmental sounds, Freesound  and FindSounds , were checked to fill in the missing word/sounds.
However, there are three major problems with the original sound clips.
First, most of them range from 10 seconds to several minutes in length.
It takes time to listen to them and they are therefore not suitable for an instant communication support setting.
Second, most of the sounds were recorded from a complex sound scene or event.
This could distract people from focusing on a particular sound source or action that we want to depict.
Third, the BBC sounds are high quality stereo and too large to store, especially for mobile devices.
To address these problems, we extracted and created five-second soundnails from the original tracks.
The original sounds were first downsampled to 16kHz, 16 bit mono to reduce the size of the sound files while maintaining their quality so that people can still recognize sound scenes.
We chose the 16kHz sample rate based on the fact that it is a conventional sample rate for speech recognition; many video games use 11.025 or 22.05kHz for their sound effects.
A pilot study  also proved that people could identify and categorize sounds in the 16kHz sample rate.
The downsampled sound clips were then randomly chopped into 5-second fragments .
Five seconds is a length sufficient to depict a sound source or a complete sound event, and not too long to listen to if used in a communication setting.
The 5-second fragments were grouped into three to four clusters using the K-Means algorithm based on different audio features  extracted from them.
The fragment which was the closest to the center of each cluster was chosen as a candidate soundnail for the intended concept.
In the last round, our group manually examined all the candidates and assigned the one that was the most representative to the target concept.
A total of 327 soundnails were generated for 184 words.
Some words were associated with more than one soundnail, each of which was from a different domain.
For example, "fire alarm," "burglar alarm," and "car alarm" sounds were all used to illustrate the concept "alarm."
The sound labeling study was carried out via the Amazon Mechanical Turk  platform , which allows people all over the world to post and participate in online surveys.
The 327 soundnails were randomly grouped into 32 Human Intelligence Tasks  with 10 to 11 sounds each.
After listening to the soundnail  is loaded, participants were asked to provide free form answers to three questions about the sound source, location of the sound, as well as the activities involved in the creation of the sound.
After they finished labeling all the sounds in one HIT, participants submitted their work to AMT.
The submission was checked both automatically and manually.
Once approved, the participants received payment for their work.
Our goal was not just to gather labels for the sounds but to determine whether, and in which cases, specific aspects of the soundnails evoked responses.
Thus, instead of acquiring a single label, we collected answers to three targeted questions.
We hypothesize that in some cases, the location, the source, or the manner of the sound production is salient, but perhaps not all of these.
We also wanted to see in which cases not all of the words in the label were named by the subjects.
For example for the "walking on snow" soundnail, "walking" and "footstep" were generated, but not "snow," suggesting that the location was not audioable here.
Since Amazon Mechanical Turk does not reveal information about the participants and all tasks were completed over the Internet, we had no knowledge of the background of the workers nor the quality of their work.
To control the quality of the collected labels and to prevent the use of scripts or robots that can automatically fill out web forms, we embedded mechanisms and checkpoints in the interface as well as in the submission approval process.
The instruction page provided a step-bystep walk-through of the study.
In order to start, participants logged in by typing the keywords revealed in an auditory captcha .
This ensured that sounds played properly and that participants listened and paid attention.
At the beginning of each HIT, a demo sound and spoken example answers to the three questions were played.
People were asked to put down the answers as instructed.
This step, too, helped to ensure that a human  performed the task.
Our system could filter out invalid responses such as "YYYY" and "08gv2" by automatically checking for valid words in WordNet , however, irrelevant answers such as "hello" and "OK" could not be eliminated.
To determine the relevance of the submitted responses to the content of the sound, we ran a pilot study with 25 undergraduate students.
Each soundnail was tagged by eight to nine students, and those labels became the ground truth data for comparing to the online responses.
If over half of the labels in a HIT had some words appearing in the tags provided by the undergraduate students, we considered the submission as acceptable.
Our group also reviewed the responses and flagged the ones considered as invalid.
After 97 days of data collection, we obtained at least 100  human semantic labels for each of the 327 soundnails.
The raw responses were mostly in sentence format, and they were transformed into word-level data, in a process similar to that applied to the BBC sound file names.
Each sentence was broken down into a bag of words; function words like "the" and "or" were removed.
Subsequently, misspellings were corrected and stemming was performed based on the word's presence in WordNet.
Thus, "woods"  was left unchanged but "dogs" and "dragging" were normalized to "dog" and "drag," respectively.
We calculated "word count"  for each word.
In general, the more people use a word in their descriptions regarding the sound source, location, and interaction, the stronger the word is connected to what the sound portrays.
However, people may use different words denoting the same concept, for example "plane," "airplane," and "aeroplane."
In those cases, words in the same sense were grouped into units called "sense sets."
The most commonly used word of each sense set was used as the representative of that whole set, and referred as a "label" in the following sections, to distinguish it from an individual word.
The word count of a sense set is the sum of word counts over all its members.
Thus, the following metrics were introduced for the measurements and comparison.
The maximum sense score was "3", suggesting that for every labeler the concept appeared in answers for all three questions.
Among all soundnail sense sets, the one with the top sense score was the concept that people most agreed on.
Although the response time could be affected by factors like how quickly a sound is perceived, how long a sentence was used to describe the sound, how fast he/she typed, and so on, it still can reflect whether or not people had difficulty identifying a sound.
Figure 2 shows that the more distinctive a sound, the less time was needed for people to listen and respond.
For each sound, we collected sense sets with a sense score no less than 0.25 .
In general, target words that were highly audioable  received a significantly high sense score .
In the following subsections, we present results of our analysis from different perspectives.
Table 2 shows the pairwise comparison between the target word part of speech and the part of speech of the most agreed-on label.
About 80% of sounds for a noun concept were labeled as a noun, while half of the sounds for a verb and almost all sounds for adjectives and adverbs were labeled using a part of speech other than the intended one.
This is consistent with interpretation of pictorial representations .
However, the parts of speech people produced changed as they answered different questions .
Since a sound source can be a person, a thing, or an action/event, mainly nouns and some verbs were used.
Responses to the location of the sound contained fewer verbs in proportion, and a few adverbs indicating positions were introduced.
On the contrary, the "how the sound was made" questions focused on the interaction involved, and thus a lot more verbs appeared in the descriptions.
For each soundnail, the initial concept  assigned in SoundNet was compared to the label  that labellers agreed on the most.
It showed that although different from what was intended, the sound  was distinctive enough to illustrate a concept.
There were 52 sounds in this category.
It suggested that the soundnail was similar to the sound associated with the agreed-upon label, meaning that the sound could be communicatively effective.
Thirty-five sounds fell into this category.
Of course, cases 2 to 4 may simply suggest problems with the scripting and sound selection.
Further investigation on why people generated those labels can lead us to refine and extend SoundNet.
Research  suggested that concrete words and words that are highly imageable are easier to name and categorize based on pictorial representations than abstract words.
Figure 5 and 6 show that concept recall via auditory representations followed the same rule.
Sense score dropped significantly as concreteness and imageability  went down for both target words and most agreed labels .
This indicates that, in general, concrete concepts and concepts that can be easily illustrated by a picture are more likely to be conveyable by an environmental sound.
Our sound labeling study showed that 89% of the SoundNet soundnails can convey a concept, and a third evoked the intended concepts.
The question arose as to how effective these environmental soundnails are when used to communicate information in a context of common phrases.
A second study "Sounds as Carriers for Communication" was designed and conducted to explore answers to the following questions.
First, will context improve the performance of soundnails?
In the sound labeling study, 46% of our soundnails evoked concepts that were directly related to the sound scenes but differed from those we intended.
It is possible that clues such as parts of speech could direct people's attention to the target.
Second, how well do auditory representations perform compared to pictorial representations?
Pictures have long been used in assistive technologies.
If we want to apply the data in SoundNet to systems that support communication, we need to verify their effectiveness compared to the use of icons.
People from 46 countries and regions participated in the sound labeling study.
Table 5 lists the countries with more than 10 participants.
In Table 6, the average length of valid tags  and average response times were compared.
The goal of the study was to investigate how well people could interpret sentences in which words are replaced by soundnails based on SoundNet's audio-concept associations.
It merely aims to explore how sounds can convey certain concepts when compared to icons and/or animations.
Thus it is a proof of concept, not a user study for a specific population.
Eighty-seven target words with different ratings from the sound labeling study were selected .
They covered all cases listed in Table 1.
The phrases used in the study came from the Ageless Project .
Ageless Project is a blog forum for senior people who fall into the same age span as our ultimate target population, people with aphasia.
The posts in Ageless project reflect popular topics among the elderly, and thus is a good reflection of the topics important to the aphasic population and their everyday communication needs.
Sentences with the selected words were crawled.
Thirty-six phrases were picked and paraphrased if they were too long.
Each phrase was of the length five to twelve words, and had one to four target words embedded.
In the Sounds as Carriers for Communication study, we introduced two other modes for comparison.
One mode used icons  and animations  from Lingraphica.
Those iconic representations have been used for almost 20 years in assistive devices to help people with aphasia to compose phrases for language rehabilitation, and therefore, are valid for comparison.
In addition, a baseline mode which shows a gap in place of the target word tested how much information the context provided.
Figure 7 shows the example phrase "It is written in the book."
Unlike pictures, which can appear at the same time, sounds in a phrase need to be played in sequence.
To ensure the proper order, all of the phrases were turned into Flash files, which displayed the words one after another.
The interval was one second for context words, and five seconds  for the words replaced with one of the modes.
It helped to estimate how much time people spent on interpreting the missing words.
The study was conducted on Amazon Mechanical Turk.
The 36 phrases were divided evenly into nine blocks, and regrouped into 27 HITs.
Each HIT contained one block in audio mode, one in icon/animation mode, and one in blank mode.
The mode assignment and position were determined using a Latin Square block design.
On the interface , the Flash file of a phrase was automatically played.
Text fields corresponding to the number of missing words were provided.
People were asked to fill in their interpretation of the picture, sound, or gap.
They could replay the Flash, as well as individual soundnails in the audio mode.
Quality control similar to the sound labeling study was applied.
The captcha was also implemented in Flash to ensure that participants had proper software installed to play the Flash files.
All of the soundnails were converted to Flash, so that people did not need an extra player for the audio files.
All typed responses were collected, stemmed, and corrected for misspelling.
To better assess the data quantitatively, four evaluation metrics were used.
A test for homogeneity of variances in the four metrics showed that results in different modes came from the same normal distribution.
This measures how well people's responses converged.
Entropy gives low scores if users agree on a concept and high scores for distributions that are more spread out, which means more words were generated and each has a lower count across all labelers.
This takes into account both the total number of different labels  that were generated as well as the sense score for each label.
As shown in Figure 9, the target words with high sense scores in the previous study were again those with significantly higher accuracy rate than the ambiguous ones  = 37.037, p < 0.01.
However, context did provide information for people to identify the sounds or concentrate on intended aspects in many cases.
Table 9 lists the 10 words with highest accuracy rate in audio mode as well as their corresponding blank mode accuracy rate.
Six out of the ten words had an accuracy level of 1 or 2 in the sound labeling study, and half of them  had an accuracy rate higher than 0.7 in the blank mode.
This meant that people could guess these words quite well purely based on the context.
An example is "I will bring an umbrella in case it rains."
In other cases, the context suggested the part of speech of the missing word.
For example, the "baby crying" sound was used to illustrate the word "cry."
In the sound labeling study, many people identified the sound as "baby."
The phrase given in the second study was "Her baby ____ a lot ..." which indicated that the missing word should be a verb.
As a result, people mostly generated "cry" instead of "baby."
With the help of context, the accuracy rate of abstract words was greatly increased .
The accuracy rate of words with an average level of concreteness  even approached highly concrete ones.
Similar effect was found in imageability .
In all respects, icon/animation mode performed significantly better.
The small eta squared effect size showed that part of speech was not as great a factor as representation mode.
Looking at the details more closely and taking entropy as an example, the results for the words can be divided into groups based on the mode with the best performance .
Specifically, the audio mode significantly outperformed icon/animation mode for seven words  in terms of score, and the scores for another 31 words were not significantly different, indicating that certain concepts can be better conveyed by a sound than by an icon or animation.
Phrase level results were similar to the word level.
The average score of target words in each phrase was computed, and the icon/animation mode significantly outperformed the audio mode  = 62.493, p < 0.01, Figure 14 green/dark columns.
However, there were still five phrases for which the scores in the audio mode was significantly higher than the visual mode .
Comparison of entropy in different modes within groups categorized by which mode had the lowest value.
Phrases for which the audio mode score was significantly higher than the icon/animation mode score.
The fire alarm went on while I was cleaning the house.
I rewound the movie several times.
We have run out of chalks.
I am too full after having so many crackers.
Phrases for which the audio mode score was significantly higher than the icon/animation mode score.
There were a few interesting facts observed in the studies.
The undergraduate students who participated in the pilot sound labeling study stated that given a sound-label pair, the association is often easily established but given only the sound, retrieving the concept can be difficult.
For example, one of the soundnails that was assigned to "telephone" was the dialing sound of an old style rotary dial telephone.
Results showed that very few undergraduate students accurately identify the sound, whereas this soundnail receive a top sense score of 0.7265 in the Amazon Mechanical Turk study.
This suggests that young people who may not be familiar with such a phone fail to recognize the source of the sound.
When trying to evoke the word "day" with a sound playing rooster crewing, clock ticking, and crickets chirping in sequence, most people put down "rooster"
The response time was computed for each phrase, calculated as the time between phrase loading and the response submission  minus the time spent on playing sounds for context words.
Although the response time could be affected by participants' behavior in the study , it still provides a rough estimate of how long people spent on trying to figure out the missing words and typing in the answers.
Figure 14  showed that overall, significantly more time was required for the audio mode  = 20.279, p < 0.01, suggesting that unlike pictures, which people can interpret at a glance, sounds may require listening to the entire clip before forming an idea.
However, in the audio mode, time spent on words for which people showed low agreement was not significantly longer than that spent on words where people showed high agreement.
This suggests that time might be an important feature for auditory representations, whether the sound was recognizable or not.
Similarly, in an attempt to illustrate the concept "down" with the "power down" sound, almost nobody named this concept in the labeling study.
Although they are closer to auditory icons, some kinds of sounds seem similar to earcons, and may require learning.
In this paper, we introduced SoundNet, a lexical network extended with environmental sounds.
SoundNet provides a vocabulary of common words with an audioability rating, as well as a five-second soundnail if the word was considered audioable.
The audioability property can be automatically scaled based on the semantic similarity of concepts.
SoundNet carries great potential for facilitating assistive technologies with auditory representations of everyday concepts, and could be used to aid people with language disorders to receive and express information.
A large scale online study was run to collect semantic human labels on the source, location, and interaction of 327 soundnails.
A further study "Sounds as Carriers for Communication" was conducted to evaluate the efficacy of environmental sound representations in daily phrase context in comparison to icons and animations.
Results showed that although the icon/animation mode had better performance overall, there were seven concepts for which the audio mode had significantly higher scores, while there were another 31 words for which the auditory and visual modes were not significantly different.
This suggests that audio has advantages in conveying certain concepts over visual stimuli and may be able to utilize in assistive systems.
We next plan to look at combined auditory and visual cues in language comprehension.
We will continue to refine and extend SoundNet, and explore applications in assistive technologies using SoundNet.
