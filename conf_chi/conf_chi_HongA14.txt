This note describes two studies of the use of a performance modeling tool, CogTool, for making recommendations to improve a user interface.
The first study replicates findings by Bonnie John : the rates at which novice modelers made correct recommendations  and supported them  are close to the values in John's study .
A follow-on study of novice modelers on the same task without CogTool produced significantly lower values.
CogTool improves the UI design recommendations made by novices.
Novice modelers at a different university, taught by an instructor with light experience using CogTool, produced and justified design recommendations for improving a specific user interface with similar accuracy and consistency.
The implication is that learning of CogTool can scale, knowledge we believe is important for shaping future generations of HCI practitioners in the classroom.
CogTool shares the goals of other task modeling techniques for HCI , but we are aware of no empirical comparison between CogTool and these techniques for making design recommendations.
A second contribution is a follow-on study in which novice modelers, without knowledge of CogTool but with the same exposure as in Study 1 to the KLM and other techniques, recommended improvements to the same user interface.
Relatively large differences in performance were observed.
In the remainder of this note we describe the two studies and discuss their implications.
CogTool  is a cognitive modeling tool for HCI; it predicts human performance for storyboarded user interfaces.
In the past few years it has been applied in real-world software development projects to good effect .
John  reports a study in 2011 of 100 novice modelers who used CogTool to produce recommendations for improving two Web sites.
Modelers show significant consistency in their results, and the majority of their recommendations are well supported with quantitative justifications based on CogTool models.
These results are important: modeling tools are typically viewed as helpful mainly to expert modelers in understanding user performance; John demonstrates that CogTool can be easily learned and applied by those new to modeling and relatively new to interface design.
This note describes a replication of John's study.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Undergraduate computer science students in an HCI course participated in the studies.
The course is a conventional survey of topics in HCI, using The Resonant Interface  as the textbook.
At the time of each study, modeling topics in class had covered Fitts' Law, KLM, GOMS, and modeling for HCI in general .
Students  submitted models as part of a homework assignment, worth 5% of their course grade.
In both studies, modelers were given an explicit task to perform on a Web site, as shown in Figure 1.
We chose a simple task comparable to John's for interpretability of our 1.
Visit the Web page http://www.amazon.com.
Click in the Search text box.
Type "kinect"  in the Search text box, and press return.
Click on the top link that is shown in the search results; for me this is "Kinect Sensor with Kinect Adventures and Gunstringer Token Code."
Click on the Add to Cart button on the right.
Determine three potential improvements to the usability of the Web site, for this task, based on the information in the visualization.
For each of your improvements, 1.
Give a short phrase that identifies the improvement.
Optional: If the phrase doesn't entirely specify what the change to the Web site should be, explain it in more detail.
Give a screenshot of the visualization, annotated to show how the improvement would change performance .
In words, summarize the pattern you've identified in the visualization.
Give the time savings, in seconds and milliseconds, that your improvement would produce.
The conditions in Study 2 were the same as in Study 1, except that the modeling task was given to students before the introduction of CogTool.
The modeling instructions for Study 2 gave more generic guidelines, without mention of CogTool: Base your structured description on the material in Chapter 7 and our discussion in class: the Model Human Processor, with its visual, motor, and cognitive capabilities; the Keystroke Level Model; various GOMS techniques.
Modelers worked without specialized tools, mostly relying on Fitts' Law and KLM.
Aside from the use of CogTool, Study 1 and Study 2 are effectively identical, with respect to the task and the background of the participants.
If our results did not match, we would have an additional explanatory factor for the differences.
Our interest is in the generalizability of CogTool across populations of users; we must leave generalization to more complex tasks for future work.
Modelers were also given instructions for building a model for the task and making three recommendations for improving the interface.
Additional instructions, elided here, directed modelers to carry out the task by hand first, to be as specific as possible in their recommendations, and to remember that  the goal was to model expert behavior and  not all potential changes, such as reducing visual clutter or changing the color scheme, would plausibly improve expert behavior, and such changes were not amenable to analysis using the techniques the modelers were to rely on.
Figure 2 gives the modeling instructions for Study 1: modelers were to build their models in CogTool and support their recommendations with annotations of a CogTool timeline visualization.
Study 1 replicates John's as follows: the participants were new to modeling and CogTool, they were given a step-by-step description of a task, and they completed their modeling and recommendation activities within one week.
The instructions were equivalent to John's, though more explicit in describing the required structure for submissions.
The participants were all computer science majors rather than HCI majors with a mix of backgrounds, and they were asked to analyze a different Web site from the ones used by John.
The introduction to CogTool, including a demonstration, took an additional half hour of class time.
The introduction was given by someone not on the CogTool development team, and unlike John's study no hands-on working session followed during which questions could be answered.
Otherwise Study 1 was as close to John's as practicable.
When we attribute performance to a specific influence-- here, CogTool--we must ask about performance in its absence.
In Study 1, 53 modelers submitted recommendations.
Two submissions were produced without using CogTool and are not included in our analysis.
Of the remaining 51, two modelers provided only two recommendations rather than three, giving 151 recommendations in all.
Two evaluators independently created categorizations over the aggregate set of recommendations.1 The evaluators then worked together to agree on a specific category for each recommendation.
As shown in Figure 3, the most common recommendations were to place an additional "Add to cart" button beside each product and to make targets for mouse clicks larger or to place them closer together.
Following John, the recommendations were judged with respect to being "correct"  and "well supported" .
The evaluators judged 68.2% of the recommendations correct and well supported , below John's 75.1%.
Figure 3 shows all of the correct and well-supported recommendations given by modelers; no others were identified.
Our emphasis is not on the categorization of novice design recommendations, but we note the challenge of consistent categorization of usability improvements based on textual descriptions and CogTool visualizations, working without an a priori set.
The two evaluators  differed in the number and assignment of categories they identified, due to ambiguities or combinations of categories in the modelers' descriptions.
There was initial agreement on 74% of the recommendations, with Kappa = 0.693.
These represent poorer performance than John's numbers, but most measures are surprisingly close.
Figure 3 also shows the median duration and interquartile range of the time savings calculated by the modelers, per category of recommendation.
The total time for the task is more than ten seconds, and most of the recommendations would each save half a second or more.
We find it notable that novice modelers can justify recommendations for improving a professionally designed interface in wide use.
Variability in estimates of time savings  is inevitable, due to the coarseness of categorization.
For example, changing the size or location of a target icon can reduce duration, but how much depends on the new size or location; Search autocomplete and Incremental search can reduce typing time, but the reduction depends on how many keystrokes are saved.
Some but not all modelers modified their models for these two design changes; others estimated the savings by subtracting some number of keystrokes.
The latter approach appeared to be common, but it will not always produce accurate results.
Incremental search savings are inflated because most modelers did not include the required shift of visual attention to a dynamically changing set of products.
Even for more straightforward recommendations, such as Autofocus in search box, modelers differed in whether the Think operator should be included in the savings.
As long as the recommendation was correct and the annotation included the relevant cognitive/visual/motor components of the task, it was considered well supported.
One category of interest not included in Figure 3 is Remove extraneous content.
Many modelers observed that removing clutter could improve performance by allowing for target elements to be closer together , but other modelers recommended reducing clutter based on incorrect arguments .
In general, incorrect recommendations were of two types: within the scope of the task but not likely to improve an expert user's performance ; or out of scope, without regard for expertise .
We evaluated the consistency of the recommendations with the Any-Two Agreement  statistic .
A2A is the number of common problems found by two modelers, divided by the total number of problems found by those two modelers, averaged over all modelers.
John's A2A value of 34% puts her study above 9 of the 12 studies in Hertzum and Jacobsen's survey ; our A2A value of 30% is above 8.
Study 1 adds evidence to the view that CogTool may reduce the evaluator effect by providing a model-based grounding for recommendations .
Two weeks after the completion of the modeling exercise a brief, informal survey was given.
47 modelers submitted free-form text answers to questions about their use of CogTool.
The most informative answers were to the question, Describe in a few sentences how CogTool fit into the process of your making recommendations for design improvements.
John has demonstrated that novice modelers can produce useful results with CogTool; our survey provides initial clues about how they integrate modeling into their analysis, as well as how CogTool improves the process.
Our informal analysis of the responses showed three categories of interest.
The most common answer, from 26 modelers , described using CogTool to identify appropriate areas for design changes.
Most then described using CogTool to justify their recommendations in quantitative terms.
Nine modelers  also mentioned making comparisons between models, to evaluate the differences between interface designs.
Thirteen modelers  only mentioned using CogTool to validate design decisions they had thought of on their own: "It helped give me proof that my suggestions would actually help by providing estimated times."
John's experience teaching CogTool to HCI majors at CMU extends to computer science majors at a different university , and detailed knowledge of CogTool is not needed for adequate instruction.
This is promising news for practical modeling, recognized by some of the novice modelers: "I would never have thought about these areas taking up users' time, so CogTool did help..." Existing research provides limited information about how modeling can best be integrated into interface design tasks , but the strategies used in Study 1 are among those typically recommended by experienced HCI modelers.
Notably, the novices were able to find effective ways to use CogTool without explicit guidance, to produce correct, well-supported recommendations.
Study 2 indicates that CogTool is a decisive factor in helping novice modelers produce good design recommendations--it was not because the task was too easy.
With CogTool, recommendations were much more often correct and well supported, and the proportion of modelers who made such recommendations was higher.
CogTool appears to help novice modelers to focus their recommendations on those relevant to a specific class of users, within the boundaries of specified tasks.
Further exploration of how and why, as well as implications for professional UI development, remain for future work.
For Study 2, in which modelers did not use CogTool, 50 modelers submitted 150 recommendations in total.
The same evaluators followed the same procedure as in Study 1 to categorize recommendations, except using references to the modeling techniques above rather than to CogTool in their evaluation.
Figure 3 shows that the correct and wellsupported recommendations in Study 2 are comparable to those of Study 1 in the numerical distribution of categories and estimates of time savings.
But the totals were considerably lower.
In Study 1, 88% of recommendations were correct, versus 63% in Study 2; for correct and well-supported recommendations the values were 68% and 43%.
Figure 4 shows the tail distributions for the count of correct/well-supported recommendations, aggregated over modelers; that is, 35/51 or 69% of modelers in Study 1 gave three correct recommendations, and so forth.
A complete statistical analysis is beyond the scope of this note, but a two-sample Kolmogorov-Smirnov exact test shows a significant difference between Study 1 and Study 2: D = 0.321, p < 0.01.
Analysis of correct and well-supported recommendations shows the same result.
In other words, performance in these studies is not determined purely by the task and the background knowledge of modelers; use of CogTool is a significant factor.
The drop from 30% in Study 1 is not unexpected, given the greater variation in modeling procedures.
No new correct and well-supported recommendations were identified, but there was a wider range of incorrect recommendations.
These fell into the same general classes as in Study 1: changes affecting performance outside the scope of the task , and not focusing on expert users as specified in the instructions .
The latter is not surprising.
In an HCI survey course, the clearest illustrations of usability issues are often based on the challenges that novice users face in complex interfaces; CogTool currently targets expert users.
The former category of incorrect recommendations is more surprising, though not in retrospect.
CogTool also appears to help novice modelers remain aware of the boundaries of a specific task when attempting to improve user performance by changing the UI design.
