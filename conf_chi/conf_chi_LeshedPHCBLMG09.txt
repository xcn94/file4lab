While most collaboration technologies are concerned with supporting particular tasks such as workflows or meetings, many work groups do not have the teamwork skills essential to effective collaboration.
One way to improve teamwork is to provide dynamic feedback generated by automated analyses of behavior, such as language use.
Such feedback can lead members to reflect on and subsequently improve their collaborative behavior, but might also distract from the task at hand.
We have experimented with GroupMeter - a chatbased system that presents visual feedback on team members' language use.
Feedback on proportion of agreement words and overall word count was presented using two different designs.
When receiving feedback, teams in our study expressed more agreement in their conversations and reported greater focus on language use as compared to when not receiving feedback.
This suggests that automated, realtime linguistic feedback can elicit behavioral changes, offering opportunities for future research.
However, unless teams are given guidance on the basics of effective collaboration skills, the tools may offer little benefit .
For instance, giving a design team a screen sharing tool in order to help them come up with a single design proposal will not necessarily help them to develop the consensus-building skills needed to complete the assignment collaboratively.
Teamwork can be a powerful tool for learning  and for accomplishing tasks .
However, if people are expected to successfully work in teams, they should be given tools to adopt appropriate interpersonal skills to overcome challenges and to achieve effective team processes and outcomes .
These skills are often taught by providing team members with feedback on their own behavior, along with guidance on how certain changes in their behavior might improve group outcomes .
Our high-level research goal, therefore, is to understand how collaboration technology can illuminate social processes and behaviors within teams, allowing members to reflect and learn to become better collaborators.
One of our specific goals is to design computer-mediated environments that both focus attention on group process and support the team task, thus enriching teamwork experiences.
Below we describe our underlying principles for providing automated feedback to teams using peripheral visualizations as the team goes about completing its tasks.
We then present GroupMeter, a research platform proposed by Leshed et al.
We conclude by presenting a distributed group-based user study of GroupMeter, focusing on four main questions: RQ1: Does automated feedback about language behavior cause people to reflect on their use of language?
RQ2: Do people modify their communication patterns when provided with automated feedback?
RQ3: Does automated feedback about language use distract from the team task?
RQ4: How do people experience the use of different feedback designs?
Growing interest in computer-supported group work  has motivated research on developing communication and collaboration technologies that support many aspects of group work: fostering informal interaction , creating awareness of colleagues' presence and behavior , and supporting shared tasks such as writing or meeting .
An implicit assumption in this work is that simply pro-
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Bosworth suggested that providing feedback on teamwork behaviors in practice situations is a key element in teaching collaborative teamwork skills, allowing team members to process and reflect on their own experiences .
Traditionally, the source for group process feedback has been either mentors--teachers, managers, or expert observers --or group members through peer assessment procedures such as interviews or questionnaires .
However, asking people to provide feedback has drawbacks.
In particular, feedback from peers, supervisors, or instructors, can be obtrusive and expensive .
We suggest an alternative approach: using automated analysis of communication content to provide feedback.
In many settings, teamwork consists mostly of conversations and meetings--often conducted through CMC tools that can capture the conversation.
In these cases, automated analysis of group conversations could reveal much about a group's collaborative processes.
This, in turn, could be presented to teams to help them improve their skills and performance.
Prior to deployment, however, an automated technique must demonstrate  analyses of linguistic features that correspond to actual desirable collaborative behaviors, and  feedback presentation that causes participants to reflect on and change their behavior.
A general strategy for making collaboration behavior visible is to use social proxies, graphical representations of users that depict their presence and activities .
For example, Babble  represents individuals in a discussion forum as colored dots around a circle that represents the group, placing individuals closer to or farther from the center based on their participation in the discussion.
Systems such as Chat Circles  and visualizations of turn taking based on audio input  together with eye-gaze patterns  have shown that social proxies can stimulate reflection on and influence collaborative practice.
Visualizing each member's participation relative to other group members, for example, has been shown to prompt dominant contributors to become aware of their behavior  and attempt to achieve more balanced participation .
Similarly, augmenting synchronous CMC environments with visualizations of contribution level and amount of group agreements led users to perceive the group experience as more positive and effective, and to perform better .
These latter studies highlight the potential of using finergrained behaviors, such as indicators of agreement obtained through linguistic analysis, as a source of feedback to help group members attend to, reflect on, and change their communicative and collaborative behaviors.
We thus posed the following research questions: RQ1: Does automated language-based feedback cause people to reflect on their language use, especially about the specific aspects of language use the feedback represents?
RQ2: Do people change their communication patterns when provided with feedback compared to when not?
Recent work suggests that automated linguistic analysis can achieve these two criteria.
For instance, Latent Semantic Analysis  of team conversations predicted team performance scores on a flight simulation task .
In that task, teams where members stated more facts and acknowledged their peers tended to perform better.
Further, machinelearning approaches have shown promise in collaborative learning settings in identifying how learners construct arguments that build on others' contributions .
Others have investigated using word-level analysis in group situations, such as using the Linguistic Inquiry and Word Count tool , a dictionary-based approach to analyzing language features.
These findings suggest that a system that analyzes language in team conversations can be used to provide effective nearreal time feedback to team members, causing them to reflect on their conversational practices.
Despite its value, feedback cannot be the star of the show.
A long-recognized principle of teamwork effectiveness is that group members must attend both to their task and to social matters .
It is thus important, in designing a system to provide group process feedback, to ensure that team members are able to focus on their task as well.
Our approach is to provide dynamic feedback in the periphery of the team activity interface.
This allows team members to maintain awareness of the social process and how their behaviors are tied to it, while still focusing on the primary team activity.
It is important that the peripheral visualization effectively conveys awareness information  in ways that minimize cognitive load and do not significantly impact users' performance on the primary task .
Still, our aim is to stimulate reflection on and awareness of language use and teamwork behavior, leading to a possible tension between attending to ambient feedback visualizations and accomplishing a task.
The chatroom text is processed using a dictionary-based word count tool based on LIWC .
LIWC counts what percentage of words in a block of text fall into various categories such as emotion words, self-references, and assents.
Unlike advanced natural language processing techniques , LIWC can produce linguistic markers in nearreal time.
Further, these markers can serve as measures of conversation style and social behavior .
For example, the use of pronouns requires a shared understanding of their referent between the speaker and listener .
In the current study we chose to present feedback on word count and percentage of agreement words , based on two sets of previous research findings.
First,  showed that the amount of contribution to discussion is positively associated with peer-rated teamwork behaviors.
Second, they also showed that critically addressing the conversation without passively agreeing was related to positive collaborative behaviors .
We implement our visualizations of linguistic feedback using GroupMeter, a platform designed to help teams reflect on their group behaviors and support research into the relationships between communication behaviors, feedback, and teamwork.
GroupMeter was conceptualized in  based on a study with a peer feedback intervention, but was not fully designed and implemented.
We present here a working prototype developed to carry out the study presented later.
The basic design of GroupMeter consists of a web-based system in which groups communicate through a chatroom to perform their tasks and see feedback visualizations based on their conversation.
The front end consists of an AJAX-driven HTML chat with feedback visualizations presented using Adobe Flash.
A server manages the chat sessions and parses the chat text to provide the feedback metrics.
An administrative UI supports management of user accounts, group creation, feedback metrics and sessions.
We tested two designs for feedback visualization.
Our main design goal for both was to give group members a clear notion of how their linguistic behaviors compare to those of other group members.
This allows individuals to compare themselves to specific others in the group as opposed to an abstract aggregate , as well as allowing them to reflect on both their own and their teammates' behavior .
At the same time, we wanted to be sure the design did not encourage competitive behavior.
Hence it was important for us to show individuals as being part of a larger group - all of this through aesthetically pleasing visualizations that "tell a story about the data" .
We also wanted to be ambiguous to some extent with respect to behavioral norms, hoping to make room for users' own interpretations relevant to their specific context .
Finally, to minimize distraction, we designed the visualizations with the principle of glanceability , aiming for easy visibility of relevant feedback information.
In the first visualization, feedback is presented as horizontal bars, one for word count and one for percentage of agreement words.
The bars change in length based on each participant's linguistic behavior .
An alternate display using a standard bar chart might cause people to try to have the highest bar.
Further, showing individuals' behavior as an aggregate bar emphasizes the idea of being part of a group.
We located the bars at the bottom of the chat box as this allowed us to place them on-screen without the need for scrolling.
The intent of the bars' design and placement is to provide ambient, unobtrusive feedback that can be efficiently processed at a glance.
Figure 2 shows an overview of the structure of a session.
Using a within-subjects design, participants were randomly assigned to seven mixed-gender groups of 2 to 5 members.
Each group went through three conditions: Fish visualization, Bars visualization, and None .
The order of the conditions was randomized for each group.
To engender a feeling of distributed chatroom collaboration, we did not require participants to come to the lab.
Instead, they were scheduled to log into the GroupMeter website at particular times from wherever they were, using either Internet Explorer or Firefox on a PC or a Mac.
Once logged in, participants were greeted by the experimenter  and informed that they would be working as a team on three tasks.
They then had a few minutes to talk, to get used to the interface and to break the ice.
The experimenter then explained the nature of the study and that in each of the tasks they would see a different visualization of the language they used.
The word count and agreements measures were then described for both visualizations.
For each task, the team was given five minutes to brainstorm ideas for solving a problem, and then five minutes to choose the top three solutions.
The experimenter sent a message at the end of each brainstorming period and each task.
Due to a small number of groups, it was not possible to counterbalance both the visualization and task orders.
Since the tasks were all of the same general type, we felt that the risk of carryover effects was smaller than it would be for visualizations.
We therefore kept the task order constant for all groups and randomized the order in which visualizations were presented to groups.
Upon completion of all tasks, participants filled out an online survey that included scale items to assess their perceived awareness of their language use during each task , distraction by the visualizations , and open-ended questions to capture their experience .
We kept the task times short and waited to the end of all three tasks to fill out the survey to avoid possible contamination between the tasks and to reduce the chance of participant fatigue.
Sessions lasted about 60 minutes total.
The second visualization  consists of an animated school of fish with each member represented by a colored fish.
We hoped that the school of fish would instill the feeling of being part of a group.
The fish start in a circular formation, all at the same size and equidistant from the center.
When a participant uses more agreement words, their fish moves closer to the center.
Fish increase in size when a participant speaks more.
Again, the shape of the visualization dictated its placement.
Locating it below the chat box would have required users to scroll to see it; placing the visualization to the right of the chat box allowed it, like the bars, to be continually visible.
The purpose of the lively nature of the fish was to draw attention to the feedback information represented by their position and size.
Our final research question, then, relates to how these different visualizations of the same feedback information affect users' experience: RQ4: How do people feel about their experience of both designs?
What aspects of each do they like and dislike?
These visualizations allowed us to address each of our four research questions in the experiment described below.
We wanted to assess: the capability of GroupMeter's feedback visualizations to trigger reflection on language use and word choice ; people's actual use of language beyond their perceptions and reflections when provided with these visualizations ; whether one or the other visualization was more distracting ; and users' felt experiences with each visualization .
Twenty-five undergraduate students  from a Human-Computer Interaction course in a large university in the United States received course credit for their participation in the study.
As HCI students, these participants are likely sensitive to interface design issues and we sought their opinions about our designs.
Our data consisted of numerical and open-ended responses to the survey and the transcripts of the chatroom conversations.
To examine communication patterns within teams, we coded each chat entry with one of six codes representing a behavioral move that we believe relate to the nature of the tasks and the feedback provided through the visualizations.
This included both new ideas as well as repeating ideas already mentioned .
An example for the wilderness task was "a water purifier thingy."
We excluded entries about study administration, troubleshooting, digressive remarks, and fillers.
Two coders coded the transcripts independently, 1269 entries in total, assigning each entry the most prominent code.
Inter-coder agreement was 80%; disagreements were resolved by discussion.
Our data were hierarchical in nature, with observations nested within participants and participants nested within teams.
We used hierarchical linear models to account for non-independence of the statistical data, calculating variations within and between participants and teams, and used post-hoc analyses for our statistical tests.
Pairwise comparisons between the three conditions show that the both the Fish and Bars visualizations were significantly different from None , and that Fish and Bars did not differ from each other.
There was no significant effect of the order in which visualizations were presented =0.62, NS, meaning that participants were more aware of their language use in the feedback conditions regardless of the visualization order.
This provided an initial answer to RQ1: feedback caused a reported increase in reflection on language use.
RQ2 asked whether the feedback visualizations affected communication patterns and group dynamics.
We therefore used the coded chat transcripts as behavioral data in answering this question.
Based on , we assumed that the dynamics and processes used by groups to accomplish the brainstorming and decision-making subtasks would also differ.
For example, while brainstorming requires divergent group thinking, decision-making calls for convergence.
We therefore divided the transcripts into two segments, according to the 5-minute subtask periods during which they were typed.
Figure 4 shows the relative proportions of each coded entry type by condition and segment.
The data for the bar charts appears as percentages in Table 1.
For each entry type, we used post-hoc testing for main effects of Condition and Segment , as well as interaction effects of these factors.
As with the questionnaire data above, we found no main effects of Visualization Order for any of the entry types, ruling out the possibility that the order in which the visualizations were administered affected the results.
We were interested in whether the experimental condition had any effect on the number of ideas generated.
Differences between the conditions might indicate, for instance, that feedback distracted people from the brainstorming task, or motivated them to produce more ideas.
Since RQ1 was framed in terms of participants' perceptions, we relied on their responses to survey questions in which they evaluated the degree to which the visualizations made them think about and change the words they used.
Three five-point Likert scale items addressing these issues were aggregated into a single scale : "I fo-
Once again, we found a main effect of Segment, with participants using a higher proportion of agreement entries during the decision-making segment than during the brainstorming segment =16.12, p<0.001.
Again, this was expected given that participants were instructed to reach consensus toward the end.
Participants in the Bars condition used a higher proportion of disagreements in the brainstorming segment compared to the other two conditions and showed a larger decrease in disagreement in the decision-making segment.
One possible explanation is that the bars visualization provided participants with the clearest feedback, which in turn, may have prompted the most behavioral change.
Why this occurred for disagreements and not for other statement types remains unclear.
This suggests directions for future research; Perhaps the bars created a more "business-like" feel to the session, and this may have led to greater sensitivity to disagreement than to agreement feedback.
Again, we observed the expected changes across segments =17.98, p<0.001, with participants reducing the proportion of disagreement entries from the brainstorming to the decision-making segment.
This complements our finding on the increase of agreement entries from the brainstorming to the decision-making segment; both changes suggest that groups attempted to reach consensus.
As expected, more ideas were presented during the brainstorming segment.
Because participants received feedback on the frequency of their use of agreement words, differences between conditions on their proportion of agreement entries would strongly suggest that the feedback affected communication behavior.
A main effect of Condition =3.45, p=0.04 was observed, supporting our expectation that the feedback triggered changes in the expression of agreement.
There was a marginally higher proportion of agreement entries in the Fish condition compared to None =1.84, p=0.07, and a significantly higher proportion in the Bars condition compared to None =2.35, p=0.02.
As with idea generation, differences in the amount of discussion between conditions would indicate that the feedback was affecting how teams performed the task.
In other words, when viewing the Fish visualization, teams tended to discuss ideas less compared to when not receiving feedback.
This calls for additional investigation, but we speculate that the visualization of agreements might have induced more focus on agreement rather than on discussing ideas.
This finding also suggests, regarding RQ3, that the fish drew more attention to the specific behaviors that influence the visualization as compared to completing the task at hand.
Across all conditions, a main effect of Segment showed that groups discussed ideas more in the brainstorming segment than in the decision making segment =11.16, p=0.001, as expected.
The interaction effect between Condition and Segment was not significant =0.70, NS.
We examined proportions of ranking entries to see if the feedback had an effect on the extent to which participants expressed their preferences for ideas.
As expected, a main effect of Segment shows that more ranking entries occurred during decision-making than brainstorming =87.72, p<0.001.
As one indicator of the extent to which different visualizations changed team interaction, we looked at the number of entries that explicitly referenced the feedback.
These could indicate that the visualizations captured the attention of participants and became a conversation topic.
They only appeared in the Fish condition, and only for 3 groups, as in the following chat excerpt: m: wait, weren't our fishes supposed to swim?
Comments such as the example above suggest that the fish visualization did not remain in the periphery to the same extent that the bars visualization did.
The fish may thus have been somewhat more distracting than the bars visualization, eliciting comments that deviated from the task.
We will present further evidence relative to RQ3 and RQ4 below.
Finally, we looked at total conversational activity to see whether the visualizations had any effect on the duration or substance of group conversation.
Neither the total number of entries nor the total word counts posted by team members had statistically significant differences by Condition, Segment, or ConditionxSegment.
Considered together, these analyses allow us to address RQ2.
While it is possible that some of the differences observed in the proportion of agreement statements stem from participants attempting to manipulate or "game" the feedback display, they nonetheless changed behavior in ways that made sense for the specific activity they were doing .
Further, we found little evidence of gaming the system in our reading and coding of the transcripts.
An important assumption in our approach is that automated linguistic analysis can effectively capture communication patterns.
To assess this assumption, we compared the results of an automated LIWC analysis with the relevant codes produced by human coders.
For example, the LIWC assent category should correlate with the human coding of agreement entries.
Likewise, the negate category should correlate with disagreement statements.
To address this question, we could not simply examine correlations given the non-independence in the group data.
Instead, we created hierarchical linear models nesting participants within groups.
One model tested whether the rate of assent terms identified by the automated LIWC analysis could predict the proportion of agreement entries per participant identified by human coders.
The second model predicted the proportion of disagreement entries identified by human coders using the LIWC category negate.
The results of both models suggest that automated linguistic analysis, such as that produced by LIWC in the GroupMeter system, is a powerful method for capturing the communication pattern and tone of entries as interpreted by human coders: the assent category significantly predicted the proportion of agreement entries =56.18, p<0.001, and the negate category significantly predicted the proportion of disagreement entries =24.72, p<0.001.
This finding is interesting, since although the human coders were looking for linguistic cues when coding for agreement and disagreement statements, they were instructed to consider the tone of messages beyond the words they contained.
For instance, the entry "me too" was coded as agreement but does not contain agreement words.
In addition to the behavioral data discussed earlier, we addressed RQ3 by asking participants to evaluate their task focus in each of the tasks, using the 5-point item "I remained focused on the task throughout the exercise" .
To specifically address the difference between the two visualizations, we asked participants to rate the item "I was paying attention to the feedback" .
People also found them marginally more distracting , based on an aggregate score of two 5-point items : "The feedback I received was interruptive" and "The visualization distracted me from the task."
Thus, with regard to RQ3, our results provide several answers.
First, the fish visualization seemed to draw more attention and was perceived as more distracting than the bars visualization.
This is also supported by two behavioral effects reported above: people in the Fish condition were the only ones to talk about the visualization, and they had a smaller number of discussion entries compared to None.
Addressing RQ1, we found that providing visual feedback made users more aware of their language use.
While it is possible that the within-subjects design led participants to pay special attention to the feedback when it was present, research on psycholinguistics suggests that much of our language production and planning is unconscious .
Thus, we believe that stimulating reflection on one's own word choice is important in making people change their communication patterns, particularly in a conversational setting.
This is a critical first step in understanding how to improve teamwork behavior via visual feedback.
With respect to RQ2, our results show that people changed their behavior when seeing feedback.
They spent more time agreeing with each other, less time discussing the brainstormed ideas when seeing the fish, and drastically decreased their disagreement when seeing the bars.
The fact that GroupMeter was able to elicit changes in communication behavior is especially encouraging given previous research suggesting that people's choice of words in conversation is largely spontaneous, unintentional, and uncontrolled .
In our study, participants were nonetheless able to effect changes in their communication patterns  in both visualization conditions.
Our results thus demonstrate the power of automated linguistic analysis for teamwork feedback in stimulating reflection on and change in behavior.
This leaves open the question, however, of which specific behavioral changes are desirable.
We did not explicitly pose normative guidelines, although length of bar and distance of fish from the center possibly implied that more agreements were preferred.
This, in addition to the mere presence of the feedback guiding participants toward self-focused awareness, might have led them to conform more to the group .
If fewer agreements and more discussion are favorable behaviors , integrating appropriate guidelines for effective teamwork into the design is an important future direction.
Regarding RQ3, our findings demonstrate the complexity in designing feedback visualizations that raise awareness of and reflection about social processes without distracting the team.
However, the fish were perceived as drawing more attention than the bars, suggesting that given the desired reflection and behavioral change, more subtle visualizations may be sufficient.
One limitation of our results is the use of self-reports to estimate how distracted participants were.
Future work could involve more objective measures of distraction, such as via eye tracking or evaluation of participant recall of specific conversational elements.
Finally, regarding RQ4, our qualitative findings suggest that the two designs elicited quite distinct experiences from users.
They enjoyed the playfulness and liveliness of the fish, and valued the efficiency of information conveyed through the bars.
Finally, we were interested in participants' qualitative experiences with and opinions about the visualizations .
We asked participants in open-ended questions to tell us their reactions to and what they liked and disliked about each of the visualizations.
They reported that they generally liked the visualizations and thought they were "cool".
They liked that the fish were "fun to look at", "cute", and "dynamic".
However, the fish were also considered by users to be harder to understand than the bars, and more distracting with their lively animations and movements on the screen.
In contrast, whereas users liked the bars' ease of interpretation, they were referred to as "boring" and "just there".
The behavioral findings presented earlier also support these self-reports.
The fewer discussion entries in the Fish condition compared to None and the three groups talked about the fish but not about the bars suggest that the fish drew attention and were worth discussing during a timed exercise.
Together with the results for RQ3, these responses suggest that our users experienced the fish visualization as engaging and even enchanting , as they offered an experience of being "caught up and carried away".
However, users did feel that the playfulness designed into the fish sacrificed ease of use, glanceability, and peripherality, which are important in the task-related setting for which GroupMeter is designed for.
They liked the unobtrusiveness of the bars, but also criticized their lack of excitement.
GroupMeter adds to this work by influencing teamwork behavior through detailed feedback about word choice and production in computer-mediated settings.
Our work also contributes to research around peripheral awareness systems .
We demonstrate that the design of such interfaces should carefully consider the balance between focus on the primary task, here the teamwork task, and maintaining awareness of peripheral information, here the nature of team communication.
Finally, our findings reveal the power of different visualizations representing the same data to bring about diverse user interpretations and experiences..
We view both our design and implementation of GroupMeter, and the experimental results, as successful preliminary efforts in a broader program of understanding the relationships between collaboration, automated linguistic feedback, and visualization design.
Extended research is needed to fully understand the ways that technology can be used to illuminate and improve collaborative behavior.
One area for future work lies in understanding the value of specific behavioral changes, and how to provide interpretable feedback that will elicit these changes.
For example, brainstorming tasks might call for encouraging people to generate new ideas while discouraging both agreements and disagreements; other tasks might require different metrics and different normative goals.
Careful selection of appropriate feedback dimensions is therefore imperative if desirable changes are to result from presenting feedback.
More work is also needed to fully understand the distinction between users modifying their language in order to manipulate the real-time visualization, and the actual adoption of effective teamwork skills outside of the laboratory.
Another direction to pursue is to examine cross-cultural issues of feedback on language use in computer-mediated communication.
For example, the use of singular versus plural first person pronouns in CMC environments differs between Western and Eastern cultures .
Providing feedback to team members on their first person pronoun word choice could thus have differing effects depending on their culture.
Also, whereas American culture is very task-oriented, European and Asian cultures value the social relationships within the team .
This can lead to major differences in how team members accept process feedback that is not directly related to the task at hand.
Dynamic feedback visualizations of teamwork process information have the potential to affect the interaction of users in CMC environments.
Results from manual coding of chat transcripts, as well as user responses to scale items and openended questions, demonstrate that automated feedback on linguistic behavior alters how people think about, communicate in, and experience their teamwork practices.
We found evidence that automated feedback can make people aware of and reflect on their language use, a first step in training them to acquire better teamwork skills.
Furthermore, our findings show that automated linguistic feedback can be powerful in more than raising awareness of one's own language use - it can also cause people to alter their communication patterns.
