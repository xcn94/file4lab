With computers being used ever more ubiquitously in situations where privacy is important, secure user authentication is a central requirement.
Gaze-based graphical passwords are a particularly promising means for shoulder-surfing-resistant authentication, but selecting secure passwords remains challenging.
In this paper, we present a novel gaze-based authentication scheme that makes use of cued-recall graphical passwords on a single image.
In order to increase password security, our approach uses a computational model of visual attention to mask those areas of the image that are most likely to attract visual attention.
We create a realistic threat model for attacks that may occur in public settings, such as filming the user's interaction while drawing money from an ATM.
Based on a 12-participant user study, we show that our approach is significantly more secure than a standard image-based authentication and gaze-based 4-digit PIN entry.
Despite these advantages, graphical passwords that rely on physical interaction with the authentication system are still susceptible to shoulder-surfing attacks .
Particularly in public spaces, e.g.
One solution to increase the security of graphical passwords is to use authentication schemes that do not require any physical contact with the system.
Amongst the methods investigated in the past, the human gaze is particularly promising for implementing such schemes .
By its very nature, gazing does not require any physical contact and therefore, potentially works over greater distances.
In addition, the human eye moves rapidly, which makes eavesdropping gaze-based passwords more difficult than touch-based input.
A key challenge in user authentication generally, and in graphical schemes in particular, is to define secure passwords.
Previous research has shown that such schemes lead to hotspots, i.e.
A password point is defined as a single fixation that is detected by the authentication system to be part of the chosen graphical password.
These hotspots render such schemes more susceptible to dictionary attacks .
The only viable solution so far has been to select single password points across a sequence of several images .
In this present work, we present an alternative gaze-based authentication scheme that supports users in selecting secure gaze-based graphical passwords.
To tackle the problem of hotspots, our scheme uses a computational model of visual attention - also known as saliency maps - to mask out those areas of the image most likely to attract visual attention .
We show that this approach significantly increases the security of gaze-based cued-recall graphical passwords.
Graphical passwords have long been investigated as a means of user authentication .
Cued-recall graphical passwords  have considerable advantages over traditional approaches, such as text passwords, as they leverage the vast capacity and capabilities of the human visual memory system .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In their simulations, they found that the gaze data partly resembled the password click points.
The authors argued that this might offer attackers an advantage over guessing click-based graphical passwords at random.
Others investigated means of making text-based and graphical passwords more secure.
For example, Forget et al.
Their system randomly added characters to the passwords defined by the users.
Afterwards, users were allowed to retrigger this process until they found a password they felt was easy to remember.
The authors found that their approach significantly increased password security, but also that participants exhibited compensatory behaviour, by choosing weaker passwords in the first place.
Poorly chosen passwords in graphical password schemes like PassPoints  were found to lead to hotspots, i.e.
These hotspots allowed attackers to mount more successful dictionary attacks .
In subsequent research, they were able to successfully persuade users to select more secure passwords, by using a viewport positioned randomly on the picture that forced users to select a click point within its area .
They described a shoulder-surfing-resistant gaze-based authentication scheme that relied on several points on a sequence of different images.
They evaluated their scheme with respect to eye tracking accuracy, password entry time, as well as the number of successful logins by users with their own passwords.
They also discussed some advantages, in particular the larger theoretical password space  and the cued-recall nature of their approach that helped users to more easily remember multiple passwords.
Gaze has only recently started to be investigated as a means of user authentication.
In one of the first works in this area, Kumar et al.
They found that gaze-based password entry required marginal additional time and that the error rates were similar to those of using a keyboard.
Overall, the users in their study preferred gaze to traditional keyboard-based password entry.
One problem with text-based passwords is that they are more vulnerable to guessing attacks, due to the predictability of passwords, particularly for weak user-chosen ones.
Several researchers have investigated gaze-based approaches that are less predictable and thus less vulnerable.
For example, Maeder et al.
Using a grid overlaid on the image, different sequences of these points were interpreted as different PINs.
They evaluated different fixation thresholds and found that in all cases, users were able to successfully authenticate, using their own PIN sequence.
A key issue with gaze-based authentication schemes is the need for accurate point-of-regard gaze measurements.
To address this issue, De Luca et al.
They argued that gestures can be detected more robustly and are therefore better suited to real-world applications, such as public terminals or ATMs.
In a subsequent work, they presented EyePassShapes - an authentication system that used a single eye gesture as the password .
The results of their usability studies indicated that EyePassShapes are more secure than common manual PIN entry and add less time to the login procedure than previous gaze-based authentication schemes.
To assess the security of EyePassShapes, they performed a qualitative evaluation where a person familiar with their system tried to attack the password using video footage.
Previous research has investigated eye tracking for gaze-based authentication, as well as means of persuading users to select more secure graphical passwords.
Only one study used a "guessing test" to qualitatively evaluate the security of gazebased graphical passwords .
That study involved a single attacker and - like the study presented in  - required the user to press a key on a keyboard for several seconds to activate the password detection.
In contrast, the current work presents a quantitative evaluation with a total of 240 password attacks from 12 users.
In addition, we are the first to investigate computational models of visual attention in order to increase the security of gaze-based graphical passwords.
Figure 2: Sample images within the different image categories used in the studies: abstract image with low image complexity , abstract image with high image complexity , nature image , holiday image , and landscape image .
Given an input image or video, these models compute a so-called saliency map that topographically encodes for saliency at every location in the image.
Visual saliency models were shown to predict human visual performance for a number of psychophysical tasks and have a large number of applications .
For example, in computer vision, saliency models were used for automated target detection in natural scenes, smart image compression, fast guidance of object recognition systems, or high-level scene analysis .
The key concept underlying this work is that by encouraging users to select password points that do not fall inside salient regions of the image, the security of gaze-based graphical passwords can be increased significantly.
This is similar to the characteristics commonly required for text-based passwords, such as a minimum number of different alphanumeric or special characters.
Figure 1a shows one of the normal login images used in this work.
Because the penguins' heads and feet are most likely to attract the user's visual attention - as predicted by the visual saliency model - these parts are masked out in Figure 1b.
In a real-world authentication system, such masked images would be shown to the user in selecting the initial password.
During operation, such as for authentication at a public terminal, the same image but without a mask would be used instead.
To calculate saliency maps, we used a Graph-Based Visual Saliency  model .
GBVS was shown to predict human fixations on natural images with superior performance to the original visual saliency algorithm presented in .
The saliency maps were calculated using the default parameters of the toolbox.
The greyscale heatmaps returned by the GBVS algorithm were first normalised and a threshold applied at the 0.5 level so as to separate salient and non-salient areas.
The salient areas were used as saliency masks that were overlaid in red onto the original images .
The login interface was implemented in Java and obtained gaze data from the eye tracker via TCP/IP.
In addition, it recorded all events triggered by the user interacting with the system, such as the detection of password points or successful and failed login attempts.
The interface included a standard 9point calibration routine to adapt the tracking system to each user and a validation routine to assess the calibration quality.
The testing routine involved the user looking at each of the calibration points in sequence until a fixation was detected by the system.
Calibration quality was calculated for each participant as the mean Euclidean distance between all calibration points and the detected fixation points.
In the user study, depending on the type of graphical password, the interface showed two different screens: * PIN: the login screen showed a grid of 10 tiles resembling a standard 10-digit keypad .
In order to perform a login attempt, the participant fixated at four of these tiles in sequence.
Sequences of fixations to correct tiles in the correct order resulted in a successful login attempt; all other attempts were considered as failed.
In contrast to PINs, the screen was not discretised by a grid, but arbitrary locations in the image could be selected.
Figure 3: Standard 10-digit keypad used in the studies for gaze-based PIN entry.
To select a password point, the participant had to fixate at a certain point in the image.
The software continuously analysed the dispersion of the gaze points within a time window of 1.5 seconds.
A fixation was detected by the system if at least 70% of the gaze points in this time window were inside a circular area with a radius of 1.7 degree of visual angle .
Figure 4: Experimental setup used in the pre-study.
Participants selected image-based passwords, gazing at images with and without a saliency mask, as well as PINs on a standard 10-key keypad.
Gazes were recorded using a remote eye tracker placed under, as well as a high-resolution video camera mounted above the computer screen.
We first downloaded a large number of images from the Internet with a resolution of 1600x1200 pixels and without any potentially offensive or sexual content.
The images were then grouped into five different categories: 1.
Abstract images showing low-complexity background images with several homogeneous areas .
Abstract images showing high-complexity scenes such as from computer games or star clouds .
Nature images showing mainly  animals in the wild, making it easy to select an individual or a feature of an individual, such as a leg .
Holiday images showing a beach or famous sites very similar to what we expected people to have among their own private holiday pictures .
Landscape images showing famous skylines or mountains .
We randomly chose four images per category and calculated a saliency mask version of each of these images, resulting in a total of 40 different images.
The experiment consisted of a pre-study and a main study.
In the pre-study, we asked one group of participants to define three different types of gaze-based graphical passwords .
The goal of the main study was to assess the security of these passwords by asking a second group of participants to try to attack and guess these passwords by analysing close-up videos of the eyes recorded from the pre-study participants.
The main study was designed as a randomised, controlled user study .
For the main study, the binary output of all login attempts per image  was the dependent variable.
The dependent variable was measured under three different conditions of the within-participants factor "graphical password": PIN-based, image-based without saliency mask, and image-based with saliency mask.
Selecting the images was an important step in the study preparation.
Allowing participants to choose their own images would have made it difficult to control for similarity across images.
In addition, pre-processing of the images would have been necessary to adjust for ratio and resolution.
Finally, we expected side effects if people had chosen familiar images, for example containing family members or friends, for which certain areas in the image would have been more likely to be selected as password points.
Because they should also cover a realistic range of motifs and different visual complexities, we preselected all the images.
The goal of the pre-study was to record a set of real gazebased graphical passwords that would be attacked in the main study.
For the pre-study, we recruited four participants , aged between 22 and 32 years , none of whom were familiar with the project.
To have as wide a variety of faces as possible, we selected one participant wearing glasses, one with an Asian appearance, and one male as well as one female European.
The pre-study was performed in the lab using the setup shown in Figure 4.
After arriving at the lab we first explained the task to the participants and asked them to sign a consent form.
Then, participants were instructed how to use the login interface, in particular how to enter and verify their passwords.
We calibrated the eye tracker, validated the calibration and started the login interface.
The interface automatically guided each participant through a sequence of screens, each involving their entering and verifying one image or PIN-based graphical password.
Each sequence consisted of 10 different PINs with four digits and a subset of 20 out of the 40 images - 10 images with a saliency mask and 10 without - shown to the participants in randomised order.
The PINs were randomly generated to avoid people choosing easy or PINs that they were actually using.
For each image we asked participants to choose one graphical passwords consisting of four password points, but did not give them any indication as to how to choose them.
We made sure that none of the participants was shown both the saliency and the non-saliency mask version of the same image and that, over all pre-study participants, each image was shown equally often.
For images with a saliency mask, we asked participants to choose passwords points outside the red areas but did not explain how these areas had been defined.
Figure 5: Sample images extracted from the video tutorial with the person looking at the top-left , top-centre , topright , middle-left , middle-centre , middle-right , bottom-left , bottom-centre , and bottom-right  points of the calibration procedure.
Participants were rewarded with 10 EUR for participating in the study.
In addition, we offered them a 1 EUR bonus for each password they managed to guess successfully.
After the participants arrived at the lab, we introduced the study and asked them to sign a consent form.
We asked them to complete the first part of the questionnaire on demographics and on their use of PINs and passwords.
We guided them to the room where we had set up our authentication system.
All participants were first shown a video tutorial on how to guess gaze-based graphical passwords.
In the first part of the tutorial, participants watched close-up videos of another person  looking at the nine points of the calibration procedure .
This was to give them a feeling of the maximum gaze range they could expect later on.
In the second part of the tutorial, we showed them close-up videos of the person's gaze, while entering a PIN-based and an image-based password, and asked them to guess the password.
In addition, we gave them some hints that we had discovered ourselves when trying to guess passwords using the same approach, e.g.
All videos in the tutorial were mirrored horizontally, so that the gaze direction matched the on-screen gaze location.
After finishing the video tutorial, we ensured that participants were seated about 60 cm away from the computer screen and faced its centre.
We calibrated the eye tracker, validated the calibration and started the login interface.
The interface guided the participants through a sequence of login procedures, each involving their guessing one image or PIN-based password.
For each password, the participants could watch the corresponding close-up video from the pre-study as often as they wished on a laptop placed next to the screen .
Participants could pause the video at any time so as to closely examine gaze direction.
In addition, a printout of the image was provided for marking potential password points.
The goal of the main study was to evaluate the security of the three types of gaze-based graphical passwords recorded in the pre-study.
We recruited 12 participants , aged between 23 and 29 years  via University mailing lists and bulletins in the neighbourhood surrounding the University building.
Three of the participants wore glasses, one used contact lenses, and five had already participated in an eye tracking study before.
A first test showed that attacking all 120 passwords recorded in the pre-study took too long and would have affected participants' motivation and ability to keep concentrated.
Consequently, for the study to take at most one hour, we had to reduce the number of passwords.
We deemed crucial to still use the same number of passwords from each pre-study participant in order to minimise potential effects from different password selection strategies.
We therefore selected five passwords per pre-study participant.
Counterbalancing these passwords for image category and study condition finally resulted in a set of seven PINs, as well as seven images with and six images without a saliency mask and the corresponding videos.
Each participant was asked to attack the passwords in this set in randomised order.
We made sure that, overall, all passwords would be attacked the same amount of times.
Figure 6: Experimental setup used in the main study.
Participants tried to guess passwords by analysing close-up videos of other users' eye movements on the laptop.
An additional printout of the image was provided to take notes and to mark potential password points.
Once participants thought they knew the password, they tried to log into the system using this password.
For each image, participants were given a maximum of three login attempts before the interface automatically switched to the next image.
If the password was guessed correctly, the system immediately switched to the next image.
Participants were asked to guess as many passwords as possible while we recorded their gaze paths.
We explicitly told them that guessing the passwords was more important than finishing the study quickly.
However, when participants spent a lot of time watching the video, we applied a soft deadline of 2 minutes, after which the experimenter asked the participant to finish.
Consequently, it was not possible to analyse post-hoc how quickly passwords could be guessed by the participants.
After the study, we asked participants to complete the second part of the questionnaire, with questions on the experiment and how they perceived the PIN-based and image-based graphical passwords with respect to robustness, security, and usability.
In addition, we asked them how difficult it was to guess passwords, whether the tutorial had been useful, and whether they had noticed any differences in visual behaviour between the various people they had observed.
We first analysed the eye tracking accuracy  in the pre- and main study using the gaze data recorded during the validation routine after calibration.
In the pre-study, the mean Euclidean distances ranged between 21 and 46 pixels ; in the main study the distances were between 22 and 52 pixels .
At a distance of 60 cm, these distances correspond to an accuracy of between 0.5 and 1.05 degree of visual angle - a range similar to the manufacturer-reported accuracy of the Tobii eye tracker.
In the main study, we collected quantitative and qualitative data.
Quantitative data was recorded from participant interactions with the login interface and the questionnaires, using 5-point Likert scales.
To verify that all login attempts were counted correctly in the post-hoc analysis, for each login attempt, we plotted the image with the password from the pre-study and the password guessed by the participants in the main study .
In total, we individually validated 541 images.
Questionnaire responses were transcribed to a spreadsheet.
Qualitative data was gathered from observations during the study, open questions in the questionnaire and from interviews following the study.
This data was transcribed and important themes identified.
Using the gaze data collected in the pre-study, we analysed whether the saliency masks actually influenced participant visual behaviour and thus the way they selected their password points.
To this end, we compared the number of password points selected inside and outside the saliency regions for all images with and without a saliency mask.
Figure 8 shows the results of this comparison for all pre-study images and participants.
On average, for images without a saliency mask, 34.5% of the password points fell inside the saliency regions.
For images with a saliency mask, this percentage dropped to 1.3%, a difference that was statistically significant .
We first calculated the theoretical password space, that is the total number of all possible distinct passwords in a system, for the different graphical password types.
The TPS for 4-digit PIN-based graphical passwords with a password length of four digits is log2   13.3.
Table 1: Logistic regression analysis of 12 participants' performance in guessing three different types of graphical passwords.
Both the "PIN" and image with a saliency mask  conditions are significantly different to the "image without a saliency mask" reference condition.
Thus, with the same number of password points per password, the TPS for image-based passwords is much larger, namely log2   25.3.
We then evaluated the security of the different graphical passwords using the data recorded in the main study.
Each successful login attempt was counted as a "1", whereas three failed login attempts on a single image were counted as a single "0".
Overall, 19 out of 81 PINs were successfully guessed by the participants compared to 8 out of 72 images without a saliency mask and 1 out of 82 images with a saliency mask.
We used a logistic regression to model the relationship between the password type and the relative probabilities of a successful login attempt.
The regression was conducted using a generalized linear mixed effect model with "image without a saliency mask" as the reference condition.
Participants was a random effect; the "PIN" vs. "image without a saliency mask", as well as "image with a saliency mask" vs. "image without a saliency mask" were fixed effects.
In the previous evaluation, the fixation detection threshold was fixed to thf = 1.7 degree of visual angle.
The lower bound of the sweep was motivated by the maximum eye tracking accuracy achieved after calibration.
For login attempts in which one of these thresholds resulted in an earlier successful attempt than in the original study, all subsequent attempts on that image were excluded from the analysis.
As can be seen from Figure 9, the fixation detection threshold exerts a considerable influence on the number of successful login attempts.
Most importantly, the analysis shows that using saliency masks consistently, results in fewer successful login attempts and thus more secure passwords than without saliency masks or using PINs.
From the questionnaires completed by the participants of the main study, we received valuable feedback on the perceived security and usability of the PIN-based and image-based graphical passwords, experiences with guessing them, preferred situations and devices, as well as general feedback on the study.
Figure 11: Distribution of responses to question "How do you rate the usability of the different graphical passwords?".
Participants found image-based passwords to be significantly more secure than PIN-based passwords, 2  = 10.0, p < .05.
Yet, the password usability was rated significantly higher for PIN-based than for imagebased passwords, 2  = 6.0, p < .05.
Figure 13: Distribution of responses to question "How do you rate the ease of guessing passwords from the videos?".
Although we did not elaborate on this, our results indicate that difficulties in guessing passwords may be related to the gender of pre-study participants.
We then analysed the perceived ease of monitoring eye movements and guessing passwords from the videos for PIN-based and image-based passwords .
We again used a Friedman analysis of variance by ranks on participant responses in both conditions.
Participants found it significantly easier to monitor eye movements for PIN-based passwords than for image-based passwords, 2  = 10.0, p < .05.
Consequently, participants found it significantly easier to guess PIN-based passwords  = 8.0, p < .05 and felt that they had to concentrate significantly more  when trying to guess image-based password, 2  = 4.5, p < .05.
We further analysed in which situations and with respect to which devices participants could imagine using the different graphical passwords.
Although none of the following findings was statistically significant, they nonetheless raise some interesting questions for future work.
For situations , we found that the image-based passwords were favoured in non-private situations, such as in a bank  or in public spaces .
These responses may have been caused by the fact that participants could not imagine mobile devices being equipped with robust eye trackers in the near future.
While currently, application domains are indeed mostly limited to ATMs or similar stationary systems, the advent of mobile eye trackers will pave the way for gaze-based authentication on smaller and thus more mobile devices .
A proper analysis of password memorability requires a longterm study and was therefore beyond the scope of this work.
However, when we asked two pre-study participants to log in with their image-based passwords two days later  they correctly remembered 14 out of 40 passwords .
13 of these image-based passwords were remembered by the pre-study participant who had selected the password points in the direction of reading, i.e.
While using such strategies seems to improve password memorability this may come at the cost of reduced security.
We plan to investigate this trade-off between memorability and security in more detail in future work and particularly how password memorability can be improved without compromising security.
In terms of security, it will also be interesting to see how saliency masks compare to other approaches, such as selecting password points on a sequence of images .
Finally, the study also reveals some of the issues researchers may face in the real-world implementation of gaze-based graphical passwords.
Participants in our pre-study reported having used visual strategies for selecting their passwords points in the images.
Two main-study participants noticed and exploited this behaviour by specifically looking for characteristic eye movement sequences such as in a vertical or horizontal direction.
This suggests that, in addition to the saliency masks presented here, measures need to be taken to prevent users from choosing closely related password points .
Additional user studies will be required to investigate whether users should be allowed to choose their own graphical passwords , or whether both should be provided by the authentication system during registration.
In the latter case, it would be useful to identify what defines such "good" passwords and images.
The qualitative feedback from the pre-study revealed that some participants developed strategies for selecting passwords.
One participant constructed stories around the password points to make them more memorable.
A second strategy reported by another participant was to select password points along vertical or horizontal lines, such as by choosing points at the intersection of foreground objects.
Participants from the main study found PINs easier to guess, due to the grid and thus the eye movements were more easily recognisable.
P2, P7, P8 found vertical eye movements more difficult to detect than horizontal ones.
P2 reported looking for visual strategies, while P8 and P9 noticed that one pre-study participant had chosen password points in the direction of reading, i.e.
The results of our study demonstrate that image-based graphical passwords are significantly more secure than PIN-based passwords, both in an actual attack and in terms of participant perception, hence verifying Hypothesis 1 .
Using computational models of visual attention to mask the most salient areas of the images does significantly increase security, compared to the standard imagebased approach, hence verifying Hypothesis 2.
In combination with the much larger theoretical password space, these results make saliency masks a promising means of increasing the security of gaze-based graphical passwords.
While image-based graphical passwords were perceived as significantly more secure than PIN-based passwords, the usability was rated lower by the participants in our study .
In this paper we have proposed computational models of visual attention to increase the security of gaze-based cued-recall graphical passwords.
We introduced saliency masks as a promising method for supporting the user in selecting more secure passwords and thus reducing the risk of hotspots in the authentication images.
In a study with a realistic threat model, we showed that saliency masks significantly increase password security on a single image, compared to a standard imagebased method and gaze-based 4-digit PIN entry.
This result is promising, as saliency masks can easily be computed.
It also raises the issue of the wider applicability of this approach, such as in the development of quantitative measures of the security of cued-recall graphical passwords.
This work was supported by the European Union 7th framework programme under grant agreements no.
We would like to thank Yordan Terziev, Andreas Kaiser, and Ken Pfeuffer for their help with implementing the authentication software, Brian Bloch for editing a draft of this paper, as well as Simon Byrne and Julian Mennen oh.
Harel, J. Graph-Based Visual Saliency Toolbox for MATLAB, http://www.klab.caltech.edu/ harel/share/ gbvs.php, 2006.
Harel, J., Koch, C., and Perona, P. Graph-based visual saliency.
In Proceedings of the 20th International Conference on Neural Information Processing Systems , 545-552.
Hoanca, B., and Mock, K. Secure graphical password system for high traffic public areas.
Itti, L., and Koch, C. Computational modelling of visual attention.
Itti, L., Koch, C., and Niebur, E. A model of saliency-based visual attention for rapid scene analysis.
Jacob, R. J. K. What you look at is what you get: eye movement-based interaction techniques.
In Proceedings of the 8th SIGCHI International Conference on Human Factors in Computing Systems , 11-18.
Kumar, M., Garfinkel, T., Boneh, D., and Winograd, T. Reducing shoulder-surfing by using gaze-based password entry.
LeBlanc, D., Forget, A., and Biddle, R. Guessing click-based graphical passwords by eye tracking.
In Proceedings of the 8th International Conference on Privacy Security and Trust , 197 -204.
Maeder, A. J., Fookes, C. B., and Sridharan, S. Gaze based user authentication for personal computer applications.
In Proceedings of International Symposium on Intelligent Multimedia, Video and Speech Processing , 727-730.
Moncur, W., and Lepl atre, G. Pictures at the atm: exploring the usability of multiple graphical passwords.
In Proceedings of the 25th SIGCHI International Conference on Human Factors in Computing Systems , 887-894.
Thorpe, J., and van Oorschot, P. C. Human-seeded attacks and exploiting hot-spots in graphical passwords.
Wiedenbeck, S., Waters, J., Birget, J.-C., Brodskiy, A., and Memon, N. Passpoints: design and longitudinal evaluation of a graphical password system.
Angeli, A. D., Coventry, L., Johnson, G., and Renaud, K. Is a picture really worth a thousand words?
Biddle, R., Chiasson, S., and van Oorschot, P. C. Graphical passwords: Learning from the first twelve years.
Bulling, A., and Gellersen, H. Toward Mobile Eye-Based Human-Computer Interaction.
Chiasson, S., Forget, A., Biddle, R., and van Oorschot, P. C. Influencing users towards better passwords: persuasive cued click-points.
Chiasson, S., van Oorschot, P. C., and Biddle, R. Graphical password authentication using cued click points.
De Luca, A., Denzel, M., and Hussmann, H. Look into my eyes!
In Proceedings of the 5th Symposium on Usable Privacy and Security , 1-12.
De Luca, A., Weiss, R., and Drewes, H. Evaluation of eye-gaze interaction methods for security enhanced pin-entry.
Everitt, K. M., Bragin, T., Fogarty, J., and Kohno, T. A comprehensive study of frequency, interference, and training of multiple graphical passwords.
In Proceedings of the 27th SIGCHI International Conference on Human Factors in Computing Systems , 889-898.
Forget, A., Chiasson, S., and Biddle, R. Shoulder-surfing resistance with eye-gaze entry in cued-recall graphical passwords.
In Proceedings of the 28th SIGCHI International Conference on Human Factors in Computing Systems , 1107-1110.
Forget, A., Chiasson, S., van Oorschot, P. C., and Biddle, R. Improving text passwords through persuasion.
In Proceedings of the 4th Symposium on Usable Privacy and Security , 1-12.
