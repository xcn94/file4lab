More and more products and services are being deployed on the web, and this presents new challenges and opportunities for measurement of user experience on a large scale.
There is a strong need for user-centered metrics for web applications, which can be used to measure progress towards key goals, and drive product decisions.
In this note, we describe the HEART framework for user-centered metrics, as well as a process for mapping product goals to metrics.
We include practical examples of how HEART metrics have helped product teams make decisions that are both data-driven and user-centered.
The framework and process have generalized to enough of our company's own products that we are confident that teams in other organizations will be able to reuse or adapt them.
We also hope to encourage more research into metrics based on large-scale behavioral data.
With additional instrumentation, it is also possible to run controlled experiments  that compare interface alternatives.
But on what criteria should they be compared, from a user-centered perspective?
How should we scale up the familiar metrics of user experience, and what new opportunities exist?
In the CHI community, there is already an established practice of measuring attitudinal data  on both a small scale  and a large scale .
However, in terms of behavioral data, the established measurements are mostly small-scale, and gathered with stopwatches and checklists as part of lab experiments, e.g.
A key missing piece in CHI research is user experience metrics based on large-scale behavioral data.
The web analytics community has been working to shift the focus from simple page hit counts to key performance indicators.
However, the typical motivations in that community are still largely business-centered rather than user-centered.
Web analytics packages provide off-the-shelf metrics solutions that may be too generic to address user experience questions, or too specific to the e-commerce context to be useful for the wide range of applications and interactions that are possible on the web.
We have created a framework and process for defining large-scale user-centered metrics, both attitudinal and behavioral.
We generalized these from our experiences of working at a large company whose products cover a wide range of categories , are almost all web-based, and have millions of users each.
We have found that the framework and process have been applicable to, and useful for, enough of our company's own products that we are confident that teams in other organizations will be able to reuse or adapt them successfully.
We also hope to encourage more research into metrics based on large-scale behavioral data, in particular.
Advances in web technology have enabled more applications and services to become web-based and increasingly interactive.
Web usage mining techniques can be used to segment visitors to a site according to their behavior .
Despite this progress, it can still be challenging to use these tools effectively.
Standard web analytics metrics may be too generic to apply to a particular product goal or research question.
The sheer amount of data available can be overwhelming, and it is necessary to scope out exactly what to look for, and what actions will be taken as a result.
Several experts suggest a best practice of focusing on a small number of key business or user goals, and using metrics to help track progress towards them .
We share this philosophy, but have found that this is often easier said than done.
Product teams have not always agreed on or clearly articulated their goals, which makes defining related metrics difficult.
It is clear that metrics should not stand alone.
They should be triangulated with findings from other sources, such as usability studies and field studies , which leads to better decision-making .
Also, they are primarily useful for evaluation of launched products, and are not a substitute for early or formative user research.
We sought to create a framework that would combine large-scale attitudinal and behavioral data, and complement, not replace, existing user experience research methods in use at our company.
A change that brings in more revenue in the short term may result in a poorer user experience that drives away users in the longer term.
A count of unique users over a given time period, such as seven-day active users, is commonly used as a metric of user experience.
It measures the overall volume of the user base, but gives no insight into the users' level of commitment to the product, such as how frequently each of them visited during the seven days.
It also does not differentiate between new users and returning users.
In a worst-case retention scenario of 100% turnover in the user base from week to week, the count of seven-day active users could still increase, in theory.
Based on the shortcomings we saw in PULSE, both for measuring user experience quality, and providing actionable data, we created a complementary metrics framework, HEART: Happiness, Engagement, Adoption, Retention, and Task success.
These are categories, from which teams can then define the specific metrics that they will use to track progress towards goals.
The Happiness and Task Success categories are generalized from existing user experience metrics: Happiness incorporates satisfaction, and Task Success incorporates both effectiveness and efficiency.
Engagement, Adoption, and Retention are new categories, made possible by large-scale behavioral data.
The framework originated from our experiences of working with teams to create and track user-centered metrics for their products.
We started to see patterns in the types of metrics we were using or suggesting, and realized that generalizing these into a framework would make the principles more memorable, and usable by other teams.
It is not always appropriate to employ metrics from every category, but referring to the framework helps to make an explicit decision about including or excluding a particular category.
For example, Engagement may not be meaningful in an enterprise context, if users are expected to use the product as part of their work.
In this case a team may choose to focus more on Happiness or Task Success.
But it may still be meaningful to consider Engagement at a feature level, rather than the overall product level.
The most commonly used large-scale metrics are focused on business or technical aspects of a product, and they  are widely used by many organizations to track overall product health.
These metrics are all extremely important, and are related to user experience - for example, a product that has a lot of outages  or is very slow  is unlikely to attract users.
An e-commerce site whose purchasing flow has too many steps is likely to earn less money.
A product with an excellent user experience is more likely to see increases in page views and unique users.
However, these are all either very low-level or indirect metrics of user experience, making them problematic when used to evaluate the impact of user interface changes.
We use the term "Happiness" to describe metrics that are attitudinal in nature.
These relate to subjective aspects of user experience, like satisfaction, visual appeal, likelihood to recommend, and perceived ease of use.
With a general, well-designed survey, it is possible to track the same metrics over time to see progress as changes are made.
For example, our site has a personalized homepage, iGoogle.
After launching a major redesign, they saw an initial decline in their user satisfaction metric .
However, this metric recovered over time, indicating that change aversion was probably the cause, and that once users got used to the new design, they liked it.
With this information, the team was able to make a more confident decision to keep the new design.
Engagement is the user's level of involvement with a product; in the metrics context, the term is normally used to refer to behavioral proxies such as the frequency, intensity, or depth of interaction over some time period.
Examples might include the number of visits per user per week, or the number of photos uploaded per user per day.
It is generally more useful to report Engagement metrics as an average per user, rather than as a total count - because an increase in the total could be a result of more users, not more usage.
For example, the Gmail team wanted to understand more about the level of engagement of their users than was possible with the PULSE metric of seven-day active users .
With the reasoning that engaged users should check their email account regularly, as part of their daily routine, our chosen metric was the percentage of active users who visited the product on five or more days during the last week.
We also found that this was strongly predictive of longer-term retention, and therefore could be used as a bellwether for that metric.
For example, during the stock market meltdown in September 2008, Google Finance had a surge in both page views and seven-day active users.
However, these metrics did not indicate whether the surge was driven by new users interested in the crisis, or existing users panic-checking their investments.
Without knowing who was making more visits, it was difficult to know if or how to change the site.
We looked at Adoption and Retention metrics to separate these user types, and examine the rate at which new users were choosing to continue using the site.
The team was able to use this information to better understand the opportunities presented by event-driven traffic spikes.
One way to measure these on a large scale is via a remote usability or benchmarking study, where users can be assigned specific tasks.
With web server log file data, it can be difficult to know which task the user was trying to accomplish, depending on the nature of the site.
The team believed that the single-box approach was simplest and most efficient, so, in an A/B test, they tried a version that offered only the single box.
They compared error rates in the two versions, finding that users in the single-box condition were able to successfully adapt their search strategies.
This assured the team that they could remove the dual box for all users.
Adoption metrics track how many new users start using a product during a given time period , and Retention metrics track how many of the users from a given time period are still present in some later time period .
What counts as "using" a product can vary depending on its nature and goals.
In some cases just visiting its site might count.
In others, you might want to count a visitor as having adopted a product only if they have successfully completed a key task, like creating an account.
Like Engagement, Retention can be measured over different time periods - for some products you might want to look at week-to-week Retention, while for others monthly or 90-day might be more appropriate.
Adoption and Retention tend to be especially useful for new products and features, or those undergoing redesigns; for more established products they tend to stabilize over time, except for seasonal changes or external events.
No matter how user-centered a metric is, it is unlikely to be useful in practice unless it explicitly relates to a goal, and can be used to track progress towards that goal.
We developed a simple process that steps teams through articulating the goals of a product or feature, then identifying signals that indicate success, and finally building specific metrics to track on a dashboard.
The first step is identifying the goals of the product or feature, especially in terms of user experience.
What tasks do users need to accomplish?
What is the redesign trying to achieve?
Some tips that we have found helpful: * Different team members may disagree about what the project goals are.
Do not get too distracted at this stage by worrying about whether or how it will be possible to find relevant signals or metrics.
Next, think about how success or failure in the goals might manifest itself in user behavior or attitudes.
What actions would indicate the goal had been met?
What feelings or perceptions would correlate with success or failure?
At this stage you should consider what your data sources for these signals will be, e.g.
How will you gather attitudinal signals - could you deploy a survey on a regular basis?
Some tips that we have found helpful: * Choose signals that are sensitive and specific to the goal - they should move only when the user experience is better or worse, not for other, unrelated reasons.
Sometimes failure is easier to identify than success .
They have generalized to enough of our company's own products that we are confident that teams in other organizations will be able to reuse or adapt them successfully.
We have fine-tuned both the framework and process over more than a year of use, but the core of each has remained stable, and the framework's categories are comprehensive enough to fit new metrics ideas into.
Because large-scale behavioral metrics are relatively new, we hope to see more CHI research on this topic - for example, to establish which metrics in each category give the most accurate reflection of user experience quality.
Finally, think about how these signals can be translated into specific metrics, suitable for tracking over time on a dashboard.
Some tips that we have found helpful: * Raw counts will go up as your user base grows, and need to be normalized; ratios, percentages, or averages per user are often more useful.
If it is important to be able to compare your project or product to others, you may need to track additional metrics from the standard set used by those products.
We have spent several years working on the problem of developing large-scale user-centered product metrics.
This has led to our development of the HEART framework and the Goals-Signals-Metrics process, which we have applied to more than 20 different products and projects from a wide variety of areas within Google.
We have described several examples in this note of how the resulting metrics have helped product teams make decisions that are both datadriven and user-centered.
