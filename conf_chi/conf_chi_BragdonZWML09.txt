Andrew Bragdon, Robert Zeleznik, Brian Williamson, Timothy Miller, Joseph J. LaViola Jr.  Brown University University of Central Florida Department of Computer Science School of EECS 115 Waterman St. 4th Floor 4000 Central Florida Blvd.
GestureBar is a novel, approachable UI for learning gestural interactions that enables a walk-up-and-use experience which is in the same class as standard menu and toolbar interfaces.
GestureBar leverages the familiar, clean look of a common toolbar, but in place of executing commands, richly discloses how to execute commands with gestures, through animated images, detail tips and an out-ofdocument practice area.
GestureBar's simple design is also general enough for use with any recognition technique and for integration with standard, non-gestural UI components.
We evaluate GestureBar in a formal experiment showing that users can perform complex, ecologically valid tasks in a purely gestural system without training, introduction, or prior gesture experience when using GestureBar, discovering and learning a high percentage of the gestures needed to perform the tasks optimally, and significantly outperforming a state of the art crib sheet.
The relative contribution of the major design elements of GestureBar is also explored.
A second experiment shows that GestureBar is preferred to a basic crib sheet and two enhanced crib sheet variations.
Gestures can also be committed to physical muscle memory which can help users focus on their task instead of the UI.
The HCI community has a long history of developing gestural UIs which demonstrate this value, going back to .
Why then, do most applications forgo the potential of gestures, relying instead on conventional WIMP paradigms, such as menus and toolbars?
We believe the basis for an answer lies in the refrain we commonly encounter when pitching gestural applications to software industry leaders: "this is great, but how will new users learn these gestures?"
High quality pen-based hardware devices have become increasingly available at successively lower cost.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We interpreted this concern broadly to mean that the research challenge for making gestural UIs mainstream is approachability, the summative experience of a first-time novice who attempts to accomplish a non-trivial, ecologically valid task, without human assistance, training or introduction.
Thus, instead of concentrating on performance and retention metrics which apply more to users who have adopted a gestural UI, our primary focus is on developing and evaluating techniques that facilitate the acceptance of a gestural UI by a novice - someone assumed to be unfami-
Our approach, GestureBar , embeds gesture disclosure information in a familiar toolbar-based UI.
Users encounter relevant gesture details only as needed, after they have formed a mental goal, searched for, and found an appropriate command - consistent with Polson, et al.
When a tool is clicked, GestureBar displays feedback that richly discloses information about the gesture, and provides an area in which to experiment without impact on the user's document.
Note that clicking an item does NOT execute the command, but rather discloses the gesture, and how to perform it.
In this paper, we explore the hypothesis that gestural interfaces can be approachable - supporting a walk-up-and-use experience.
First we discuss the evolution of GestureBar from a set of design principles and prototypes.
We then present two user studies, conducted in the context of a full gestural application, to test our hypothesis and yield insight into the relative merits of our major design elements.
Our work draws from this result and applies this basic design notion to the disclosure of gestures, as clicking GestureBar items does not execute commands, instead disclosing the appropriate gesture.
InkSeine  presented a variation of the crib sheet theme in which gestures were shown in situ as highlighter annotations over application widgets; the annotations could be toggled on and off with a button press.
This technique was well suited toward disclosing simple gestures associated with explicit UI widgets.
However, with only a few gestures, the technique cluttered the workspace but did not provide support for accessing more detailed information about subtle or complex gestures or for displaying gestures that required a document context .
In addition to searchability problems, the iconic representations used in crib sheets are not always effective at expressing the essential characteristics of all gestures, including the context where they apply and possible gesture variations.
Kurtenbach  explored extending crib sheets with animation to make gestures more learnable.
In this system, pressing and holding within the document invoked a contextual crib sheet and pressing a crib sheet item presented a series of animations illustrating examples of the gesture within the active document.
Users could then trace the gestures to develop the physical skill required to perform the gesture correctly and execute the corresponding function.
However, this system did not support demonstrations of geometrically parameterized gestures, did not have a mechanism for highlighting geometric gesture nuances, and did not support browsing through functions that required an existing context unless the user had already created that context in their document.
In addition, users needed a priori knowledge to know about the pressandhold "gesture" to bring up the crib sheet and special, unmarked contexts like the margins of a page.
Hinckley explored a related variant in which text prompts and traceable extensions were displayed when partially-completed pigtail gestures had been entered .
As an alternative to crib sheet organizations, marking menus  and zone and polygon menus , are organized by the hierarchical, radial nature of the gestures they support.
In either case, users learn to perform a gesture by following its trail while receiving continuous feedback about their performance - gesture trails are pruned or reinforced based on how closely the user follows that trail.
With all these techniques, gesture labels are spatially arranged based on the geometry of their corresponding gesture which often results in related labels being spatially separated.
Gestural UI research spans a broad set of topics, including: creating gesture sets  , disclosing gestural functions and teaching individual gestures  , recognizing individual gestures  , and correcting recognition results  .
Although each of these areas influences the usability, our GestureBar work focuses specifically on novice approachability.
We note that we are unaware of any prior evaluations that study the approachability of gestural UIs in which pure novices are given a full-scale gestural application but no a priori training or advice on its use.
Prior gestural UIs have commonly treated gestures as an organizing principle in themselves which can compromise searchability.
For example, crib sheets often display gesture sets as gesture-icon/command-name pairs in a two-column table , or a 2D grid .
Within this layout, gestures may be clustered alphabetically or according to the similarity of their functions .
Such crib sheets, although effective as reference guides, are generally complemented by additional novice training material such as videos and interactive tutorials.
The Graffiti gesture set  was a notable exception, as it was rapidly adopted by a broad base of users who had access only to a crib sheet.
We believe Graffiti's crib sheet was successful because it leveraged intimate a priori familiarity; its crib sheet may likely have been perceived as an organized collective whole - the alphabet - instead of a cluttered set of gestural commands.
Fluid Inking  treated gestures analogously to command-key shortcuts by embedding their mnemonic description in a menu system.
However, because both the mnemonic depiction of the gestures was crude and also non-gestural alternatives were available, this technique was not successful with novices.
Trail-based approaches also do not adapt well to many common gesture types  and they cannot be used with all gesture recognizers, for instance those that do not provide incremental feedback.
Color-blind users also may have difficulty interpreting OctoPocus' color codings.
A different approach to disclosing and teaching gestures is to make a clean separation between disclosure and invocation, such as with step-by-step training videos or interactive tutorials.
Microsoft provides an interactive tutorial for users to experiment with Flicks and receive textual disclosure about what they did right and wrong.
This tutorial presented the gestures in the gesture set simultaneously as faded out annotations over a partly finished musical example; users could then trace any trail to finish that part of the score.
However, these approaches come at a higher cost - they require a significant, upfront time and attention commitment from the user as well as additional production time by the system developer.
Sketch-based UIs  attempt to recognize hand-drawn diagrams based on their visual appearance the way a human would.
In this sense, users have less of a need to explicitly learn a UI.
However, our work is still applicable to these systems since most novice users will not know all of what can be sketched  or how to perform an abstract gestural command such as Zoom or Copy.
Gestures were displayed as static images, each labeled with the appropriate command name, in a toolbar-like layout across the top of the screen.
However, on hover, the gesture icons were animated to demonstrate the dynamic nature of the gesture.
Each animation was designed as a canonical example, showing appropriate context; a shape around which a selection lasso was being drawn, for instance.
We tested the mockup on three users with no Tablet PC experience; they performed a series of command execution tasks  in a think-aloud protocol.
We found that the toolbar did not in fact look familiar to users who were accustomed to seeing icons that depict functionality - they fruitlessly looked for "standard" toolbar icons  while being continuously interrupted by small, inscrutable gesture animations.
In essence, the gesture animations were confusing since they confounded the natural function-browsing workflow.
We expanded Kurtenbach, et al.
Searchability: Since a user's first priority when using a new application is to find relevant commands, the UI must facilitate command browsing and identification.
Expressivity: Unlike traditional GUIs, gestural UIs must be capable of fully disclosing compound physical interactions that are recognized by complex algorithms.
Low Cost: UIs must be practical both for the system designer and for the end user and be compatible with traditional UIs; they should not limit recognition technology, require programming effort beyond what is needed for standard UIs, or fundamentally alter user workflow.
For our first real GestureBar prototype , we changed the toolbar to display function icons which expand on hover to show a larger animated demonstration of the associated gesture.
We performed an initial pilot test on three users who had never used Tablet PCs, with the same tasks as before, and found that although they were ultimately able to accomplish the tasks given to them, they experienced significant initial confusion.
They did not expect either the on-hover behavior, or the animation; and they were puzzled why commands were not executed when they tapped on the toolbar buttons.
They also missed most or all of the animations since the animations played quickly and without an initial delay.
In addition, they complained that the expanded hover animation area covered nearby buttons making it hard to see the commands they needed.
We adjusted the toolbar to be more familiar by getting rid of hover animations and displaying instead a Gesture Explorer dropdown when a tool item was clicked .
We also added a short "attention getting" introductory animation  prior to playing the gesture animation and we supported secondary animations, intended to disclose gesture variations that could be tap-activated.
Another set of four pilot users in a third, identical pilot test indicated the toolbar was no longer confusing, but new problems had arisen.
Our expectation that animations would clarify gestures was not fully supported.
Users found the secondary animations to be useful, but visually overwhelming.
In addition, we observed that animations did clarify directional requirements of gestures, but seemed to obscure geometric content.
For example, our lasso gesture requires users to draw a loop enclosing the objects of interest that ends in a small tail, but users did not notice the tail and simply circled the objects of interest.
As another example, a text gesture required an underline terminating in a sharp upward hook to be drawn below handwritten text, but many users overlooked the hook.
We concluded that no single silver-bullet strategy was expressive enough to present all nuances of even simple gestures, and that multiple strategies needed to be combined with careful attention to visual complexity.
In addition, we observed that as users underwent the trial-and-error muscle training process of performing gestures, they often ended up doing significant damage to their document as failed gesture recognition led to unexpected results such as stray lines and text, and unintended command invocation.
Usage scenario: hovering over AutoShape reveals a tooltip , clicking AutoShape opens the Gesture Explorer; Clicking the Thin Arrow tab displays an animation with detail tips ; a successful gesture attempt in the practice area ; user adds an arrow to their document .
To support command set scalability and infrequently used commands, we added tabs across the top of the GestureBar, similar to the Ribbon in Office 2007 , and we added toolbar items which directly execute functions instead of displaying gestures.
Switching tabs allows users to easily browse for commands in a single place , and related sets of commands can now be grouped together.
We added labeled groups within each tab , for example, the drawing commands might be in one group, "Drawing" and the text commands might be in another group, "Text".
In addition, we added tooltips which display on hover, showing the command name and a brief description of its use.
Finally, to reinforce awareness of which gesture was recognized, we gradually fade the corresponding toolbar icon in and out of the document at the end of each completed gesture, much like Windows Vista Flicks  and Bau, et al.
We made several improvements to the expressivity of the Gesture Explorer .
Tabs are provided for switching to related gesture variations so that each tab page can show a single animation at a larger size to accommodate the addition of pen sprites which "write" each stroke, highlighting movement to the starting location of the gesture and transitions between segments of multi-stroke gestures.
To facilitate muscle training without affecting the document, we added a Practice Area that shows a semitransparent, static traceable overlay of the gesture, and application content, where appropriate - for example, lasso select provides a square for the user to practice selecting.
The Practice Area is an instance of the same WPF control as the application document which makes it easy to simulate the in-document recognition experience without fear of "messing up" the document.
If an executed gesture matches a set of success gestures, "Nice Job!"
If a reported gesture is in a set of intermediate actions, no result is shown, allowing users to complete multi-stroke gestures.
Finally, if a gesture is in neither set, "Not Quite Right" is displayed.
Tapping this notification resets the practice area for another attempt.
One can imagine proffering rich feedforward  and explanatory feedback about "why" a gesture failed, but this may have increased the implementation and design costs and imposed gesture class or recognition technology constraints.
Distinct from the GestureBar design, is the development of application-specific content.
Similar to WIMP menu/toolbar content, creating content for GestureBar does not involve writing code.
A developer specifies content values such as icons, text, and gesture strokes as WPF properties in a GUI editor; gesture demonstration animations are procedurally generated by GestureBar using WPF.
In addition to actually specifying content, designers must also consider whether each gesture variant warrants special attention requiring its own command button , and whether important gesture sequences should be treated as a single command with its own button .
This distinction is particularly relevant for gestural systems which often rely on interaction strategies that may not be obvious given only isolated gesture descriptions.
To evaluate the approachability of GestureBar, we opted to study the performance of an unassisted novice user in a complex "real" application with a range of gesture types.
We felt that easier-to-control synthetic tests that artificially constrained user workflow would be inadequate to assess approachability.
We also implemented variant UIs to span the design space between GestureBar and a crib sheet so as to identify the relative value of our main design choices.
We chose to evaluate GestureBar  relative to the status quo of a basic crib sheet  similar to that used by the Mouse Gestures Add-on  to Firefox.
This baseline is important because it presumably represents the problems of approachability that are perceived with gestural interfaces in general.
By demonstrating a significant advantage of GBAR over crib sheets, we hope to persuade those who believe gestural UIs are inherently unapproachable that the problem may rather be a function of a particular UI style.
Our evaluations were conducted in terms of ecologically valid diagram replication tasks using Lineogrammer , a research system that we have developed for creating simple diagrams, that we feel is representative of a wider class of gestural applications.
As expected in a gestural application, unscripted and unanticipated actions can occur when users fail to perform gestures correctly or when gestures are misrecognized.
In Lineogrammer, such errors typically generate stray lines or other geometry within the user's document.
Thus we were able to observe the complete openended process by which users approach an unknown interface, including forming goals, searching for commands, performing gestures, and assessing results.
During pilot testing we had to adapt our baseline from the basic crib sheet style used by Mouse Gestures to a crib sheet that displayed a tooltip  explaining the concept of a gesture and a crib sheet whenever the crib sheet was clicked on.
This change was prompted by four pilot users of the basic crib sheet who were essentially unable to find or perform any gestures other than the implicit line, polyline and rectangle gestures; instead they treated the crib sheet as a toolbar and repeatedly clicked on it to change modes only to find that nothing happened.
We also eliminated the confound of scrolling within the basic crib sheet since it fit the height of the display of the Tablet PC.
To gain perspective on the value of the major components of GBAR, we also tested two intermediate designs that blended GBAR features into the basic crib sheet.
These demonstrations were shown in a small window with replay and close buttons, in response to clicking on a crib sheet entry.
The second of these intermediate designs, EXPLOR, is the same as ANIM but extended to show the full gesture explorer UI - including animation demonstrations, detail tips, a text description, and practice area.
The content used in these three conditions - command names and groups, animations, Gesture Explorer design, etc.
Participants were told to go at their "normal working pace."
Participants were told that the diagrams they created did not have to be "100% perfect" but to "do as well as you can."
Before starting the experiment, the concept of a gestural command, and how to use one, was explained to each participant.
Participants were also told that there would be a help system on the side of the screen that would show them how to perform the gestures, and that they would see four different help systems.
In addition, participants were told that the person running the experiment would not answer questions about the software, but could clarify task descriptions.
We felt that handling failed gesture attempts is an important usability aspect of any gestural system, and so coping with failed gesture attempts was left to the participants.
Some participants ignored the stray lines and geometry failed attempts generated, while others sought to delete or undo it.
As the purpose of the experiment was to collect qualitative feedback, we believe that this added significantly to the realism of using a variety of gesture disclosure interfaces.
After the introduction, participants were randomly assigned an ordering of the conditions.
Users were then given one task set per condition.
Users were asked to perform each task from each task set sequentially.
If users made 10 or more failed gesture execution attempts, or if they said that they were stuck and wished to move on, they were given the option of continuing on to the next task.
To ensure participants were aware of the change in conditions, before the start of tasks 2, 3, and 4, participants were told that the help system had changed and that clicking on it would produce a different result than before.
A questionnaire was administered at the end of the study to assess users' overall experience using the conditions.
To help participants identify the conditions in the questionnaire, a color figure was provided showing screenshots of each condition, labeled anonymously "Help System 1," "Help System 2," etc.
We recruited 24 participants  from the general student body of the University of Central Florida.
Each participant was paid $15.
Participants were required to be able to operate a Tablet PC.
We advertised the study widely to get a sample of participants with diverse backgrounds and levels of experience using computers; 5 participants identified themselves as "Experts" in terms of computer expertise, the average rating across the participants was just above "Intermediate," the middle of a 5-point Likert scale.
3 participants had used a Tablet PC before, and 2 participants regularly used a Nintendo DS.
All trials used an HP tc4400 Windows XP Tablet PC with 1 GB of RAM, in slate configuration.
We created four task sets, each comprised by an ordered sequence of tasks.
Each task required the use of a gesture; several examples are "make a medium-sized rectangle," "zoom in on one of the circles," "make a star autoshape in an unused portion of the screen."
Each task set contained five gestures the user had not yet seen in a prior task set as well as several simple gestures  that the user had already seen.
Not all gestures were assigned to task sets; six gestures were used as distracter commands, so that there would always be commands the user was unfamiliar with and would have to search through, even if the user successfully executed every task set.
The majority of the participants felt the GBAR made it easier for them to both find and learn commands.
Participants were split between GBAR and EXPLOR in terms of which technique they liked the best.
However, the distinguishing characteristic appears to be the layout of the help system as some participants felt that GBAR was a well organized approach to providing help learning gesture while others preferred the fact that EXPLOR was laid out all once and showed gesture example icons.
CRIB was almost unanimously voted as the worst help system; participants remarked that it was very difficult to find things using it and that it really did not offer any help to learning and using the gestures.
Finally, overall comments regarding the four help systems indicated that several participants thought the organization found in GBAR was very helpful although they wanted the help to stay on when they needed it and to go away when they didn't.
These comments suggest a hybrid approach of GBAR and EXPLOR is worthy of future investigation.
This approach allowed us to give users rich tradeoffs between accomplishing a task in a substandard and inefficient way, or by searching for an appropriate command to make the task easier, learning the gesture, and executing it.
For example, users had to choose between making an arrow shape by free-handing it , or by searching the application for an appropriate command, finding the solid arrow command, learning the solid arrow command, and executing it, to make a perfect solid arrow, identical to the one depicted in the task.
In another example, a user might have to choose between redrawing a complex item  from scratch after deleting it, and searching for and learning how to use an undo command to undo the deletion.
The single-part tasks gave a sense of how users responded to each condition when given no direction; the multi-part tasks created scenarios in which commands such as Delete, Undo, and Zoom In were part of the task learning set.
The recruitment procedure and equipment was the same as for Experiment 1.
We recruited 44 participants  from the general student body of Brown University.
In addition, to ensure that users met our definition of novice, after a participant completed the experiment, we read them a description of a pen gesture, and asked if they had ever encountered anything similar prior to the experiment; none had.
11 participants had used a Tablet PC before and 1 participant regularly used a Nintendo DS.
We observed each participant's interactions with the software and identified each gesture attempt, as defined here: Gesture Attempt: An instance of a user attempting to perform a gesture in the document .
We identified the gesture participants had attempted to perform by the command they had tapped on, their own thinkaloud verbalizations, and the structure of the ink they wrote.
In all cases user intent was clear from the information available, and the distinctiveness of each gesture.
Gesture attempts were then classified as either failed or successful.
We created four tasks for Experiment 2.
Tasks 1 and 2 asked a participant to replicate a given diagram .
Tasks 3 and 4 asked participants to replicate a diagram and then to change it to look like another given diagram.
Each task was designed so that there was a single, unique, optimal combination of gestures that would produce the highest quality result with the least effort.
Tasks were specifically designed to not require artistic ability.
Each task, to be done optimally, required learning a specific set of new gestures ; we distributed the gestures in Lineogrammer evenly across each task, with six distracter gestures left unused.
Discovery Rate: The number of unique non-distracter gestures attempted  divided by the total number of non-distracter gestures.
Overall Coverage Rate: The ratio of successfully performed to total number of non-distracter gestures.
Performance Category: For each gesture in the combined task learning set, for each participant, we assigned one of five categories: successful on first attempt , successful within three attempts , successful in more than three attempts , attempted but all attempts were unsuccessful , and did not discover .
Similar to Experiment 1, participants were read an introduction explaining that they would be asked to create a series of diagrams using a program.
As before, participants were asked to think-aloud.
Participants were told to go at their "normal working pace" and to press the Start button to begin a task, and the Stop button once they had finished a task.
Participants were told that the diagrams they created did not have to be "100% perfect" but to "do as well as you can."
They were also told that they did not have to match the color and exact alignment of the target diagram.
To test approachability, it was important to give users no training, warm-up tasks, or introduction.
Participants were not introduced to the concept of a gesture to ensure that their a priori knowledge would reflect what end users might have in the real world, as most users have no experience with pen gestures.
However, believing that users of a specific application would probably have heard, at the very least, a minimal amount of high-level information about the application, we gave each participant one sentence of background, "You will be using Lineogrammer which has powerful tools for creating and editing diagrams."
Participants were told that the experiment moderator would not be able to answer questions about the software itself, but could clarify a given task if it was unclear.
Experiment moderator was only permitted to answer specific questions directly related to the task.
Nothing related to the capabilities, features, or use of the application was mentioned at any time during the experiment.
Although GBAR and EXPLOR are better suited for gesture discovery than CRIB and ANIM, GBAR is best in terms of overall performance.
Surprisingly, there was little change in mean overall coverage between CRIB and ANIM.
The discovery rate and overall coverage data lets us examine the effectiveness of the four experimental conditions in assisting users in successfully finding gestures and invoking them.
Figure 7 summarizes this data.
To further explore each metric we conducted a post-hoc analysis, performing pair wise comparisons on the four conditions using independent sample 2tailed t-tests.
To control for the chance of Type I errors, we used Holm's sequential Bonferroni adjustment  with 3 comparisons,  = 0.05 for each.
This indicates users were much better at discovering the gestures needed to optimally perLevene's test for equality of variance was significant for this comparison yielding a correction in the degrees of freedom.
We collected the performance category data in an effort to break down overall coverage to see how many attempts it took to successfully invoke gestures.
A summary of this data is shown in Figure 8.
We were most interested in ONE and FAILED because being able to complete a gesture on the first attempt or not at all is important to the overall usability of a gestural system; using a one way ANOVA, significant differences were found for ONE  and FAILED .
We also conducted a post-hoc analysis using the same criteria as the overall coverage and discovery rate metrics.
The results from Experiment 2 support our core hypothesis, as GBAR users were able to successfully execute nearly 90% of the gestures needed to optimally perform the tasks they were given - a value which was significantly higher than the other conditions.
This suggests that, with a well designed gesture set, users could achieve over a 90% mean coverage rate of an application's gestural commands, which we believe is sufficient to support walk-up-and-use experiences for commercial software.
Experiment 1 also showed that users overwhelmingly chose GBAR over the other conditions for finding and learning gesture commands.
Figure 7 shows that for GBAR, the mean overall coverage rate is closer to the mean discovery rate than in EXPLOR which implies that GBAR users are learning a higher percentage of the commands they discover than in EXPLOR.
This is surprising as there is no additional information in GBAR to aid the gesture learning-in fact there is less, since EXPLOR has gesture demonstration icons in the crib sheet.
Anecdotally, we noticed that GBAR users appeared more comfortable and confident perhaps as a result of using an interface which is familiar.
This confidence may have led them to attend more closely to the Gesture Explorer content, resulting in more successful learning.
One can think of performing a gesture correctly on the first attempt as an ideal experience, whereas one can think of failing to perform a gesture correctly at all - probably after a number of failed attempts - as a poor experience.
Based on this framework, one can say that users of GestureBar had a better experience as they successfully performed significantly more gestures on their first try, and failed significantly fewer gestures than CRIB or ANIM, and there was no significant difference in failures from EXPLOR.
The category data also supports this as in Figure 8 one can see that with GestureBar a high level trend appears to be away from the worst categories  toward the better categories .
The difference in performance between CRIB and ANIM was minimal, suggesting that animation alone does not necessarily improve performance.
We suspect that the speed of the animation may have de-emphasized geometric nuances.
One participant said of ANIM, "learning how to make certain shapes was vague and a bit difficult," which was mirrored by several other participants.
Interestingly, animation did appear to help with the disclosure of stroke direction.
We noticed that a number of users drew strokes in the reverse direction, resulting in recognition failures, but only when using CRIB.
CRIB and ANIM was initial uncertainty about the gesture interaction model, which in some cases persisted throughout the participant's use of the system.
In some cases, users would tap repeatedly on the crib sheet and then tap on objects in the document in an attempt to execute commands.
This behavior was largely absent in EXPLOR and completely absent in GBAR.
This suggests users were driven to find an outlet for their habitual "point-and-click" behavior which was most naturally accommodated by GBAR.
A related observation is that for all conditions, some users felt that they needed to click on a tool before performing a gesture.
CRIB and ANIM users complained about having to do this, but surprisingly EXPLOR and GBAR users did not.
We believe this reflects an overall sense of confusion and dissatisfaction with CRIB and ANIM.
However, at least one user in each condition commented that it was frustrating to find a command, such as Undo, and then have to learn a gesture in order to perform it .
It is not clear how severe a problem this is since Grossman's related work  observed that users had a positive perception of his disabled technique in which menu items were disabled to force usage of hot-keys.
An intuitive way to visualize the difference in performance between GBAR and CRIB is to review sample diagrams made by users .
These sample diagrams clearly show a significant difference in terms of real productivity between the average coverage rates of 58.37% for CRIB and 87.56% for GBAR.
GBAR also had a higher mean discovery rate than the crib sheet designs which we largely attribute to its less cluttered layout that makes higher level categorizations stand out .
However, the higher discovery rate for EXPLOR over the other crib sheet conditions, suggests that finding high-value information during one search has the added benefit of reinforcing future searching behavior.
In almost all cases, participants using ANIM and EXPLOR opened the animation/gesture explorer for each gesture they wanted to learn; however, in a handful of cases, users opted to simply read the crib sheet icon.
In two of these cases, the participant failed the gesture attempt and then opened the gesture explorer before continuing.
It is possible that over time, some users may become comfortable with the concept of a gesture and find referencing a crib sheet more convenient, perhaps as an optional power-user feature.
It is also possible that after a sufficient introductory period, users might switch to an "expert mode" to save screen space.
The fundamental nature of gesture UIs implies that, at times, gestures will be misrecognized.
We are thus interested in techniques which either reduce the likelihood of errors or at least make it easier to recover from them.
One approach is to encourage users to make more use of the Practice Area.
This might happen as a byproduct of just improving the feedback of the Practice Area, perhaps by integrating the disclosure techniques used in OctoPocus , or by incorporating explicit textual answers to the question "why didn't my gesture work?"
Alternatively, we might exploit workflow context, such as knowledge of which gestures had recently been encountered in the GestureBar, either to improve recognition tolerance or to provide suggestive recovery or performance feedback after ensuing failed attempts.
GestureBar could also be applied to multi-touch and speech UIs.
We have presented GestureBar, a familiar-looking toolbar UI, which, instead of executing a command when clicked, richly discloses the gestural interactions needed to execute the command.
Thus, unlike crib sheets which present an entirely unfamiliar UI metaphor, GestureBar users naturally and necessarily discover gestural interaction as a byproduct of approaching the GestureBar as if it were a conventional toolbar.
A quantitative, ecologically valid study of user performance supports our motivating hypothesis that even novices, with no prior exposure to gestural interaction, can immediately and successfully use a gestural application without the need for human assistance or up-front training.
A qualitative study of user preferences shows that GestureBar is preferred over the other crib sheet-based conditions for finding, and learning commands.
Finally, GestureBar presents no barriers to widespread adoption in terms of required recognition technology or gesture set constraints, and it can be easily unified with standard toolbar elements.
Alvarado, C. Sketch Recognition User Interfaces: Guidelines for Design and Development.
Alvarado, C. and Davis, R. SketchREAD: a multi-domain sketch recognition engine.
Bau, O., and Mackay, W. OctoPocus: A Dynamic Guide for Learning Gesture-Based Command Sets.
Buxton, W. Chunking and phrasing and the design of humancomputer dialogues.
