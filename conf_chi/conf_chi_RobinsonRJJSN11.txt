Much of the mobile work by HCI researchers explores a future world populated by high-end devices and relatively affluent users.
This paper turns to consider the hundreds of millions of people for whom such sophistication will not be realised for many years to come.
In developing world contexts, people will continue to rely on voice-primary interactions due to both literacy and economic reasons.
Here, we motivate research into how to accommodate advanced mobile interface techniques while overcoming the handset, data-connection and user limitations.
As a first step we introduce TapBack: back-of-device taps to control a dialledup, telephone-network-based voice service.
We show how these audio gestures might be recognised over a standard telephone connection, via users' existing low-end devices.
Further, in a longitudinal deployment, the techniques were made available on a live voice service used by rural Indian farmers.
Data from the study illustrates the desire by users to adopt the approach and its potential extensions.
Unlike the state-of-the-art, future-looking devices often studied by HCI researchers, though, a large proportion of these mobiles are likely to remain relatively dumb-phones with only a low proportion being routinely served by a data connection.
Furthermore, the users themselves add additional challenges to the goal of universal access: many have a low level of textual literacy, and their prior exposure to computing technology is often very limited.
To meet these challenges, a class of network-level audiobased services have been proposed.
These often combine automatic speech recognition  and touch tone dialling  to allow people to create and browse through spoken content.
The Spoken Web , for example, is a collection of interconnected voice sites.
These interactive audio applications provide content on topics such as farming or health information over the public telecom network.
Individual voice sites are accessed using any type of phone by dialling unique telephone numbers .
Although both ASR and DTMF allow a level of control and interaction with audio content, we believe there is still much work to be done in terms of improving the expressiveness and range of interactions.
As a first step towards richer mobile voice interfaces, we present TapBack: an extended interaction method for voice sites that aims to allow callers to smoothly navigate through and control the content they are listening to without having to unnecessarily interrupt its playback.
Our approach uses simple back-of-device interactions - audio gestures - on the phones users already own.
In contrast, the majority of our target audience use relatively lowend mobile phones, so we have focused on providing these additional interaction features without requiring users to own a specialised device.
The users themselves are also from very different backgrounds to those often studied by other researchers, bringing additional insights and challenges.
The contribution here, then, is an exploration of ways that impoverished platforms and their users can be afforded the sorts of advanced interactions being imagined for people living in the `developed' world.
Automatic speech recognition promises intuitive, low cognitive load interaction with audio content, with the benefit that no base level of literacy or numeracy is required.
However, the pauses or cues that are needed to prompt speech input and detection can easily upset the interaction flow, especially for short inputs such as `yes' or `no' .
DTMF key tones are quick to enter on keypads, designed to be unambiguous when recognising, and offer many possible input sequences.
However, because the tones generated are echoed in the phone speaker , interactions and responses are often fragmented, unlike in our design.
A further key issue with DTMF is that it is often necessary for the caller to take the phone away from their ear to see the keypad and respond to any input cues.
In our design we have concentrated on removing this disruption - instead we allow callers to interact on the back of their device during the normal call flow.
As mobile devices have continued to offer more features in increasingly-compact form factors, researchers have recognised the need for interactions beyond the screen and keypad.
This has culminated, recently, in touch-based back-ofdevice interaction with almost no need for a device at all .
However, although these designs offer touch interaction anywhere, they also require users to own specialised hardware.
We believe it is possible to offer a subset of these input methods to users who might not have the most modern devices.
However, this was aimed at allowing use of the phone's functions during a call, unlike our approach, which uses back-of-device interaction to control the call itself.
The TapBack system allows callers a richer experience with interactive voice sites by enabling audio gestures to be used at any time during a call.
By using the back of users' phones as an input surface while a call is in progress, we remove the interruptions of ASR/DTMF and allow users to keep the phone by their ear throughout the call.
Unlike previous backof-device methods, we use the phone's inbuilt microphone to pick up the sounds generated on the back of its case.
These sounds are loud enough to be transferred to the other party in the call, but, unlike DTMF tones, are not so loud on the caller's end that they drown out the audio being played.
For a simple user introduction to audio gestures we chose to apply tapping recognition to the control of audio playback speed.
Previous voice site analyses  have shown that callers would appreciate finer control of playback, so this was a natural application for our system.
In our implementation, when users tap two or three times, the time compression is 25% and 35%, respectively, while still retaining intelligibility.
Tapping once returns playback to its normal speed.
We build upon previous research into interaction that appropriates a device's surface as an input channel.
A microphone inside the device captured the sounds, allowing complex scratches to be recognised as distinct commands.
Our approach, although not capable of the same diversity in inputs, affords similar interaction on a normal phone.
Harrison and Hudson  built on this work to allow scratches and taps to be used as inputs on any solid surface, capturing sounds using a modified stethoscope.
They found that inputs were reliably recognised by a fast, lightweight recognition engine - an approach we adopt in our system.
Mobile possibilities for these interactions have recently been demonstrated as a commercial prototype1 that can detect tapping locally on a dumb-phone.
The TapBack system is installed on a remote server, monitoring low-level network packets to track incoming phone calls to individual voice sites.
When a call is established, realtime audio capturing, decoding and analysis is initialised.
The audio is first filtered to remove frequencies below 3KHz, greatly reducing the problem of ambient noise.
Tap recognition itself is very unsophisticated, simply searching each window for short, highintensity, high-frequency sounds.
Detected tap events are then fed to a higher-level audio gesture classifier which uses timeouts and basic heuristics to classify each tap type.
When an audio gesture is found, the system sends a command to the Spoken Web server, which adjusts the voice site playback speed in response to the request.
Users are also free to control the speed of playback by using DTMF inputs instead of taps.
In this case, keys 4, 5 and 6 correspond to single, double and triple taps, respectively.
The system logged call details and any input actions .
We supplemented this data by conducting detailed telephone interviews with 15 users.
Ten of these were selected at random from the set of those who had used TapBack during the deployment period; the remaining five were randomly selected from callers who did not attempt to use the tap interaction.
The average age of participants was 31, and all except one were male.
During the interviews these users were asked about their reactions to the approach; how usable it was; and, any issues they had encountered in its use.
Interviews were conducted in the participants' native language .
We conducted a user study to measure and improve the recogniser's accuracy over a standard telephone connection.
18 users of an existing, popular farming information voice site based in a rural region in India were recruited .
To ensure a cross-section of user expertise, participants included people who access the voice site very regularly and also those who are only casual users.
All users were male, and the average age was 32.
The set of phones used by the participants consisted of 14 different  handset types produced by four manufacturers.
All participants had already used the DTMF speed control methods detailed in .
Each participant was called by phone to explain the study method and the concept of audio gestures.
The calls were made to participants when they were at locations from which they usually interact with the voice site services.
The participant was then connected to a test voice site, which asked them to tap the back of the phone while holding it to their ear, in response to four sets of cues.
Each cue set asked users to tap once; twice; then, three times.
Each participant, therefore, provided 12 tap commands.
High accuracy rates for single and double taps were encouraging given: the minimal explanation of the concept to users; this form of interaction with a service was entirely novel in users' experience; a diverse set of low-end phones were involved; the audio channel was of standard telephone quality; and, the study was in a live, not laboratory setting.
A large proportion of the errors in recognition were due to participants tapping slower than the recogniser expected.
To deal with these errors, the tap classifier was modified to employ simple correction heuristics so that, for example, a 2-tap shortly followed by a 1-tap was interpreted as a 3-tap instruction.
The remaining errors were caused by taps not being distinct enough for the recogniser to extract from the input.
286 calls to the voice site were recorded over the study period, from 52 unique callers.
1293 tap interactions were recorded in total.
36 callers used the TapBack feature .
Of the 36 participants that used tap interaction, 25 used the feature on more than one call.
Two others called more than once but only used tap interaction on their first call; the remaining 9 TapBack users called only once over the study period.
The 16 participants who did not use tap interactions did not use DTMF for speed control, either.
Tap interactions consisted of 772 single, 301 double, and 220 triple taps.
Few of the callers that wanted to control the speed of the call used the DTMF method - 52 speed control DTMF events were recorded in total.
Considering first the ten callers who had used the TapBack feature.
Of these, the majority were positive in their comments about the approach.
Benefits mentioned ranged from those related to utility to those concerning the less-tangible `user experience'.
Several respondents commented on the tapping being easier to use and quicker than DTMF.
Another interviewee talked of the `fun' of the new interaction.
Interestingly, one participant said, "This is like having a touchscreen, this is a modern thing to use - it's cool."
Negative comments from these 10 adopters included the predictable, such as frustration when a tap-event was not recognised: one respondent said he would always use buttons because, "they always work - end of story."
However, there were also issues related to the use-context.
Two interviewees worried about using the system regularly as the tapping, to their mind, might damage the phone.
For one of these interviewees this was particularly worrying as they often lent their phone to others to use .
Another respondent said they tended to listen to the service with a group of people using the speakerphone.
There were two explanations for the non-use of the approach by the five other interviewees.
For some, their environments  were too noisy;
However, one commented on the use of drumming to skip through voice content; and many others wanted a fast way to jump to particular sections of the audio.
2 shows how we might widen the set of audio gestures in response to the studies.
We have introduced TapBack to illustrate the potential of audio gestures to complement voice-based interaction over the phone.
While the technique might appear unsophisticated or less exciting compared to the methods proposed for high-end mobiles, we argue that it is far more likely to have impact in the sorts of context that concern us.
Of course, there is much work still to be done to improve our lightweight recogniser.
Future work could focus on refinements to the recognition algorithms to improve accuracy.
Alternatively, increased robustness might be achieved by simplifying the gesture set to allow only single taps.
In the current speed control application, this could be applied as a toggle between options so each individual tap would change playback to the next speed preset.
We have shown how tapping can be used as input by rural Indian farmers via their basic mobile handsets.
Further, these users' responses to the method, along with their suggestions for additional gestures, indicate the viability of the approach for groups of people with very low exposure to computing.
The logged data provides evidence that callers are willing to adopt the tapping method, with the majority of callers using the approach.
It should also be noted that the functions controlled by TapBack - speeding and slowing the audio - are optional: users are able to listen to and navigate through content without employing them.
We would expect, then, that some calls during the study would not show tap interaction.
Furthermore, 93% of callers who used TapBack during their first call also used it again in their subsequent calls.
Callers that used TapBack did so several times in each call.
The comments about the system's utility value are, of course, encouraging, especially when considering the accuracy of our simple recogniser.
However, of more note, perhaps, are the responses relating to the user experience - a fun, `modern' interaction is something that is not usually associated with the low-end devices these users have access to.
The negative comments are spurs to improve the recognition engine and explanation of its use.
The social issues raised suggest extensions to our approach - we will need to ensure the tap-models used are tuned not just to individual phone numbers but the set of users that might use that phone; and, in speaker-mode use, we might be able to consider a wider set of audio gestures as suggested in .
In order to further understand the needs of the target users, we conducted a study to gather more potential audio gestures and corresponding actions.
The 15 participants questioned during the deployment were also asked to suggest additional back-of-device interactions they were comfortable with, and to discuss what actions they felt these gestures might initiate.
The majority of interviewees identified the value of two single-handed, back-of-phone interactions: drumming fingers and scratching.
Many users suggested finger-clicking .
Most users also raised the possibility of making non-verbal utterances - e.g., "I could make the noise I make when shooing away cows."
