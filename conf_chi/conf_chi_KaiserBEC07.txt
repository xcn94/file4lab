Lecturers, presenters and meeting participants often say what they publicly handwrite.
In this paper, we report on three empirical explorations of such multimodal redundancy -- during whiteboard presentations, during a spontaneous brainstorming meeting, and during the informal annotation and discussion of photographs.
We show that redundantly presented words, compared to other words used during a presentation or meeting, tend to be topic specific and thus are likely to be out-of-vocabulary.
We also show that they have significantly higher tf-idf  weights than other words, which we argue supports the hypothesis that they are dialogue-critical words.
We frame the import of these empirical findings by describing SHACER, our recently introduced Speech and HAndwriting reCognizER, which can combine information from instances of redundant handwriting and speech to dynamically learn new vocabulary.
In Figure 2, after a meeting facilitator has spoken the phrase, "Information Questions," while handwriting its abbreviation, Information Q's, on a flipchart, he then pauses, points at the abbreviation and says "right?"
These actions ground and entrain the meaning of the handwritten abbreviation.
Herbert Clark's Principle of Least Collaborative Effort  argues that humans expend all and only the necessary conversational energy to accomplish dialogue grounding and entrainment .
It is clear that multimodal redundancy -- e.g., both handwriting and speaking a term -- requires more energy than unimodal communication alone.
Therefore, there must be important communicative purposes driving its use.
Our working hypothesis is that people use redundancy as a conversational strategy to bolster their communicative effectiveness by drawing attention to the meanings of dialogue-critical terms.
In support of this hypothesis, we consider two derived claims.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In support of this hypothesis we will show that multimodal redundancy is indeed typical in certain communicative situations.
Secondly, we will show that words presented redundantly are dialogue-critical, as measured by their greater tf-idf  weights.
To introduce these empirical findings we will first discuss related and motivating work.
After analyzing multimodal redundancy, we will outline how SHACER, our Speech and HAndwriting recognizer, can leverage its occurrence to dynamically learn important new words, like proper names and their handwritten abbreviations.
The system's function is to unobtrusively collect, recognize, integrate and understand the information it observes in those public spaces, and produce useful background artifacts.
For example, our ACI Charter application  automatically populates an MS ProjectTM Chart by observing a scheduling meeting and integrating recognized whiteboard Gantt chart sketch and handwriting elements with their associated speech events .
Since an ACI perceives and processes natural interactions, new terms  will inevitably occur, which are not covered by the system's dictionaries and language models as discussed next.
In multimodal command systems, redundancy has been shown to occur for only between 1%-5% of interactions .
Thus the prevailing view in the literature is that for most multimodal commands, complementarity rather than redundancy is the major organizational theme .
In contrast to this prevailing view, Anderson et al.
This paper confirms and expands upon the findings of Anderson et al.
We examine three empirical data collections:  online whiteboard presentations,  a ninety minute spontaneous brainstorming session, and  multi-party discussions of photos printed on digital paper.
All three of these interaction contexts, as is also true for the studies of Anderson et al., are of human-human interactions where participants share a public writing space.
New language constantly emerges from complex, collaborative human-human interactions like meetings, lectures or presentations -- such as when a presenter handwrites a new term on a flipchart, like the Information Q's abbreviation shown in Figure 2.
Fixed vocabulary recognizers tend to fail on such new terms; therefore, we argue that multimodal ACI systems need to be able to adapt dynamically to newly introduced vocabulary.
In a recent analysis of lecture speech , Glass pointed out that the ideal vocabulary for speech recognition is not the largest vocabulary, but rather one that is both relatively small and has a small Out-Of-Vocabulary  rate.
A small vocabulary minimizes substitution errors, and a small OOV rate minimizes insertion errors.
The problem is that in general the size of vocabulary and the rate of OOV are inversely proportional .
To illustrate this difficulty Glass compiled a small, 1.5K vocabulary of words common to college lectures in three different course areas, and found that still the 10 most common subject-specific words in each lecture area were OOV.
Thus the presence of technical, subject-specific OOV terms makes deriving a vocabulary and language model for lecture speech and other natural speech contexts like meetings a significant challenge.
When a lecture's topic area is known ahead of time, automatic vocabulary expansion can be used  to leverage textbooks or targeted web searches to augment recognition dictionaries and language model statistics.
Kurihara et al., in their work on the use of predictive handwriting during lectures given in Japanese , assure full coverage with such methods.
Work in the area of Spoken Document Retrieval   also must deal with OOV terms.
SDR researchers aim to retrieve specific recordings from audio databases, employing queries like those used in searching the web for text-based documents.
Recently we have introduced a new class of multimodal system.
Instead of supporting a direct human-computer interface for command/display turn sequences, it accumulates ambient perceptual observations during structured multi-party interactions, like the construction of a Gantt schedule chart during a meeting .
Within this Ambient-Cumulative-Interface  there is no direct human-computer command interface; instead, there is ongoing background computer perception and processing of natural human-human interactions.
For the ACI we have implemented for testing SHACER, the perceived interactions occur in public spaces that are shared by the participants -- e.g.,  a shared interactive whiteboard or a piece of digital paper for public sketching and handwriting ,  a shared conversational space for speech captured by close-talking microphones .
Using the same vocabulary and language model on Switchboard data itself had only a 6% OOV rate.
As the OOV rate increased in moving from Switchboard to Teleconference data so too did the recognition word-errorrate with an attendant loss in precision-recall of spoken document retrieval.
To lessen the harmful effect of OOV terms in SDR, practitioners use sub-word units -- like phones or syllables -- as a basis for both recognition and query representation .
The OOV rate for query words in , even with small OOV rates for the SDR data itself, was still found to be 12%.
When sub-word units are used as a basis for recognition, the OOV problem is mitigated.
Query terms can automatically be transformed into appropriate sub-word units, and sequences of sub-word units can then replace words as the basis for querying an index of spoken documents.
SHACER, which is at the core of Charter, our ACI multimodal system for processing multi-party Gantt chart meetings, also employs a sub-word unit based recognition and alignment strategy as a basis for learning new vocabulary dynamically.
The methodology was to annotate all handwriting and speech.
For redundancy analysis, the frequency with which handwritten words were accompanied by redundant speech was examined.
For tf-idf analysis documents were constructed by concatenating the transcripts of both the spoken and handwritten words for a discourse segment.
Moreno and Mayer's theory of multimedia learning  is founded on three working assumptions drawn from cognitive psychology :  humans have separate processing systems for visual/pictorial versus auditory/verbal channels of information ,  each processing channel has limited capacity , and  that meaningful learning requires mental processing in both verbal and visual channels, building connections between them.
Given these assumptions, Mayer and Moreno  can explain why presenting text that is also spoken helps students learn more effectively, while presenting visual animations or graphics along with visual and spoken text hurts learning.
When the redundancy is across two channels  then processing proceeds in parallel in both channels and the effect is complementary.
The import of Mayer and Moreno's findings is that students have better recall and learn more effectively when textual information is presented redundantly in both visual and auditory modes.
Next we will show that in some humanhuman interactions speakers typically present information in just this way, redundantly across both visual and auditory channels, by handwriting words and also saying them.
Tf-idf word weights are commonly used in search and retrieval tasks to determine how important a word is relative to a document .
Words that occur with highfrequency in a document, but are relatively rare across the set of documents under consideration, provide a good indication of the document's content .
The handwritten abbreviations shown in Figure 3  exemplify the relation between dialogue-critical words and tf-idf weight.
They are dialogue-critical words because without knowing how they are grounded in speech, as shown by the call-outs in Figure 3 , the underlying visual representation lacks meaning.
They also have high tf-idf weights because they occur frequently within the presentation, but not so frequently across the entire set of presentations.
Thus the abbreviations in Figure 3 are both dialogue-critical and highly weighted.
We examined 34 short  whiteboard presentations offered on ZDNet's At The Whiteboard site .
Figure 1 shows a partial frame from one of these presentations.
There was an average of 11.6 handwriting events per presentation, and within those events were 15.9 annotatable handwritten words.
In the 34 presentations there were 33 different presenters.
The presentation videos were professionally made, and the speakers were in general practiced at presenting information via a whiteboard.
Twenty nine of the presenters were male, and four were female.
Audio and video annotations were done by hand using WaveSurfer's  video transcription plug-in.
A second annotator scored five randomly selected presentations from among the thirty-four, i.e., a 15% random sample.
Compared to the first annotator there was a 100% match on what the handwriting events were, a 96% match on the handwritten words within each event , and a 99% match on the spelling of matched words.
Between annotators the word start times varied on average by 71 milliseconds and the end times by 49 milliseconds.
Rounding up, the handwriting annotation timing accuracy was reliable to within 0.1 seconds.
In natural language processing tasks, a stop-list typically contains closed class words like articles, prepositions, pronouns, etc., which tend to occur with equal relative frequency in most documents.
When computing tf-idf weights , the stop words  are removed from consideration, because they tend to add little to the determination of which words are important representatives of a particular document.
In  we reported on some aspects of a pilot study in which photos printed on digital paper were discussed and simultaneously annotated with a digital pen .
There were four annotation sessions.
In this paper we further analyze data from the two native English speakers' sessions.
All speech for these photo annotation sessions was hand annotated, but the handwriting gestures were automatically captured via digital paper and pen .
Multimodal redundancy also occurs in less formal situations.
For example, we recorded a spontaneous brainstorming session, which occurred during a two day planning meeting with 20 participants.
Ninety minutes of the session were recorded.
Figure 2 is an example of handwriting and speech that occurred during this session.
Annotation of handwriting events followed the same procedure used in annotation of the ZDNet whiteboard meetings .
For audio transcription, only speech that was associated with a handwriting event was annotated.
All handwriting was performed by the session leader, but the speech associated with the handwriting events was spoken by various participants in the meeting.
Only 52% of the speech accompanying the presenter's public handwriting during the brainstorming session was spoken by the handwriter.
The other 48% was spoken by seven out of the other 20 meeting participants.
The percent of contributions from each of those seven roughly matched their positions in the organizational hierarchy underlying the meeting.
In a document, each unique word is referred to as a word type, while each individual word occurrence is referred to as a word token.
If while saying "hand over hand" a presenter also wrote the word hand, then concatenating the speech and handwriting transcripts would yield the word token list, "hand over hand hand," with three tokens of the word type, hand.
We refer to the word types in this combined token list as overall types  because they can originate from either speech or handwriting.
The subset of ALL word types that were handwritten are HW word types.
The subset of HW types that were redundantly handwritten and spoken are RH types.
Participants were asked to choose some photos they'd like to discuss .
They then spoke about their photos to a small group of others , having been told that they could annotate freely and that the software would process their annotations so they would get back labeled photos.
Photos were automatically projected on a shared display , since audience members sitting across the table could not easily see the paper versions.
The projected images were updated when the digital pen touched a photo sheet .
For the ZDNet whiteboard presentations examined here, the presenters spoke on average for 192.9 seconds  and handwrote on average for 38.9 seconds .
Table 1 shows the number of handwritten words that occurred in each of the three corpora , along with the number of handwritten words that were also spoken redundantly .
The bottom row of Table 1 shows the percent of handwritten words that were spoken redundantly .
The average number of handwritten words accompanied by redundant speech over all three corpora was 96.5%.
These results support the claim, which is derived from our working hypothesis, that multimodal redundancy is typical of human-human interaction settings where multiple modes can be perceived.
Our findings are thus numerically more significant.
We also examine three different scenarios, none of which was based on the use of a tablet PC as in the study by Anderson et al.
Of these six different types of redundancy, SHACER can currently take advantage of three --  exact,  abbreviation exact, and  almost exact redundancies.
These three categories represent 87% of the handwriting events reported on in this paper.
Within the no match category  there was a sub-category dubbed semantic matches.
Such semantic matches occurred in about 1% of redundant instances.
Both semantic matches and approximate matches could conceivably be processed by SHACER in the future.
Figure 5 shows the types of redundant matches that occurred, averaged over all three corpora.
The preponderance of matches were exact lexical matches , where the handwritten terms were spoken exactly as written.
Almost exact matches differ only in number or tense .
Category examples are shown in the Categories inset of Figure 5.
For the ZDNet corpus the percentage of handwritten words that were abbreviations was 44.3%, while for photo annotations it was 5.7%.
Our result of 74.3% exact match with 96.5% overall redundancy closely parallels the 74% exact match and 100% redundancy found earlier by Anderson et al.
For the 34 presentations of the ZDNet corpus, we found that 24% of redundant inputs were presented sequentially with either handwriting occurring first followed by speech , or speech occurring first .
For simultaneous  constructions, which were 76% of instances, speech preceded handwriting in 13% of cases, handwriting preceded speech in 60% of cases, and neither preceded in 3% of cases .
When we superimpose the timing data from the spontaneous brainstorming  session onto to that of the ZDNet presentations , the timing contours are closely matched .
Negative values mean speech occured first.
During the spontaneous brainstorming session, when handwriting was spoken redundantly by others rather than by the leader, there was a marked shift in the peak amount of time by which speech preceded handwriting .
Thus, when speaking about his own handwriting the leader's timing pattern closely matched that of the average ZDNet presenter -- with handwriting slightly preceding speech and simultaneously overlapping it.
However, when the speech of other meeting participants was reflected in his handwriting, then that handwriting occurred a few seconds after the terms had been spoken.
Each input pair of the sequential inputs shown in Table 2 is by the same ZDNet presenter.
Of these inputs 33% were speech followed by handwriting, a pattern which for speech and sketched graphics in Oviatt et al.
This may suggest that because handwriting requires more cognitive effort than sketching it is therefore delayed in presentation compared to simple locative sketches.
For sequential patterns, a preponderance of inter-modal lag times  was less then 2 seconds: 80% in the speech first case, and 76% in the handwriting-first case.
For the speechfirst condition all lags were within 4 seconds.
For the handwriting-first condition 8% of the lags were longer than 4 seconds, with the longest being a full minute and a half.
Row 1 of Table 3 shows that across all 34 presentations 15.81% of word tokens were shared, while only 0.25% of word types and just 1.03% of handwritten types were shared commonly.
With no stop list removal the average number of handwriting types per presentation was 15.9.
There were 209 average overall word types per presentation.
The percent of overall word types occurring in only one presentation  was 59.95%, and of handwritten types was 48.32%.
Such presentation-specific words will be OOV for a shared common vocabulary.
In the lower three rows of Table 3  we show the percentage of shared types remaining after basic stop list removal and with increasingly larger removal dictionaries: SL = basic stop list; 20k = a 20,000 word dictionary of the most common words in a corpus of meetings; and 170k = a 170,000 word dictionary from the Festival Speech Synthesis Toolkit .
As the number of common word types removed increases the remaining word types tend to be more and more presentation-specific.
However, it can be seen that as dictionary size increases the number of average handwritten types per presentation  decreases from 14.6  to only 3 .
Thus using large dictionaries does reduce the number of presentation-specific words that are likely to be OOV; but, as Glass  has pointed out, this is not ideal.
Larger dictionaries require more computational resources and are susceptible to higher word-error rates due to substitutions.
Perhaps, if we had many more training presentations to examine we could hope to find a shared vocabulary with fewer OOVs.
Figure 7 shows a power regression prediction that addresses this question.
As in row 2 of Table 3, the data points in the left side of Figure 7 are computed with no stop list removal.
As discussed above, Glass et al.
They found that subject-specific words from lectures were not well covered and often missing even from the vocabularies of larger corpora like Broadcast News 2 and Switchboard.
Here we perform a similar examination of word type sharing across the 34 presentations of the ZDNet whiteboard presentation corpus.
If we choose a vocabulary of all words that are not presentation-specific what level of coverage will there be?
The plot shows that the percent of presentation-specific overall word types  and handwritten types  decreased steadily as set size increased.
But the rate of decrease appeared to be leveling off around 40%.
The power regressions were computed in MS Excel.
The R-squared values indicate goodness of fit: 0.95 for overall and 0.97 for handwritten types.
Regression equations are shown in Figure 7.
In the plot on the right side of Figure 7 we have extended the power regression formulas from the left side plot to see what rate of presentation-specific handwritten words might still be present after examining a training set ten times the size of our ZDNet corpus.
Trend lines are extended to 360 presentations.
Even with this simulated order of magnitude larger training set there was still about 30% of handwritten types predicted to be presentation-specific .
Thus for natural speech contexts, even when a large training corpus is available, these findings suggest that as much as a quarter or more of redundant handwritten words would still be presentationspecific and thus out-of-vocabulary.
In earlier work  we showed that for photo annotation sessions, redundantly introduced words had a 90% higher average frequency than overall word types.
In this paper we calculate the average tf-idf weights of overall word types  versus redundant handwritten word types , for not only the two native English-speakers' photo annotation sessions but also for the ZDNet corpus.
For this combined data set, Figure 8 shows the average tf-idf weight increase for RH types compared to ALL types.
Table 4 shows examples from three ZDNet presentations of the top ten most highly tf-idf-weighted word types .
In some presentations - like the left-most, Detecting Greynets - all of the top ten are redundantly presented words.
On average for all 34 meetings only 7.66% of overall types are present in the top ten most highly weighted words for a presentation.
But of the redundant handwritten  types, 61.47% are present in the top 10, which represents 48.64% of all top ten words for all presentations.
Thus, the likelihood of a word being one of the top 10 most highly weighted words is less than 1 in 10  for overall word types, while for RH word types it is about 5 in 10 , meaning that RH words as a class are significantly more representative of a presentation than non-RH words , p<0.0001, one-tailed.
Similarly, on average, for all 19 individual photo discussions, just 11.5% of ALL types are present in the top 10 most highly weighted words.
But of the RH types, fully 81.77% were ranked in the top 10, which represents 48.95% of all top ten words for all photo discussions.
Table 4 shows that redundantly handwritten and spoken word types  as a class are better representatives of their respective presentations or discussions than other words.
Since they have significantly higher tf-idf weights than other words, they should be effective search query terms.
To test this claim we performed retrieval queries on an indexed directory of speech and handwriting transcript documents, one such document for each presentation in the ZDNet corpus.
The search engine we used was a state-ofthe-art, open-source search application called Seekafile , which works on both small and large data sets.
Thus for randomly chosen three word queries the retrieval accuracy was 27% higher using RH rather than non-RH words .
For two word queries the right side bar chart in Figure 9 shows that randomly chosen words from the RH set yielded 137% higher accuracy than randomly chosen words from the non-RH set.
Thus for two-word queries the retrieval accuracy was significantly higher using RH as opposed to non-RH words .
These results support the claim that redundantly presented words, which as a class have significantly higher tf-idf weights than non-redundantly presented words, are more effective search query terms.
From the work of Moreno and Mayer  on multimedia learning we know that redundantly presented words are easier to recall, and support better learning.
This means that, after seeing redundantly presented words during a presentation, those words will later come to mind more readily for use in retrieval queries.
We have also shown that redundant words are likely to be presentation-specific and thus OOV.
Allauzen & Gauvain in  have reported that up to 70% of OOV words are named entities, like proper names.
In the section below we show how SHACER can leverage multimodal redundancy to learn OOV proper names and their handwritten abbreviations.
Understanding these redundant OOV terms is critical for background understanding of a Gantt chart created during a meeting.
We performed searches with both three-word and two-word queries .
Retrieval accuracy measured how often the best-scoring retrieval result was the correct result.
SHACER's goal is to dynamically learn OOV terms , as they are presented redundantly during the course of an interaction.
In the lower pane of Figure 10, the recognized inputs row shows recognition results from both the handwriting recognizer and speech recognizer.
For example, the Fred Green handwriting, which labels the Gantt chart taskline beneath it, is incorrectly recognized as i-redesign , and the speech is incorrectly recognized as, "Fred's Green," because the proper name is not in the system's language model.
After SHACER combines the redundant handwriting and speech information, both labels were corrected as shown in the integrated inputs row.
In  SHACER corrected 22 of 29 such Gantt chart labeling errors across its development test set -- a significant 76% relative error rate reduction .
In the upper pane of Figure 10, the recognized inputs row shows two labels beneath a diamond-shaped Gantt chart milestone.
Neither of these handwritten abbreviations  is semantically grounded.
They have no call-outs that indicate their spoken expansions, and are thus considered incorrect abbreviation recognitions.
The WPS spoken recognition, together with the nearby first-letter abbreviation matches, triggers the association to spoken semantics .
SHACER uses sub-word unit recognition for characterizing OOV terms, similar to recognition techniques used in spoken document and spoken name retrieval systems .
However, SHACER's aim is not retrieval but rather learning and enrolling the spelling and pronunciation of new words into the system's vocabulary, improving understanding over time and usage.
SHACER learns from as little as a single instance of multimodal redundancy, but it can also benefit from repeated associations.
Currently such repetitions expand the list of pronunciation variations enrolled in SHACER's Word/Phrase-Spotting recognizer, thus improving the chances of subsequent recognitions.
Figure 10: SHACER example: learning abbreviation expansions through Multimodal Integrative Grounding  followed by Multimodal Semantic Acquisition .
On a held-out set of five related test meetings SHACER corrected 6 of 16 such abbreviation label errors, for a significant 37% absolute reduction of error rate .
These results clearly support our earlier findings in  that combining information from redundant handwriting and speech is significantly more reliable for the recognition of Gantt chart labels than depending on either mode alone.
The system learns dialogue-critical OOV proper names and abbreviations on its own, with no supervision but that provided by multimodal redundancy itself.
Our working hypothesis was that people used multimodal redundancy to focus attention on important words.
Derived from that hypothesis was the claim that if multimodal redundancy is a general communicative strategy, then it should be typical in human-human interaction settings.
Averaged across three separate contexts we found that 96.5% of handwritten words were also spoken redundantly, which supports the view that such redundancy is typical.
Furthermore we have shown that  as much as a quarter of redundantly presented handwritten words are likely to be out-of-vocabulary in relation to ideally sized recognition vocabularies, regardless of training set size,  that such redundancies are good mnemonic representatives of a presentation , and  that as a class they are significantly more representative of a presentation than other non-redundant word types, as measured by higher tfidf weights and significantly better accuracy in search retrieval results.
The second claim derived from our working hypothesis was that if redundant words are dialogue-critical they should be measurably more important than other words.
These results support this claim.
In describing our work with SHACER, we have shown that redundantly presented terms are dynamically learnable by unsupervised, boot-strapped methods.
Figure 10 shows a sequence of two meetings during which SHACER learns the expanded meaning of two new OOV terms and their abbreviations.
The lower pane of Figure 10  illustrates Multimodal Integrative Grounding, in which the spelling and pronunciation of new terms are dynamically learned by integrating redundant information from handwriting and speech.
Information stored in that WPS recognizer can be serialized and thus carried across meeting boundaries.
When an enrolled new term is spoken later, as for example in Meeting 4 , it is recognized by the WPS recognizer, and its spelling is compared to temporally nearby handwriting.
This material is based upon work supported by the Defense Advanced Research Projects Agency  under Contract No.
Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the DARPA or the Department of InteriorNational Business Center .
