During a live sports event, many sports fans use social media as a part of their viewing experience, reporting on their thoughts on the event as it unfolds.
In this work, we use this information stream to semantically annotate live broadcast sports games, using these annotations to select video highlights from the game.
We demonstrate that this approach can be used to select highlights specific for fans of each team, and that these clips reflect the emotions of a fan during a game.
Further, we describe how these clips differ from those seen on nightly sportscasts.
Crowd-sourcing; Video summarization; Sports; Twitter; Microblogging; Broadcast sports; Video annotation.
Prior work uses peaks in the rate of incoming data to detect when interesting events occur in a live broadcast .
We build on this approach by also separating the incoming tweet stream by home/away teams to provide more nuanced interpretation of the stream.
Using this approach, we can annotate the video with time spans that indicate "interesting snippets" of the video; this data can help us clip the video stream, producing different highlight reels to suit fans of each team.
Thus, we use Twitter as an additional semantic layer to understand the video stream.
In our work, we evaluate this approach by using a dataset of 15 American football games, drawing on live national broadcasts from both the Canadian Football League  and National Football League .
In comparing the highlight reels generated from our tool with those on broadcast news, we found that our approach is able to generate team-specific highlights that capture "in-the-moment" excitement well .
In contrast, clips from broadcast news better capture which plays were ultimately meaningful to the outcome of the game.
This work makes two contributions:  we demonstrate that crowdsourcing techniques can be applied to the domain of sports video summarization, and  with a deep probe into the sports problem domain, we uncover and highlight some interesting subtleties in both the data and requirements for crowd-sourced sports summarization.
To set the scene, we review existing approaches to sports video summarization, which widely focus on extracting information from the video data itself.
We further describe a growing body of work that examines crowd-sourcing as an approach to assist in annotating and summarizing events.
Many researchers have focused on summarizing sports videos for two reasons: first, sports have underlying structure that can be exploited ; second, sports broadcasts are limited to a small range of shot types, which eases the recognition burden.
However, truly interesting footage is only a small fraction of the entire broadcast.
Sportscasts summarize these interesting events into highlight reels, though these are limited by:  time , and  the inherent bias of the video editor.
Most automated approaches to highlight reel generation rely heavily on the video itself, using computer vision to detect motion, field orientation, and score .
Others use statistics and play-by-play information published to the web by on-site statisticians to help annotate the video .
Watching broadcast sports can be a social experience-- many viewers watch with other fans.
Today, this social experience can transcend physical boundaries: for example, during the event, viewers might microblog on Twitter as part of their viewing practice.
In this work, we make use of this Twitter data, using it to address the problem of sports video annotation and summarization.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
These data streams can be of textual nature , or related to our approach, analyze the video's audio  .
There is a growing body of research  demonstrating the utility of using crowd-sourced data to summarize and interpret events.
This crowd-sourced data allows for identifying sports  or political events  that are temporarily interesting/relevant to the population.
In addition, the actual text of tweets can be used to construct textual summaries of the event itself .
Applying crowd-sourcing to video summarization is a far less explored area.
Unfortunately, preference data for this system was mixed--potentially because these were general fans  rather than fans of the teams involved in the games themselves.
Our work builds on this work, applying algorithms originally designed for social event summarization  to the sports video domain.
Here, we focus on understanding the types of video clips generated by the algorithm rather than evaluating preference data.
American football is a team-based contact sport where the goal is to move the football into the other team's end zone.
Points are scored for touchdowns  or field goals .
Other plays can result in 1 and 2 points depending on the situation.
In North America, two professional leagues play American football .
In their respective countries, both leagues broadcast games live nationally, and command huge fanbases.
To serve this fanbase, both leagues have web-based play-by-play trackers that report summaries of each play as they occur .
For our interest in crowd-sourced sports video summarization, American football thus has a number of useful characteristics:  it is a highly structured game, with welldefined beginning and ends of plays ; 
Scoring plays are generally considered significant unless the game score is already decidedly lopsided.
In games where the score is close, plays toward the end of the game often carry far more significance than those earlier in the game.
Most importantly, however, is that web-text provides summaries of plays as they occur, but they do not always capture the significance of  each play to the game.
Thus, web-text is inadequate in picking out these plays; we believe that Twitter, on the other hand, can help find such plays.
This is not a clean separation--some fans use hashtags of both teams for two reasons:  greater visibility ;  fans of one team may complain about a "dirty play" involving the other team.
However, the vast majority of tweets were about one team.
With this data, #EpicPlay computes two different sets of highlights using the TwitInfo algorithm  on the teamspecific streams.
We calculate local peaks, as we believe that a peak in activity reflects interest by fans of that team .
From the beginning of a peak we move one minute backward in the video stream, and select a one-minute video clip.
This is based on the heuristic that tweets will likely appear after the end of the play , and that plays typically last less than a minute, thus ensuring that the clip contains the play in question.
What constitutes a good set of highlights from a sports event is necessarily subjective, complicating the evaluation of the highlight selection process.
As a quantitative benchmark, we compare #EpicPlay's selected highlights with those of sportscast news highlights ; however, our primary interest is in understanding the types of highlights selected by these different processes.
A visualization from #EpicPlay that shows the twitter activity during an NFL football game, along with our annotations:  tweets with the home/away team's hashtag;  the start and end of the game;  are peaks detected by the TwitInfo algorithm for home/away team's tweets;  the red ticks represent scoring events in the game, where ticks above the centre are home team scoring events, and ticks below are away team scores and  the darkened area represent tweets with both teams' hashtags.
For comparison purposes, we also collected highlight reels from two nightly sports newscasts  for each game.
In our sample of ~680k tweets across these games, an average of about 10% of tweets occurred within a minute following a scoring play.
We used TwitInfo's peak-finding algorithm as we assumed that "interesting events" would be highlighted by sudden bursts of activity.
The TwitInfo algorithm's sensitivity is controlled by two parameters:  the difference in number of tweets in the current bin as compared to the last one classified by the mean deviation  , and  the correction of the mean deviation over time  .
As illustrated in Figure 1, different peaks found for home/away teams reflect the differential excitement by the fans but are not necessarily aligned with scoring plays--as expected.
Table 1 shows the average calculated precision  and recall  for the games, with the highlights from sportscast considered as relevant items.
From a purely information retrieval perspective, crowd-sourcing video highlights on a per-team basis is not comparable to nightly sportscast highlights.
Thus, they typically reflected a build-up toward the final score .
In contrast, the crowd-sourced highlights typically reflected the fan emotion in the moment.
In practice, this resulted in several different types of plays being selected by the crowdsource method, summarized in the following:  Only scoring plays that are interesting were selected.
That is, if a score is "expected" or considered "easy", the play was not considered interesting; in contrast, a score that changes the lead was often selected.
Plays that required a high degree of skill, or resulted from extraordinary effort are often selected by the crowd.
These were sometimes considered more interesting than a possible subsequent scoring play.
Since the algorithm identifies spikes in activity, this also resulted in "lowlight" selection.
Plays that resulted in fans becoming frustrated with their team were selected.
Plays that resulted in a controversial penalty called by the referee were often selected, with fans voicing their displeasure through Twitter.
Unusual things that occurred during the broadcast were often captured.
In one game, for example, a player dribbled the football in soccer-style around the field.
Crowd-sourcing using Twitter does have some shortcomings.
We noticed, for instance, that many fans would tweet at the beginning and ends of each quarter .
These did not typically result in an interesting highlight.
Why does #EpicPlay exhibit such poor precision and recall scores?
To address this, we performed a more qualitative analysis on the highlight reels themselves.
When we examined the differences between sportscast highlights and our crowd-sourced highlights, we noted several differences:  Median # clips sportscasters can only show a selected Avg # tweets / min limited number of highlights Games Home Away Home Away .
Quantitative summary of our dataset.
Fortunately, both these shortcomings are systematic in nature, suggesting that they can be programmatically eliminated if necessary.
We considered two alternative methods for selecting highlights:  using the TwitInfo algorithm on aggregated tweets for both teams, and  selecting highlights based on scoring plays.
The first variation selected highlights that were interesting to fans of either one or of both teams .
The resulting set of highlights was perhaps more balanced, but sports fans in our lab suggested that they would be more interested in highlights selected by likeminded fans.
Using solely scoring events for highlight selection missed other "interesting" events that occurred in the game .
TwitInfo requires a meaningful rate of tweets to detect peaks reliably.
For some CFL games, we found that fans did not tweet with sufficient frequency  for meaningful peaks to be found.
In general, NFL games had considerably higher rates of tweets , resulting in more meaningful peak detection.
For instance, tweets might actually be used as a source for annotating highlights, or even provide commentary for a sports event .
The utility of Twitter data is that it adds a semantic layer of interpretation atop the broadcast and web-text feeds.
If used in concert with existing techniques, this layer would add considerable strength to work in this area.
As a consequence, the plays fans are interested in differ depending on their team allegiance.
This is also information that is reflected in the Twitter data.
In this work, we show that we can extract meaningful highlights for sports events using Twitter.
The resulting set of clips does not closely match those of nightly sportscasts; instead they are more tied to the drama and emotion of the game as experienced by fans.
As such, they are a reflection of the fan's excitement and interest during the game.
We believe this information can be used as another data source for information retrieval: rich information being generated as a consequence of fans' everyday viewing behavior.
To better understand the quality of #EpicPlay's selected highlights, we considered two approaches:  some form of user evaluation, and  comparing highlights against a suitable benchmark.
With due consideration to others' work in this space , we felt that highlight reel selection was necessarily extremely subjective.
Rather, we expected the strongest reaction from team-specific fans to highlights generated about those specific teams.
These fans would be able to relate better to the emotional content or meaning of particular plays within the context of the specific games.
However, given our dataset, recruiting such a select group of participants would have proved extremely challenging.
Exploiting "Normal Behaviour" for Crowd-sourcing.
This work builds on a considerable body of research that explores crowd-sourcing.
What sets this work apart is that we are actually mining data that results as a consequence of existing behaviors.
That is, we do not ask viewers to tweet about their experiences; instead, they already do this as a part of their everyday sports-viewing activities.
We simply use this data to perform a task that is challenging for computers to do on their own: understand the event.
