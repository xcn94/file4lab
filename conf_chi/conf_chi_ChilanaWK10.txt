Although usability methods are widely used for evaluating conventional graphical user interfaces and websites, there is a growing concern that current approaches are inadequate for evaluating complex, domain-specific tools.
We interviewed 21 experienced usability professionals, including in-house experts, external consultants, and managers working in a variety of complex domains, and uncovered the challenges commonly posed by domain complexity and how practitioners work around them.
We found that despite the best efforts by usability professionals to get familiar with complex domains on their own, the lack of formal domain expertise can be a significant hurdle for carrying out effective usability evaluations.
Partnerships with domain experts lead to effective results as long as domain experts are willing to be an integral part of the usability team.
These findings suggest that for achieving usability in complex domains, some fundamental educational changes may be needed in the training of usability professionals.
Consider a project for evaluating how consumers shop using an e-commerce website  versus scientists using a bioinformatics analysis tool .
How does a usability professional trained as a generalist deal with these seemingly two different projects?
Human-Computer Interaction  researchers, practitioners, and educators hold differing opinions about usability in complex domains.
Some might argue that usability work in any domain requires a learning curve so complex domains are not fundamentally different.
Others may propose that complex domains are different, but the issue has been resolved with ethnography and HCI field methods for observing work, eliciting requirements, and understanding domains .
However, what actually happens in usability practice in complex domains has not been established by prior research.
What we have learned from existing surveys of general usability practice is that low-cost, informal or "discount" methods like usability testing and heuristic evaluation are most widely used .
The popularity of these informal methods is not surprising, considering the rapid pace of industry product cycles, the organizational constraints on design, and resistance from engineering cultures to employ formal methods.
Unfortunately, assessments of usability practice to date have largely been agnostic to the domain of practice or subject-matter knowledge.
Given the increasing role of computing in all domains and the appeal of making software user-centered, understanding how domain complexity affects usability practice is more important than ever, especially for training the next generation of usability experts.
To fill this knowledge gap and to establish a better understanding of usability evaluation in complex domains, we conducted semi-structured interviews with 21 usability professionals working as in-house experts, consultants, or managers in a broad range of complex domains.
In just a few decades, usability evaluation has moved from an academic endeavor  to a practical reality, with industrial practice growing by as much as 5000% in the last 25 years .
And yet, despite the rise of the usability professional1, there is growing concern that achieving usability in some domains is an overwhelming challenge .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Our interviewees worked in medical imaging, software development, network security, aviation, healthcare, test and measurement devices, genomic analysis, financial derivatives, statistical analysis, and business-process support.
We specifically investigated how empirical usability testing in these domains differs from conventional GUI and web applications, and how it changes the dynamics of collaborations among usability professionals, users, and developers.
We have gained three key insights from our initial results about usability evaluation in complex domains:  Even highly experienced usability professionals regarded work in complex domains to be more challenging than working with GUIs and web applications.
Some went to extreme measures to understand a complex domain-- even taking night classes--but generally found their efforts to be insufficient.
Some developed partnerships in which domain experts acted as consultants, contributing on an infrequent but regular basis.
Others formed deeper, persistent relationships, incorporating domain experts into their team and working hand-in-hand through all aspects of their evaluations.
Some managers admitted that they were becoming reluctant to hire usability experts who lacked significant domain expertise.
There are many ways to respond to these findings.
Domain complexity clearly imposes new knowledge demands and the need to change dynamics of collaboration in usability practice.
In our discussion, we particularly focus on the implications of these findings for training the next generation of usability professionals.
The main contribution of this paper is that it provides empirical data that establishes the nature of usability work in complex domains and lays the groundwork for further refining usability practice and education.
While these surveys are important for understanding usability practice, they have not considered the role of domain expertise.
They highlight challenges faced by practitioners, such as working under time pressure and how, in order to maintain their credibility, usability professionals have to compromise their methods.
A study by Folstad  compares the results of group expert walkthroughs carried out by usability professionals to results from domain experts for three different domain-specific mobile applications.
The results show that domain experts identify fewer but more severe usability problems compared to usability professionals with no domain expertise.
While Folstad is cautious about generalizing his results given the limited scope, the results suggest that domain experts make higher-impact findings than usability professionals in usability inspections.
One contribution of our work is unveiling the causes of this difference in empirical usability methods and detailing the strategies that usability professionals employ when domain experts are not available.
Other domain-specific studies have investigated one or two specific domains in-depth.
For example, Viitanen et al.
Redish  discusses usability testing of complex information analysis visualization tools.
These studies explore only a small number of domains, making their findings difficult to generalize.
Our study includes a much wider range of domains, allowing us to identify common themes across a range of usability work.
Finally, several studies have explored HCI aspects of complex domains other than usability evaluation.
For example, Mirel  explores interaction design for complex information visualizations.
Gulliksen and Sandblad  discuss the development of a domain-specific style guide to deal with the shortcomings of generic graphical interface elements.
Roesler and Woods  discuss design and focus on the roots and characteristics of domain expertise itself.
Our empirical results complement these conceptual discussions of design work in complex domains.
We carried out semi-structured interviews with 21 usability professionals at their workplaces  or on the phone, following the strategies outlined by Rubin and Rubin .
We developed a list of structured questions and used the critical-incident technique , probing into interesting responses with an unstructured follow-up conversation.
Each interview lasted about one hour.
During the conversations, we first asked interviewees to describe a project that required significant domain expertise that they lacked.
We then used this critical incident in subsequent questions, asking interviewees to compare it to an experience working on a project from an "everyday domain."
In particular, we asked them to think of projects where they used task and scenario-based observational usability testing  and to compare these projects to those in which other evaluation techniques, such as heuristic evaluation, were used.
The interviewees were asked about difficulties they faced in each phase of a usability test--during planning, observations, and analysis.
We also probed into the strategies interviewees used to cope with the challenges in different phases.
The rest of the interview focused on credibility, allocation of time, and communication.
We left time at the end for open-ended discussion and allowed interviewees to share anything else that would help us understand usability practice in complex domains.
Our 21 interviewees worked across five large corporations, three national-level research institutions, and three independent consulting companies.
They had experience working in a range of domains: medical imaging, software development, network security, aviation, healthcare, test and measurement instruments, genomic analysis, financial derivatives, statistical analysis, and business-process support.
The interviewees included 11 in-house usability experts, 5 external usability consultants, and 5 managers, and held titles including Usability Engineer, User Experience Researcher, Usability Consultant, Human Factors Engineer, User Experience Manager, and Sr.
The median work experience for the interviewees was 10 years.
Seventeen of the interviewees had formal training in HCI or a related field such as Psychology, Technical Communication, Computer Science, Information Science, or Human Factors.
Fourteen of these interviewees had training at the Masters or Ph.D. level.
Many interviewees mentioned extensive on-thejob learning of evaluation techniques.
To recruit interviewees, we used direct contact through email, word-ofmouth, and snowball sampling, where current interviewees helped to identify other participants.
All interview transcripts were encoded digitally and by hand.
They were organized, coded, and analyzed using ATLAS.ti,
We followed an iterative process of applying open coding and axial coding to discover relationships among emerging concepts in our data, followed by selective coding to integrate the results .
Through this process, we continually explored different facets of the data and identified recurring themes.
We now report our main findings, namely the challenges faced by usability professionals in complex domains and the coping strategies used in such situations.
We first illustrate the characteristics identified by our interviewees that made working in complex domains difficult.
Next, we highlight some of the specific challenges that the interviewees encountered during the planning, execution, and analysis phases of a usability test.
Last, we highlight common strategies our interviewees used to deal with their lack of domain expertise.
Every new project for our interviewees involved an initial learning phase enabling familiarization with a particular domain.
However, the majority of our interviewees said ecommerce websites and other web applications were now familiar domains and routine work in usability.
Our interviewees described other types of domains that required deep subject-matter knowledge and experience as more complex .
They also described a number of challenges of working in any new domain, but consistently identified three key characteristics that are pronounced in complex domains:
Even after several years of working in the same area, some interviewees were not able to understand all the unique situations and frequent exceptional conditions that arose in complex domains: If you're working on an e-commerce site, you know how it works, there's a shopping cart, you check out...You're very familiar with how that works and that overlaps with other web apps...
Here , everything is very unique comparatively--there is no overlap with anything I've done before... Say you go to the emergency room, to understand the basic workings of a hospital.... when a patient is brought into the ER, they bypass a lot of the procedures which may be used when you're just getting admitted... Limited access to domain experts.
That person does have a job to do and can't answer questions all the time... One great difficulty is finding representative users--the true target audience.
These experts are hard to find, highly paid, have busy schedules... have much better things to do with their time... need significant incentives to participate...
Is it something they  care about or don't care about--trying to get at that is important...if they make a mistake and start all over, is this a problem or an issue...if you're watching a procedure, if it's going 10 minutes, is that too long or not long enough...how to interpret that?
In trying to understand the challenges of evaluation, we focused on the widely prescribed task and scenario-based observational usability testing method .
The interviewees described a number of challenges they faced in planning the test, observing user behavior, and analyzing results in complex domains.
Planning for a usability test posed the greatest challenge.
The interviewees described difficulties in knowing where and how to begin, asking the right questions, knowing what to look for, devising appropriate tasks, and coping with new domain-specific concepts.
One of the interviewees who had been a usability consultant for over six years said that he normally started his evaluation projects by simply looking at the current system and its requirements, and then devising tasks for users.
However, for complex domains, his experience was different:
The process of analyzing and reporting the final results raised another set of difficulties.
For example, one interviewee who had over 12 years of experience as a user researcher and currently managed a consulting group explained how analysis of results in complex domains required extra time and effort:
For a  website or something we don't go back as much to review tapes...for an expert domain system I have to watch each video end-to-end again, stopping and taking notes, filling in gaps...I may have to Google a term sometimes...I have to take baby steps along the way...analysis is more difficult overall...
It's more about my own confidence level--my recommendations are more like suggestions because I don't want to give a hard recommendation if I'm not confident it's going to work or not with their system...I use qualifiers more, so my recommendations become more like suggestions, like, "if possible, then do this"...
Many of the in-house professionals said that they had sufficient exposure to the domain they were working in and could generally understand users' tasks.
However, the professionals struggled because each project in a complex domain was unique.
For example, one interviewee doing aircraft design explained that finding realistic tasks for users was always a new endeavor, regardless of familiarity with the domain:
These initiatives are summarized in Figure 1.
One of these initiatives was particularly striking: a third of the interviewees mentioned enrolling in specialized training on top of their regular jobs to learn about the domain they were working in.
For example, one interviewee who worked on electronic measurement instruments explained:
This interviewee stressed that even though observing users' environments and tasks provided a clearer picture towards understanding the domain, this process took months.
When site visits were not possible or if additional information was required, participants mentioned interviewing domain experts.
Some described using formal structured interview protocols, while others opted for unstructured approaches.
The interviews allowed usability professionals to capture thought processes and decisionmaking rationales used by domain experts, rather than relying on observations alone:
When you start talking to people, you start to get how these other people think about it vs. how we understand it... you can't do this kind of stuff without evaluating what these people are doing.
I learned that...for more expert  systems, you're trying to be the expert on an expert, so it requires you to really understand.
Now, before letting them  work on a product, I try to get them training...as a manager, it's my responsibility to look over everything.
A lot of people previously haven't been useful because they are not domain experts...so  teams write them off...but now I really require people to prove themselves.
Although such collaborations are related to the contextual inquiry  technique used to inform design , none of the interviewees mentioned following all the steps prescribed by CI or deriving the CI models.
One of the managers was rather explicit about why CI does not work for the types of complex projects he deals with:
It's like you can spend hours doing it because someone told you it was a good thing and then you come up these affinity diagrams and then don't know what to do with it...
All interviewees agreed that acquiring domain expertise on their own was a good starting point, but insufficient for understanding the nuances of a complex domain.
Thus, the interviewees relied heavily on domain experts to learn and clarify domain-related details.
Three models of collaboration between usability professionals and domain experts emerged from our data: iterative elicitation, persistent partnership, and upfront investment.
In this section, we describe these models and their strengths and weaknesses.
One form of collaboration described by our interviewees was upfront investment .
With this strategy, usability professionals relied on domain experts extensively during the planning phase to understand scenarios, tasks, and the target users of the domain, and then little or never thereafter.
Despite the high regard for upfront investment, this strategy was not feasible in the working lives of most usability professionals we interviewed.
We found it mainly being used in research settings or large corporations with formal user experience teams.
Those professionals who worked on projects that had quick turnaround cycles or limited resources could not afford to make such investments for the purpose of evaluation.
Furthermore, participants who used this strategy stressed that field observations or interviews of domain experts must still be interpreted correctly:
Other interviewees were generally meticulous about their final reporting, mainly to maintain their credibility.
By acquiring feedback from domain experts, usability professionals felt more confident about their analyses and believed they made stronger recommendations.
Despite its popularity, iterative elicitation exhibited drawbacks.
The majority of interviewees cited getting regular and repeated access to the right domain experts as the major hurdle.
For example, one of our interviewees working in the aviation industry pointed out:
The most common form of collaboration described by interviewees consisted of regular back-and-forth exchanges between usability professionals and domain experts.
We call this iterative elicitation .
As mentioned above, one challenge for usability professionals in complex domains is coming up with relevant questions or tasks for a usability test.
With iterative elicitation, our interviewees outlined tasks and then sought feedback and revisions from domain experts:
Even just basic things explained in ordinary language, even when my questions were answered, they weren't always in normal English.
It was still hard to understand the answer...with email, these people  don't have a lot of time... the tendency is to be efficient... give short, concise answers, but then I end up sending emails back, "so when you said, `blah blah,' what were you talking about?"
For this interviewee, the focus was on verifying whether his tasks had covered all relevant issues for the test.
Others described a similar back-and-forth exchange with a domain expert to ensure no details were overlooked, and no unimportant or unrealistic test situations were devised.
Our interviewees also used iterative elicitation to make sense of their observations and findings:
A third and most direct form collaboration that interviewees used was a persistent partnership with domain experts .
Partnerships were sustained from the initial planning through the analysis phase.
Usability professionals worked hand-in-hand with domain experts to create and verify tasks, co-facilitate observations, and analyze results.
Likewise, other interviewees worked with domain experts to figure out what to look for, where potential problems may be, and to develop sample data sets to use during tests.
The expertise contributed by usability professionals was in phrasing the tasks, for example, by ensuring that tasks did not include biased questions.
With persistent partnership, domain experts also cofacilitated the test sessions, similar to the idea of cooperative usability testing .
The domain experts not only helped interpret what was being observed, but also answered domain-related questions posed by users:
I have certain domain experts who are good for certain kinds of questions, and it is good to know who to ask for certain types of questions...there's also who am I getting along with right now?
For several months I might collaborate with one domain expert, and then it might not feel like a positive interaction, so I'd move on to someone else.
Domain experts also helped analyze results and prioritize findings with usability professionals.
Most interviewees said that for everyday domains, such as consumer websites, they were able to make judgments about what to fix with little effort.
However, with complex domains, there were, as we expected, additional challenges:
We now turn to two dynamics that underlie all collaborations in usability practice: credibility and persuasion.
Usability professionals have long struggled in gaining credibility with management and persuading developers to make changes based on user data .
Our interviewees explained that these challenges exacerbate when domain expertise is lacking.
These additional challenges manifested in complex domains as additional communication demands.
Apart from collaborating with domain experts, our interviewees explained that they spent extra time communicating with design team members, managers, and software developers.
For instance, usability professionals often had more conversations in their teams about the planning, execution, and analysis of tests when working in a complex domain:
Before our test, the whole team walks through how the test will work out ...when it's something simple we can come up with something.
With the more complicated projects, we work very closely with the  researcher and developers.
There will be more frequent meetings, more time on the whiteboard.
Restrictions and regulations in domains such as healthcare or aviation appeared to be particularly stringent and often overshadowed usability recommendations.
By using persistent partnership and having domain experts co-located during the analysis, usability professionals felt less at risk of making untenable recommendations.
Although persistent partnership is similar to participatory design , the partnership appeared to be stronger and lasting and not necessarily a partnering of equals.
Furthermore, the partnership centered on evaluation and analysis, not on idea or concept-generation and the partners were often domain experts who were not necessarily the endusers of the resulting system.
And yet, while this model yielded successful results, our interviewees were clear about the downsides of involving domain experts in usability evaluations.
One interviewee told a story that occurred during user testing:
The problem with  is that the subject matter experts were so bold in their perspective...I would say in every case...at the end of the session they were trying to talk the user into doing it their way or explaining or justifying why they built it this way, and so on...so not very effective...
Although such a connection with a colleague did not make up for a lack of domain expertise, interviewees found it helpful for discussing results and sometimes even for recruiting domain experts via the colleague's contacts.
In addition to increased communication demands, usability professionals struggled even more to make usability an integral part of software development, instead of being viewed as overhead .
Although project managers and others in similar roles seemed to appreciate the role of usability more than before, they still wanted to see outcomes directly related to the scope of their projects.
When usability professionals lacked domain expertise, more work was needed to convince management about the value of usability findings:
Still, most interviewees felt that sometimes their lack of development expertise, in addition to their lack of domain expertise, got in the way of persuading developers.
Our manager interviewees particularly emphasized that usability professionals needed to master the basics of how developers talk because ultimately all design changes were implemented by developers:
There's this whole mentality  dividing wall between usability and developers.
We have to take the time to learn the language...that doesn't mean we have to become those people or experts...people can help fill in the gaps...but you have to make effort to learn first.
It's like going to a foreign country and being annoyed that they don't speak your language...have to learn the basics.
I try to get the users to express what the issue is...
I may interact with a client to figure out if I'm expressing this in the right way.
I usually feel comfortable with the point of contact, but then to gain credibility you have to make sure that they don't feel silly having hired us.
If it's an important problem I usually put a video clip together--have 3 or 4 domain experts speak about the issue.
The results from our exploratory study show that despite efforts to be effective in complex domains, our interviewees' lack of domain expertise was a significant hurdle in carrying out usability tests.
There are several implications from these findings for improving current usability practices in complex domains.
For example, organizations seeking usability services could take into account the extra overhead that a complex domain imposes and adjust work schedules accordingly.
However, as indicated by our results, domain complexity introduces a number of problems in usability work that cannot be resolved merely through spending additional time on a given project.
For achieving best results in complex domains, our findings suggest that either  usability professionals take on formal training in a particular domain, or  usability professionals commit to a long-term relationship within a domain, start small, and iteratively work the way up to doing more complex testing, or  domain experts carry out usability evaluations themselves.
Many of our interviewees felt that generalist usability professionals can still succeed as consultants and work in a range of domains, but they would benefit by learning about the adaptations necessary for working in complex domains early on.
In our discussion below, we reflect on the ideas of combining domain expertise and usability expertise, and augmenting generalist usability education for helping usability professionals succeed in complex domains.
According to our interviewees, the more that domain experts were involved, the less credibility was an issue.
However, since accessing domain experts was such a challenge, most usability professionals felt that establishing credibility with management was a constant battle.
Establishing credibility was particularly difficult with developers.
Some of our interviewees who had been in the usability field for over a decade explained that they had been seeing increased acceptance and even enthusiasm about gathering user data.
However, convincing developers to make changes was particularly problematic when usability professionals lacked domain expertise because developers were skeptical about usability recommendations:
To be more persuasive, participants described some initiatives that they took to get the developers involved in the usability process early.
For example, they invited developers to sit-in during interviews when gathering information upfront from domain experts or asked developers to watch usability sessions, often via web-cams from their desktops:
In addition, the managerial participants in our study asserted that for complex domain software, their preference was to hire usability professionals with domain expertise, although finding this combination was rare.
A response may be to train usability professionals to specialize in a particular domain from the outset during their formal education.
Although this approach is appealing, one problem is that the types of domains we investigated required graduate or professional training.
It is not feasible or even reasonable to expect someone who has a graduate degree in HCI to pursue an additional degree in medicine or financial analysis.
In contrast, it is possible for individuals to acquire training in usability methods in just one or two years.
Most physicians or financial analysts with doctorates will probably not seek HCI degrees, but it is possible that some professionals could become expert evaluators in their respective domains.
The idea of empowering domain experts is not new in HCI.
For example, the idea of meta-design  extends the notion of user-centered and participatory design by allowing users to contribute to design throughout the whole process.
As mentioned in our related work, Folstad's study  also suggests that domain experts produce higher-impact results than usability experts, although his finding is within the limited context of inspection methods.
But even if domain experts are willing to specialize in usability, there are drawbacks even apart from the obvious one of the unlikelihood of a doctoral-level expert taking on usability work.
A common feeling among many of our interviewees was that when domain experts got involved in an evaluation, they got too "bogged down" in domainspecific details.
In contrast, although generalist usability professionals may struggle in complex domains, they are able to provide a broader perspective on interface design and user interaction, and can be strong user advocates in the face of daunting domain-specific challenges.
Whether a middle-ground solution can be achieved through joint training programs is another open question.
Students may be graduating with an extensive repertoire of methods and skills in understanding users, but they do not get enough exposure to examples from complex domains where, as our study shows, the need for understanding domain experts and their work is daunting.
Arguably, many students will still be working in everyday domains after they graduate, but as our participants pointed out, usability services increasingly are being sought in all sorts of complex domains.
Thus, preparing future practitioners with the appropriate skills to work in complex domains is a necessity, not just an enhancement.
As a start, educators can consider including the three collaboration models that emerged in our results in lessons on usability evaluation.
The persistent partnership model produces the best results in the view of our participants.
The spirit of this partnership approach is not new--pluralistic usability walkthroughs , participatory heuristic evaluations , and cooperative usability testing  all emphasize the involvement of end-users.
However, our data makes clear that it is not enough for a domain expert to simply cooperate or be a participant.
Rather, the hand-holding aspect is crucial in all phases for producing effective results.
To form and make effective use of these partnerships, students could benefit from learning about interdisciplinary teamwork skills, how to ask the right questions, and how to best apply the domain knowledge that they gain in their designs and recommendations.
In addition, most practitioners currently develop skills in dealing with short turnarounds and credibility issues on the job.
More emphasis on fostering these skills in the classroom would also be useful.
Interdisciplinary team projects are a continuing must, but also domain-specific projects that pair usability students with students from other disciplines entirely to perform evaluations of specialty software.
For example, a usability student may be assigned to conduct an evaluation of a bioinformatics tool with a student in biomedicine.
We will be looking at the effectiveness of implementing such an approach as part of our ongoing research.
Apart from the possibility of training specialists to be both domain experts and usability experts, in the short-term there is value in reflecting on current generalist HCI training programs.
The ACM SIGCHI Curricula for HumanComputer Interaction  prescribes the incorporation of interdisciplinary skills from four traditional areas relevant to HCI: computer science and engineering, psychology and cognitive science, media and design, and social science.
We informally surveyed current training programs and courses in top HCI universities across the United States and found that they generally follow this pattern.
Since we have only considered the experiences of usability professionals from North America and do not have any corroborating evidence such as direct observations, we generalize our results with some caution.
Also, our findings about persuasion, team structure, and credibility are worthy of further study since we did not investigate or control for organizational and corporate culture.
Still, the results of this study do corroborate with recent published opinions of practitioners .
Lastly, while we focused our interviews mainly on empirical usability testing, other methods like heuristic evaluation did arise in our open-ended discussions with interviewees, which made it clear that similar challenges exist for usability inspection methods .
Although recognition of the value of usability has been growing in industry, our study has highlighted fundamental challenges in usability practice in complex domains.
We have learned about the strategies used by usability professionals to cope with domain-related challenges, but our findings suggest that for usability practice to truly succeed in complex domains, perhaps long-term educational changes are needed.
In future work, we will follow up on the insights gained from this initial study to further understand the relationship between domain expertise and usability expertise and focus on improving pedagogy.
We plan to develop and implement a new curriculum for introductory HCI classes that takes into account the demands of complex domains.
Our long-term research goal is to eventually bring together perspectives from designers, software developers, requirements engineers, technical writers, testers, and others involved in the software development and evaluation process.
With this broader view of software development in complex domains, the HCI community can be better equipped to tackle the challenge of designing for domains of the future.
We thank Barbara Endicott-Popovsky, Kelly Franznick, Jonathan Grudin, David Hendry, Shaun Kane, Judy Ramey, Tristan Robinson, Axel Roesler, Jean Scholtz, Jim Thomas, and Gayna Williams.
This work was supported in part by Intel Research, Microsoft Research, and the Social Sciences and Humanities Research Council , Canada.
Engineering for Complex Interactive Systems Development.
Proc Human Systems Integration Symposium, 1-13.
Gould, J. and Lewis, C.  Designing for usability: key principles and what designers think.
Gulliksen, J., Boivie, I. and Goransson, B.
Gulliksen, J. and Sandblad, B.
Hudson, W.  User-Centered Survey Results email posting to CHI-WEB@ACM.ORG.
Millen, D. Rapid ethnography: Time deepening strategies for HCI field research.
In: Schifferstein, H. and Hekkert, P.
Thousand Oaks, CA: Sage Publications.
Schuler, D. and Namioka, A.
