In this paper, we describe an experiment designed to evaluate the effectiveness of three interfaces for surveillance or remote control using live 360-degree video feeds from a person or vehicle in the field.
Video feeds are simulated using a game engine.
While locating targets within a 3D terrain using a 2D 360-degree interface, participants indicated perceived egocentric directions to targets and later placed targets on an overhead view of the terrain.
Interfaces were compared based on target finding and map placement performance.
This work can assist researchers of panoramic video systems in evaluating the optimal interface for observation and teleoperation of remote systems.
The video feeds from the mobile system are usually monitored in real-time and require significant vigilance to examine their contents.
The design of the view interface can be essential in providing the effectiveness of observation.
An interface that provides observers with a complete view at a single glance without leading to perceptual distortion of spatial information would represent an improvement.
Human eyes have a horizontal field of view  of approximately 200 degrees, and so displaying a 360-degree panorama to a human observer requires compressing the display horizontally.
The resulting horizontal distortion could disrupt the viewer's ability to accurately perceive spatial relationships between multiple objects in the camera's view.
Egocentric  directions of objects in the display will not necessarily correspond to the egocentric directions of the objects relative to the person wearing the camera.
Human spatial orientation is thought to be largely based on the egocentric directions and distances to known landmarks .
Misperception of those egocentric directions could result in large errors in determining one's position within a remembered space.
In light of the potential disruption of normal spatial cognitive processes, the interface should augment the view to leverage our natural sense of presence and spatial awareness.
In this paper, we describe an experiment investigating the effectiveness of three designs for a 360degree view interface, part of an ongoing project to build a wireless 360-degree life-sharing system.
A surveillance system typically involves monitoring multiple video feeds of cameras in an extensive area.
We can divide this system into two categories: stationary systems and mobile systems.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The two approaches to producing 360-degree video are combining images from multiple cameras, each with limited field of view , and using a single panoramic camera.
The first approach requires use of software techniques such as image blending , piecewise image stitching , or 2D projective transformation .
The single camera approach may use a fish-eye lens , omnidirectional camera , or a conic mirror camera .
This approach is similar to piecewise image stitching, and the choice of multiple cameras enabled tight control of distortion, which is reduced by using multiple cameras with small FOV .
The interfaces were designed to maintain the same size of objects in the 3D scene across all three interfaces.
The primary reason to transform a 360-degree view to a 2D flat interface is to offer observers the ability to use standard computer displays and to enable monitoring of multiple video feeds simultaneously .
The 360-degree interface should help observers attend to both center and peripheral information and maintain spatial orientation within the displayed environments.
Past research has used a variety of interfaces to display a 360-degree video feed on a single monitor.
Three examples highlight the diversity of interface designs.
Kadous et al , for a robot search and rescue system, displayed a main front view with smaller additional views  arranged around the border of the main view.
Meguro et al , for a mobile surveillance system for an autonomous vehicle, presented two views, each with 180degree FOV .
Greenhill and Venkatesh , for a mobile surveillance system for city buses, presented a single uncut 360-degree view.
A remote 360-degree surveillance system allows the surveillance operator to detect objects of interest in any direction, thereby exceeding the abilities of the camerawearer.
For example, a surveillance operator could detect a navigational goal or target behind the wearer and direct him/her to approach the goal.
Alternatively, the operator could remember the goal location for later use.
In both of these tasks, performance might be impaired by displaying 360-degree video on a 2D monitor because of disruptions in judging egocentric directions.
Accurate perception of the egocentric direction of a goal relative to the camera-wearer requires mapping the 360degree image onto a representation of the wearer's body.
When displaying a 360-degree view in a single, uninterrupted image on a monitor, an object appearing on the far left or right of the image is actually directly behind the camera-wearer.
This mapping between display and body coordinates is unnatural and will likely cause errors in estimating egocentric object locations.
Adding visual borders to the image might improve judgments of egocentric direction by providing multiple points of reference between the image and the wearer's body.
The first interface, "90-degree x 4," is a combination of four views: front, left, right, and rear.
As illustrated in Figure 1a, each view has 90 degree FOV and is placed 10 pixels apart from the other.
The rear view is placed underneath the front, left, and right views.
This first interface is designed based on the common size of FOV for a video game with additional views to cover 360 degrees.
The borders of the four views provide visual landmarks indicating the camera boundaries at the front-left, frontright, back-left, and back-right relative to the user's heading within the virtual environment.
We designed this interface based approximately on the natural horizontal FOV of human eyes.
The left and right borders of the images provide visual landmarks indicating the camera boundaries directly left and right relative to the user's heading within the virtual environment.
The last interface, "360-degree x 1," is a single 360-degree panoramic view as illustrated in figure 1c.
We believe this interface may reduce visuospatial working memory load since the views are grouped into a single element.
While this interface has been perceived by some to be the obvious "winner" because of its familiarity, the results below suggest that a careful analysis is merited.
The left and right borders of the image both provide visual landmarks indicating the camera boundary directly behind the user's heading within the virtual environment Each interface was displayed on a 22-inch monitor with the participant sitting approximately one foot away, yielding a visual angle on the eye of ~30-40 degrees horizontally and ~10 degrees vertically.
Errors in judging egocentric directions to objects may or may not lead to errors in generating a cognitive map of the environment, and different spatial cognitive theories make different predictions regarding this relationship.
For the purposes of this study, a cognitive map is considered to be a mental representation of object-to-object, or exocentric, locations in the environment.
A cognitive map is required when retrieving remembered locations, such as when drawing a map of the environment or navigating to a remote goal.
If accurate perception of egocentric direction is necessary in order to construct an accurate cognitive map, then there should be a direct relationship between egocentric and exocentric errors.
That is, misperception of a target's egocentric direction should lead to errors in placing that object within a cognitive map .
This would result in a positive correlation between egocentric pointing errors made within the environment and map placement errors made when reconstructing the layout from memory.
However, if exocentric relationships are stored independently of egocentric directions, then errors in egocentric and exocentric judgments should be unrelated.
If cognitive maps are constructed on the basis of object-toobject relationship , then an accurate cognitive map could be formed even if egocentric directions to individual objects are misperceived.
This would result in no correlation between egocentric pointing errors made within the environment and map placement errors made when reconstructing the layout from memory.
These alternative hypotheses are also tested in the current experiment.
The goal of this project is to identity which of three design interfaces offers the best spatial task performance in an observed remote environment.
For this study, active navigation is used instead of passive observation since active navigation benefits peripheral perception in a large FOV display .
Participants performed several spatial tasks using each interface, and their performance was recorded and used to determine the effectiveness of the interfaces.
The best interface should provide appropriate utilization of the edges of the display, including the corners, as is commonly used in first-person shooter games to display status information, and should promote accurate judgment of egocentric object directions as well as accurate creation of a cognitive map of the virtual environment.
The virtual environment used in this study was created using the graphics game engine Irrlicht  with C++ and OpenGL.
Our experiment utilized a 22-inch 3M multitouch display to present the views from virtual cameras of the game engine and receive participants' responses in given tasks.
Participants used the keyboard arrow keys to navigate through the virtual environment.
For each interface, the experiment was broken up into three phases, a familiarization phase, a target search phase, and a map reproduction phase.
Interface order was counterbalanced to prevent order effects from contaminating the analyses.
The familiarization phase occurred at the beginning of each interface session.
During familiarization, participants were given 5 minutes to familiarize themselves with the navigation controls and the interface.
Figure 2, left: Participants tapped 10 red barrels in the environment to "clear" them to green.
At right: Compass rose, showing front, right, left, and rear, where a participant has tapped to indicate that a target barrel sits about 100 to the right of her.
Original 3D model at left created by HORSE-A-F; used with permission.
During the target search phase, participants had 10 minutes to locate 10 targets .
Participants were instructed to select the target as soon as it appeared anywhere on the display.
To select a target, the participant tapped it with a finger, and the target color changed from red to green .
To accommodate colorblindness, red barrels also lost their radioactivity logo when touched.
Immediately after selecting the barrel target, participants were asked to identify the target direction relative to their current heading in the virtual environment by tapping a compass rose , herein referred to as the pointing response.
The map phase began after participants had located all 10 targets or after 10 minutes had elapsed.
Participants were asked to place 10 targets on an overhead map , herein referred to as the map response.
Barrels were not uniquely identifiable except by their spatial location.
After placing all 10 targets on the map, participants were introduced to the next interface and the entire process was repeated.
Three different target layouts were created and randomly paired with the three interfaces so that each participant experienced all three layouts.
The primary dependent measures were errors committed on pointing and the map responses.
We also recorded the total number of targets found during the target search phase, the total time required to find all targets during the target search phase, and the egocentric directions and distances of selected targets during the target search phase.
To gauge their ability to understand a video-game-like virtual environment and navigate using the keyboard control, participants were asked for number of hours of video game playing per week.
The median number of hours was one; most participants did not routinely play video games.
Data from two of the 20 participants were excluded from all analyses because their pointing errors were extremely high  and inconsistent with errors made by the remaining participants.
Across all three interface sessions, 13 of the 18 participants found all 10 targets.
The remaining 5 participants sometimes found only 8 or 9 targets.
Participants who found all 10 targets spent 5 minutes on average.
However, times could range from 2 to almost 10 minutes.
The frequency distribution of actual target directions at the time they were selected is shown in Figure 4.
Directions displaced rightward from the participant's heading at the time of selection were arbitrarily labeled as positive angles, and leftward directions were labeled as negative angles, where 0 degrees is straight ahead and 180 degrees is directly behind.
In all three interfaces, the peak frequency occurs at 0 degrees and decreases with increasing distance from 0 degrees.
This indicates that participants tended to select targets when they were near the center of the display.
Furthermore, the distributions of target directions vary across interface.
When using the 90-degree x 4 interface, 52% of targets were selected when they fell in the center 45 degrees of the display.
Considering an even smaller window defined by the center 15 degrees of each interface accounts for 31% of targets in the 90-degree x 4 interface, 23% of targets in the 180-degree x 2 interface, and 15% of targets in the 360degree x 1 interface.
These differences in target direction distributions indicate that the interface influenced participants' focus of attention, such that attentional distribution across the display was broadest in the 360degree x 1 interface and most narrow in the 90 x 4 interface .
Target distance  affected the size of the target in pixels within the interface.
The size of the target was expected to have little impact on pointing performance , since the pointing action is based on the horizontal position of any green barrel pixels on the display.
As predicted, target distance was not significantly correlated with pointing error, r = .135.
However, target distance was positively correlated with the time required to indicate the target's direction on the compass rose, r=.570.
Reaction time was evaluated using a log of participants' actions in the virtual environment.
This log was an output text file that contained participants' directions and positions that were recorded every second during the experiment.
The reaction time was analyzed by replaying this data and then measured the time delay between target appearance and target selection.
The differences of average reaction times were small across three interfaces  as shown in Figure 5.
Reaction time was analyzed in a one-way repeatedmeasures ANOVA to determine the effect of interface on reaction time.
The main effect of interface was not significant indicating that reaction times did not differ across the interfaces, F=0.871, p = 0.428.
Absolute pointing errors were evaluated in order to assess the accuracy with which participants could map the 360 degree images onto egocentric target directions relative to the participant's heading in the virtual environment.
Pointing errors were computed by finding the minimum absolute difference between the actual angle of the target and the angle indicated by the participant.
Pointing errors  otherwise were generally low with approximately 75% of pointing responses being within 20 degrees of the actual target direction.
Absolute pointing error was analyzed in a one-way repeated-measures ANOVA to determine the effect of interface on pointing error.
Post-hoc analyses indicated that pointing errors in the 90-degree x 4 and 180-degree x 2 interfaces did not differ from one another , but pointing error when using the 360-degree x 1 interface was significantly greater than when using the 90-degree x 4  and the 180-degree x 2 interface .
Map error was calculated as the mean Euclidean distance between the participant's selected locations on the map task and the actual target locations.
In many cases there was a clear correspondence between the barrel placed by the participant and the actual location of a barrel in the environment .
In other cases, participants placed barrels in locations midway between two actual barrel locations .
Because the barrels were non-unique, it was impossible to determine which barrel response corresponded to which actual barrel location.
As such, we calculated the sum of squared distance errors for all possible pairings of barrel responses with actual barrel locations for each participant on each map.
The barrel pairings which minimized the sum of squared errors  were used to calculate average barrel placement error .
The map task was challenging for some participants, and map error varied considerably across individuals.
The virtual environment was 360 feet x 360 feet in dimension.
Figure 7: Two maps showing large differences in map error.
The top map shows targets placed highly accurately, while the bottom map shows a more typical placement.
Pairings show the map error scoring method, minimizing the sum of squared errors across all possible pairings.
Original 3D model created by HORSE-A-F; used with permission.
Thus, individuals varied but were always better than chance performance.
Also, because participants varied in how far they travelled through the environment during the target search phase  it could be posited that a longer travel path would lead to more exposure to relevant landmarks and object-toobject relationships, which in term may lead to more familiarity with the map and lower map error.
However, map error and travel distance were not significantly correlated, r = -.172 Map error was analyzed in a oneway repeated-measures ANOVA to determine the effect of interface on map error .
The main effect of interface was not significant, indicating that map errors did not differ across interfaces =1.589, p = .219.
Survey results  serve as additional data for triangulation of participants' experiences using the interfaces.
The remaining responses are evenly split between the 90 degree x 4 interface and the 180 degree x 2 interface.
When comparing these preferences with performance on the pointing and map tasks, Q1, Q2, and Q3 results match the pointing error performance data, in that participants performed significantly worse when using the 360-degree x 1 interface compared to the other two interfaces.
Q3 highlights a slight difference between self-reported preferences and pointing errors: a majority of participants preferred 90-degree x 4 over 180-degree x 2 for accurate pointing, even though their pointing error performance did not significantly differ in the two interfaces.
This result suggests that the 90-degree x 4 may be preferred for reasons other than the tasks themselves, such as familiarity of the 90-degree FOV from first-person shooter video games.
Previous work has shown that video game experience improves general spatial skills such as spatial allocation of attention and mental rotation .
We investigated whether individual differences such as video game experience influenced performance on the pointing and map tasks.
Participants with greater or equal to 3 hours per week of video game experience were considered gamers, and those with fewer than 3 hours per week of video game experience were considered non-gamers.
Although the primary analysis of map performance data revealed no differences between the three interfaces , we also considered the possibility that there may be individual differences in the effect of the three interfaces on map error.
To evaluate this, we divided participants into two groups based on their average map error.
A cutoff of 25 feet was used to distinguish participants who committed high and low map errors.
This cutoff was chosen because it was near the median error score, and because there was a large gap between the next largest and next smallest error scores.
Map errors were analyzed in a mixed-model repeatedmeasures ANOVA with terms for map score  and interface.
The interaction between the two variables was not significant, F=2.143, p=.143, nor was the main effect of interface , indicating that participants who scored high or low on the map task were not differentially affected by the three interfaces.
These results show that the lack of significant difference in map error across interfaces is not likely due to individual differences.
In all three interfaces, participants preferred to allocate their attention in the direction they were moving.
In the 360 degree x 1 interface, participants were able to spread their attention horizontally over a relatively wide area of the display, centered on their heading.
In the 180 degree x 2 interface, participants needed to attend to the upper 180 degree view in order to center their attentional focus around their heading, which reduced their ability to detect targets in the far periphery .
In the 90-degree x 4 interface, participants focused their attention on the central  view, and the visual borders around the central view restricted their attention to that view.
Furthermore, since the reaction times between target appearance and target selection were not influenced by different interfaces, it suggests that participants were able to focus their attention to detect targets in the periphery of the wider view.
We interpret these findings as evidence in favor of the 360 x 1 interface, which allowed participants to make better use of the 360-degree view of the environment.
Pointing errors when judging the egocentric directions of targets also depended on the interface.
Errors were lowest in the 90 x 4 and 180 x 2 interface, compared to the 360 x 1 interface.
This difference might be due to a benefit conferred by the visible image borders, which could serve as landmarks to identify key orientations relative to the participant's heading.
However, the additional borders in the 90 degree x 4 interface did not lead to superior pointing performance compared to the 180 x 2 interface.
One possible explanation for this is that the borders in the 180 x 2 interface were more helpful than those in the 90 x 4 interface because they distinguished between the front and back hemi-fields.
The front-back distinction in body coordinates is more salient than the left-right distinction .
Unlike the pointing errors, which favored the 90 x 4 and 180 x 2 interfaces, map errors did not differ as a function of interface.
This suggests that participants were able to create a cognitive map of the environment equally well in all three interfaces.
The distinct error patterns in the pointing and map tasks suggest that the processes might be dissociable.
Specifically, the data indicate that cognitive maps may have been formed on the basis of exocentric  directions rather than egocentric directions.
This conclusion is contrary to the notion that cognitive maps are formed by integrating egocentric directions with perceived self-position and orientation .
The lack of correlation between egocentric pointing errors and map errors provides further evidence for the dissociation between perception of egocentric object directions and creation of a cognitive map.
During the target search phase, participants attempted to find 10 target barrels distributed throughout the environment.
Upon finding each barrel, participants judged its egocentric direction relative to their heading in the virtual environment.
The actual egocentric directions of selected targets varied as a function of the interface used to display the 360-degree view of the environment.
Participants tended to select targets close to their heading at the time of selection, and this tendency was most pronounced in the 90 degree x 4 interface, somewhat reduced in the 180 degree x 2 interface, and further reduced in the 360 degree x 1 interface.
One possible explanation for this finding is that image segmentation affects spatial allocation of attention.
We plan to extend our findings on the usage of peripheral views to evaluate the mechanism underlying the observed differences across the three interfaces.
By experimentally placing targets in the periphery and measuring eye and head movements, we will be able to evaluate whether selected target direction is dependent on eye gaze direction.
Adding visual boundaries  on a wider view interface such as 360-degree x 1 may significantly improve the ability to determine target directions.
However, it may also interrupt the distribution of attention across the display.
This enhancing technique will need to be carefully investigated to the extent of designing an effective interface.
We also plan to expand our work to a passive viewing situation rather than active navigation, since passive viewing corresponds more closely with a real-world surveillance task .
Additional field agents can be added for observation to explore the maximum number of agents that can be observed reliably and the ability of an observer to build a cognitive map successfully by integrating 360degree views from multiple field agents.
Results of such a passive study may differ significantly from the current study given Noe's suggestion that the ability to act on an environment is critical to perception of it .
An important extension of this work will be to evaluate how experience and training with each interface affects task performance.
Participants in the current study experienced each interface for a total of 15 minutes , but professional operators might acquire hundreds of hours of experience with a given interface.
Therefore, more extensive training studies are needed before the design implications can be fully realized.
The results of the current study that favor those with more video game experience suggest that additional time with a novel interface may lead to higher performance and an increased sense of naturalness.
This idea is consonant with early prism glasses research that showed that over time participants could adapt and perform normally even if their right-left and up-down were reversed or distorted .
A practical contribution of this work is the identification of a previously unreported tradeoff that designers face when choosing the optimal interface for observation and teleoperation of remote systems capable of capturing live 360-degree video.
If accurate judgment of egocentric directions is of critical importance to the task, then the 90degree x 4, the 180-degree x 2, or another interface with visual angle markers or boundaries will be most appropriate.
However, if surveillance of the entire display is the primary task, then the 360-degree x 1 interface, or perhaps another seamless interface, would be more appropriate, since it would allow for the broadest distribution of attention.
This applies to situations in which an observer provides a simple but fast alert about the presence of a person or object near the person carrying the cameras, but egocentric location is of secondary importance .
This project represents a first attempt to determine the necessary display characteristics that allow viewers to correctly interpret 360-degree video images displayed on a 2D screen.
By examining performance on two spatial tasks and user preferences using three interfaces, we have established that the best design of the interface is not obvious and have offered practical guidance for practitioners creating such displays.
Also, we have revealed evidence that egocentric and exocentric spatial tasks may be dissociable, which has broader implications for the design of any navigation system or virtual environment.
Foote, J. and Kimber, D., FlyCam: Practical panoramic video and automatic camera control.
Franklin, N. and Tversky, B. Searching imagined environments.
Greenhill, S. and Venkatesh, S. Virtual observers in a mobile surveillance system Proceedings of the 14th annual ACM international conference on Multimedia, ACM, Santa Barbara, CA, USA, 2006, 579-588.
Hirose, M., Ogi, T. and Yamada, T. Integrating live video for immersive environments.
Liu, H., Javed, O., Taylor, G., Cao, X. and Haering, N. Omni-directional surveilance for unmanned water vehicles 8th International Workshop on Visual Surveillance, 2008.
Meguro, J., Hashizume, T., Takiguchi, J. and Kurosaki, R., Development of an autonomous mobile surveillance system using a network-based RTK-GPS.
Park, J. and Myungseok, A.
A novel application of panoramic surveillance system IEEE International Syposium on Industrial Electronics, Seoul, Korea, 2009, 205-210.
Control in Human Factors and Ergonomics Society Annual Meeting, , 1411.
Schmidt, M., Rudolph, M., Werther, B. and Furstenau, N., Remote airport tower operation with augmented vision video panorama HMI.
Some preliminary experiments on vision without inversion of the retinal image.
Panoramic video capturing and compressed domain virtual camera control Proceedings of the ninth ACM international conference on Multimedia, ACM, Ottawa, Canada, 2001, 329-347.
Szeliski, R. Image mosaicing for tele-reality applications DEC and Cambridge Research Lab Technical Report, 1994.
A system for real-time panorama generation and display in teleimmersive applications.
Vincent, L. Taking Online Maps Down to Street Level.
Discriminative conditioning of prism adaptation.
Xiong, Y. and Turkowski, K., Creating image-based VR using a self-calibrating fisheye lens.
