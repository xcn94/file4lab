We use the term user model to refer to a finite state machine  representing the user's understanding of a device .
In contrast to the user model, we use the term target model to refer to the FSM representing the behavior of the device, which the end user is trying to learn .
The user may attain such a model if her multiple user models progressively approach the target model, while interacting with the device to learn it.
Relatively little attention has been paid to how user models evolve as a part of a user's learning process.
FSMs have long been used to represent mental models .
In this paper, we build on the past work in FSMs, contribute an edit distance metric for comparing the differences between FSMs, and apply this to represent the process of how a user learns to use a new device, by tracing the evolution of user models towards a target model.
We study user learning through a progressive comparison of the user model and target model over time.
However during learning, user models may be incomplete, erroneous and contradictory .
It is therefore important to quantify the amount of learning.
This translates to the problem of determining the proximity between the two models.
To quantify the gap between a user model and a target model, we introduce edit distance for measuring behavioral proximity between them.
The edit distance metric is based on two propositions:  A target model can simulate a user model by deleting some edges from the user model.
We define edit distance as the sum of the minimum number of such deletions and additions, as proposed above.
We use a heuristic procedure for manually calculating edit distance between FSMs and employ this procedure on our experimental data.
Our proposed representational technique provides an intensional description of the process involved in learning a new device.
When a user learns to use a new device, her understanding of it evolves.
A progressive comparison of the evolving user models towards the device target model, for analysing learning, involves determining the behavioral proximity between them.
To quantify the gap between a user model and a target model, we introduce an edit distance metric for measuring their behavioral proximity using a bisimulation-based equivalence relation.
We define edit distance to be the minimum number of edges and states with incident edges required to be deleted from and/or added to a user model to make it bisimilar to the target model.
We propose an algorithm to compute edit distance between two models and employ the heuristic procedure on experimental data for computing edit distance between target and user models.
The data is organised into two experiments depending on the device the user interacted with:  a simple device resembling a vending machine and  a close to real-world vehicle transmission model.
The results validate our proposed metric as edit distance converges with progressive user learning, increases for erroneous learning, and remains unchanged indicating no learning.
The rapid proliferation of technology challenges users to quickly become familiar with any new device that they may encounter.
User interface  designers, on the other hand, are challenged to provide UIs that are easy to learn and use.
In order to do so, they need to understand how a user gets acquainted with a new device.
Insight about how a user learns to use a device helps in designing better UIs, creating new interventions to assist learning, quantifying the speed of learning, all of which improve both the efficiency and effectiveness of the user's interaction and experience with the device.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Do successive user models converge for all users?
The key here is not to show a mere convergence of the user models, but to capture and represent the process of their evolution to show that the convergence  follows a user's ability to learn the system.
Such a representation allows designers to identify the problem areas in a UI by differentiating instances when a user model is improving from instances where it is not.
The paper is laid out as follows.
First we provide the conceptual foundations for our edit distance approach.
We then describe related literature that highlights the need for a novel approach.
Then we report on the exploratory study that helped us formulate our solution.
Next, we describe the edit distance approach in detail and demonstrate the computation of edit distance between two models.
We then present our first experimental study that heuristically validates our approach using a simple machine.
Later, we strengthen our findings through another study conducted using a close to real-world vehicle transmission system.
Using our algorithm, we calculate the heuristic edit distances of successive user models, and demonstrate that they converge or diverge for progressive or regressive user learning respectively, validating our metric.
A bisimulation relation is a simulation relation whose inverse is also a simulation.
Formally, we define the simulation and bisimulation relation as follows.
A relation R  X1 x X2 is called a simulation of L1 by L2 if, for each pair   R, 1.
Whenever s1 makes a transition, then s2 makes a transition such that the corresponding pair of next states are also in a the relation R, i.e., if s1 - s1 , then there exists a state a s2  X2 such that s2 - s2 and   R. D EFINITION 0.0.3 .
Simulation establishes that all behaviors in the simulated machine are reflected in the other  machine.
Bisimulation is a two-way simulation that establishes behavioral equivalence between two machines.
The concept of simulation and bisimulation thus allows for behavioral comparison and equivalence respectively .
Successful simulation of the user model by the target model implies that the user's learning is coherent with the behavior of the machine.
However, this does not indicate whether the user has learnt the behavior of the machine entirely.
To ascertain that the user's understanding is complete and coherent, we require a bisimulation relation between the user model and the target model encompassing all the states of both the models.
We now discuss past research that we build upon, and use it to motivate our FSM and bisimulation approach.
Most of the devices that people use are interactive in nature.
An interactive machine can be succinctly described as a control system whose user inputs can  be administered by a user through a UI.
Apart from the UI, an interactive machine also has a set of capabilities that a machine offers to the user.
It is essential for a user to get acquainted with the UI in order to learn its functional behavior and use it effectively .
In our paper, we represent an interactive machine as an FSM, and refer to it as a target model.
An FSM has internal states, transitions between these states and observable outputs associated with each state.
An FSM with outputs is a tuple X, q, , I, Y, H where * X is a set of states, q  X is the start state, I is the set of user inputs and Y is the set of outputs.
An FSM is deterministic if | |  1 for all s  X and a  I , else nondeterministic.
In this paper, we restrict ourselves to deterministic FSMs since we model deterministic devices.
Here, we discuss earlier efforts by others related to the topics of quantitative analysis of UIs, formal methods in HCI, model representation and simulation.
Various approaches have aimed to improve user productivity through quantitative analysis of UIs .
Such approaches require task analysis at a low level and hence are suitable when a user's behavior is either well understood  or error-free .
The inability of these approaches to deal with incomplete and imperfect user behavior  limits their utility for analysing user learning.
We therefore propose a new approach to trace and measure user learning when defined as the progressive convergence of the gap between the target and the user model.
Mental models have long been in use in Learning and Cognitive Sciences .
Moreover, there is considerable debate on whether a mental model in general , or its FSM representation in particular, is in itself a sufficient description of what the user has in her mind.
In our work, the novelty is not in using FSMs to represent mental models, but in representing the evolution of an individual's mental model during a guided learning process.
Most works on guided learning are domain specific.
Cognitive tutors  use separation of memory types into declarative and procedural to provide individualized support.
We, therefore, focus on measuring a user's knowledge specific to learning an interactive system.
Formal methods have been widely used to specify, model and verify user interactions .
Several investigations have reported different methods for identifying and bridging the gap between what users know and what they need to know .
Systems engineering principles have helped to hide the irrelevant complexity of a machine from the UI .
Several of the limitations of automatic UI generation have been described in detail .
Moreover, most of these techniques assume every user behaves in a similar and ideal manner, and therefore do not apply to novice users.
This encourages us to propose a new technique to understand, evaluate and analyse individual user interactions with machines without the assumption of a generic user model.
The representation of a model is a key aspect that determines the effectiveness of any analysis technique.
Different notations like maps, graphs and mathematical specifications have been used to represent user models .
Some of these representations offer poor support for scalability and hinder a detailed analysis of the user models.
Traces of user interactions with a machine form the primary source of data for user modeling and should ideally be analyzed as a time series.
The formal language of automata has been used by others like  to represent both the target and the user model.
This facilitates the use of mechanized techniques like model checking to automatically discover any scenarios that cause any divergence in the behaviors of the two models .
It also allows us to contribute towards safety and due diligence in safety critical interactive systems by identifying any instances in the machine that may lead to incorrect user understanding .
In our paper, we capture and compare the user models of an individual to understand the process of user learning.
Milner used bisimulation to demonstrate behavioral comparison of two FSM models .
In , Combefis and Pecheur used a bisimulation-based equivalence relation to address the mode confusion problem.
They demonstrate the possibility of generating user models for an imperfect user and take into account the deviations in her behavior.
This motivates us to extend the idea of bisimulation-based equivalence to study how users learn a new machine by extracting, not one but multiple user models in sequence as the user interacts with the machine.
To investigate the potential of our proposed idea, we conducted an exploratory study as described below.
Not surprisingly, users trust an unfamiliar machine quickly if it is simpler, due to their ability to predict the machine's behavior .
This trust in the machine's behavior assists users in breaking out of any incorrect user understanding and further learn the machine correctly.
Therefore, we chose a simple machine  for our study.
We labelled the states as q0 , q1 and q2 with observable outputs, A or B, alongside.
The UI consisted of one output text box and two input buttons labelled 1 and 2.
The output display language consisted of letters A and B.
As a subject pressed either of the two buttons, the output would display the previous history of outputs appended by either letter A or B following its deterministic machine behavior.
These strings of letters A and B would remain in the text box over time.
Also, it does not output tea or coffee without accepting any coins.
In Figure 1, the transitions labelled 2 correspond to coin transactions, while the transitions labelled 1 correspond to a user accepting tea or coffee.
Thus, we present an unfamiliar machine to the participants by introducing small changes into a familiar machine.
A confirmatory approach to extract, represent and analyze a user's understanding of a concept allows researchers to validate only a subset of a user's understanding .
We therefore chose the exploratory approach for this study.
The unfamiliarity with the machine compelled the user to construct a model of her understanding in order to be able to successfully predict and use the machine.
The users predicted the next character in sequence after each click of the buttons labelled 1 and 2 and thought aloud what they believed to be the behavior of the machine at regular intervals.
Later, we attempted to represent the user knowledge gathered through verbal responses as FSMs.
From our experiences in this exploratory study, we drew the following conclusions: 1.
The users were able to perform similar tasks better when they interacted more with the machine.
Therefore, we hypothesize that user learning can be represented as a series of user mental constructs.
The uncertainty within user responses to reason about a change in the machine made it difficult to translate well the user narratives of machine's behavior into an FSM.
Therefore, we decided to recruit computer science literate partic-
Investigating evolution of user models can be of tremendous help for reasoning how users learn.
Our proposed measure of user understanding should therefore allow for a rigorous analysis of the evolving user models, which may be incomplete and/or incorrect by nature.
These realizations helped us formulate our edit distance solution approach, as explained in the following section.
Then there exists an FSM Lu obtained by addition operations on Lu such that Lu is bisimilar to Lt by the relation defined by R extended to the added states.
Furthermore, the additions can be restricted to correspondence with only those transitions in target FSM not simulated by user FSM.
We have described the formal proofs for both the propositions in detail in .
Proposition 0.0.4 says that a user model can be simulated by the target model by deleting a limited number  of edges from the user model.
Further, Proposition 0.0.5 says that if a user model is simulated by the deterministic target model, then addition of a limited number  of nodes and edges to the user model would make the two models bisimilar.
Based on this, we define the edit distance and hypothesize that it captures behavioral proximity.
This hypothesis is validated through the experiments in the paper.
Once the user model and the target model are formally captured and rendered, then learning can be operationally treated as the process of evolution of the user model towards the target model.
In most cases, a user model deviates from the target model because of incomplete, as well as, incorrect learning.
For a fine grained analysis of user learning, we need to quantify the gap between these two models.
To measure the behavioral closeness of these models, we rely on the graphtheory technique of edit distance, a quantitative value which represents the gap.
In our work, the user learning process is then hypothesized as a reduction in the edit distance: the progressive convergence of the gap between what the user believes to be the system's behavior  and what truly is the system's behavior .
To address the concerns raised in our exploratory study, we used a bisimulationbased technique to calculate edit distance ensuring that our metric allows analysis of incomplete and incorrect user models.
Next, we discuss the process of making a model bisimilar to another model by performing some edit operations.
In general, an edit operation on an FSM can be any change in terms of deletion or addition of an edge or a node.
But it is easy to observe that the presence or absence of an isolated node  would not affect bisimilarity.
Thus, deletion of a node simply corresponds to deletion of multiple edges modulo bisimilarity.
Therefore, we do not consider removing a node as an edit operation.
We define an edit operation on an FSM as deletion of an edge, addition of an edge or addition of a node with only one incident edge.
It is also easy to see that any two FSMs can be made bisimilar to each other by deleting all edges of one FSM and adding nodes and edges corresponding to the other FSM.
This is a naive way of making two FSMs bisimilar.
Furthermore, this would ensure structural  proximity and not behavioral proximity.
We have had examples of user models from our experiments that were very structurally different from the target model but were behaviorally close to it.
Therefore, we ensure that our chosen algorithm for edit distance modulo bisimilarity not only captures behavioral proximity as opposed to structural proximity, but also restricts the required edit operations using the following propositions.
Through the proof of Proposition 0.0.4 , it is easy to see that the model is unique to a relation defined.
We can define multiple relations and for each relation there is a model.
Let S  denote the set of all such models.
The minimum number of delete operations required to generate Lu from Lu such that Lu is simulated by Lt is denoted as Md .
Now, there exists an Lu , the FSM obtained by making addition operations on Lu so that Lu is bisimilar to Lt by an extension of the relation R to added nodes.
This follows from Proposition 0.0.5.
Denote Ma  as the minimum number of additions required to obtain one such model among all possible bisimilar models.
Although it is computationally difficult to find the edit distance between two arbitrary FSMs due to the number of possible relations from states of one automaton to other, it may be found easily for small FSMs by associating the start states of the target and user FSM by the simulation relation and applying heuristic calculations.
The catch here is the start states of the two FSMs normally correspond to each other.
So, we take them as related states of the desired bisimulation relation.
We illustrate the heuristic calculations with an example case taken from our experimental study I.
Our example is described by the series of Figures 2, 3 and 4.
We denote the user and target model in consideration as Lu and Lt in Figure 2 and find the edit distance between them.
This is done by recursively associating states of Lu to those of Lt and deleting edges from Lu as necessary.
The start state u0 of Lu should unequivocally be associated to the start state of Lt i.e.,   R. This association is correct with respect to 1 1 the transitions u0  u0 and t0  t0 and hence we keep these 2 2 transitions intact.
This can be clearly seen in Table 1 that the user model is missing a state equivalent to state t1 of the target model.
We setup a simple within-subjects experimental study to validate our proposed measure of user learning.
We presented participants with a newly created unfamiliar machine and tasked them with learning its functionality by interacting with it.
Each participant was routinely asked to self-report her impression of the functionality as an FSM, which represented her user model for that round.
Over the course of an entire interaction, several rounds of user models and one final user model were captured.
We compared this series of user models to the target model.
Finally, we derived a trend by comparing each subject's stream of user models against the singular target model.
The reliability of the proposed measure is confirmed by observing that the user models converge towards the target model with progressive learning of the user.
The functional behavior of the machine, that was presented to the participants as a novel machine for learning, was modeled after the FSM described in Figure 1.
The software also captured experiment duration, inputs provided, outputs generated, and the timing of such events.
We used this information along with the self-reported user models for data analysis.
We captured the demographic information of each participant and provided them with an overview of the experiment.
A set of two questions were then asked:  Have you studied FSMs before?
After obtaining her consent, the subject was lead into the experiment.
For the experiment, we initially trained the participant using a counter that allowed her to increase or decrease the displayed number by one ranging from 0 to 4 with the help of two buttons.
The UI of the training machine was similar to the one described in Figure 1.
Now after making Lu and Lt bisimilar, we calculate the edit distance as the sum of the total number of deletions and the total number of additions = 1+4 = 5,
After the training period, all participants were instructed to learn the functionality of the new machine, which was about to be displayed to them, by interacting with it.
Operationally, each interaction meant clicking on the buttons labelled 1 or 2, observing the corresponding outputs in the form of letters A and B, and studying the history of earlier outputs .
The participants were informed that while they were interacting with the machine, they would be interrupted after every 3 interactions, each interaction being one click, and asked to draw an FSM that represented their understanding of the machine.
This drawing served as their user model for that round.
The subjects would also have to answer the question: "How well does your user model represent what is in your mind?"
The possible answers were:  this is a very good representation of what I have in my mind,  this is a partial but a fair representation of what I have in my mind, and  this is an incomplete representation of what I have in my mind.
We allowed the participants to interact with the machine for any number of rounds.
When the participant felt that she had a complete understanding of the machine, she could stop interacting and draw a final FSM, which was labelled as final user model  for that subject.
To conclude the experiment, participants answered 3 final questions:  was this machine new to you?
13 of the 20  felt that when learning a new machine, it is the interactions with the machine  that is important for better understanding the machine.
The remaining 7 felt that interactions and the time spent on the machine contribute to learning.
However, when asked about how they have learnt about the functionality of a new machine in the past, 6 of these 7 said that they would consult a manual, and 1 said that he would look for a pattern.
All agreed that they would indeed interact and explore the machine to learn it.
Our data revealed that every participant spent an average of 9:31 minutes in learning this new machine .
All interacted with the machine for at least 3 rounds; 15 completed 5 rounds, 7 completed 6 rounds, and only 1 person went up to round 8.
The average time spent in each round was typically around 1:56 minutes.
The participants self-reported their user models as handdrawn FSMs .
Almost all  had the output symbols  as states, and input buttons 1 and 2 as the transition arcs.
One participant chose to represent the state machine with multiple letters .
Structurally, there were dissimilarities between the hand-drawn models across different users, but common features included A as the start state, at least 2 states, 4 transitions, etc.
We computed the edit distance for each user model using our algorithm and plotted the trend against the multiple rounds of interaction for each participant in Figure 6.
The overall convergence in the trend of edit distances validated our metric.
A detailed comparison of the self reported user models and the resultant trend of edit distance can be found in the Discussion Section of this paper.
All 20 participants had previously studied FSMs in their university curricula.
In addition, 18 of them felt comfortable in using an FSM to represent their mental understanding of the machine.
Most of the participants  felt that the machine was sufficiently new to them.
Although the remaining 2 participants suspected a partial exposure to a similar device in their past, neither of them guessed the FSM correctly.
The trend of edit distance as it converges for different users across several rounds of user interaction.
The dotted line indicates that the average of edit distances calculated for models of all users during the corresponding rounds of interaction shows a monotonic convergence.
A sequence of user models self-reported by a participant during study I.
Each model represents her understanding of the machine represented in Figure 1 at different rounds of interaction.
The participant was shown a simple FSM with 2 states and 1 transition, and was asked to think aloud to explain it.
After ensuring a correct understanding of the concept of an FSM, the participant received training on how to represent an interactive behavior as an FSM.
We used the same training example of a number counter, taken from the first study.
The participants were provided with a brief explanantion of a VTS model having a few main gear levels and a few sublevels within each of the main gear levels.
We informed them that we would be using a variation of a VTS with 3 main gear levels: low, medium and high, and each level would have a minimum of 1 and a maximum of 4 sub-levels.
It was possible to separately identify all the sub-levels within a main gear level through sufficient interaction.
The participants were tasked to learn the behavior of the machine  by interacting with it for an unlimited number of rounds, where each round consisted of four clicks.
They were free to distribute the 4 clicks in each round amongst the 4 buttons on the UI  as they saw fit.
Then, they were asked to self-report their understanding of the machine as an FSM, one on each sheet of the paper with the possible states indicated on them.
We notified the participants that the most succint representation of the VTS would not take more than 9 states; but they were free to utilise all 16 states shown on the provided sheets.
A round of interaction was complete when a participant answered the following questions about her user model for that round:  What fraction of the machine's behavior did she understand till then?-  0-25%  25-50%  50-75% or  75-100% and  How well did the FSM represent her understanding of the machine?-  poor  fair but partial or  fair and complete.
The response of the former question reflected the gain, if any, in participant's understanding of the machine; while the latter reflected the efficiency and completeness of her representation, similar to the first study.
At the end of the second and the final round of interaction, we tested the participants' understanding.
We asked them to indicate, using the smallest number of clicks, how to transition between two states of the model, e.g., start state of the machine to any of the medium sub-levels.
These responses reflect if the participant learnt anything during the several rounds of interaction or not.
Each participant answered the same set of 3 final questions, taken from the previous study, after her interaction session with the machine was complete.
The protocol for this study remained largely similar to the previous study, except for our approach for extracting the user models.
From the different techniques available for extracting user models, such as structured interview, questionnaire and talk back, an open method is less useful than a more structured approach when the complexity of a machine increases .
The digital VTS model presented to the participants, even with the variations meant to simplify it, was sufficiently complex for an unfamiliar user and necessitated a structured approach to extract user models.
Therefore, we provided partial guidance to the study participants in identifying the states of the target model.
The participants received this guidance in the form of sheets of paper having a minimum of all the states, which were necessary to represent the target model, printed upon them.
They were expected to use these sheets to self-report their understanding of the machine, thus reducing the task of drawing FSMs to identifying the valid states and mapping the transitions between them.
This helped the researchers to minimize the noise captured within a user model due to a participant's inefficacy, while reproducing her understanding as an FSM.
We provided a few superfluous states on the sheets to  ensure that particpant were not influnced by the number of states marked on the sheets and  allow them to express their understanding as an FSM that may be structurally different, but bisimilar to the VTS target model.
While conducting the study, we started by collecting the demographics of the participant and her prior knowledge about FSMs and her ability to draw one.
9  participants had studied FSMs and thought they would be able to plot an interactive behavior as an FSM.
Except one, all participants had an average of 6.1 years driving experience .
The brief explanation of a VTS model  ensured that each participant had at least some knowledge of how a car gear box works.
The edit distances for all user models, computed using our algorithm, collected in the experimental study II performed using the vehicle transmission model.
We have grouped the users as per their final understanding of the machine, reflected by the number of correct user responses.
X axis denotes the normalised duration of time that each participant spent trying to learn the machine and actually made any changes to her user model.
One of the participants, with prior driving experience, reported the behavior to remotely resemble a game to launch a rocket at different speeds depending on the player interaction with the knob to release the rocket.
She also tried to relate it to a new type of clutch that decides how to shift gears, depending on how hard it is pressed.
5 participants reported the number of interactions allowed with the machine to be the primary reason for them to understand the behavior of the VTS model, while 6 participants gave an equal priority to number of interactions and the time to think between them.
One participant found her knowledge of the previous state where he left the machine to be more important.
When questioned about the preferred way to understand the working of an unfamiliar machine, 9 preferred to interact with it; 2 preferred using a manual, while one said that she would interact with a simple machine but refer to a manual if the machine were sufficiently complex.
The participants interacted with the machine for an average of 28.5 rounds each , with a standard deviation of 11.04.
In the end, all the participants reported having understood most of the machine  and were confident about their representation to be fair and complete, 5 participants stopped interacting once their model was complete, while 7 continued to interact further and used their final few rounds to confirm their previous understanding as represented by their last FSM.
The participants took an average of 55:30 minutes  to interact with the machine before they thought they understood it completely; the minimum was 27:37 minutes while the longest was 108:57 minutes.
We classified the users depending on the correctness of their responses  to perform certain state transitions as requested by the researchers into 3 groups.
We plotted the edit distances for user models, as computed by our algorithm, across the normalised duration of time that each participant spent trying to learn the machine, separately for each group in Figure 8.
The normalised duration excludes the time spent by participants in past few rounds for verifying their previous knowledge of the machine during which they neither changed their user model, nor their confidence about their understanding of the machine or their representation of it.
A detailed discussion of these follows in the next section.
By using the self-reported user models in both studies as our primary source of data, we trace user learning and measure the gaps between the user models and the target model in progressive rounds of interaction.
We quantify this gap using our proposed measure edit distance.
We then compute the edit distances for each of the user models per participant, and plot them against their corresponding rounds of interaction as observed during study I in Figure 6.
The simplicity of the machine used in study I ensured that all the participants learnt some fraction of the machine's behavior through user interaction.
Considering the complexity of the VTS model, we classified the participants of study II into 3 separate groups to facilitate a better analysis of the results as shown in Figure 8: participants who answered all 3 understanding questions correctly, answered 2 correctly, and 1 or none correctly.
The graph in Figure 6 clearly indicates that the overall trend of edit distances converges with every passing round as a user interacts more with the machine.
A similar trend of convergence can be seen in Figure 8a.
This graph reflects how edit distance converges for a user who learns the VTS model correctly .
5 participants from experimental study I learned the machine partially.
This is captured as a converging but non-zero edit distance.
A similar convergence can be seen in Figure 8b  despite their inability to perform all the given tasks correctly.
Our measure successfully illustrates their process of acquiring this partial understanding.
As Figure 6 indicates, a few participants in study I have their trend of edit distances higher than the average.
On examining their user models, we realised that initially the users who contributed to the 2 highest peaks, struggled with the concept of FSMs since they could not distinguish between the outputs and the states of the machine and resorted to plotting their user models as a sequence of outputs as opposed to an FSM.
We also notice an outlier in Figure 8b showing no significant convergence in edit distance despite providing correct user responses to our state transition questions.
A further probe into her user models indicated that the she identified behaviorally similar states as separate without mapping all the necessary transitions between them to ensure bisimilarity.
The replication of behaviorally similar states in a user model, in turn replicates the number of missing transitions, if any.
We consider that a user with a partial understanding and a larger number of states in her model is more likely to be confused than a user with similar understanding but fewer states.
We suspect this to indicate higher cognitive load.
Our metric thus identifies these cases as partially incorrect user learning.
The next peak in Figure 6 comes from a participant who demonstrated an erroneous learning of the machine thus justifying an increase in the edit distance towards round 2.
This participant's user models indicate that she recreated her user model following round 4 when she realised her error, as is evident from the steep decline in her edit distance plot indicating a positive knowledge gain.
Similar instances of erroneous learning can be seen in trends of several users during study II.
The rise in edit distance for one participant in Figure 8a resulted from an erroneous learning which was reflected in her correct, but not the shortest, sequence of clicks for the third task.
Figure 8c especially highlights cases where users either could not rectify their erroneous learning or were unable to learn constructively about the VTS model.
The absence of any convergence in this graph implies no user learning, thus further indicating the reliability of our proposed edit distance.
Thus, our proposed measure edit distance successfully traces cases of complete, partial, erroneous and no user learning.
These trends will allow us to identify users that need some help in learning an interface or UIs that need to be redesigned.
In the future, this approach can be used to investigate the rate at which a user attains her understanding of an unfamiliar device by investigating how fast do the user models converge towards the target model.
Our proposed technique of edit distance is useful for allowing a behavioral comparison between any two models that can be represented as FSMs.
Ideally, edit distance would be the minimum number of edges and states with incident edges required to be added and deleted from the user model such that it bisimulates the target model.
However, it would be computationally very expensive to compute the edit distance by an automated algorithm due to a large number of possible relations between the states of the user model and the target model.
The heuristic edit distance that we computed manually using our algorithm, therefore, may vary from the ideal edit distance and compromise its minimality; however it is logically and scientifically consistent, as shown in Proposition 0.0.4 and Proposition 0.0.5.
A non-deterministic user model can certainly be made bisimilar to the target model by using a naive O algorithm, which essentially deletes G1 edges and reconstructs G2 edges.
However, such an edit distance is not minimal.
The problem of computing the minimum edit distance is NPHard.
On the other hand, efficiently finding the minimum edit distance between two non-deterministic FSMs modulo bisimilarity is an open problem.
While not optimal, our present algorithm performs better than the naive O algorithm and measures the edit distance between two deterministic models.
Although we have defined simulation in the context of FSMs, bisimulation is a general concept used to capture the notion of behavioral equivalence and extends across different domains e.g.
Thus, the idea presented in the paper can be applied even when user interactions are represented in different notations and is not restricted to FSMs.
The paper focuses on a behavioral comparison of user and target models when represented as finite state machines.
We propose the edit distance metric for mathematically capturing the gap between evolving user models and a target model using bisimulation equivalence.
In our experiments, we observed that the edit distance between the user and target model converges with progressive user learning, increases for erroneous learning and remains unchanged indicating no learning.
Thus, the paper demonstrates that user learning can be witnessed, captured, and measured formally, allowing for a better understanding of how users learn to use a new device.
Our proposed edit distance metric is of significant importance to UI designers to identify instances when user understanding is not improving and to develop dynamic adaptive interventions to address them .
A quantified measure of user learning will help researchers to develop techniques that expedite the process of training a user for successfully operating a new device.
In the future, we plan to use our proposed framework for segmentation of the feature space of a device depending on which features a user could understand completely or partially, and those, which he could not use at all.
Ensuring complete knowledge of some features will be crucial to safety critical systems for meeting minimum standards of safety.
Such an effort will also help in identifying new design principles leading to better user interaction.
Mental models in human-computer interaction: research issues about what the user of software knows.
Comb efis, S., and Pecheur, C. A bisimulation-based approach to the analysis of human-computer interaction.
Dhawan, R., M. O., and Borman, M. Mental models and dynamic decision making: an experimental approach for testing system methodologies.
In 24th International Conference of the System Dynamics Society .
Doherty, G., Campos, J. C., and Harrison, M. D. Representational reasoning and verification.
A bisimulation approach to verification of molecular implementations of formal chemical reaction networks, 2012.
Falb, J., Popp, R., Rock, T., Jelinek, H., Arnautovic, E., and Kaindl, H. Fully-automatic generation of user interfaces for multiple devices from a high-level model based on communicative acts.
Fischer, G. User modeling in humancomputer interaction.
Cognitive Science - Lawrence Erlbaum Associates.
Lawrence Erlbaum Associates, Incorporated, 1983.
Harrison, M. Modelling user structures within system specifications.
Harrison, M., and Thimbleby, H. Formal Methods in Human Computer Interactions.
Hermann, M., and Weber, M. When three worlds collide: a model of the tangible interaction process.
Formal analysis and automatic generation of user interfaces: approach, methodology, and an algorithm.
Hinckley, K., Cutrell, E., Bathiche, S., and Muss, T. Quantitative analysis of scrolling techniques.
Ippel, M. J., and Beem, A. L. Mental models as finite-state machines: Examples and computational methods.
Rep. 9911, United State Air Force Armstrong Laboratory, October 1998.
John, B. E., and Kieras, D. E. The goms family of analysis techniques: Tools for design and evaluation.
Johnson-Laird, P. N. Mental models: Towards a cognitive science of language, inference, and consciousness, vol.
