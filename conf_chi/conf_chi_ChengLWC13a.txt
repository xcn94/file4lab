Heatmap visualization of preferred keyboard layouts and positions for three grasp conditions in our 64-user study:  not grasping the devices,  grasping the devices with one hand,  grasping the devices with both hands.
The most preferred keyboard modes are merged+docked, merged+undocked, and split+undocked, respectively.
Multitouch tablets, such as iPad and Android tablets, support virtual keyboards for text entry.
Our 64-user study shows that 98% of the users preferred different keyboard layouts and positions depending on how they were holding these devices.
However, current tablets either do not allow keyboard adjustment or require users to manually adjust the keyboards.
We present iGrasp, which automatically adapts the layout and position of virtual keyboards based on how and where users are grasping the devices without requiring explicit user input.
Our prototype uses 46 capacitive sensors positioned along the sides of an iPad to sense users' grasps, and supports two types of grasp-based automatic adaptation: layout switching and continuous positioning.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Multitouch mobile devices, such as iPhone, iPad, and Android tablets, support virtual keyboards for text entry.
While text entry performance for virtual keyboards has been extensively studied and improved, such as through gestures  and adaptive techniques including predicted target zones  and motion compensation , most work has focused on fixed keyboards.
This paper explores automatic adaptation versus manual adjustment of the keyboard layouts  and vertical keyboard positions .
We asked the participants to report their preferred keyboard layout and position for each of the three grasp conditions.
Figure 1 shows a heatmap visualization of the results.
Most users preferred merged+docked mode when not grasping the devices, merged+undocked when grasping the devices with one hand, and split+undocked when grasping the devices with both hands .
In addition, 98% of the participants preferred two or more distinct keyboard modes, for an average of 2.49 distinct keyboard modes across the three grasp conditions.
Furthermore, among the 17 iPad owners in the study, 76% were not aware that its keyboard were adjustable, indicating a potential discovery problem for manually adjustable keyboards.
We present iGrasp, which automatically adapts virtual keyboard's layout and position based on users' grasps without requiring explicit user input.
We built an iGrasp prototype using 46 capacitive sensors placed on the sides of an iPad, which are connected to an Arduino board and then wired into iPad's serial port.
It is capable of sensing the users' grasp position in 1 cm resolution, and can distinguish between the three grasp conditions.
We conducted the other two user studies to evaluate iGrasp for two adaptation modes.
For iGraspSwitch, our results show that participants strongly preferred iGrasp over manually adjustable keyboards .
Also, the time to type the first character improved by 42%, from 2.57 seconds to 1.49 seconds.
For iGraspPosition, our results show no statistically significant improvement in performance nor preference over iGraspSwitch.
Based on our findings, its possible to implement iGrasp using only four capacitive sensors, with each sensor having a sensing area that covers half of each side of the device and having similar width as our prototype.
The rest of the paper is organized as follows: first, we discuss related work, then describe the first observational user study and finding.
We then present iGrasp design and implementation, and discuss evaluation methodology and results for the second and third user studies.
Last, we conclude with discussion, contributions, and future work.
Modern mobile devices often have different kinds of sensors such as accelerometers, gyroscopes and cameras, to support contextual sensing .
The users' context then can be used for adapting interfaces.
For example, WalkType  uses touch and accelerometer data while users are sitting and walking to built displacement models that can be used to compensate for imprecise input during walking.
Researchers also use accelerometers and cameras to rotate screen orientation .
Visible and invisible key-target adaptation have been widely studied to improve text entry performance of virtual keyboards.
Examples of visible adaption include Himberg et al.
Text Text Revolution  is a typing game that helps users improving typing performance.
The game provides targeting practice, highlights areas for improvement and generates training data for keytarget resizing.
BigKey  expands the key size of the next entry character predicted from tables of single letter and diagram frequency counts .
Invisible input adaptation changes the touch size, shape, and location of keys, but does not modify their on-screen appearances to minimize the possibility of visual distraction.
Each key's target area would change invisibly without crossing other key's anchor.
Because iGrasp changes the layout and position of the entire keyboard, it can augment visible and invisible key-target adaption techniques.
Many sensing technologies have been used to sense grasp.
GripSense  leverages the built-in inertial sensors, vibration motors, and touchscreens of smartphones for grip and pressure detection.
After monitoring device rotation, touch size, and thumb swiping, users' hand postures could be inferred.
Capacitive sensing is commonly used for researching touch sensing input devices  and users' grasp .
Many new technologies are invented based on capacitive sensing.
Touch e  proposes Swept Frequency Capacitive Sensing  which sweeps through a range of frequencies to obtain a multitude of responses to know how a user is touching the object.
Midas  is a software and hardware toolkit for automatically synthesizing capacitive touch sensor that enables designers to build touch sensitive prototype in any shape.
Time domain reflectometry , which originally was used to diagnose cable faults, can be used to locate a users touch on form factors such as guitar strings and conductive ink.
Hand-Sense  uses four capacitive sensors on two long edges to recognize left-hand and right-hand usage.
Our iGrasp prototype uses 46 capacitive sensors.
Pressure sensors  and impedance sensors  have also been used to sense grasps.
Tango  uses 256 pressure sensors to make a hand-size ball as a haptic interface for 3D objects.
The sensors use two layers of electrically conductive strips separated by a compressible foam rubber.
Pressure compresses the two layers, thus increasing the capacitance.
Arrays of piezo-resistive elements have been used identify the grip patterns on smart guns in order to identify the user for safety purposes .
Light and infrared sensors have also been used to sense grasps.
TouchString  concatenates units of LED phototransistor pairs to form a multitouch sensor rail that can surround surfaces of objects such as bottles and mobile phones.
SideSight  uses infrared proximity sensors embedded along the side of small devices to detect the presence and position of fingers.
FlyEye  also uses infrared light and a camera to detect touch and proximity by measuring the changes in light reception through optical fibers that are embedded in the surface.
Rock-paper-fibers  uses a bundle of optical fibers observed by a webcam and recognized how the bundle is shaped and touched by matching the resulting graph with its widget database.
GRASP  proposes a model of human grasping that describes five meaningful factors - goal, relationship, anatomy, setting and properties, and offers a basis and framework for further research and discussion.
BiTouch and BiPad  studies users' holds of tablets and designed bi-manual interaction techniques such as chords and gestures on the bezel of devices.
They collected users' grasps in different postures and orientations by the capacitive sensors on the edges and the back of their phone-sized prototype, and then they used support vector machine  to classify grasps into viewing orientations.
However, our paper focuses on investigating how users' typing experience can be improved by our proposed adaptation -- changing the virtual keyboard layout and position based on how and where users are grasping their tablets.
We designed a study to understand users' preferences for keyboard layouts and positions.
We targeted the portrait orientation because it is a common usage and users' grasp positions would have more variation.
We developed a custom text entry application for iPad that recorded the preferred keyboard layout  and the Y-axis position of the keyboard.
Using the same manual adjustment methods as the built-in iPad keyboard, participants learned to change the keyboard's position by dragging the bottom-right button up and down.
In addition, participants learned to split and merge the keyboard by performing pinch-in and pinch-out gestures on the keyboard.
76% of iPad owners reported that they did not know how to adjust the keyboard, indicating a potential discovery problem for manual keyboard adjustment.
We did not constrain how users grasped the devices.
For example, in the single-handed grasp condition, most users grasped an edge of the tablet, but a few users supported the tablet with their forearms instead.
After participants felt they had found a preferred keyboard layout and position for each condition, the layout and position were recorded.
The average session duration was 10.4 minutes.
Graspables  uses several capacitive touch sensors as discrete, binary input to sense touches.
They built two prototypes that can switch between the multiple functions of a device based on grasp: Bar of Soap switches between different applications, including camera, gamepad, phone, and remote control.
Ball of Soap selects different pitches such as fastball and curveball.
There is further research discussing grasp interaction on mice and pens such as MT mouse  and MTPen .
Table 1 shows the mean and the median of keyboard Yaxis position offset of all participants for each of the grasp conditions.
The Y-axis offset value is 0 for the docked position.
The high standard deviations in all three conditions show that users preferred very different keyboard positions.
The mean of Y-axis position when users grasped the devices by one hand was 245 pixels , and 223 pixels  when users grasped the devices with both hands.
These values may be used by iGrasp as the default keyboard positions.
Figure 1 shows the heatmap visualization of keyboard layout and position preferences for all participants.
Figure 2 shows the distribution of the most preferred keyboard mode for the 64 participants for each of the three usage conditions, showing that the most preferred keyboard mode is different for each condition.
When typing without holding the devices, 74% of the participants preferred the merged+docked keyboard.
This is the default keyboard mode supported on all the tablets that we surveyed, including iPad, Android tablets, RIM Playbook, and Palm TouchPad.
When grasping the device by one hand and typing using the other hand, 88% of the participants preferred merged+undocked keyboard.
When holding the devices using both hands, 70% of the participants preferred the split+undocked keyboard while 25% of the participants preferred merged+undocked keyboard.
Participants chose an average of 2.49 distinct keyboard modes across the three grasp conditions, and 98% of the participants chose at least two distinct keyboard modes.
The detailed distribution is shown in Figure 3.
Adaptive layout switching augments manual keyboard adjustments by helping users automatically switch between different keyboard modes.
It senses the current grasp condition  and then automatically shows the keyboard using the same layout and position that was last used for that grasp condition.
This requires the system to have sufficient sensor resolution to distinguish between different types of grasp.
Also, the system needs to save the keyboard layout and position for each grasp condition.
Although this approach may not always position the keyboard to exactly where the users' hands are, it provides users with the option to manually adjust the keyboard positions and layouts.
Instead of showing the keyboard at the last-used position, iGraspPosition senses the current grasp position and shows the virtual keyboard at that location.
It shows the same keyboard layout that was last used for the current grasp condition.
To avoid constant movement of the keyboard while typing, the system stops re-positioning the keyboard once the user has started typing.
There are two key challenges to this adaptation.
First, the system must be able to locate the grasp position with sufficient resolution in order to accurately position the keyboard.
Second, the system must have an accurate model that maps the user's grasp position to an optimal keyboard position.
Because users' hand sizes may vary significantly, personalized calibration can be used to improve the system's performance.
The trade-off of this approach is that it replaces manual position adjustment so that if the keyboard is poorly positioned due to an inaccurate model, it will always be positioned poorly for that user.
Our goal is to build a tablet-sized device that is capable of sensing the grasp position and distinguishing between different grasp conditions: no grasp, one-handed grasp, and twohanded grasp.
The form factor and weight should be similar to typical tablets to minimize changes in users behavior.
After exploring several sensing approaches, including light sensors and clip-on sensors , we decided to use capacitive sensors which are not sensitive to lighting conditions and can be placed more densely than clip-on sensors.
Our iGrasp prototype consists of the following components: an Arduino Pro Mini 328 circuit board, four Freescale capacitive touch sensor controllers MPR121, an iPad case, and an iPad 2.
As shown in Figure 4, the circuit board and the controllers are placed in the center on the back side of the iPad.
We placed 46 copper foils along both of the long sides of the iPad, with 0.2 cm gaps between the foils.
Each copper foil is 4.0 cm x 0.8 cm in size and is connected to the capacitive sensor controllers, and the Arduino board samples each sensor as binary readings at 60Hz, then transmits the data to iPad via the serial port.
All subsequent processing is done on the iPad, which runs iOS 5.1 and is jailbroken to enable the serial port.
Its weight is 662 g, 18 g lighter than iPad 1 and 10g heavier than iPad 3.
To determine where to display the keyboard based on the sensed grasp position, the user could optionally complete a calibration process.
Our calibration process has each user grasp the top, middle, and bottom third of the iPad and also manually move the keyboard to a comfortable position K for each of the grasps.
We then use a linear regression function generated by a set of  pairs to calculate the keyboard position given a sensed hand position.
We evaluated iGrasp's automatic layout and position adaptations by conducting two typing studies under several grasp conditions and postures: 1. comparing iGraspSwitch, which automatically switches to the memorized layout and position, to the manually adjustable keyboard.
2. investigateing whether iGraspPosition, which additionally positions the keyboard to users' grasp location, improves upon iGraspSwitch.
For both user studies, we measured the initial placement time, which is the time that elapsed between the keyboards appearing on screen until the time that users begin to type.
The more closely the keyboard layout and position matches the users' preferences, the more quickly the users would be able to begin typing and the lower the initial placement time would be.
Conversely, if the keyboard layout and position are not ideal, users would either need to move their hands to accommodate the keyboard layout and position, or need to manually adjust the keyboard to suit their preferences before they begin to type.
We also recorded the total task completion time, and typing speed which additionally captures how well the layout matches the users' preferences.
We used one way ANOVA and paired t-test to see if the result was significantly different.
Furthermore, users reported subjective preferences using five-point Likert scales.
Based on how users held the iPads in our observational study, we divided the 23 sensors on each side into top-half and bottom-half groups, for a total of four sensor groups.
We merge all sensors in each group into a binary ON/OFF reading.
Because all the sensor readings were already in binary form, merging sensor readings is simply an OR operation.
The first one is grasping a tablet by its edge using only one hand.
This results in one or both sensor groups on one side showing ON, but both sensor groups on the other side would be OFF.
In order to compare our automatic keyboard adaptation and the manual adjustment, we are interested in understanding how each approach performs for the following three grasp conditions: no grasp , one-handed grasp , and twohanded grasp .
Specifically, there are 6 possible grasp transitions that users would normally make as they use their tablets in different settings: 0G  1G, 0G  2G, 1G  0G, 1G  2G, 2G  0G and 2G  1G.
At the beginning of the user study, participants went through a 5-minute training session to practice how to adjust the keyboard layout and position.
They also practiced typing tasks to identify their preferred keyboard layout and position for each of the grasp conditions.
During the user study, participants could freely adjust the keyboard layout and position.
In order to have participants start each typing task while grasping the devices naturally, each participant first performed one of the six grasp transitions before starting the task.
For each typing task, participants touched an input text field to activate the keyboard, then typed a phrase that was randomly selected from the Makenzie and Soukoreff phrase set .
If the input field was occluded by the keyboard, the test app would scroll automatically to make it visible.
Each users went through all six possible transitions for the iGrasp condition and for the manual condition, for a total of 12 typing tasks.
The order of iGrasp and the manual keyboard, as well as the starting grasp conditions were counter-balanced.
Participants filled out a questionnaire on demographics and preferences after they finished all the tasks.
The two posture transitions to the nograsp condition  have no data because the participants were not grasping the device.
For all other conditions, the average distance reduced by 53% from cm to cm.
We also found that iGrasp had some improvement in terms of words per minute  and error rate; however, the difference was not statistically significant in our 18-person user study.
A much larger user study may help draw statistically significant conclusions.
In addition, we asked participants to rate the ease of use of both iGrasp and the manually adjustable keyboard on a fve-point Likert scale, and the participants rated iGrasp significantly higher at 4.2 vs 2.9 for manual adjustment.
We recruited 18 participants  from our university population.
The age of participants ranged from 21 to 25.
All were regular computer users and had experience with touchscreen devices.
Three of the participants had their own tablets for more than three months.
The initial grasp distance moved was calculated in the same manner but summed up to the time the first character was typed.
We also ran a two-way ANOVA of techniques  and conditions .
The main effect of conditions is not significant  and the interaction effect of techniques and conditions is not either .
This study focuses on comparing different approaches to position the keyboard.
In order to control for the different layouts  and layout transitions, we focus on the two-handed grasp that supports more usage scenarios, and only use the corresponding split layout that was preferred by the most participants from our first study .
At the beginning of the user study, participants went through a 5-minute training session to practice how to adjust the keyboard layout and position.
Participants were then asked to do calibration in each posture for us to build a personalized model on positioning the keyboard based on each users' particular grasp positions.
Our testing application instructed participants to grasp the tablet at three different positions and asked the participants to adjust the keyboard to their most comfortable position.
Participants then typed a short phrase while the participant's sensed hand position H and the keyboard position K was recorded to build a regression model.
To thoroughly evaluate how well each positioning approach works, we needed the participants to perform typing tasks with a large number of postures that people would normally take when using tablets.
Also, because we were primarily interested in the initial placement time between the different keyboard positions, shorter words that enable more trials would be preferable.
We asked the participants to go through the following sequence of five postures: lying down, lying on one side, sitting and leaning back, sitting, and standing.
For each posture, the participants first picked up the tablet, activated the keyboard, completed a typing task, then placed the tablet down before transitioning to the next posture.
Similar to our second user study, picking up the tablet at the beginning of each posture is important to ensure that the participant would be grasping it naturally, and not affected by the previous task or posture.
The short typing tasks we used was based on the 100 mostfrequently-used words that were three characters long and contained characters on both sides of the split keyboard.
In order to prevent the participants from anticipating where the keyboard would appear, we did not reveal which positioning approach was used and we randomly shuffled the positioning order for each posture.
Each participant needed to go through the posture sequence three times to experienced all possible keyboard positions and posture combinations.
We asked each participant to perform two trials, so each user went through the sequence of postures six times for a total of 30 typing tasks .
After the participants finished the two trials, they completed an additional trial in which the keyboard positioning approach was revealed to them.
We then asksed the participants to fill out a questionnaire and the NASA perceived workload index  .
We also interviewed them to get their feedback.
All were regular computer users and had experience with touchscreen devices.
Six of the participants owned tablets.
Figure 6 shows the average initial placement time and also the initial grasp movement, which is the distance the participants' hands had moved before typing the first character.
The results show that iGraspPosition was the most accurate in placing the keyboard at users' comfortable typing location, causing the users' hands to move the least distance.
The average total grasp movement of iGraspSwitch was 2.58cm.
The average initial grasp movement distance of iGraspPosition was also the lowest at 2.14cm.
The 63% improvement over the default position is statistically significant but is not statistically significant  compared to iGraspPosition.
The results of subjective workload with the NASA TLX procedure are shown in Figure 7.
The result indicates that on average the workload of iGraspPosition and iGraspSwitch are lower and the difference is statistically significant  compared to the default keyboard.
Howerver, there is no significant difference in each subscale between iGraspPosition and iGraspSwitch .
In addition, participants rated the ease of use for each keyboard positioning approach using a 5-point Likert scale.
The difference is significant  between the default keyboard and iGrasp's approaches, but the difference is not significant  between the two iGrasp modes.
To summarize our findings from the third user study, both iGraspPosition and iGraspSwitch approaches are significantly better than the docked keyboard position in terms of task performance, perceived task load, and participants' preferences.
However, we did not observe statistically significant difference between the two iGrasp approaches in any of the metrics we analyzed.
Each sensor should have a sensing area that covers half of each side of the device and have similar width as our prototype.
Four sensors are needed instead of two because we observed some one-handed grasps in our observational study where users rested the bottom corners of the devices on their forearm and grasped the opposite top corner of the devices.
Results from our third user study show that even though continuous positioning had shorter grasp movement distance, participants' typing performance was comparable.
It suggests that users are able to tolerate some amount of positioning error.
To better understand this tolerance, we analyzed each user's grasp position from iGraspPosition keyboard from our third user study.
The optimal grasp position HO was recorded when each user chose their preferred keyboard position.
We then define tolerance as the distance from HO to users' grasp positions at the end of each typing task .
We use the position of the last character typed instead of the first character because users' hands may still be moving during the task, and the ending position is likely to be the more comfortable typing positon.
For each user, we calculated the maximum tolerance across the 10 tasks.
The average tolerance across all users  is 4.2 cm.
If we further drill down and separate the grasps above HO and below HO , we get a tolerance of 4.2 cm when users' grasp above the optimal typing postion, and a tolerance of 0.4 cm when users grasp below.
This large difference suggests that it is much easier to reach below to type on a keyboard that is positioned too low, than to reach above to type on a keyboard that is positioned to high.
Figure 8 shows the cummulative distribution function of tolerance.
50% of the users have a tolerance of 4.0 cm.
In our pilot study, a participant wearing shorts reported that the keyboard position was far from his grasp position.
It turned out that he was resting the device on his legs to support it, and the sensors near the bottom were reporting touch events.
We adjusted the sensor layout and removed the two sensors at the bottom, leading to our current 46-sensor prototype.
We are currently building an iGrasp prototype to support both landscape and portrait usages, and are experimenting with touch point clustering to identify contiguous grasp regions and then applying machine learning techniques to build a more robust grasp classifier.
Capacitive sensors have some inherent limitations.
For example, wet hands and users wearing gloves would be difficult to detect correctly.
When no grasps are detected, iGrasp would present the keyboard used in the no-grasp condition, which is most likely docked, and would provide the same user experience as the default keyboards.
Users in these situations may also have trouble typing on the capacitive touchscreen keyboard.
We implemented real-time keyboard positioning and tested it by our study with additional 5 users .
The system would track users' grasps and move the keyboard to match the users' hand position even when users were typing.
All users who tested real-time positioning commented that the constantly-moving keyboard was distracting, and they could not type correctly due to keyboard drifting caused by slight changes in grasp while typing.
We modified the positioning to stop updating its position after the first key is pressed in our user studies.
When users grasp a tablet, movement such as stretching a thumb to reach a far target may cause the sensed hand position to change.
We used moving average approach which buffered the last 30 sensor readings  and returned the average value to minimized such drifting to improve positioning accuracy.
The second challenge was building an accurate personalized model to map a sensed grasp position to users' desired keyboard position.
During our pilot studies, we used the second order polynomial regression.
By examining users' models built from sets of  pair collected in different postures, we found all models were close to linear, so we used the linear regression in our final prototype to avoid overfitting.
However, two participants in our pilot study reported that the keyboard positions were not accurate.
One was lower than the desired position, and one was higher.
We analyzed users' models again and found that the models in different postures were different.
Therefore, the predicted result may be accurate in one posture, but exceed the user's tolerance region in another posture.
Posture detection may be needed to switch between different models.
The reasons may be that users' eyes relative to the hands and the tablets vary between postures.
Also, the comfortable typing range may also change depending on the relative angle between users wrists and the tablets.
As shown in Figure 9, with the same grasp position, the user can reach the 'Y' key when holding the device at an 90 degree angle relative to his wrist, but cannot reach the same key when holding the device at a different angle.
Although our grasp condition classification heuristics worked well for the participants in our user studies, we are currently exploring machine learning techniques to generalize the types of grasps we can support.
We are also exploring other types of grasp-based adaptive user interfaces, such as resizing the interaction area based on the grasp size of the users.
Last, we plan to open-source our sensor design and grasp sensing framework to make it easier for the research community to explore grasp-based interface and interaction.
Our studies have shown that most users have a different preferred virtual keyboard layout and position depending on how they were holding the tablets.
We proposed iGrasp, a novel approach that uses grasp-sensing to automatically adapts the keyboard layout and position.
Our evaluations show that participants were able to begin typing 42% earlier using the iGrasp's adaptive keyboard compared to manually adjustable keyboards.
In addition, participants also rated iGrasp significantly easier to use .
We also found that continuous position adaptation shows no statistically significant improvement over users' last-used positions.
