Phishers are fraudsters that mimic legitimate websites to steal user's credential information and exploit that information for identity theft and other criminal activities.
Various anti-phishing techniques attempt to mitigate such attacks.
Domain highlighting is one such approach recently incorporated by several popular web browsers.
The idea is simple: the domain name of an address is highlighted in the address bar, so that users can inspect it to determine a web site's legitimacy.
Our research asks a basic question: how well does domain highlighting work?
To answer this, we showed 22 participants 16 web pages typical of those targeted for phishing attacks, where participants had to determine the page's legitimacy.
In the first round, they judged the page's legitimacy by whatever means they chose.
In the second round, they were directed specifically to look at the address bar.
We found that participants fell into 3 types in terms of how they determined the legitimacy of a web page; while domain highlighting was somewhat effective for one user type, it was much less effective for others.
We conclude that domain highlighting, while providing some benefit, cannot be relied upon as the sole method to prevent phishing attacks.
Phishing attacks are a method whereby fraudsters try to steal user's credentials - user names, passwords, bank accounts, credit card numbers, and so on - via technical means and by social engineering.
A typical phishing attack would involve a message  sent to a user, purporting to be from a legitimate entity like a bank; the message would try to convince the user to follow a URL to the phisher's web site.
At this web site - which is the focus of this paper - the phisher visually mimics the real web site, and will save and later exploit the sensitive information sent to that web site by the user/victim.
Of course, the web security landscape is not blind to phishing attacks, and consequently many methods try to thwart them.
Domain name highlighting in Google Chrome  and Internet Explorer  internet providers.
Other methods try to help web users identify suspect sites, as discussed in the related work section.
Yet in spite of these efforts to combat the threat of phishing, recent statistics paint a bleak picture of their overall effectiveness.
The 2008 Gartner study  estimated that upwards of 5 million people in the U.S. alone were subject to financial losses due to phishing , even though safe-browsing features were used by 36% of the people surveyed.
Furthermore, at least 9400 phishing attempts were identified in the month of August 2010 alone .
Thus phishing continues to occur, and users continue to fall victim despite efforts to deter attacks.
One problem is that a well-crafted phishing site will be visually identical to the original site.
Indeed, the sole telltale sign of a well-crafted site may only be that its domain name is incorrect.
The new user-centric antiphishing method of domain highlighting  exploits this: it visually enhances the domain name portion of the web address in the browser's address bar.
Figure 1 illustrates how domain highlighting is incorporated in the default interface of recent releases of Google Chrome  and Microsoft's Internet Explorer  .
In both cases, the host domain is drawn in black, while the rest of the address is rendered in gray.
The only visible difference between the two browsers is that Explorer does not highlight the "www" prefix.
Users can recognize legitimate domain names, likely from their knowledge of the web, their expectations of what a well-formed domain name should be, and their prior experiences visiting particular web sites.
Users will naturally attend to the address bar as part of their normal browsing practice, where they will quickly examine the highlighted domain name portion of the address to judge the legitimacy of the web site.
We ask in this paper if domain highlighting really meets its potential.
Specifically, our objective is to provide insight into the effectiveness of domain highlighting in user identification of phishing sites.
The remainder of the paper unfolds as follows.
We begin by briefly reviewing related work that investigates phishing security as part of interface design.
We then describe our study, its methodology, and the results obtained.
Our discussion identifies limitations and faults with domain highlighting and its assumptions, and offers a few suggestions for security developers who wish to improve the effectiveness of phishing deterrents.
We also identify three types of users.
One type consistently cited brand, content and previous experiences with the web page as major factors of trust.
In contrast, another user type scrutinized the address bar as part of their evaluation.
A third type was a hybrid between these two.
Researchers  have investigated other common visual security cues integrated into web browsers: lock icons and pop-up warnings on fraudulent certificates.
They found these are also ineffective, as participants often continued their task in spite of these indicators.
As with the toolbar, subjects typically ignored icons located at the periphery of their attention.
Yet even when confronted with a blatant warning , users often did not care enough to stop.
The problem is that the intuition used to develop the above visual security cues assumes that what works for protecting the technically-knowledgeable specialist must, by extension, also work for the average consumer.
Yet when researchers examined what users actually use to form their opinion of trust while browsing, they found that subjects from a non-technical demographic judge security primarily from the visual content of the page itself and its relevance to one's own task .
Seldom do users consider browser elements such as icons and security toolbars.
Other approaches move security indicators directly into the user's field of view, although this has to be carefully done as it could affect the usability of a browser.
With enhanced sign-in indicators, first-time users of a legitimate site are asked to choose an image and/or enter some personal text during login.
That image and text are then displayed on all subsequent logins.
If they are missing or wrong, then he or she should suspect that they have been misdirected to a visually similar but fraudulent phishing site .
Domain highlighting takes an inbetween approach, where the highlighted domain name is  more prominent and meaningful than the security indicator, but not incorporated in the page itself.
To our knowledge, the actual effectiveness of domain highlighting has not been examined previously, which is why we do so now.
While somewhat similar in nature to the above-mentioned studies, it differs from them in three significant ways.
First, ours is the first formal study of domain highlighting.
Second, our methodology masked our interest in domain highlighting , where participants only knew that we were conducting a security study of some kind.
This allowed us to evaluate domain highlighting in the context of how users may behave outside of a lab setting.
Third, during a second evaluation round, we did tell people to focus their attention on the address bar, but we did not tell them about domain names or how to identify phishing sites.
This allowed us to evaluate how people used their own knowledge while removing problems associated with attention as found in prior studies of security icons.
Much prior work on anti-phishing security focuses on its technical aspects.
Yet many technical solutions are often thwarted because they expected users to understand them , or because phishers and other spammers used social engineering methods to fool users into taking actions they would not normally do.
This is why researchers are now seriously focusing on the gap between technical approaches to security vs. user interface design.
In particular, the Symposium on Usable Privacy and Security  has become, since 2005, the main venue for publishing research on user aspects of security.
Within the context of phishing and the web, the majority of attention on user-oriented security methods has been on security toolbars and visual security indicators.
In general, these methods situate visuals  on the edges of the browsing window or in the address bar that display the security status of a particular web site .
Where a phishing site is blacklisted, more prominent and active notifications may be used ; we are focusing instead on indicators for non-blacklisted sites.
Yet why, despite the existence of such measures, do phishing attacks continue to be effective against users?
First, people rarely noticed them because these were located in the peripheral area of a browser.
Second, users did not necessarily care about the toolbar even if they did take notice.
Our study was a controlled experiment, where 22 participants were shown 16 web pages  in randomized order1 across two phases.
In the first phase, subjects were merely asked to classify how "safe" each page appeared to be.
In the second phase, they repeated the exercise except this time they were told to focus on the address bar area.
Each ~45 minute study, conducted in an office, was comprised of an initial interview, 2 study phases, and an exit interview.
The initial interview asked participants about computer experience, prior use of web sites similar to those used in our study, and knowledge of security concepts.
We wanted to see if any had prior experience that would affect their performance.
In phase 1, no particular instructions were given on how to evaluate the web sites' trustworthiness.
In phase 2, we asked subjects to re-evaluate the web sites' trustworthiness where we directed them to focus on the address bar area .
If subjects changed their ratings between these two phases, then we could attribute this to their using the extra information supplied by domain highlighting in the address bar.
If there is no change, then we could assume that the participant likely could not recognize nor exploit the extra information supplied by domain highlighting.
In each phase, participants were shown a series of 16 web pages representing web sites  and asked to gauge how "safe" a site was on a 5-point scale.
In case subjects were unfamiliar with a site, we explained generally what each site was for .
As they did this, we asked participants to describe their rationale for their ratings as well as what visual element they were currently inspecting on the web page via a think-aloud process.
In our exit interview, we told study participants about domain highlighting and how it could help people evaluate phishing attacks.
Participants were asked if they knew about this feature, whether they had noticed it, and were asked for their thoughts on its potential effectiveness and what could be improved.
Participants were not told or instructed about domain name spoofing, nor domain highlighting, nor that we were specifically concerned with phishing attacks.
Rather, we told them that they would be involved in a study on the visual security aids as generally found in Internet Explorer 8, and they were consciously aware that they were being asked to judge a web site's trustworthiness.
While this sensitized them to security, it was up to them to decide what informational cues were present and how to use them to evaluate a site's legitimacy.
This is because we wanted to evaluate the effects of domain highlighting in a semirealistic context.
Had we made participants specifically aware of domain highlighting, they would likely have knowledge atypical of everyday users, and they likely would have given that feature far more scrutiny than they would normally do when browsing.
In real world contexts, security concerns are secondary goals in relation to the primary goal of completing a specific task 
Our emphasis on security shifted their primary goal to security rather than the task.
Consequently, we expect that participants will perform somewhat better in our study than in a typical browsing scenario where judging security is at best a secondary goal.
That is, if domain highlighting performs poorly when people are asked to judge a web site's security, it would certainly perform more poorly under more routine browsing conditions.
Thus our study portrays a reasonable upper bound on the effectiveness of domain highlighting in standard browsing environments2.
Participants, recruited from the University of Calgary through posted ads, an electronic newsletter, and e-mail group lists, were compensated with $15 cash.
Ten were graduate engineering students, six were students from science or social science fields, two were computer science graduate students, and four were university employees or previous university graduates from non-technical fields.
We also ran a pilot study involving 7 participants to help form and debug our study.
The method and results obtained in the pilot are essentially identical to those found in this full study.
Identifying domain highlighting as a study goal and instructing people on typical phishing attacks would produce a `true' upper bound, but one likely not reachable  in the real world.
The computer used ran Windows XP and Internet Explorer 8.
The only change we did to the standard environment was to point the web browser to a proxy server, which cached and presented the spoofed domain name / URL address in the participant's web browser for the fraudulent web pages.
Legitimate pages were loaded by the proxy server directly from the real location and displayed without modifications.
We used 16 different web pages representing 16 web sites.
Company / Description / Information requested Royal Bank of Canada.
Online auction for posted items; browsing, purchasing and paying for various items.
University library login page to access online library services.
Requests user account credentials that could be used to access other university services.
Login page to syncronize user devices, e.g., iPod, iPhone, iPad, and laptop or desktop computer.
Phone company login page for text messaging services.
Clothes store log-in page for online shopping.
Social network invitation sent by another person.
Supposed Company / Description Facebook.
Requests user account credentials for multiple social networking sites.
Describes how to pay tuition fees, but doesn't require any personal information.
Login page for payment of goods via bank / credit cards.
Online store for browsing and purchasing books and other items.
Photo-sharing login page that requests user account credentials.
As is common in the better phishing sites, each phishing page was a perfect replica of the original site.
Thus the only discernible visual difference between a fraudulent and its legitimate page was the difference in the URL, particularly in its domain name.
For realism, the 16 web sites in Table 1 were selected according to one or both of the following criteria.
We wanted participants to be either familiar with the displayed site or other sites like it.
Thus we selected popular sites  within our geographic region, where those sites were also representative of online tasks our target group would likely encounter during routine web browsing.
If participants were unfamiliar with a particular site as shown by the web page they saw, we described  the kind of organization hosting that site, and then presented a sample scenario illustrating the purpose of that site.
As described in the right columns, most pages in Table 1a/b are login pages to those sites that ask for account credentials.
If our fraudulent pages were actual phishing attacks, then people entering information into them would likely incur financial misappropriation, identity theft, or both.
Phishing Domain Names / URLs.
Phishers use domain name obfuscation techniques to fool people into believing that the domain name / URL in the address bar is not fraudulent.
For our fraudulent sites, we crafted URLs representative of typical obfuscation techniques  as listed below.
While we name these methods below, the community has no consistent names for most of them.
Similar-name attacks are addresses that are similar sounding to the legitimate address.
For example, the fraudulent amazon.checkingoutbooksonline.ca appears similar to the legitimate www.amazon.ca, even though the second-level domain portion is quite different .
Letter substitution attacks substitute one or more characters in the domain name with a character that is visually similar in appearance.
These can be seen as crude homograph attacks .
An example is `www.uca1gary.ca' rather than `www.ucalgary.ca' .
The user may miss this in a quick glance.
Complex URL attacks are addresses that span the length of the address bar or that may contain nonsensical characters, making interpretation of the URL difficult.
Question 1 considers differences that happen before and after a person is told to pay attention to the URL, i.e., if they even notice and exploit domain highlighting more effectively after being told to look at that area of the page.
Question 2 considers differences between measures when we compare rates on legitimate vs. fraudulent pages.
Question 3 considers how well people can or cannot detect particular types of domain name attacks.
Our primary quantitative metric used to evaluate the effectiveness of domain name highlighting was derived by comparing user ratings  to web page legitimacy .
This led to the measures below, illustrated in Table 2 as a matrix.
Correct decision is the percentage of participants who correctly classified  a page as legitimate or fraudulent.
It is the sum of the two measures below.
Correct: safe occurs when one correctly rates a legitimate web page as safe.
Correct: unsafe occurs when one correctly rates a fraudulent web page as unsafe.
Wrong decision is the percentage of participants who incorrectly classified  a page as legitimate or fraudulent.
It is the sum of the two measures below, which are equivalent to Type I  and Type II  errors in hypothesis testing.
Wrong: safe occurs when one incorrectly rates a fraudulent web page as safe.
This is very critical factor, because this incorrect decision directly measures the vulnerability of participants to a phishing attack.
Wrong: unsafe occurs when one incorrectly rates a legitimate page as unsafe.
This means that the participant will not continue to use the page even though it was, in fact, safe to do so.
While a  inconvenience, this error is not a security threat.
We use a combination of descriptive statistics plus hypothesis testing.
Our null hypothesis is a rephrasing of Questions 1 and 2 above to state that no such difference exists between decisions over legitimate vs. fraudulent pages across the two phases.
For hypothesis testing, we used a 2x2 ANOVA: Phase  x Web Page Type  with a threshold of p < .05 .
We calculated frequency values of correct/unsure/wrong decisions , and - as is common with proportional data in ANOVA that has limits of 0 and 1 - we transformed them to arcsin values.
For correct and unsure decisions, a significant difference was found between the frequency of people's rating of Web Page Type ; there was no significant difference between the phases.
That is, people rated fraudulent and legitimate web pages differently in terms of their correct / unsure frequency, but the fact that they were aware of the address bar in Phase 2 did not change these frequencies.
For incorrect decisions, an interaction effect existed for the frequency of people's ratings.
Post-hoc tests revealed there was a significant difference between people's incorrect frequency ratings of legitimate vs. fraudulent sites in both phases , and between their ratings of fraudulent pages between Phases 1 and 2 .
However, there was no difference in people's ratings of legitimate sites between Phases 1 and 2 .
The actual values of these differences are detailed below along with other descriptive statistics, and include brief discussions.
Correct vs. wrong decisions .
Figure 2 is a visualization of each participant's decision data for each web page seen in Phase 1.
Each column represents an individual participant, where columns are sorted by participants with the most correct results on the left.
Each row represents a particular web page, with the 8 legitimate pages at the top and the 8 fraudulent pages at the bottom.
Our quantitative analyses were driven by three areas of interest.
Is there a difference in web page ratings in terms of correct decisions, wrong decisions, wrong:safe, and wrong:unsafe 1. regardless of whether or not subjects were told to pay attention to the URL address?
2. regardless of whether the page is actually legitimate or fraudulent?
Green cells are correct decisions, red are incorrect, and yellow are those rated as unsure.
Participant numbers are shown in the topmost row, whereas participant type  is shown on the bottom row .
For the legitimate pages, participants rated 54% of them correctly , 15% incorrectly , and were unsure of 31% of the pages.
Figure 2, top, illustrates this by the moderate amount of green squares showing the correct ratings.
For fraudulent pages, participants rated 25% of them correctly , 57% incorrectly , and were unsure 18% of the time.
Figure 2, bottom, vividly reveals the high number of incorrect ratings shown in red at its bottom.
Correct vs. wrong decisions .
Figure 3 is a visualization of how people changed their ratings of a page after they were told to attend to the address bar.
White cells are those that were unchanged, or where the new rating was insufficient to move that page into a different safe/unsafe/unsure category.
Pale green cells are page ratings that improved by one category .
The dark green cells occurred when a person correctly changed their decision between safe and unsafe, while the  dark red cell occurred when a person incorrectly changed their decision between safe vs. unsafe.
For the legitimate pages, the overall percentages of ratings barely changed between Phases 1 and 2; as stated previously, no significant difference was found for legitimate pages between these phases.
While changes were made, the number of incorrect re-ratings approximately equaled the correct re-ratings to produce no net gain.
This can be seen visually by the balanced scattering of red and green cells in Figure 3, top.
For fraudulent pages, participants' correct ratings improved from 25 to 34% , with a decrease in incorrect ratings from 57 to 44% .
As mentioned, these differences were statistically significant.
Unsure ratings increased from 18 to 23%.
The overall gain in correct performance in re-rating these fraudulent pages can be seen visually by the predominance of light and dark green cells in Figure 3, bottom.
Participants appeared reasonably accurate at identifying legitimate sites, where they wrongly rated only 15% of them as unsafe .
Yet their performance was effectively unchanged between the phases.
We suspect that this good performance is due more to people's disposition to trust familiar-looking pages vs. their ability to recognize the domain name as legitimate.
As we will see, this suspicion is verified in the qualitative analysis.
For fraudulent pages, participants fared poorly, where they correctly identified phishing pages only about 25% of the time.
Fully 57% of these pages were incorrectly rated as safe , which would have put them at substantial risk of a successful phishing attack.
Even when they were told to attend to the address bar, their performance only improved marginally ; fully 44% of the pages were still incorrectly rated .
We interpret this as follows.
Domain name highlighting is, at best, only somewhat effective at helping people identify fraudulent sites.
Participants do not always attend to the address bar  to determine site legitimacy.
If they had attended the address bar in Phase 1, we would not have found a difference between the two phases.
However, when participants were told to attend to the address bar, they did do somewhat better - but not much better - in correctly identifying fraudulent sites.
Evident from these figures is that there is marked variation in participant performance.
We will revisit this difference between participants in the qualitative analysis section, where we identify three different user types.
The order of web pages within Tables 1a and 1b correspond with the row ordering in Figures 2 and 3.
That is, they are ordered from those with the most correct ratings  to the most incorrect.
As evident in Figure 2, the majority of people frequently and incorrectly rated a fraudulent page as safe, regardless of the type of obfuscation method used.
When people were asked to look at the address bar, people did improve the correctness of their ratings somewhat  across all phishing methods.
The exception is the IP-address attack .
More people correctly identified the IP-address attack as a fraudulent page: 9 people in Phase 1, and 14 people in Phase 2 after being told to look at the address bar.
Legitimate web sites rarely use an IP address as a domain name.
If the domain name was displayed as an IP address, this should be an obvious cue suggesting a phishing attack.
Our results indicate that this is the case, but only marginally.
IP-address attacks are still an effective phishing method.
Even when participants were asked to attend the address bar, only 64% of them were able to spot the attack.
That is, people either did not notice or, more likely, they did not understand the implications of seeing an IP address in the domain name.
The other three types of phishing attacks seem equally effective at fooling people, as there is little practical difference between them in terms of people's ability to spot them.
This strongly suggests that people are equally bad at judging domain name legitimacy when slightly more sophisticated obfuscation methods are used.
Within these categories, we then grouped comments into two sub-categories that asked what specific information they scrutinized, and how they used that information to form their judgments.
We then looked at differences between any of these categories as described by a participant between Phase 1 and Phase 2.
Finally, during our analysis, we noted that participants did not use these categories equally.
Consequently, we sorted participants with similar traits into a typology of groups.
Perhaps the most interesting behavioural pattern we identified concerned the differences between people in how and when they attended different elements of the browsing interface to judge a site's legitimacy.
These differences suggest a typology, where people can be  categorized as clustering along points of a spectrum in terms of how they scrutinized a mix of information covering a web page's:  institutional brand;  content as presented in the main pane of the browser;  information in the address bar; and  other security indicators.
The three types are briefly described below and illustrated in Figure 4, where the sections that follow detail how each type used this information.
Type A people focus solely on information present in the browser's content pane .
Information brand as identified in the content pane was critical.
They did not consider information in the address bar or other security indicators shown outside of the content pane.
Type B people consistently focused on the information found in the address bar, the information being requested, and other security indicators .
While they did use information visible in the content pane , it was not considered the sole source of their judgement.
Type AB people were a composite of the above .
Like Type A, they relied primarily on the content pane and institutional brand.
However, like Type B, they sometimes considered information in the address bar and the kinds of information being requested.
They were much more variable in the information they considered from page to page.
Using a form of open coding , we identified categories from our user comments and behaviours observed during the think-aloud phases and the interview session.
This led to four overarching category types, which will be described in more detail shortly.
Institutional brand of the site, especially people's familiarity with it, as exhibited by materials visible in the content pane of the browser;  Informational content and layout found in the content pane;  Address and security status information found in the browser's address bar, the URL within it, and the status bar;  The information requested and perceived risk of providing that information, i.e., user input requested in the content pane.
It is rather concerning that a good portion of our participants did not seem to even look at the URL, let alone correctly recognize one of them as a phishing site, for these high-risk banking pages.
Other recognizable brands, such as Amazon, Google, the University of Calgary , and Yahoo!
This suggests that for many users, the perception of safety and legitimacy of a site is tied heavily to their preconceived perception of the brand itself.
Type A and Type AB subjects were also often quick to identify symbols and logos associated with familiar brands.
This included logos by other companies on a page, such as the Google logo on the YouTube page, the Yahoo!
Any sign of these `famous' and familiar big-brand logos seemed to instill these participants with a measure of  trust.
In contrast, the seven Type B participants showed a different pattern in their use of brand and their familiarity with it.
Five mentioned brand for only two of the pages.
Even when brand was mentioned, these participants never used brand as the sole category of information to form their ratings.
Most Type B comments involved only a passing mention of brand, typically to say whether they did or did not recognize the brand depicted or connected to the site.
Generally, participants were also inclined to distrust particular brands or types of sites.
For example, a few participants mentioned privacy issues with Facebook, YouTube and Meebo as a reason to distrust the site.
If anything, participants  showed greater caution toward social networking sites, especially if they had little or no personal experience with that site, or had heard negative things about it.
Of course, a phisher can easily spoof all this information.
Security and privacy related information.
Type A and Type AB subjects were often quick to identify symbols that associate a site as being verified as `safe' by an external authority.
An example is the appearance of the Verisign logo.
They also pointed out internal indicators of security, such as a "security guarantee" logo on the Royal Bank page .
The presence of a virus scan advertisement was interpreted by two participants as being a positive sign of site security, and a few mentioned the option to forget user login credentials  as a sign that it would be safe to enter their login information.
Indirect security-related content such as an `updated' copyright date and links to contact/support information were also frequently reported by Type A, and some Type AB participants.
In fact, Type A  participants frequently spoke of the security and non-security-oriented vocabulary typical found on the edges of the content pane .
In these instances, links to legal, privacy and security policies implied that the site had safeguards to ensure that disclosed information would be held in the sole possession of the particular site.
Content amount, the kind of information, and its layout within a page were also important factors contributing to Type A and AB interpretations of a site's trustworthiness.
One notable example involved customer reviews and book information in the amazon.com product page.
The amount and layout of information on that page instilled a sense of confidence in its legitimacy .
The customer reviews on that page instilled a sense that others had visited the site before, and that others had purchased the depicted product.
All but one of our pages requests people to enter different kinds and amounts of information.
Some pages request sensitive identification information, and/or financial information, and/or unusual or excess information that could seem disproportionate to the that page type.
Curiously, financial information requests did not seem to warrant greater caution by participants, and most participants did not even comment on it.
Type AB people made the most comments about pages that seemed to request excessive or unusual information whereas Types A and B provided fewer comments; we have no explanation for this discrepancy.
While it is natural to interpret page content as an indicator of trust, this is irrelevant in the detection of a phishing page that replicates the look and feel of the legitimate page without any discernible difference.
What is alarming that Type A  instinctively relied on this content of the page to determine legitimacy.
In contrast, Type B participants, for the most part, were not strongly influenced by page content in their evaluations of site legitimacy, where they instead spoke mostly about the content of the address and status bars.
Type A and AB participants formed their judgement around the content information as present in the main content pane of the page.
They were particularly sensitive to security and legitimacy indicators within the content pane, where thre presence of these indicators positively influenced their perception of that page as trustworthy.
Type A people focused almost exclusively on information present in the browser's content pane, and they rarely considered information in the address bar.
Even when asked to look at the address bar, most Type A people clearly had no idea about `unusual' URLs, nor what to do with the information contained within it.
Comments included "this  doesn't mean anything to me" or "this  doesn't change anything for me".
Type A people mentioned they were uncertain of how to interpret `.net' or `.ca' suffixes, or the `https' prefix.
Some incorrectly considered the URL length as meaningful , while others thought that certain words in the URL had positive connotations, e.g., the appearance of the word `secure', `deliveryaddress', the brand name, and so on.
Long URLs and those containing seemingly nonsensical argument characters  prompted frequent comments about what the URL meant and why it was so complex.
For IP attacks , the three Type A people who noticed the IP numbers in the domain name mentioned their suspicions, but still rated that site as trustworthy: "the URL are just numbers".
Type A people tended to use brand identity and site type to over-ride any suspicions they may have had when seeing unusual URLs.
To illustrate, one subject who was judging a fraudulent bank page commented "the highlighted  part is strange...  banks are secure" and then rated that page as trustworthy.
Another subject spotted a letter substitution attack , yet also rated that page as trustworthy because of site familiarity.
Unlike Type A, Type AB paid more attention to URLs and to other security indicators, although not consistently.
The content pane still seemed to be their primary source of information across all pages, but they did occasionally look at the URL on some pages.
At least some of them seemed aware that the URL may contain security and other indicators that could be used to form their judgement of trust.
For example, one mentioned "https is more secure".
Another recognized the "1" substitution attack as strong evidence that a page was fraudulent.
Yet Type AB people were often unsure of what to look for in the URL, and did not have a full understanding of phishing indicators.
For example, 3 of the 6 Type AB people caught the "1" in the `paypa1' domain name, but were confused as to how this should influence their ratings.
They continued to choose to trust the phishing site.
Unlike Type A and AB people, Type B participants paid heavy attention to the address bar.
They knew of the security indicators built into the address bar area.
They knew the URL was important to determine page legitimacy, and they consistently looked for clues in the address bar to appraise the legitimacy of the sites.
For example, many of them consistently looked for `https' and the lock icon in the URL.
Some knew about the importance of the domain name, where they tried to match it with the page being displayed.
Meebo page said "the URL is not meebo.com, and it looks phishy".
While all this sounds like good news, Type B people still missed many fraudulent pages .
While one Type B person did correctly identify all fraudulent pages, four other Type B people missed 50% or more of the fraudulent pages.
Perhaps surprisingly, when asked about it during the exit interview, most participants - including Type B - said they were not even aware that the domain name part of the URL was highlighted.
Subjects with Type AB or Type B classifications demonstrated a marked improvement in successful identifications of phishing sites during the second phase.
Contrary to Type A, these participants focused their attention more on the technical rather than superficial aspects of the address bar.
Secure protocol indicators like 'https' or the lock icon were noted; the entire URL was examined, and , the obfuscation method was seen.
Between Types and correctness of ratings.
Recall that the bottom row of Figure 2 indicates the participant type  and that columns are sorted by participants with the most correct to least correct rating, from left to right.
Not surprisingly, almost all Type B people are clustered on the left side, i.e., those with the most correct ratings.
Types AB tended towards the middle, and Type A tended towards the right  side, although the correlation appears much weaker.
Correlation of types to formal technical training.
Some of our participants were trained as computer scientists and engineers.
Most - but not all - of those fell into the Type B category.
Similarly, most Type A people were in nontechnical fields, with AB being a mix.
However, this correlation was not perfect.
For example, two of the nine Type A people were either computer scientists or engineers.
Our study provides reasonable `upper bounds' of how likely people are to detect phishing attacks based on domain name highlighting.
That is, participants in Phase 1 were primed to judge security as their goal, and in Phase 2 they were specifically told to attend to information in the address bar.
Our participants were also highly educated , with several of them being technically knowledgeable.
What we found was that domain highlighting works, but nowhere near as well as we would like.
Our participants still incorrectly rated about half to two-thirds of the fraudulent pages as safe, depending on whether they were told to attend to the address bar.
For Type A users, domain name highlighting is rarely effective, as they judge a page almost solely by its content area .
And even Type AB and Type B people who do scrutinize this area are at best `hit and miss' at detecting domain name anomalies.
To make matters worse, most people don't even bother to look at the address bar unless they are told to do so.
Consequently, in everyday life, we expect people to be much worse at identifying phishing attacks via domain name highlighting, as they lack technical knowledge and they will be focused on their normal web browsing activities rather than security.
Two questions worth asking are: should we abandon it, and are there ways to improve it?
Our results showed several problems that could be associated with a lack of user knowledge.
First, Type A and AB people mistakenly place great reliance on the information in the content area and the brand, which can be easily mimicked by phisers.
Second, Type A  and Type AB  attend to the address bar, likely because they do not know of its primary importance.
Third, even if they do look there, almost all participants - even Type B - frequently miss the spoofed domain name, likely because they are unaware of the common obfuscation methods used.
Alternately, they use information in the URL to form their judgement that is simply not relevant.
People need to know that they cannot rely on the content area, that they should scrutize the domain name, and that they should be aware of common phishing methods.
As one person said: "highlighting the domain name is fine for people who normally look at the URL, but we need to reach the general population... more knowledge would help."
The challenge is to find effective ways to train users without getting in the way of their primary task.
In summary, domain highlighting gives only marginal protection and cannot be relied upon as the sole means to identify a phishing site.
Still, it is worth including as it comes at almost no cost in terms of interface clutter, browser performance, and interference with a user's task.
Clearly, it must be used with other anti-phishing tools, where the combination of those tools may provide better protection .
We believe that domain name highlighting can be made somewhat more effective by making the domain name even more obvious, by drawing the person's focus onto the address bar, by reducing URL complexity, and - most importantly - by educating people about the importance of the domain name in judging web sites, and what typical obfuscation methods are used by phishers.
This research is partially supported by NSERC, in part via ISSNet.
When people browse, they tend to focus on the content area as it is the reason why they are browsing.
Security is a secondary concern , and consequently even the most careful  may fail to scrutinize the domain name if they are immersed in their task.
This makes us question if there are methods that we can use to draw people's attention to the domain name, or if we can present that information another way.
Solutions here may try to make the domain name even more obvious, or draw the person's focus onto the address bar , or somehow place the domain name in a portion of the page where the person is more likely to attend to it.
URLs can be quite complex, and it was clear that many of our participants were confused by this complexity.
This could perhaps be resolved by either presenting the domain name by itself in a dedicated area separate from the rest of the URL, or by somehow reducing complexity of the URL by  hiding extra information in URL address.
