End-user interactive concept learning is a technique for interacting with large unstructured datasets, requiring insights from both human-computer interaction and machine learning.
This note re-examines an assumption implicit in prior interactive machine learning research, that interaction should focus on the question "what class is this object?".
We broaden interaction to include examination of multiple potential models while training a machine learning system.
We evaluate this approach and find that people naturally adopt revision in the interactive machine learning process and that this improves the quality of their resulting models for difficult concepts.
This note re-examines an implicit assumption about how people should interact with machines that is common in prior interactive machine learning research .
Machine learning systems learn by generalizing from examples of classes of objects.
Prior work has thus focused interaction on prompting a person to answer "what class is this object?"
We propose that a better approach may be for a person to consider "how will different labels for these objects affect the system in relation to my goals?"
Based on this approach, we examine end-user comparison of multiple potential models.
We situate this research in the context of CueFlik, a system that allows end-users to train visual concepts for re-ranking web image search results .
Consider a person attempting to train a "portrait" concept, as in Figure 1.
Given a large and diverse set of images from a web query "Bill", the person may label Bill Gates and Bill Clinton images positive and dollar bill and Bill of Rights images negative.
Machine learning is a promising tool for enhancing human productivity and capabilities with large unstructured data sets.
For example, consider a scientist trying to annotate segments of X-rays containing a specific "abnormality" in a medical imaging dataset or an office worker who wants a smart environment to automatically screen "unimportant" phone calls whenever it senses they are "busy".
Interacting with individual objects to achieve these goals becomes difficult because of the vast amount of data .
With end-user interactive concept learning, people provide examples to interactively train a system to recognize concepts, such as "abnormality", "unimportant", or "busy".
Automated processing is then based on those concepts.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Figure 2: We investigate strategies examining multiple potential models while training CueFlik, a system for end-user interactive concept learning of visual concepts.
Support for examining multiple potential models includes revision capabilities such as label removal, undo/redo, and a history visualization of recently explored potential models.
These are unambiguous given the person's goals.
However, they may also encounter images they are less certain about, such as Bill Cosby on a magazine cover.
The image features a prominent face, but also a variety of magazine graphics the person does not intend as part of the "portrait" concept.
We propose a person should be able to experiment with either potential labeling, compare the resulting concepts, and then decide upon a label that guides the system to learn the desired concept.
We examine this in CueFlik with  support for removing labeled examples directly and via undo/redo, and  a history visualization showing recently explored models, estimates of their reliability, and support for rolling back to previous models.
The specific contributions of this note are: * A discussion of human-computer interaction and machine learning perspectives on the notion of examining multiple potential models.
Bridging these perspectives suggests new strategies for end-user interactive concept learning.
We find that end-users naturally adopt revision as part of an interactive machine learning process and that undo/redo improves the quality of their resulting models for difficult concepts.
Furthermore, supporting undo/redo is explicitly called out in Nielsen's ten usability heuristics .
In contrast, traditional interactive machine learning emphasizes collecting data to maximize information gain .
Interaction with a person has therefore generally been limited to asking "what class is this object?".
Such an approach permits simulated experiments with fully-labeled datasets.
However, treating a person as an oracle neglects human ability to revise and experiment.
Deleting training examples, whether explicitly, by undo, or by rolling back to a previous model, is a poor fit from this perspective because it deletes information that has already been made available to the machine learning system.
Prior research at the intersection of human-computer interaction and machine learning has implicitly assumed the machine learning perspective.
Fails and Olsen discuss the interactive machine learning process as providing additional training examples to improve the current classifier , but never raise the possibility of providing different training examples to improve a model.
This note considers end-user interactive machine learning from a revision approach, wherein a person assigns different combinations of labels over time as they examine and choose from multiple potential models.
CueFlik is a system that allows end-users to train visual concepts for re-ranking web image search results .
End-users train CueFlik by providing examples it should match  and examples it should reject .
CueFlik uses these examples to learn a distance metric based on a set of visual features, which it then applies with a nearest-neighbor classifier to re-rank images.
CueFlik guides end-users to provide informative examples using overviews of the current positive and negative regions of a learned concept  .
It also combines these overviews with a presentation of the entire set of images  ranked by their likelihood of membership in the positive class  .
We made two sets of enhancements to CueFlik in this research.
Participants trained three models in each condition, corresponding to concepts such as "pictures with products on a white background" and "portraits of people".
CueFlik automatically issued a query for each concept  to obtain a diverse set of 1000 images previously retrieved from the web.
Participants were then given a sheet of paper with ten target images on it and were asked to train the system to re-rank the images such that those like the targets are ranked highly .
Based on prior experience with these target concepts, we categorized them as easy or difficult and pseudo-randomly selected queries such that the first task in each interface was easy  and the next two were difficult.
We fixed the order of the queries selected because we did not expect them to lead directly to carryover effects.
Participants were asked to train the system as accurately and quickly as possible and we imposed a maximum time limit of four minutes for each task.
All actions were automatically logged and timestamped.
After each condition, participants were given a short questionnaire about the interface they had just used and the models they trained with it.
At the end of the study, a final questionnaire collected overall assessments of CueFlik.
The experiment lasted approximately 90 minutes, and participants were given a software gratuity for their time.
We augmented CueFlik to include a history visualization of recently examined potential models .
The history contains a plot of each model's estimated reliability, updated after each addition or removal of examples.
Model reliability is measured using leave-one-out-cross validation on the current set of training examples, with confidence intervals computed according to a binomial distribution .
Estimating reliability via training data is necessary because standard measures  require labeled data .
The history also contains snapshots of the top ranked images for each model.
The history is intended to help people visually compare and assess the relative quality of the models they have trained.
CueFlik includes several mechanisms for people to revise their currently trained model.
First, a person can undo or redo actions.
Second, they can remove labels of individual examples if they feel the example could be hurting the model's performance.
Finally, a person can click directly on a data point or a snapshot in the plot to revert back to that model.
The person can then continue providing examples from that stage, effectively enabling a simple branching mechanism for exploring multiple models.
Analysis of logged data from our experiment shows that participants made use of the revision mechanisms to explore multiple potential models in CueFlik.
When the history visualization was available, participants made revisions in 68% of their tasks.
To make revisions, participants used the undo/redo feature in 19% of tasks, the remove label feature in 5% of tasks, and the ability to rollback to previous models via the history in 42% of tasks.
When the history visualization was not available, participants made revisions in 41% of tasks.
Interestingly, their usage of the undo/redo and remove label features increased to 30% and 11%, respectively, likely because these were the only revision mechanisms that were available in this condition.
For the tasks in which people made revisions when the history was available, 3% of their total actions  were undo/redo actions, 1% were removing labels, and 9% were rolling back via the history.
Without history, they relied more on the undo/redo and remove label features, using undo/redo in 11% of actions and removing labels in 3% of actions.
This suggests participants were able to make progress by providing CueFlik with examples, but in some cases felt it necessary to explore or revert back to previous models.
We conducted an experiment to understand how people would use our history and revision enhancements to examine multiple potential models in CueFlik and to determine the effectiveness of a revision-based approach to interactively training a machine learning system.
We used a 2  x 2  within-subjects design.
Conditions were counterbalanced using a Latin Square.
Nineteen participants  volunteered for the study .
Score is defined as the mean ranking of the target images by the final learned concept, where a lower score indicates a higher-quality concept .
We perform these analyses using mixed-model analyses of variance.
All of our models include History , Revision , and their interaction HistoryxRevision as fixed effects.
To account for any variation in individual performance, query difficulty, or other carryover effects, we include Participant and Query as random effects.
We exclude easy concepts because these were intended for practice with each condition and because we expect our enhancements to be less relevant in situations where there is little ambiguity.
There was no effect of History on Score.
There were no effects of Revision on Time or NumImages.
There were no interaction effects on any of our dependent measures.
This research re-examines a traditional interactive machine learning focus on the question "what class is this object?"
Without such support, our study participants found it difficult to recover when model quality appeared to drop .
Furthermore, prior research has found that, after a certain point in the interactive machine learning process, continuing to provide examples can become detrimental to model accuracy .
Our research shows that including revision mechanisms can improve end-user interactive training of machine learning systems.
Our evaluation shows that participants made use of revision mechanisms while interactively training a machine learning system and that this led them to achieve better final models in the same amount of time .
Furthermore, being able to examine and revise actions is consistent with how people typically expect to interact with applications.
One participant commented that without revision "it felt a little like typing on a keyboard without a backspace key".
In contrast, our history visualization enhancement led participants to spend more time and perform more actions to train concepts without improving overall model quality.
While some participants seemed to find the history helpful for examining different models , observations during the study and other participant comments indicate that the plot was generally distracting .
Although the plot used an accepted machine learning metric to estimate model reliability , end-users seemed to use it less like an approximation tool for helping them interpret model quality and more like a quantity to maximize .
Participants did, however, use the history for reverting back to previous models, suggesting that the history may be beneficial as a facility for enabling revision .
Amershi, S., Fogarty, J., Kapoor, A., and Tan, D. Overview-Based Example Selection in End-User Interactive Concept Learning.
Proceedings of the ACM Symposium on User Interface Software and Technology , 247-256.
Beygelzimer, A., Dasgupta, S., and Langford, J.
Proceedings of the International Conference on Machine Learning , 49-56.
Dey, A.K., Hamid, R., Beckmann, C., Li, I. and Hsu, D. a CAPpella: Programming by Demonstrations of ContextAware Applications.
Proceedings of the ACM Conference on Human Factors in Computing Systems , 33-40.
Proceedings of the ACM Conference on Intelligent User Interfaces , 39-45.
Fogarty, J., Tan, D., Kapoor, A., and Winder, S. CueFlik: Interactive Concept Learning in Image Search.
Proceedings of the ACM Conference on Human Factors in Computing Systems , 29-38.
Kohavi, R. A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.
Proceedings of the International Joint Conference on Artificial Intelligence , 1137-1143.
Information-Based Objective Functions for Active Data Selection.
Usability Inspection Methods, John Wiley & Sons, New York, NY, 1994.
Side views: persistent, ondemand previews for open-ended tasks.
Proceedings of the ACM Symposium on User Interface Software and Technology , 71-80.
Tong, S. and Chang, E. Support Vector Machine Active Learning for Image Retrieval.
