3D modelers often wish to showcase their models for sharing or review purposes.
This may consist of generating static viewpoints of the model or authoring animated flythroughs.
Manually creating such views is often tedious and few automatic methods are designed to interactively assist the modelers with the view authoring process.
We present a view authoring assistance system that supports the creation of informative view points, view paths, and view surfaces, allowing modelers to author the interactive navigation experience of a model.
The key concept of our implementation is to analyze the model's workflow history, to infer important regions of the model and representative viewpoints of those areas.
An evaluation indicated that the viewpoints generated by our algorithm are comparable to those manually selected by the modeler.
In addition, participants of a user study found our system easy to use and effective for authoring viewpoint summaries.
As a result, automatic viewpoint selection  has been an active area of research in the Computer Graphics community.
However, the associated algorithms that have been developed typically consider only the final geometric models.
H.5.2 User Interfaces: Interaction Styles Recent years have witnessed significant progress in 3D modeling and 3D printing.
As the technologies are becoming essential to the industries and even people's daily life, the demand of high quality 3D models also increases rapidly.
This trend has resulted in a variety of 3D model libraries and websites where 3D modelers can share models they created.
Understanding each model through 3D navigation can be a time-consuming and confusing experience.
Thus, to achieve efficient browsing of large 3D model data sets, it is necessary to have visual summaries that can concisely show important aspects of the 3D models.
An effective visual summary usually consists of static viewpoints and animated fly-throughs of the model.
However, manually creating such views can be a long and tedious process.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
Furthermore, it is not clear how to integrate these automatic techniques into a modeler's view authoring process in practice.
Alternatively, the HCI literature has investigated techniques for navigating along constrained view paths and view surfaces to obtain effective overviews of 3D models .
However, these systems require predefined constrained views.
The authoring of such views, either manually or automatically, remains an open problem.
Recent work has demonstrated the utility of instrumenting a 3D modeling environment for visualizing editing operations and model differences .
Guided by a set of observations and interviews, we hypothesize that enhancing such instrumentation to also include the camera history could be useful for authoring viewpoint summaries.
By recording and analyzing these workflows, it may be possible to derive effective summary views.
In this paper, we contribute a history assisted view authoring and navigation system for 3D modelers .
Our system stores a model's editing and viewpoint history along with discrete versions of that model.
By analyzing the history and the model, our system can suggest informative view points, animated view paths and view surfaces of the selected model region.
These can be used for automatic visual summaries of 3D objects or in our interactive view authoring environment where modelers can select and customize such summaries.
Our system produces an interactive summary of the 3D model, allowing viewers to easily navigate between authored views and along view paths or surfaces, and even compare different versions from the model's revision history.
Alternatively, our system can also automatically arrange the authored views into user-defined layout templates for a static brochure or catalogue-like display.
In a first study, we compare the viewpoints generated by our algorithm to those generated by Secord et al , and to manually created views by 3D modelers.
The results show that our algorithm selects viewpoints with comparable quality to manual selection.
In a second qualitative user study of our interactive system, participants expressed high enthusiasm and a desire to incorporate the system into their daily workflows.
The collected data is used to summarize viewing behaviors  or assist in the future presentation of the model .
In contrast we instrument the authoring environment so that viewpoint information can be captured, and informative viewpoints can be inferred as soon as the model is created.
Numerous systems have been proposed to automatically record users' workflows within a software application.
Captured interaction history and workflows have been shown to be useful for creating step-by-step tutorials  or allowing users to explore and replay editing history of a document .
Our work on viewpoint summarization demonstrates a previously unexplored way to utilize captured workflows: to select views and create model summarizations.
A core problem related to our work is the automatic selection of informative viewpoints of a 3D model.
Our premise is that the behavior of modelers as they create the model can provide useful cues for viewpoint selection.
To better understand the validity of this premise, and gain other insights for guiding our work, we conducted multiple interviews and observation sessions with professional 3D modelers.
This section summarizes the details of these sessions and some of the insights gained from them.
We conducted three separate 45-minutes interview sessions with three 3D modelers.
Two of the participants were internal to our institution, and one was externally recruited.
There were five additional observation sessions with the two internal modelers.
Each session is 1 hour long of 3D sculpting  followed by 20 minutes of discussions.
An observer took notes during the sessions, and the modeling processes were screen-captured for later reference.
Many existing techniques have attempted to simplify 3D navigation, which is often a challenge for inexperienced users .
ShowMotion  replaces static camera bookmarks with pre-authored animated shots, and StyleCam  supports constrained navigation along preauthored camera paths.
Both works greatly improve users' viewing experience of a 3D model.
However, these systems only consider the viewers of the 3D model and it is unclear how a modeler can author the required views.
For example, ShowMotion  assumes pre-authored viewing motions and StyleCam  pre-authored surfaces and paths.
Our work bridges the gap by assisting the modeler to author such viewpoints, view paths, and view surfaces.
At the core of our system is the viewpoint selection algorithm.
Previous work  infers the importance of viewpoints based on the visible geometric attributes of a given 3D surface.
By additionally considering the editing history of a 3D model, our approach could potentially create viewpoint selection results that better convey the original author's intentions.
We observed a camera movement pattern consistent across modelers, where they oscillate between sculpting at camera position A, move to position B for inspection, then move back to position A for further editing.
Such oscillation patterns are a necessity in sculpting tools.
Typically, the 3D viewpoint which allows the modeler to precisely position a brush  is looking directly down the normal of the sculpting region.
Yet the best view for assessing the effect of the brush  is typically perpendicular to the plane normal.
Since these oscillations are often used for inspection, capturing and analyzing the inflection point of these oscillations may allow us to infer preferred viewpoints for specific areas of the model.
In the following sections, we first introduce the user interface of our authoring and viewing environments, followed by algorithm and implementation details.
As a prototype system, we instrumented the 3D sculpting tools in the MeshMixer  modeling software for recording the editing history, and integrated the authoring environment inside MeshMixer.
Our view authoring system supports both authoring and viewing modes across three main categories of view widgets: view points, view paths, and view surfaces .
As an author, a user can generate a summary of the model through a series of informative viewpoints, constrained view paths and surfaces, and comparison views across versions of the model in its editing history.
As a viewer, the user can easily inspect and navigate the model with the aid of these authored views.
Figure 1 shows the user interface of our system, which consists of three main components: the main window, the overview panel, and the navigation panel.
We also observed that modelers tend to sculpt with short brush strokes localized to surface features, e.g.
Presumably this is because 3D sculpting is largely a process of accumulating small additions and removals of local details.
Similarly, modelers tend to work in a spatially- and temporally-coherent fashion.
Both observation and interviews confirmed that modelers tend to focus on one area of the model at a time, before moving on to the next.
This indicates that we may be able to segment the 3D model into meaningful parts and to identify important regions, for the purpose of viewing or summarization, based on a modeler's editing history.
However, in 3D sculpting, we sometimes observe the exact opposite.
These smooth "features" are usually marked as unimportant by geometric analysis, but are considered important by the modeler.
We observed that when authoring static views, the modelers first quickly selected many candidate views, followed by gradual pruning and refinement.
Similarly, when authoring animated views, the modelers first assigned rough control points and then refined the paths and surfaces in detail.
Such iterations between selection and refinement happened frequently throughout the view authoring process.
From our interviews we learned that the process could be tedious, spanning from several minutes to more than an hour, if animated views for walk-throughs or fly-bys were included.
In general, the modelers welcomed aids for automation but also showed strong demand of some level of manual control over the final results.
The main window  of MeshMixer is used to display the 3D model.
It is also the main area for inspecting the 3D model and for authoring the views.
The purpose of the overview panel is to show all authored views within the spatial context of the current 3D model.
Different visualizations are used to represent viewpoints, view paths, and view surfaces .
The observations and insights in the previous section motivate us to build a view authoring system that interactively suggests candidate static and animated views to the modeler during the view authoring process.
The core of the system is an automatic view suggestion algorithm driven by the modeler's own editing history.
It is based on the observation above that there are significant amount of regularity in the camera movements and sculpting strokes and they may provide useful information for modelers' view authoring that is not readily apparent in the final 3D model.
The version slider allows users to navigate through previous versions of the model that were manually saved by the modeler or automatically saved by the system based on user-specified elapsed time and operation count.
Each version has its own unique color code, and the corresponding model version will be displayed in the main window as the user drags the slider.
The thumbnail list shows the representative images of all authored views with a color-coded stripe indicating the corresponding versions.
Suggested view surfaces are 3D spherical patches that spatially constrain the viewing camera for easier navigation.
Suggested view paths are 3D paths consisting of a series of camera viewpoints.
Once a candidate is chosen, the modeler can adjust the size of a view surface and the position and length of a path by dragging the mouse.
The modeler can inspect the 3D model in the main window with standard camera controls.
When viewing the model from a desired viewpoint, the modeler can store the viewpoint.
This authored viewpoint will then be added to both the overview panel and the navigation panel.
Per our initial observation of the iterative authoring process, the modelers might desire a list of view candidates they can choose from and customize.
Thus our system provides the function of interactive candidate viewpoint generation for a user-specified region of interest.
First, the modeler specifies a region of the 3D model by directly painting the region on the surface of the model using the triangle selection function in MeshMixer .
A list of viewpoints associated with the specified region is then displayed in the main window.
The additional temporal dimension embedded in the editing history enables the modeler to revisit previous model states in the modeling process and associate authored view with the version.
The arrow in the overview panel used to visualize the viewpoint is color-coded to represent its associated version.
A context menu is used to toggle the comparison view mode on and off.
Once enabled, the user can drag the version slider to set the versions being displayed.
In a comparison view, the camera navigation of both versions is synchronized, so that the modeler can easily compare specific areas of the model across versions.
This function is particularly helpful when the modeler would like to highlight the evolution and differences of a 3D model in the modeling process.
Selecting any two-color arrow from the overview panel will bring up the associated sideby-side comparison view into the main window.
To further facilitate view authoring, our system can visualize various aspects of the editing history, including time spent on each surface region and the camera paths.
By accumulating the sculpting time spent on surface regions we can render a time map on the surface , while for camera history we create 3D ribbons along the camera paths .
Note that the homogeneous colors around features, such as the eyes, the wings and the chest, support our observation that brush operations tend to be localized.
The modeler can author views while the editing history visualization is displayed.
The sculpting history may consist of thousands of operations, thus we also provide a region-specific history filter function .
The modeler can then navigate between the filtered camera positions.
A viewer can explore the model using both the navigation and overview panels.
For example, the viewer can double click on a preview thumbnail in the navigation panel, which will load the associated version and viewpoint into the main window.
When the viewer selects a pre-authored path or surface, the user can perform the associated constrained navigations of the model, e.g.
This allows the viewer to easily insect the model and experience high-quality views that the modeler has created.
To further enhance the portability of the authored viewing experiences, our system provides the ability to export the authored views as a collage of representative 2D images.
Our system pre-renders the authored views and arranges them according to pre-defined template layouts.
The tool is particularly useful in helping modelers showcase their work on printed mediums or devices with low graphics capability.
Our system provides two kinds of collages: summary collage and progress collage.
Given the modeler-specified region, we identify sculpting views as those camera positions where the modeler applied a sculpting brush to that region.
We then apply spatial clustering to these views to pick representative ones.
Our algorithm takes a set C of sculpting-view cameras, and builds an octree based on the camera positions.
We then compute the accumulated camera time in each cell by summing the camera times in its sub-tree, and search for the dominant cell bottom-up until we find the one whose accumulated time is the largest among cells at the same octree level and is over 70%  of the total time.
Finally, we calculate the weighted average center  of the viewpoints in the dominating cell, and select the camera in C that is closest to the weighted center.
We denote this camera viewpoint csculpt, the representative sculpting-view for the specified region.
To identify inspection views, viewpoints used by the author to assess the model, we search for camera manipulations which occur between two sculpting operations applied to the same segment.
We treat these as inspection operations.
For each such camera manipulation, the sequential camera positions create a 3D piecewise-linear curve L. We compute the opening angle at each vertex  of L, and choose as a turning point, cturn, the vertex with largest opening angle .
For a given region, we take the 3D cameras associated with all of the turning points and then apply the same clustering technique described above to extract the inspection-view, cinspect, for a segment.
Note that the camera controls between sculpting might not always be inspections.
However, the experiment results seem to show that the modeler tend to stay on noninspection cameras position much shorter and our algorithm is capable of identifying meaningful cinspect.
Furthermore, the goal is to provide suggested views for users to refine and choose and thus 100% accuracy is not necessary.
Iterating the procedure described above, we can extract multiple sculpting views and inspection views.
We then treat these extracting views as candidate viewpoints and the oscillating camera traces these views belong to as candidate view paths of the specified region.
A progress collage is meant to be used for design review and workflow summarization.
The progress collage displays a series of version snapshots from a fixed viewpoint .
In each snapshot the surface is colored using a heat-map based on the accumulated editing time difference from the previous snapshot.
This visualization enables users to rapidly assess which areas of the model have been changed between versions.
The editing history recorded by our instrumented MeshMixer comprises of a complete log of the two most significant interactions in a 3D sculpting tool: interactive sculpting brush strokes, and 3D camera manipulations.
For each sculpting brush operation we store: a unique operation ID, elapsed time of the stroke, a list of the affected vertices, and the current camera viewpoint.
For the camera history we store all intermediate cameras  during each camera transformation operation, as well as the elapsed time of the entire camera manipulation.
Our system interactively suggests candidate views based on the region specified by the modeler.
The following paragraphs describe the algorithms that generate candidate viewpoints, view paths, and view surfaces.
Our approach is motivated by the camera oscillations discussed in our initial observations.
The captured sculpting views and inspecting views tend to form dense, disjoint clusters whose weighted centers we interpret as candidate viewpoints for the surface region specified by the modeler.
From these candidate views, we interpret the associated oscillating camera traces as candidate view paths.
Given a surface region of a 3D model, we calculate the average center vcenter of the surface vertex and obtain its dominating octree cell of sculpting views and representative sculpting-view csculpt with the algorithm described above.
Next, we construct a spherical patch with the center at vcenter, radius as ||csculpt - vcenter||, and spanning angles that cover the octree cell.
For complex models with longer editing history, a faster GPU implementation might be needed to achieve good performance.
The two threshold values in camera clustering  and region growing  can be interactively adjusted by the users.
The results will be visualized on-the-fly as in Figure 6 and Figure 11.
In our current system, we hand-picked the value of T1 = 0.7 and T2 = 0.3 for all results in the paper.
It is also worth noting that previous work  assumes a 2 DoF camera model, where cameras lying on a fixed-radius sphere facing the center of the model.
On the contrary, our framework extracts cameras from the editing history and preserves the 6 DoF camera model.
In addition to suggest views for user specified regions, our system can also automatically segment the 3D model into meaningful surface regions and suggest good viewpoints for each of them.
This allows our system to recommend global viewpoints that summarize the entire 3D model.
Per our initial observations, the modelers sculpt sequentially and locally.
This leads to a feature-centric approach - we explicitly segment the surface into  regions based on the editing history and then rank them by an importance factor.
We first define a segment as a region in which each vertex has been given a roughly similar amount of attention measured by accumulated sculpting time, by the modeler.
We then extract segments via standard greedy regiongrowing .
To create a segment S, we first choose as the segment seed the vertex vs with the largest accumulated sculpting time T.
We then incrementally include vertices v adjacent to S which have a T similar to the average accumulated sculpting time for S, denoted as T.
The set S is then removed from the candidate set and we repeat the above process until either the required number of segments are found, or all vertices have been consumed.
Finally, we apply a refinement post-process.
For each modeling operation that affects any of the vertices contained in segment S, we grow S to include all vertices of that operation.
This is based on the previous observation that brush strokes tend to be local and stay on same feature.
The practical benefits of our view authoring system strongly rely on a premise that the history-assisted viewpoint selection algorithm generates viewpoints suitable for understanding the modeling process and the workflow history.
In particular, it is important to verify the underlying assumption that good editing views are correlated with good display views.
To check this premise, we compare our automatic global viewpoint suggestion algorithm to two alternatives: viewpoints generated automatically by the state of art method in Secord et al.
Note that it is not our goal to outperform the manual selections.
Instead, we hope to get close to the quality of manually generated viewpoints with the advantage of a time saving automatic approach.
We recruited a 3D modeler to create three 3D models from a sphere using our instrumented version of MeshMixer.
Table 1 provided a summary of the 3 models.
These models had varying levels of complexity and geometry features.
For example, the hydra model had fine-grained details that would have required careful close-up viewing and editing while the squirrel model had a smooth shape suitable to be viewed far away.
Given surface segmentation, we then collect the csculpt and cinspect for each segment, as described above, and treat them as the initial candidate viewpoints.
On average, two task took approximately 30 to 40 minutes to finish.
In the end of the session, we briefly interviewed the participants about the reasons for their ratings.
Unless otherwise stated, we analyzed non-parametric participant data  with the Friedman test and pair-wise comparisons with the Wilcoxon signed-rank test.
Figure 13 shows the result of the first question in task 1.
Although there is no statistically significant difference among the data, the responses indicated that our approach covers similar combination of shape and detail to the manual approach.
These results are consistent with a visual inspection of the viewpoints generated by the three algorithms .
It can be seen that the Secord et al.
Once the 3D models were created, we generated 5 best views of each model using three different methods: our automatic viewpoint selection algorithm , Secord et al.
For the algorithmic approaches we used the 5 top-ranked views for each 3D model.
For the manual approach, we instructed the 3D modeler to "assume you are going to showcase your model on a website where one can only upload 5 images per model".
It took the modeler about 15 minutes to pick all 15 static images.
Figure 12 shows some viewpoints generated by these three methods.
Please refer to the appendix file for complete materials.
For task 2, we measured the average number of images picked from the three different methods.
Our algorithm had the highest rate of being picked by participants, but there were no significant differences among the three groups .
In summary, the results showed that good views from the editing history are indeed correlated with good display views.
The viewpoints generated by our algorithm are similar in quality to those handcrafted by the 3D modeler,
After the walkthrough and short tasks, we replayed the sculpting video of the "hydra" model  at 5x speed to the participants.
We then asked them to author views of the model using viewpoint, comparison view, view path, and view surface, at least once.
We replayed the modeling process before the view authoring session to allow participants to get a sense of the modeler's workflow and the viewpoints that were used.
We did not ask the participants to perform the 3D sculpting on their own, mainly because it could take too much time.
At the end of the study, an interview was conducted and the participants filled out a questionnaire regarding individual features of the system.
In addition, our study demonstrates the limitations of using existing automated techniques for assisting modeler's viewpoints authoring process.
In the post-interview, we found that even when two participants gave the same view an equal score, the provided reasons could differ dramatically.
For example, for the bottom right image of Figure 12, a participant rated it as 5 because it showed the shape of the horn, while another rated it as 5 because it showed a clear silhouette of the head.
It suggested that people could observe the same 3D model for different personal purposes, and it is difficult, if not impossible, to have a single automatic algorithm to cover everyone's preference.
This reiterates the findings in our initial interview sessions, where the 3D modelers welcomed the automatic assistance on the view authoring while also considered the manual adjustment/modification over the final result necessary.
The second user study was designed to provide a qualitative evaluation of the entire view authoring system, and to gain insights, observations, and feedbacks.
To evaluate the authoring system, we recruited three professional 3D modelers, two computer graphics researchers, and one architecture researcher.
We used the same 3D models and editing histories in Table 1.
Overall, the participants were enthusiastic about the system, and considered it a potentially useful component in a modeler's toolset.
Participants were all able to complete the provided tasks without difficulty.
Figure 16 summarizes the subjective ratings of our system.
Overall, responses were quite positive.
When asked about their favorite functions, participants were enthusiastic about the access to previous versions, the comparison view function, and the time map.
In particular, the comparison view function received very positive comments from the professional modelers.
P2 commented that when looking for critiques from peer modelers or asking for help on detailed topology issues, it is common for 3D modelers to post before-and-after image pairs on a forum.
The evaluation sessions began with a 20 minute walkthrough of the view authoring system.
We first demonstrated individual features of our system, and then asked the participant to complete related short tasks.
There were a total of 24 short tasks, such as "add a view path for the body region", and "add a comparison view for the head region for version 3 and version 5".
One professional modeler  particularly liked the time map function and commented that it would be very useful for workflow management.
He stated that he usually works on multiple projects at the same time, and sometimes he might overlook some detailed regions on a 3D model.
The time map function could serve as a good reminder about progress.
Another modeler  also commented that the combination of the time map and version slider could be very useful for an art director to track modelers' progress.
However, participants did express concern about learning so many new functions in such a short session.
Still, participants did comment that the regions-specific authoring techniques would be very useful in certain scenarios.
One 3D modeler  commented that the region-specific authoring function is particularly useful for parts that are difficult to inspect, e.g.
Compared to their previous view authoring experiences in 3D modeling software, the responses were quite positive.
Participants were excited about the additional temporal information provided by our system and thought it would be beneficial to have candidate views suggested during the authoring process.
We focus mainly on 3D digital sculpting users and tools.
We are interested in extending our methodology to other categories of 3D modeling tools.
Our work does rely on the assumption that the 3D modeling tool has been instrumented at a very detailed level.
The internal data model in MeshMixer is a straightforward triangle mesh; however the geometric representation in other tools might be more complicated.
Currently our system is designed for scenarios where the modeler sculpts a single 3D object from scratch.
However, some 3D objects or 3D scenes  incorporate a large number of objects or import 3D assets from a database.
Our instrumentation needs to be extended to handle deletion and creation of parts, copy-and-paste, and so on.
A merge algorithm that can handle editing histories from multiple objects might also be needed.
Finally, we focus our current method only on the editing and viewing history of a 3D model.
However, this does not mean that the model geometry information should be ignored.
We believe even more effective view selection and navigation systems can be designed by considering both the final model and its editing history.
We hope our work can inspire future work in this fruitful research direction.
We propose a history assisted view authoring system for 3D models.
Our system provides modelers the ability to traverse and author the camera views, paths, and surfaces across different model versions.
We introduce the regionspecific view authoring technique where candidate views are automatically calculated based on the specified surface region.
At the core of the view authoring system is a unified algorithm framework for 3D model segmentation and automatic view selection based on the editing history.
Our user studies show that the quality of the viewpoints generated by our algorithm is comparable to ones manually selected by the modeler, and the authoring system elicits a high level of interest from potential end-users.
