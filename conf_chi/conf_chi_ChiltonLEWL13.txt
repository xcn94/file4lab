Taxonomies are a useful and ubiquitous way of organizing information.
However, creating good organizational hierarchies is difficult because the process requires a global understanding of the objects to be categorized.
Usually this is done by an individual or a small group of people working together.
This approach does not work well for the quickly changing, large datasets found on the web.
Cascade is a crowd workflow that allows crowd workers to spend as little at 20 seconds each towards collectively making a taxonomy.
To achieve these results, we introduce a design pattern of generate-apply-edit and a novel technique called adaptive context filtering that allows the crowd to do robust categorization.
We evaluate Cascade and show that on three datasets its quality is 80-90% of that of experts.
Cascade has a competitive cost to expert information architects, but takes six times more human labor.
However, that labor can be parallelized such that Cascade will run in as fast as four minutes instead of hours or days.
Most successful crowdsourcing systems operate on problems that naturally break into small units of labor, e.g., labeling millions of independent photographs.
In contrast, Cascade develops a unique, iterative workflow which is non-trivial to break down and distribute amongst hundreds of people and demands no more than one minute of labor from any worker.
To create sophisticated crowdsourcing workflows, we need to employ design patterns that guide us to breaking down work.
Just as iterative design is a guide to developing usable software, and map reduce is a design to help process distributed data, crowdsourcing needs to follow design patterns to achieve results.
In Cascade, the crowd is led to follow a design pattern that we adapted from the creative process - generate lots of ideas for categories, and edit down the group of ideas to a cohesive whole by applying those categories to the raw data.
In this paper we first present the Cascade Algorithm and describe the three human intelligence task  primitives used to implement it.
We then demonstrate the results of running Cascade on three representative data sets scraped from the Internet.
We evaluate Cascade in three ways: we compare its time and cost to that of four expert information architects we paid to taxonomize the same data, we count the number of mistakes in Cascade's output and interpret it as an error rate, and we compare the coverage of the categories in Cascade to those of the expert-made taxonomies.
In summary, this paper makes the following contributions: 1.
A novel crowd algorithm, Cascade, that produces a global understanding of large datasets from the actions of individual contributors, none of whom see more than a fraction of the data.
The design pattern of generate-apply-edit that provides the framework for the Cascade algorithm.
A technique called adaptive context filtering that allows the crowd to do robust categorization when the matches are sparse.
An evaluation of Cascade on three datasets showing that Cascade can perform close to expert level agreement  for competitive time and cost.
Taxonomies are a useful and ubiquitous way of organizing information.
However, creating good organizational hierarchies is difficult because the process requires a global understanding of the objects to be categorized.
Currently, most taxonomies are created by a small group of experts that analyze a complete dataset before identifying the essential distinctions for classification.
Unfortunately, this process is too expensive to apply to many of the usercontributed datasets forming on the Internet.
Despite recent progress, completely automated methods, such as Latent Dirichlet Allocation  and related AI techniques, produce low-quality taxonomies.
They lack the common sense, flexibility and language abilities that come naturally to people.
This paper presents Cascade, a novel method for creating taxonomies.
Cascade applies automated techniques to combine the suggestions of many humans, none of whom have a global perspective of the data or the taxonomy under construction.
Cascade uses crowdsourcing, a distributed method for solving a task by broadcasting an open call for solutions to its subtasks and composing the responses into an integrated answer.
Crowdsourcing has become a popular way to solve problems that are too hard for today's AI techniques, such as translation, linguistic tagging, and visual interpretation.
However, our method is novel because of the global nature of the taxonomy-creation problem.
Iterative improvement is a general crowdsourcing pattern first described in TurKit.
Iterative improvement has proven successful at using multiple workers to build on and improve each other's image descriptions, and to collectively decipher blurry text or bad handwriting.
For example, given blurry text to transcribe, one worker will transcribe as much as he can, and another worker will iterate on the first worker's answer.
The workflow can then ask a third worker to vote on whether the second worker has made an acceptable contribution.
This can be repeated until a stopping condition is met - such as the entire text being transcribed or workers no longer being able to improve the transcription.
We applied iterative improvement to taxonomy creation by giving workers a list of tips and an editable hierarchy interface.
Workers were asked to improve the taxonomy by adding, deleting, or moving categories or by placing tips in the taxonomy.
We tested two iterative improvement interfaces: A text-based Wikipedia-style outline editing interface 
We observed that both iteratively improvement interfaces suffered from the same two problems.
The taxonomy grows quickly making the tasks more time consuming and overwhelming as time goes on.
The first tasks are very easy - creating the first category and placing a few tips in it is quick and easy.
However, when there are 50 categories and you have to read all 50 categories to figure out whether or not it belongs in any of the existing categories, the work becomes so challenging that single workers have a difficult time making contributions in a short time frame.
Additionally, taxonomy structure editing tasks such as merging two categories are proved difficult for workers because they require understanding a large fraction of tips to decide whether all the tips under "air travel" should be merged with the category "flying".
The task had many options for how to contribute 
Although giving workers options for how to contribute made the task very flexible, it also meant workers had to decide what was important to do next.
One of the things that made it so hard to know what to do next is that you can't coordinate with future workers about what they are willing to do and how they will interpret your contributions.
From our observations, we concluded that we needed to break down the taxonomization task into manageable units of work.
Although iterative improvement may work for tasks where the task selection is obvious and the ultimate output is a manageable size for a single worker, it is less successful for large tasks that require significant coordination between workers in order to know what to do next.
Many crowdsourced workflows start with a step for generating data , and follow up with a step that uses those initial generated data.
For example, the ESP game uses two people to generate image labels, and accepts only the shared labels.
We tried a similar approach for Cascade.
We first asked workers to read tips and generate categories for them.
Next, we wanted to organize the categories into a cohesive taxonomy by removing duplicate categories and nesting subset categories under superset categories .
When identifying the relationships between categories, A and B, we want to know if A equals B, if A is a subset of B, if B is a subset of A, or if A and B have no relationship.
To find this out, we pick two categories and ask workers which relationship they have.
In one design, we explicitly asked whether categories were equal, supersets, subsets, or unrelated .
In another design we showed the workers 20 categories and let them select the pairs they wanted to compare and then say whether they were the same concept, or related concepts 
Although this approach seems natural and straightforward, regardless of the design, our workers did not agree with each other and didn't agree with our "gold standard" judgments.
From these observations, we concluded that it was a mistake asking workers to compare abstractions to abstractions - the differing assumptions people make about abstractions are too hard to write down and render individual worker's judgments incomparable.
Instead, judgments should be made relating actual abstractions to data, in this case, relating categories to tips.
The underlying problem in category comparison is the inherent difficulty of comparing two abstractions.
In order to compare two abstract category names, workers must make assumptions about their meaning - some people may interpret categories broadly and some narrowly, some people may see connections that others may not.
There are many assumptions that go into comparing two abstractions and without a way for workers to write down their assumptions, their judgments of the relationship between two categories weren't comparable.
For example, in the dataset of travel tips, our "gold standard" considered categories "air travel" and "flying" to be the same concept.
Some workers agreed with this but many workers considered these to be different.
They thought that "air travel" was a superset of "flying."
In fact, both interpretations are correct.
The category "flying" can mean both the entire set of activities around flying  and can mean just the part of air travel where you are on an airplane and in the air.
Difficult comparisons also included "TSA liquids" and "removing liquids", "packing" and "what to bring", and "advice" and "general advice."
In order to effective compare these two abstractions workers need more context to ground the meaning of the abstract category labels.
In this case, the context is the tips.
In effort to cluster tips, we tried putting tips in small, randomly generated groups and asking workers which tips were the most similar.
Workers were eager to do this task, but unfortunately, their judgments were not consistent.
The problem is that there are many potential similarities among data that is very open-ended in its coverage.
In order to compare workers' judgments of tip similarity, we not only need to know which tips they think are most similar but also the dimension along which their similarity is being judged.
For example, if we have three tips, A, B and C. We could have that A and B are both about "air travel", B and C are both about "saving money" and C and A are both about "websites."
In this case, all there is no clear pair of similar tips in this set.
The pair of tips that you pick as similar depends on the dimension on which they are being compared.
If this problem were rare, we could perhaps work around it, but in an open world, there are many 
We concluded that to combat the incomparability of judgments based on unstated assumptions it is necessary to extract the dimension of similarity as well as clusters.
University of Washington Technical Report UW-CSE-12-11-02 workers with a small number of tips  and asked them to suggest categories that fit at least two tips .
Although workers found the task easy and intuitive, the quality of the categories was not as good as when we generated category suggestions for single tips.
Restricting workers to naming categories that satisfied multiple tips seemed to encourage people to name overly broad categories that fit all the 8 tips such as "good tips" or "advice" and prevented them from naming the category that would fit one tip perfectly, and would perhaps fit other tips which didn't happen to be in the random group of 8.
For example, workers can use their common sense and intuition about travel advice and infer that "TSA Security" might fit multiple tips even though it only fits one tip in the 8-tip subset.
We decided that the clusters workers found in small subset of the data were often unnatural and forced and that it was better to allow workers to suggest categories that fit one tip very well rather than fit multiple tips more loosely.
However, "saving money" doesn't strictly apply to "air travel" it could apply to nearly anything.
We learned from Category Comparison prototype that comparing abstraction to abstraction is hard - given two categories labels it is often difficult to say whether one is a proper subset of the other.
If the suggested subcategories are in fact not subcategories, then we aren't partitioning the data effectively, which was the crux of the efficiency gain from a divide and conquer approach.
One approach to fixing the problem of ambiguous hierarchical relationships of categories is to narrow down "saving money" to "saving money on air travel."
Thus, by construction, "saving money on air travel" is a subcategory of "air travel."
The danger of doing this is that "saving money" might be a subset of many large categories: "saving money in international travel" "saving money on hotels", "saving money on train travel" and possibly many more.
It's possible that "saving money" is a big category, and we don't want it appearing multiple times in the taxonomy split across multiple categories.
If we allowed this, then the taxonomy would have a very large branching factor and explode quickly.
We concluded that the divide and conquer approach to creating taxonomies did not work because of the difficulty deciding whether a category was truly a strict subcategory of another or a cross-cutting facet without looking at all the data.
Without lines along which we could effectively partition the data and then only work on that partition, divide and conquer is not an effective strategy.
One approach to taxonomy creation is to partition data into several the large categories, then recursively divide large categories into subcategories.
For example, in the travel data, you might find that "air travel" is a large category.
By recursing into the "air travel tips" you could then sub categories of "air travel" such as "in-flight meals", "layover", and "TSA security."
We found that this approach has one critical problem which is related to the difficulty of comparing abstractions in the Category Comparison prototype.
In our experience, when soliciting subcategories for "air travel" workers do suggest strict subcategories of air travel such as "in-flight meals" but workers also suggest categories which may or may not be strict subcategories.
The categories they suggest might in fact be cross-cutting facets that apply outside of "air travel."
From initial approaches to crowdsourcing taxonomy creation, we learned several things.
We first learned that the task needed to be deliberately broken down into subtasks.
Next, we learned that comparing abstractions to abstractions was noisy and thus it was better to compare abstraction to data .
We learned that to get better tip similarity judgments, we needed to extract the dimension of similarity along with tips were being compared, and make sure that each dimension of similar fit at least one tip very well as opposed to fitting multiple tips more loosely.
Lastly, we learned that it is better to solicit multiple categories that overlap  and figure out the hierarchical relationships between them later rather than trying to identify the biggest categories and then recurse into them.
These approaches lead us to our final algorithm design wherein we first generate many category names, we next vote on the best category names to remove spam, chose the best of multiple alternatives, and to eliminate vague categories, and categories that otherwise are sub-par.
Cascade's behavior is guided by a set of parameters, which we now summarize and name our default values where applicable.
Let mn denote the number of tips considered in Cascade's initial, generative pass, default = 32 * Let k be the replication factor , default = 5.
By changing the values of these parameters, the designer can trade off cost and running time against taxonomy quality.
Decreasing k or increasing t, c, and s will lower the cost of execution.
In this section we discuss the inputs and output of Cascade, the parameters that govern its execution, the steps of the algorithm, the three HIT primitives used to implement it, and the conditions for running Cascade iteratively on a dynamic data set.
We include a running-time and cost analysis of Cascade.
Cascade takes two inputs: a set of items  to be categorized and a descriptive phrase  identifying this set.
Although "items" is a more general term, and has the correct level of generality, we find that in talking about Cascade, the concreteness of the term "tip" is less confusing than "items."
An example of inputs to Cascade are a tip-set of 100 responses to the question "What is your best travel advice?"
This paper demonstrates Cascade only on textual items .
However, we believe Cascade is general enough to work for a wide range of entities  as long as each item can be judged independently by a human.
Cascade is not designed to run ordered sequences of items .
Cascade is designed to run on tips that take less than one minute for a human to process and judge.
In theory, Cascade can handle tip-sets of arbitrary cardinality.
The smallest set we've run is 22 items,1 and the largest is close to 200 tips.
The algorithm's expense grows super-linearly in the number of tips, so although there is no theoretical limit to the size of the tips-set, it does become more expensive.
Future work can be spent on optimizing the cost for large tip-sets.
Before describing Cascade's overall control flow, we first present the three types of HITs that are presented to workers.
The order in which these tasks are generated is dependent on the characteristics of the input tips and can be complex, but the primitives are individually quite simple.
Here we present them abstractly; Figure 6 displays concrete instances of these tasks.
SelectBest -> 1 category The SelectBest HIT shows a worker a single tip and c distinct category tags and asks her to pick the single best tag.
Categorize -> bit vector of cardinality s The Categorize HIT is similar to SelectBest, but is used at a different point in the Cascade process.
It presents a worker with a single tip and s possible category tags and asks the worker to select all of the categories which are relevant to the tip.
The output of Cascade is a taxonomy consisting of labeled categories and associated tips.
More precisely, Cascade generates a tree whose nodes are labeled with a textual string, called a category; the tree's root is labeled with the topic input and an `other' node is added as a child of the root if necessary.
Cascade appends categories to the tree root and appends categories to non-root categories.
The depth of tree Cascade produces is often greater than one.
It is important to note that Cascade allows tips to appear in the taxonomy in multiple categories.
Tips generated in the wild often span categories.
For example, it is easy for a travel tip to be both about "air travel" and "saving money."
Described at the highest level, the Cascade algorithm takes every tip and solicits multiple suggested categories for it from different workers.
A new set of workers then votes on the best suggested category for each tip.
Cascade then asks workers to categorize every tip into every best suggested category.
With that data, it computes a taxonomy where duplicate categories are removed, empty categories are removed and related categories are nested appropriately.
Cascade uses steps 1-4 to generate new category labels for these tips.
Cascade concludes by iterating over the complete set of n tips to label them with the new categories where appropriate.
The rest of this section explains how these steps are implemented using a combination of automated algorithms and the Generate, SelectBest and Categorize HITs.
Step 3, which we term adaptive context filtering, is the most expensive and also the most novel, since it ensures globally meaningful results from individual contributors with a local perspective.
HIT Primitives used: Generate is called  *k times.
Output: k suggested categories for each of the m tips.
The first step of Cascade is to show each tip to k=5 different people, to tell them the topic the tip belongs to, and to have them suggest a category it might belong in.
Although tips could be presented individually, we present tips in groups of t=8 using the HIT primitive Generate.
The instructions explain that although multiple tips are displayed together, category suggestions should be independent, meaning that workers do not have to try to come up with categories that apply to multiple tips in that HIT.
By showing a group of tips in each HIT, workers get more context about the tip-set as a whole.
This is useful particularly for a worker's first HIT in the tip-set, by providing the worker with an idea of the spread in the tips before she suggests a category.
Another benefit of showing multiple tips in a group is that workers can easily "pass" on one or two tips in the HIT that are difficult to categorize 
University of Washington Technical Report UW-CSE-12-11-02 In Step 1, k=5 individual workers attempted to categorize each tip, resulting in up to five suggested categories for each tip.
There will be fewer than k=5 suggested categories if one or more workers "passed" on the tip or categories were removed for being exact-string duplicates.
In Step 2, we show each worker one tip and all of its suggested categories and ask workers to pick the one they think is best .
Any suggested category that gets two or more votes will get passed on to the next step.
We call these the best suggested categories.
The suggested categories that do not meet the threshold will be filtered out by the algorithm.
We use this matrix to edit down the list of best suggested categories in the following ways: Remove duplicate categories.
For any two categories that share more than 75% of their tips, we remove the category with fewer tips  2.
For any category, c_small that shares more than 75% of its tips with another category, c_large, make c_small a subcategory of c_large.
Remove categories that are too small.
Remove any category that has fewer than two tips.
This results in a taxonomy where all categories have at least two items, sibling categories are distinct, and subset categories are properly nested under their super category.
HIT primitives used: Categorize is called m*ceil*k + m*k times Output: a list of best suggested categories and the tips that fit in them Phase 1 In Step 1 we generated more categories than we needed for each tip.
In Step 2, we filtered out categories that were not necessary for each tip.
In Step 3, we filter out categories that are not necessary for the tip set as a whole by asking workers to vote whether each tip fits each of the best suggested categories.
To do this, we present workers with one tip and a group of s=7 best suggested categories and for each best suggested category they have to choose whether they think it "fits" or "doesn't fit."
If two or more out of five workers agree that a tip fits one of the best suggested categories, then it passes the filter and goes to the next phase of Adaptive Context Filtering.
Phase 2 The results of Phase 1 give us a list of potentially applicable categories for each tip.
These categorization decisions are deceptively difficult - tips range in quality and in clarity, and best suggested categories range in how vague they are.
It is easier to make difficult categorization decisions when the group of categories presented together are all potentially applicable, as opposed to the applicable categories being spread very sparsely across hundreds of HITs.
Phase 1 of this step aggregates the potentially applicable categories for each tip, and now in Phase 2, we categorize again on those potentially applicable categories.
Although we are using the same HIT primitive , the context we are asking it in  has improved, which allows workers to be more discriminating.
If four or five out of five workers agree that a tip fits one of the applicable categories, then it passes the filter and goes to the next step of Cascade.
When you apply Cascade to a tip-set there are several reasons you might not want to run it on all the tips at once.
One reason is that the tips may be generated dynamically and as new tips come in, you want Cascade to update its Running time Step 1 : Intentional Category OverGeneration Step 2: Best Category Suggestion Vote Step 3.
Adaptive Context Filtering - Phase 1 Step 3.
Adaptive Context Filtering - Phase 2 Step 5.
The second reason is that you may try to save time and money by applying Cascade to a subset of the data, then deciding later whether to run the rest of the tips depending on how "done" you think the taxonomy is.
Whatever the reason, Cascade can be run on a hold-out set of tips to grow the taxonomy as follows:
This is probably because it is hard to update as new responses appear and because the time and effort required to compile a fair, global picture of free-text responses is non-trivial.
The three datasets were picked are summarized in the table below.
Abbreviation editWriting sideProjects Topic "What are some tips for editing your own writing?"
No HITs Output: a tip-set From the taxonomy, determine which of the hold-out tips are not in any category and which of the hold-out tips are only "loosely categorized."
A tip is loosely categorized if it is not in any category that has fewer than 20 tips.
20 is the minimum number of tips we use in Cascade in order to recurse.
The idea behind this step is that if a category that is exceedingly large ends up in the final taxonomy, we don't want to treat tips that are in it as being categorized to satisfaction.
For example, the category "all tips" could be suggested.
All the tips would be in this category, but it wouldn't contribution to our understanding of the data.
This is actually a big problem.
Workers often produce vague categories that have 60-70% of the data in them.
In our "travel advice" tip-set the category "travel organization and convenience" had 68% of the tips, but clearly does not make a useful contribution to our understanding of how the data breaks down.
Often, a single response will contain multiple tips in bulleted lists, numbered lists or separated by paragraphs.
We manually broke these responses into their separate tips.
We changed the text minimally to make individual tips readable by means of capitalization, removal of leading bullet points, and reiteration of pronouns .
Previously, we have had the crowd do this breakdown.
It is a trivial process but not a part of the Cascade Algorithm.
We randomized the order of the tips to avoid any effects of our workers seeing tips in the order they were generated.
We ran Cascade on three datasets scraped from Quora.com, an online Question and Answer site reputed to be of high quality.
Many of the questions on Quora are fact-based, such as "How much did it cost AOL to distribute all those CDs back in the 1990s?"
But many questions have no single best answer and all the responses are valid answers to the question, such as "What are your best travel hacks?"
These types of questions get many responses and it is time consuming to get a sense of what has been said.
This is the type of domain where a taxonomy would help users get a global picture of the data and navigate the responses.
We ran Cascade on three tip-sets.
The smallest, editWriting, required only one iteration of Cascade, starting with all the tips.
The mid-sized tip-set, sideProjects, was first run with 32 tips .
The taxonomy produced in the first iterations was then applied to the hold out tip-set and there was an insufficient number of uncategorized or loosely categorized tips to run a second iteration of Cascade.
The largest tip-set, travel, was first run with 32 tips, .
When categorizing the hold-out tips to the first iteration taxonomy, 51 tips were not adequately categorized, so a second iteration of Cascade was run.
Here are details of how Cascade ran on each tip-set, which will be discussed below.
University of Washington Technical Report UW-CSE-12-11-02 Cascade on.
This tip-set required only one round of Cascade, wherein we created a taxonomy for all 22 tips.
None of the tips were uncategorized at the end and we ended up with 15 categories, 8 of which were top-level categories .
To get to the final 15 categories, we started with 83 suggested categories, filtered that down to the 33 best suggested categories, then did exactly-string matching to filter that down to 27 unique best suggested categories.
After applying the best suggested categories to all the tips, we removed 4 categories for having too much overlap  and 8 categories because they had 0 or 1 tips in them .
The resulting taxonomy can be seen in Figure 7 Larger Category Self-Editing Read out loud getting help working off an outline Smaller Category Editing Read aloud asking for help editing Outlining Overlap % 82% 100% 100% 100% categories, we removed 2 categories for having too many associated tips, and nine categories which had too few  tips in them.
After generating the first-round taxonomy, we applied the remaining 35 tips to it and found that it explained all but 2 of the tips and had no loosely categorized tips and thus we did not need to start a second round of Cascade because no new categories were required to categorize the tips.
To get the final 22 categories in the first round of Cascade, we generated 120 suggested categories and filtered that down to the 37 best suggested categories.
After doing exact-string matching we were left with 34 unique best suggested categories to apply the data to.
Travel is a large tip set with 100 items, of which 32 were used in the first round to generate a taxonomy with 7 categories.
After generating the first-round taxonomy, we applied the remaining 68 tips to it and found that it there were 15 tips which it did not categorize and 51 loosely categorized tips.
The loose tips were all in the category "travel organization and convenience."
University of Washington Technical Report UW-CSE-12-11-02 To get the final 7 categories in the first round of Cascade, we generated 149 suggested categories and filtered that down to 45 best suggested categories.
After doing exactstring matching we were left with 40 unique best suggested categories to apply the data to.
After applying the 32 firstround tips to all 40 unique best suggested categories, we removed 4 categories for having too much overlap, and 29 categories which had 0 or 1 tips in them.
The fact that one round of Cascade left 66 items unclassified gave us the opportunity to run a second round of Cascade.
This resulted in a taxonomy with 51 items.
The most important things to notice about the performance of Cascade is that in all three datasets, we started with many more tips than we intended to include in the final taxonomy, and effectively edited it down to a better and more cohesive set of categories.
Most of the categories that were eliminated for having fewer than 2 tips had 1 tip .
This makes sense because one would expect each category to contain at least the one tip that originally generated it.
What fraction of tax1-tax4categories are named in another taxonomy in tax1-tax4?
We want to compare the fraction of taxC categories used by experts to the fraction of categories used by at least two experts for tax1-tax4.
The comparison may seem slightly unfair in favor of taxC because taxC gets compared against 4 other taxonomies and tax1-tax4can only compare against 3 other taxonomies.
However, Table 6 contains the results.
For all three datasets, about 50% of Cascades categories were also named by an expert.
For example, in the editWriting dataset, four out of four experts named a category closely matching Cascade's category "working off an outline."
When comparing experts to themselves, the average expert matching fraction was 32%, 70%, and 64% for the three datasets.
This averages to 55% of tips matching another expert's tips across these three hierarchies.
Therefore, Cascade had 91% of the category agreement the experts did among themselves.
The goal of Cascade is to produce a taxonomy that provides a global understanding of independent tips.
There are three questions we want to answer to determine how well Cascade performs: 1.
Are the category labels in the taxonomy as good as labels created by experts?
Do we create an appropriate hierarchical structure in the taxonomy?
Is the cost and running time of Cascade competitive with that of hiring experts?
Taxonomies are inherently subjective; there is no right answer.
One would not necessarily expect two experts to produce the same hierarchy.
However, given a small pool of experts independently categorizing a dataset, one would expect some of the same categories to appear in multiple experts' taxonomies.
In order to compare Cascade's categories to those of experts, we paid four information architects to produce taxonomies independently for our three datasets.
We performed the following comparison on the taxonomies.
For each data set, we took the Cascadeproduced taxonomy, taxC, and the four expert taxonomies: tax1-tax4.
We wanted to know two things: 1.
What fraction of taxC categories are also named in tax1-tax4?
Cascade infers a global understanding of the data from the tip membership of categories.
Cascade removes categories that do not have enough tips in them, removes categories that have a high tip overlap, and creates a parent-child relationship for categories where one category has high tip overlap with the other.
These inferences are based on many small judgments by potentially hundreds of different people.
We want to know if all those judgments come together to infer a sensible hierarchy.
In particular, we are looking for three types of mistakes in the Cascade hierarchies: 1.
University of Washington Technical Report UW-CSE-12-11-02 To find the error rate in the hierarchical structure, we divide the number of errors by the number of categories in the taxonomy.
Both were duplicate categories errors.
The categories "tips to edit better" and "how to edit better" should have been the same, but Cascade did not remove one of them.
This came from 3 incorrect parent-child relationships: `prioritizing' was the parent of `commitment,' `prioritizing' was the also parent of `consistency,' and `motivation' was the parent of `relaxation.'
In our judgment, there is no clear reason that prioritizing should be a parent of commitment or consistency, or that motivation should be the parent of relaxation, and thus it is a mistake in the hierarchical structure of the taxonomy.
These are errors produced by the machine step - the Edit Matrix Operations - which created a parent-child relationship any time more than 75% of the tips of a smaller category were also in a larger category.
Concretely, the Edit Matrix operations nested commitment under prioritizing because more than 75% of the tips about commitment were also about prioritizing.
However, although these categories share many tips in common, they aren't semantically related: this is a danger of machine steps.
Perhaps a solution would be to have humans check the resulting taxonomy for obvious errors.
Across the three datasets, the average error rate was 18.5%.
There was an impressive number of correct parent-child relationships, especially in the travel dataset.
Many airtravel and flight related categories with complicated nesting are expressed with coherent hierarchical structure.
For example, "Air Travel Tips" is a parent of "flights" which is a parent of "flight layovers."
It is non-trivial to compare the costs associated with creating a taxonomy with Cascade versus experts.
There is a cost-quality-time trade-off.
For example, on MTurk, if you under-price a HIT, it will eventually get done, but it will take a long time.
The most basic comparison we provide is the actual costs and times in our run of Cascade and that of our recruited experts .
Cascade took ~6.5 times longer to complete the HITs, and was 1-3 times as expensive.
However, the prices were set fairly arbitrarily.
We paid our experts $25/hour as a set wage.
We paid MTurk workers $0.05 per HIT.
The average time to complete a HIT was 21.46 seconds.
This equates to $8.39/hour which is high for MTurk.
That would reduce the cost of Cascade by a factor of 2, making Cascade's cost competitive with the wage we offered experts.
Comparing time is also difficult.
The total time spent on all three datasets by the average expert was 6 hours and 50 minutes.
And the total time spent by MTurk workers was 43 hours and 3 minutes.
This is a factor of 6.3 more time spent by MTurk workers.
Seeing as the work done by workers is basically replicated k=5 times over, the time it would take one person to run Cascade on themselves would be competitive with the expert's time.
More important than comparing total time spent on the algorithm is to think about the amount of time that it would take to run the algorithm if infinitely many people work in parallel, as is supported by Cascade.
Each worker spends on average 21.3 seconds per HIT, and all the HITs in any step can be run completely in parallel.
Cascade is driven not only by human judgments, but by human judgments based on other human judgments.
Since humans are difficult to predict, it is impossible to guarantee how Cascade will perform with different worker populations.
In the worst case, the workers could be unfamiliar with the domain and not generate any useful category suggestions in the first step.
Figure 11 Cascade applied to 100 randomly generated colors.
On the right is the output of Cascade - a taxonomy organizing the colors.
Colors can appear in multiple categories, and 12 of the 100 colors are in the "other" category - meaning they were not categorized into a category with at least two items.
Cascade would not produce any taxonomy at all beyond the root node.
If the categories were good, but workers voted erratically, the resulting taxonomy would have essentially random structure - repeated categories, parents and children that do not relate semantically, and missing parent-child relationships.
The worst case will always be bad for crowdsourcing problems, but the performance of Mechanical Turk workers represents a reasonable lower bound for the quality of the resulting taxonomy.
Given a community that cared about the data and had domain specific knowledge, they would probably do it very well.
Moreover, we believe that this task is as fun for some people as the ESP game and other game-based crowdsourcing and thus people will probably be willing to do it for free.
Every step involves reading tips and the tips are interesting.
It is hard to read one without wanting to read more.
There is probably potential to gameify the interface to encourage contributions.
Cascade has five steps, including two slightly unusual steps - over-generation and adaptive context filtering.
In this paper we did not seek to prove that these steps are required.
It seems plausible that we could just generate n categories, categorize all the tips and then do the edit-matrix step.
These two steps are the result of design decisions we made based on running the algorithm many different ways early in its development.
Things we noticed that encouraged us to keep these two steps.
First, the quality of the taxonomy is most strongly correlated with the quality of the categories.
If we did not over-generate tags, then ask people to pick the best, we would have ended up with lower quality categories.
In particular, we often end up with vague categories which are problematic for Cascade.
Vague categories contain a lot of members.
For example, in the travel dataset, vague categories might be something like "comfort and convenience" or "organization and advice."
These categories encompass most, but not all of the tips.
And are indistinguishable from large categories which are not vague, such as "air travel."
A combination of trying to filter out vague categories in the Best Suggested Categories phase and in Phase 2 of adaptive context filtering is the design that we settled on.
Although the evaluation in this paper only deals with text data, we have applied Cascade to visual data as well.
Figure 11 is an example of creating a taxonomy for 100 randomly generated colors.
Part of the future work for Cascade is to push the boundaries of what types of data humans can taxonomize For example, can we create taxonomies for images, audio clips, videos, and xixed media such as websites?
Nothing about the Cascade algorithm is particular to text.
We believe that any data type that humans can process will be applicable to use Cascade on.
In the past three years, there have been several crowd workflows that produce outputs more complex than the results of worker's local contributions.
TurKit is a programming environment that allows you to easily compose the results of tasks and issue new tasks built upon the previous tasks.
Turkit has been used to iteratively improve image descriptions, to pick the best photo from an album, and to decipher nearly-unintelligible handwriting.
These iterative tasks are a step beyond simple image labeling .
Much work was able to build upon this simple iterative framework .
Considering workflows that go beyond TurKit, CrowdForge uses a MapReduce-like framework for writing articles by mapping separate workers to different aspects of the article 
Mobi solves problems like travel planning that have global constraints which are met by workers creating to-do items for other workers to do.
Turk-o-matic asks workers to break down the task and then creates subtasks for more workers to do.
Real-Time Audio Capture  uses a combination of novel interface and sequence alignment to combine work.
Complex tasks can be tracked and managed .
Automated text-clustering such as LDA could be employed recursively to create hierarchical taxonomies.
One drawback of these automated approaches is that they tend to work best on very large datasets - 50 to 500 short responses are insufficient.
In practice, AI clustering algorithms require substantial tuning, e.g., manually removing stop words and choosing the number of categories.
Additionally, there are categories that LDA would not be able to produce because they are not based strictly on the text.
For example, LDA would not be able to create clusters that distinguish jokes with observational humor from jokes with puns because similarities within the groups are not present in the words, but are properties of the meaning as a whole.
Card Sorting is a technique for members of a group to contribute to an organization of their data.
In this paper we present a crowd-algorithm that produces a taxonomy for a set of independent data items, such as travel tips or ideas for how to edit your own writing or strategies for working on personal projects after work.
We show that using three HIT primitives - Generate, SelectBest, and Categorize, we can create an algorithm where each worker can do as little as 20 seconds of work and produce a taxonomy competitive in price and quality with expert information architects, but which will require more total time put in by people, mainly due to the replicative factor we use to ensure the crowd agrees on judgments.
