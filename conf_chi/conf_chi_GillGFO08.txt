Being able to automatically perceive a variety of emotions from text alone has potentially important applications in CMC and HCI that range from identifying mood from online posts to enabling dynamically adaptive interfaces.
However, such ability has not been proven in human raters or computational systems.
Here we examine the ability of naive raters of emotion to detect one of eight emotional categories from 50 and 200 word samples of real blog text.
Using expert raters as a `gold standard', naive-expert rater agreement increased with longer texts, and was high for ratings of joy, disgust, anger and anticipation, but low for acceptance and `neutral' texts.
We discuss these findings in light of theories of CMC and potential applications in HCI.
Knowing the emotional tone of comments circulating about one's company can be useful business intelligence.
Blogs  around the world can discuss a company's performance one day, and perhaps influence its share price the next.
Some tools exist which look at usage of mood terms in blog posts, analyzing large amounts of text to capture national responses to news or sporting events .
These tools measure emotion at a very coarse  level, however, often greater specificity is needed: Smaller text segments reflecting particular opinions may need to be extracted and classified for opinion or emotion.
Indeed, detecting emotion from short sections of text may facilitate the development of technologies to automatically detect emotion in email clients or in a friend's recent blog posts.
Eventually, user interfaces which can automatically detect and adapt to user emotion may be possible.
Additionally, there is an empirical question regarding the text-based communication of emotion, with different theories proposing varying degrees to which it is possible to understand social information, such as emotion in a computer-mediated environment.
One extreme perspective put forward in Social Presence Theory  is that less rich environments, such as text-based CMC environments, inhibit communicating emotional expression.
While in much richer environments  in which intonation and non-verbal cues are available, interlocutors are able to communicate a full range of emotional and interpersonal information due to greater social presence.
Alternatively, another theory  proposes that interpersonal cues, such as emotional information, are present in computermediated environments, but it just takes longer to derive the same information.
Therefore, in a CMC environment with potentially unlimited time, interlocutors would be expected to derive the same perceptions as is possible in face-to-face communication, either by placing greater emphasis on existing cues , or by developing new strategies such as emoticons.
Face-to-face or on the phone, people can often guess a speaker's emotion `just from their tone of voice': that is, without being able to identify the words being used, let alone their specific meanings .
But would we ever want to rely on words alone - without using information from the speech signal?
In some cases we have to: computer-mediated communication , email, textchat and websites all offer reduced media richness.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Naive judges  could accurately perceive their interlocutor's emotion, and were less likely to enjoy or want to meet the authors of negative messages relative to positive ones.
Additionally, a linguistic analysis of the transcripts found that authors portraying positive emotion used a greater number of exclamation marks, and used more words overall, whereas authors' texts portraying negative emotion used an increased number of affective words, words expressing negative feeling, and negations.
Punctuation features matched the self-reported strategies used by the portrayers of emotion, with this regarded as evidence for the Social Information Processing hypothesis .
In this paper, we further explore the text-based communication of emotion in CMC, and build upon Hancock et al.
Previous work has shown that perception of personality is possible using `thin slices' of email texts .
After all experts had assigned an emotional category rating to each of the 135 texts, 20 were selected as expressing strong and clear emotional content.
This was based on all expert raters agreeing on the emotion assigned, and having the strongest emotion rating .
For each of these 20 texts we use two versions in the subsequent analysis: For the long version, we retain all 200 words; for the short version we extract the middle 50 words of the 200 word text.
Note that in doing so, we ignore the sentence boundaries and so the start and finish of the sections may occur mid-sentence.
This avoids bias resulting from the experimenters selecting texts which they believe contain features important for emotion expression.
Comparison of long and short text versions revealed consistency of language and topic.
The 65 judges of emotion were students at a Scottish university .
Debriefing revealed all were frequent email users ; conversely, very few used blogs frequently , with 33 participants never using blogs.
All were naive raters of emotion.
An additional two participants are excluded from this analysis because they provided multiple or unclear responses.
The target blog texts used for emotion rating were taken from a previously collected corpus in which authors contributed their writings from one pre-specified month .
Blog lengths ranged from a few short postings to near daily posts of a few thousand words.
Authors granted permission for further use of each blog before collection.
From our blog corpus, an `expert' research assistant  selected the first 200 words of each post if they contained some emotional content or were `neutral', that is, apparently contained no emotion.
We do not have author ratings of emotions for the blog texts.
Therefore for each extract, expert raters rated these texts as expressing one of eight emotions  or neutral.
All 20 texts were presented to the naive raters of emotion in random order.
Participants were randomly assigned to two groups in a counter-balanced design: One group saw 10 long versions, then 10 short versions of the texts; The second group saw 10 short, then 10 long text versions.
All participants used the activation-evaluation wheel shown in Figure 1 .
Imagining x- and y-axes: Evaluation  is on the x-axis, with positive values on the right, and activity on the y-axis, with high activity at the top.
The strength of emotion corresponds to the distance from the center of the circle , with the center of the circle used to score 0 or `neutral' emotion.
This model is considered well-suited to computational work , has previously been used for rating emotion in speech , and allows comparison with findings for valence .
Alternative approaches to emotion are described in .
In the rating instructions, the judges were asked to rate `how they perceive the author's emotions' but `not to spend too long thinking about their answer, as we are particularly interested in  initial response'.
All ratings took less than 30 minutes, and were combined with another  text rating task not reported here.
Nominal logistic regression was run on the emotion judgment data.
We ignored the strength of emotion rating , simply coding expert-naive rater agreement as a binary value , and entered as a dependent variable.
Text emotion , and Text length , were entered into the equation as categorical variables; and an Expert text emotion x Text length interaction variable was included.
A participant variable was included to account for individual judge biases.
We avoid drawing conclusions from the ratings of `neutral' texts, given the lower probability of assignment due to the emotion wheel design.
This reveals that the judges were able to accurately rate these emotions in the text regardless of length.
For Acceptance a main effect indicates significantly lower agreement between Naive and Expert judges .
The results show greatest naive-expert judge agreement for the ratings of texts expressing joy, disgust, anger and anticipation.
Additionally, we note that overall greater text length increases naive-expert agreement, however examination of the interactions indicates that this is mainly for the texts with low agreement .
In the case of disgust, for which there are already high levels of agreement, the extra availability of textual information in the longer text slightly hurts naiveexpert agreement.
We note that the greatest naive-expert rater agreement is related to strongly positive and negative emotions : Apparently naive judges were better able to rate texts with strongly marked valence .
Conversely, texts characterized more by their activity appeared to be assessed around chance levels, and in the case of acceptance showed disagreement.
Discussing our findings in the context of CMC theories indicates that some emotion can be accurately expressed and perceived in short blog excerpts.
This contradicts predictions by the Social Presence Theory regarding lessrich media such as asynchronous text-based CMC.
However, what sense can be made of the behavior of individual emotions in CMC?
For the emotions which strongly express valence, these appear to be clearly discernable through thin slices of textual CMC, regardless of length.
In the case of perceiving emotions primarily related to activity, here the naive judges seem to have more difficulty.
The improvement in performance resulting from increase in text length appears to offer some support for Social Information Processing theory, however exposure to a much greater length of text may be required for significant agreement with the expert judges.
Additionally, since we are not able to contrast emotion perception performance for blogs with either synchronous CMC or other media, we do not make stronger theoretical claims.
Figure 2 illustrates expert-naive judge agreement as a percentage .
Additionally, the experts may have only selected blog texts which express emotion very saliently, although that may not be the case given the lack of agreement in some cases between the expert and naive raters.
Future studies would ideally draw upon self reports or even physiological measures of emotion from the authors during writing, and also contrast this with other forms of communication.
We leave this to future work.
This study builds upon previous work to study the way in which emotion is expressed and assessed in CMC.
We note that previous work in this area has been limited to positive and negative emotions , that the naive judges of emotion had a 30 minute interaction upon which to base their judgments, and finally that emotions were acted out through a confederate.
In the current study, we show that naive raters with little experience of using blogs are able to:  identify four emotions  with relatively high agreement with expert judges from naturally occurring data;  perform these accurate ratings based on short, asynchronous blog texts, which are  genuine emotions collected from real authors.
Further, these findings suggest that some emotions are expressed and perceived through asynchronous text-only environments, apparently contradicting Social Presence Theory which would expect such emotional expressions to be inhibited.
Rather, the fact that emotion rating agreement improves with text length is in line with the Social Information Processing theory.
However, we are reserved in generalizing this claim since our study does not contrast blog performance with other media.
Potential applications may include emotion monitoring of blog posts, or dynamic interfaces which adapt to user state based on linguistic features of the texts.
We thank Scott Nowson for the use of his blog corpus, Jonanthan Ellis for preparing rating materials, and our reviewers for their helpful comments.
