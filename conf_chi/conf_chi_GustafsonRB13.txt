Imaginary Interfaces are screen-less ultra-mobile interfaces.
Previously we showed that even though they offer no visual feedback they allow users to interact spatially, e.g., by pointing at a location on their non-dominant hand.
The primary goal of this paper is to provide a deeper understanding of palm-based imaginary interfaces, i.e., why they work.
We perform our exploration using an interaction style inspired by interfaces for visually impaired users.
We implemented a system that audibly announces target names as users scrub across their palm.
Based on this interface, we conducted three studies.
We found that  even though imaginary interfaces cannot display visual contents, users' visual sense remains the main mechanism that allows users to control the interface, as they watch their hands interact.
While these findings are primarily intended to deepen our understanding of Imaginary Interfaces, they also show that eyes-free interfaces located on skin outperform interfaces on physical devices.
In particular, this suggests that palmbased imaginary interfaces may have benefits for visually impaired users, potentially outperforming the touchscreenbased devices they use today.
Typically a chest-worn camera observes the user's hands and determines the position of the pointing finger with respect to the other hand.
By abandoning the screen, Imaginary Interfaces allow for ultra-mobile form factors.
The primary goal of this paper is to provide a deeper understanding of Imaginary Interfaces, i.e., not what they allow users to do, but why they allow doing it.
We perform our exploration with an example interface: we create a browsing interface  for imaginary interfaces and then use it to explore which inherent properties of palm-based imaginary interfaces cause it to perform the way it does.
Browsing matters, because one of the key design challenges of Imaginary Interfaces is to enable users to operate an unfamiliar interface.
The strongly asymmetric abilities of Imaginary Interfaces make this challenging: input is spatial and precise but, by definition, an imaginary interface cannot show users an overview.
Our previous work restricted users of Imaginary Interfaces to what has been taught offline.
For instance, with the Imaginary Phone  users are able to learn an imaginary interface by first using a physical device of identical layout and transferring the layout knowledge to the imaginary interface.
Unfortunately, this transfer learning is limited to the comparably small number of functions that users use on a frequent basis, such as the home screen launch icons  and therefore provide no basis for using the thousands of applications available for today's mobile devices.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Inspired by touch-and-explore interfaces designed for visually impaired users  we created an audio-based interface that announces targets as users scrub across them .
Based on this interface, we investigate which of the particular properties of palm-based imaginary interfaces allows users to operate such interfaces.
Imaginary Interfaces  allow users to perform spatial interaction despite the lack of visual feedback.
Imaginary Phone  allows users to interact on their palms, by mimicking the layout of a familiar mobile device .
The main contribution of this paper is an exploration into the inherent properties of palm-based imaginary interfaces and how the properties are responsible for user performance.
We find that  visual cues, i.e., observing ones hands performing the interaction;  tactile cues sensed by the palm and  tactile cues sensed by the pointing finger all contribute to performance, in that order.
These findings deepen our understanding of Imaginary Interfaces and suggest that palm-based imaginary interfaces enable stand-alone eyes-free use, including interfaces for visually impaired users.
We investigate this final implication with an exploratory study and interview with one blind participant that confirms our findings.
Many systems have been developed to help visually impaired users operate the predominately visual interfaces present on modern computing devices.
Visually impaired users rely heavily on tactile cues but modern touchscreen-based devices lack the tactile discoverability of button-based devices.
To address this, McGookin et al.
The Talking Tablet  uses tactile and audio feedback to complementarily reinforce learning through dual modalities.
EarPod  and BlindSight  combine liberal amounts of audio feedback with a tactilerich form factor to enable eyes-free operation.
Touchscreen-based interfaces allow for highly dynamic interfaces where the user cannot predict where a given function will be located.
To address this, researchers have turned to audio feedback to "explain" the interface to the user.
For instance, Pirhonen et al.
Beyond research prototypes, visually impaired users regularly employ mobile technology to gain more independence .
Commercially available mobile phone interfaces come in two categories: cursor-based and touchand-explore interfaces.
Cursor-based interfaces, such as Mobile Speak , have a cursor that announces the current function as the user moves around the interface in single steps, allowing the user traverse the interface in a predictably way.
Alternatively, touch-and-explore interfaces allow users to navigate the interface by dragging freely on the touch screen and listening to the auditory feedback in response .
The touchand-explore interaction mode allows users to access familiar items faster than the linear effort imposed by a cursorbased list.
However, to do this, they must build up spatial memory to be able to target a memorized location.
The availability of the user's own body as a surface for mobile interaction has been exploited in many research projects.
Sixth Sense , Brainy Hand , Skinput  and OmniTouch  all combine on-body interaction with visual feedback from body worn projectors.
In situations that do not afford projection, the user's familiarity with their own body allows for non-visual interfaces that exploit the user's tactile and proprioceptive senses.
Other projects have used the user's palm as an interaction surface due to its abundant tactile features and natural divisions: as a number pad , television remote control , for text entry  and for elaborate input/output such as with the Mobile Lorm Glove .
Other interface concepts have exploited users' intimate familiarity with their peripersonal space and their proprioceptive abilities.
When interacting spatially in the world, humans gather information from many senses 
Because of this, even though modern touchscreen interfaces rely heavily on vision, proprioception and taction also play an important role.
For instance, proprioception alone is not precise enough to enable fine-grained interaction .
Instead, eyes-free interaction typically involves proprioception and taction working together since taking either away degrades performance substantially .
Similarly vision and taction work in concert, at least for the hand .
Touch itself is multi-faceted and has three distinct flavors: active touch ; passive touch ; and intra-active touch  .
Each has its own capabilities: active touch is a scanning mechanism that allows the actor to build up an understanding of the scene over time , while passive is limited to "being touched".
However, this is mitigated by the high spatial resolution of the hand .
On the other hand, intra-active touch, as is used in palm-based imaginary interfaces, combines the capabilities of both, allowing users to actively explore the interface while passively noting the location of discovered targets.
We were careful to leave the users' palm and pointing finger unobstructed in order to not interfere with the interaction between the two hands.
In order to create an appropriate interaction style for browsing imaginary interfaces we searched the related work for appropriate concepts.
Interfaces for visually impaired users display some obvious similarities with Imaginary Interfaces in that neither relies on visual feedback.
Narrowing down our search to spatial interfaces led us to focus on the touch-and-explore interaction style described in the previous section.
We adapted this interaction style for use with imaginary interfaces.
Figure 2 shows the resulting interface based on the Imaginary Phone .
As users drag their fingers across the palm surface, they enter different buttons and the system responds by announcing the name of the target, such as "clock".
If users continue further, the auditory feedback is immediately interrupted and the new name is announced.
Users familiar with the layout can shortcut this exploration and acquire a target by tapping directly on it.
When the user's finger is within 3mm of the hand's plane it is in the touching state and the system uses a space-filling Vornoi layout  to snap selection to the closest target.
Users can freely move their finger around the interfaces and listen to the audio feedback.
We stabilized selection by adding a small amount of hysteresis.
To activate a target, users double tap their hand.
During early testing, the palm-based touch-and-explore interface performed better than expected.
Encouraged by this, we went a step further and not only formally evaluated the technique's performance but investigated what caused it to perform so well.
We conducted three user studies with the goal to determine which properties of Imaginary Interfaces are responsible for their performance.
Asking this question allowed us to learn more not only about browsing with an imaginary interface but also about the very nature of Imaginary Interfaces themselves.
While Imaginary Interfaces share properties with interfaces for visually impaired users--neither relies on visual feedback--it has extra cues that are potentially relevant: 1.
Visual cues: While the lack of a screen prevents imaginary interfaces from providing actual dynamic feedback, they do offer a very particular style of visual feedback from users watching their hands interact.
Tactile cues: During interaction users' hands touch.
This provides them with tactile cues in both directions: the pointing finger feels the palm and the palm feels the pointing finger.
To explore the role of these cues we ran three user studies: Study 1: Visual Cues.
Does watching ones own hands interact support browsing?
We explored this by comparing blindfolded with sighted use  on the phone and the palm.
However, it remained unclear if the extra tactile cues on the palm were responsible.
To explore this we ran two more studies: the first focused on the tactile sensation in the pointing finger, the second on the tactile sensation on the palm.
Study 2: Tactile Cues Sensed by Pointing Finger.
We created three versions of the phone interface, all of which participants operated while blindfolded.
The first was a plain touchscreen phone and the second was the same with an engraved tactile grid.
In addition, we were wondering whether the phone was really featureless or whether touching the bezel and the supporting hand helped users orient.
To investigate this we added a third condition that embedded the phone interaction surface into a large clear piece of acrylic, thereby preventing participants from using the bezel to obtain tactile cues.
We included sighted use as an additional baseline.
Study 3: Tactile Cues Sensed by Palm.
To study this, we created another three interfaces that participants used while blindfolded.
We compared interaction on the palm to interaction on a silicone cast of a hand and to interaction on the palm with a covered pointing finger that minimized fine tactile cues sensed by the pointing finger.
Again, we included sighted use as an additional baseline.
Figure 4 summarizes the six different form factors participants used throughout the three studies.
If the selection was incorrect, the trial was discarded and the participant was required to repeat the trial.
Before beginning the experiment, participants received instructions on how to use the system and performed a series of practice trials with each interaction surface until they indicated they understood the interaction style and were comfortable with the system.
During each of the four blocks  participants had to repeatedly locate five targets out of the 20 available targets in the interface.
The five targets  were presented to the participants eight times in random order.
We presented the conditions in a counter-balanced order using a balanced Latin square.
Each condition used a different set of target names derived from a survey of the most popular iPhone apps used by local students.
At the end of the experiment participants completed a short questionnaire to gather their preference of interaction surface when blindfolded and not.
As shown in Figure 6, the participant sat in front of a table with a monitor showing instructions located directly in front of them.
A footswitch was used to confirm selection.
For the palm condition, participants used the prototype system described earlier.
Their non-dominant hand was placed in a fixture molded to the back of their hand.
This allowed the participant to replace their hand in the same position when switching between the PHONE and PALM conditions while maintaining a consistent calibration.
For the PHONE condition, we tracked interaction with the same optical tracker system used in the palm condition.
This kept any potential tracking errors consistent across conditions.
The phone used in the study was a nonfunctional replica of an iPhone 3G with identical surface area but thinner .
In each trial participants searched for and selected a prompted target.
They started the trial by pressing a footswitch and the system spoke the target name and showed it on a screen.
The participants touched the interaction surface with their finger and as they moved it around the system announced the name of each target .
When participants found the required selection they pressed the footswitch to complete the trial.
H1: Participants will be faster when SIGHTED.
However, since taction far outperforms proprioception , we believe that the tactile cues available on the palm are more likely to be able to fill in for visual cues when they are not present, compared to the mostly featureless phone surface.
Therefore our second hypothesis is: H2: When BLINDFOLDED, using the hand as an interaction surface will result in faster search times.
We defined outlier response times as three standard deviations above the mean for each condition and repetition.
Participants completed the study within 30 min.
There was no overall significant difference between PHONE and PALM  but when participants were BLINDFOLDED they were 50% slower than when SIGHTED , which confirms our first hypothesis that watching your hands improves interaction.
To investigate these results further we aggregated the repetitions into two equal blocks: learning phase  and trained phase  and analyzed each with a separate 2x2 repeated-measures ANOVA.
When BLINDFOLDED, 11 participants preferred to use their PALM  and 10 participants rated the PALM faster than the PHONE with the remaining two rating the PHONE faster.
When SIGHTED, the preference was split with five participants for each interface  but eight indicated the PHONE was faster and two that the PALM was faster .
Participants commented that when blindfolded the palm offered more tactile cues and the phone lacked a "reference system".
One said, "There are more features on the hand.
On the hand you can relate terms to fingers."
However, many commented that when not blindfolded the straightforward grid of targets on the phone was easier to traverse: "When not blindfolded the grid helps to be more efficient."
One participant noted that the tactile cues were sufficient even when not blindfolded, stating, "Even in `sighted' mode I'd rarely look at the phone/hand anymore once I learned the positions."
We fabricated the phone prototypes in three layers: a 4mm base of acrylic, a printed sheet of paper for phone screen and a 1.5mm acrylic top layer.
The tactile grid on the surface of the phone used in the TACTILE PHONE condition  was etched using a laser cutter.
The interaction area of each phone was identical  but for the LARGE PHONE the interaction area was centered on a 22.5cm x 16.5cm panel to prevent the participants from orienting using the device's bezel.
First of all, this study shows that our proposed browsing interface works.
The interface functioned reliably and as participants familiarized themselves the task time dropped to 2.66s for locating a target on the palm.
This has implications as a browsing interface that users can operate reliably could one day form the basis of a standalone imaginary interface.
Previous work assumed that users were already familiar with the interface before using it, thus offered no solution when encountering an unfamiliar interface in the "wild".
The results also show that participants performed better when they could see their hands interact and we gathered first insights into how tactile cues on the palm contribute to eyes-free use.
However, we did not know which tactile cues were responsible for this.
To explore this we ran another two studies.
The first study focused on the tactile cues sensed by the pointing finger and the second on the tactile cues sensed by the palm.
By observing participants in pilot studies we noticed they regularly orient using the device's bezel when blindfolded.
We therefore believe this is an important tactile cue and we wished to confirm that depriving participants of it would result in worse performance.
H1: When blindfolded, participants will be slower with the LARGE PHONE than with the PHONE.
Based on Study 1, where the palm, with its rich tactile cues, performed better than the smooth phone surface, we expected that adding tactile cues to the surface of the phone would also enable more efficient interaction.
H2: When blindfolded, participants will be faster with the TACTILE PHONE than with the PHONE.
In this study, we explored how far tactile cues sensed by the pointing finger contribute to browsing an imaginary interface.
We used three phone-based conditions: a normal phone; a phone with tactile cues added in the form of a tactile grid; a phone with all cues removed by placing the interaction surface in a large featureless sheet of acrylic.
We were interested in blindfolded use but included sighted use as an additional baseline.
As in Study 1 the results were analyzed using repeated measures ANOVA.
All post hoc comparisons used Bonferroni corrected confidence intervals.
Each participant took approximately 45 minutes.
Our results suggest that, although a touchscreen phone appears featureless, some features exist to guide interaction: the presence of a bezel provides a substantial benefit to the unsighted user.
It allows users to find the extent of the interaction area and concentrate their searching within that area.
It also brings the participant's non-dominant hand near the interaction area, allowing it to be used as an additional tactile cue.
Based on the results from Study 1 we expected that adding tactile cues to the surface of the phone would lead to a large improvement.
However, this was not entirely the case: adding additional tactile cues only improved performance during the learning phase.
Once the participants had learned where the targets were, they performed similarly with and without the extra tactile cues.
This indicates that it is important to have some passive tactile cues that can be sensed by the pointing finger but they are only effective up until a point.
As in Study 1, for the PALM-based conditions we placed the participants' non-dominant hand in a fixture  that provided a consistent reference for calibration.
For the FAKE PALM condition, we built a realistic replica of one author's left hand  formed with liquid silicone.
The replica has all of the fine ridges and features of a real hand and remains slightly compliant.
For the PALM WITH FINGER COVER condition we covered the tip of the participants' pointing finger a piece of Velcro backing.
The cover removed the fine cutaneous sensation from the participants' fingers but the participants could still sense pressure and large features like the palm outline.
First, since the PALM condition allows the participants to use both palm and finger taction  we expect it would outperform the other conditions when blindfolded: H1: When BLINDFOLDED, participants will be faster with the PALM than with the other interface conditions.
Secondly, we expect the FAKE PALM, which only involved active touch, to be comparatively worse to the PALM WITH FINGER COVER, which is dominated by passive touch.
Passive tactile discrimination on the palm is very good , allowing the participants to directly localize the sensation instead of integrating the position while scanning with the finger tip.
Therefore our second hypothesis is: H2: When BLINDFOLDED, participants will be slower with the FAKE PALM than with the PALM WITH FINGER COVER.
While we initially expected the pointing finger to sense the majority of tactile cues, we found the opposite to be the case, as the passive tactile sensing by the palm allows users to orient themselves.
The most likely explanation is that the cues sensed by the pointing finger are ambiguous, while the cues sensed by the palm are unique and easy to locate spatially.
The results indicate that it is the passive touch on the palm that contributes most to browsing an imaginary interface.
The active touch feedback received by the tip of the pointing finger, in contrast, contributes comparatively little.
Although the FAKE PALM condition contained equivalent tactile cues to be sensed by the finger, participants performed substantially worse using it as an interaction surface compared to the user's own hand.
We cannot say equivocally that the pointing finger contributes nothing to the interaction as in the PALM WITH FINGER COVER condition, large-scale tactile features  could still be felt but it is apparent that the fine tactile cues on the surface of the palm contribute very little.
We believe the difference occurred because the high touch discriminability of the palm makes it inherently spatial-- touch occurs at an easily resolvable location--whereas tactile cues sensed by the pointing finger are inherently ambiguous as all fingers provide similar tactile cues.
Users are apparently able to resolve this by integrating tactile information over time to develop an understanding of where they are located on the palm.
However, this integration process takes time and is prone to error, which would explain the longer interaction times in our studies.
The same reasoning can also explain the limited performance improvement of the TACTILE PHONE in Study 2.
Since only the pointing finger could sense the added tactile cues, they contribute less than if the participant's palm could be used for sensing.
The second point in this list, i.e., the fact that tactile cues between pointing finger and supporting hand can in part fill in for the absence of visual cues has an additional implication: it suggests that the palm-based interaction from imaginary interfaces might be relevant for eyes-free use and in particular for visually impaired users.
We showed that when the interaction surface is located on the user's body, additional passive tactile sensing becomes available that increases performance compared to an eyesfree interface on an ordinary surface .
Thus, while this project started by borrowing from the related work on interfaces for visually impaired users, we propose exporting our findings back to that community.
More concretely, the Imaginary Interface hardware, e.g., sensing the hands with a chest-mounted camera, might allow visually impaired users to perform better than with the touchscreen-based devices they use today.
While such a claim obviously requires a substantial amount of additional research, we want to conclude this paper with a one-user pilot study we conducted to inspire this discussion.
We recruited one blind person to perform the experiment task of Study 1 and to supply feedback.
Our participant was a 33 year-old male, right-handed, and a musician by trade.
He had been blind since age two and has zero sensitivity to light.
In his daily life, he uses screenreading software on his PC and on his non-touchscreen Nokia mobile phone.
He was familiar with the VoiceOver for iPhone interaction style but has not used it regularly.
The participant performed the task from Study 1.
He performed four blocks  of 40 trials each.
We used ABBA counterbalancing to balance learning effects.
His results are shown in Figure 15.
First, the three studies combined show that imaginary interface browsing works well.
This is an important finding because it suggests that future imaginary interfaces may use such an interaction technique to allow for stand-alone use.
Furthermore, the three studies provide an understanding of what enables palm-based imaginary interfaces: 1.
Even though these interfaces cannot display visual content, users' visual sense remains the main mechanism that allows users to control the interfaces because it allows users to watch their hands interact.
In conditions where users are able to watch their hands interact, this overrides the other cues we studied, i.e., all tactile cues.
In the absence of visual cues, the tactile cues available when the pointing finger touches the palm fill in for the lacking visual cues.
As a result, palm-based imaginary interfaces remain usable even when operated eyes-free.
Following the study, we conducted an informal interview.
He was overall very positive about the palm interface and preferred it to the phone, saying that he preferred "the material of  palm."
Assuming the sensing technology was reliable, he said that he could imagine himself using such an interface.
He also commented that using the palm might actually have less social stigma in public because it wouldn't appear out of the ordinary, especially compared to specialized equipment like Braille readers.
Clearly we must be careful generalizing from the outcome of one participant but the results here are promising and will hopefully inspire future work in the area of imaginary interfaces for visually impaired users.
In this paper, we explored which inherent properties of palm-based imaginary interfaces are responsible for user performance.
We conducted our exploration using the example of an interface that allows users to browse unfamiliar imaginary interfaces.
We learned that visual cues, tactile cues sensed by the palm and tactile cues sensed by the pointing finger all contribute to the performance of imaginary interfaces, in that order.
In addition, we obtained good results with the browsing interface, suggesting that this interaction technique has the potential for forming the basis of future stand-alone ultra-mobile devices.
