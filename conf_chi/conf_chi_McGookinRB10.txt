We present a tangible user interface  called Tangible Graph Builder, that has been designed to allow visually impaired users to access graph and chart-based data.
We describe the current paper-based materials used to allow independent graph construction and browsing, before discussing how researchers have applied virtual haptic and non-speech audio techniques to provide more flexible access.
We discuss why, although these technologies overcome many of the problems of non-visual graph access, they also introduce new issues and why the application of TUIs is important.
An evaluation of Tangible Graph Builder with 12 participants  revealed key design requirements for non-visual TUIs, including phicon design and handling marker detection failure.
We finish by presenting future work and improvements to our system.
This problem is compounded by a lack of efficient tools to allow construction and browsing by visually impaired users.
The most common way to provide access to a visually impaired user is to print a graph on special heat sensitive paper.
When the printed graph is passed through a heat printer, the surface raises up creating a tactile relief that can be explored using the fingers.
However, this technique is cumbersome, requiring special formatting of the graph and two passes through a printer.
Whilst a visually impaired person can create such graphs by him or herself, the graph cannot be inspected until it has been produced in tactile form, causing any necessary changes to be costly, as the graph cannot be modified after creation.
This lack of interactive construction is especially problematic in education settings; students are learning about coordinate systems and graph drawing for the first time and mistakes are common.
In such situations a tactile paper grid attached to a corkboard is used.
The user inserts map pins into the board, which are connected with rubber bands to create graph features - such as data lines and axes.
Whilst this technique is common, inexpensive and flexible, it also brings problems.
Firstly, the pins used are sharp to allow them to be pushed into the board and take the tension of the rubber bands.
This creates a risk that the pin may be pushed into the finger.
As the rubber bands are under tension, it can be difficult to pull them far enough to fit over the pins, with any failure to do so rewarded with the band flying off which the user may, or may not, notice.
Additionally, if the graph must be modified, it is necessary to partially deconstruct by removing both bands and pins.
The graph, once created, cannot be stored as would be the case for a sighted user drawing on paper, as the materials must be reused .
This limits the practicality of the corkboard technique to simple teaching scenarios and makes it unsuitable in business settings such as modifying a graph produced in Microsoft Excel.
Understanding spatial visualisations such as mathematical graphs, charts and maps is an important life skill.
They are common in newspapers, magazines and within many scientific disciplines.
Jones and Careras  report that over 2.2 trillion graphs were published in 1996 and this number has been steadily rising since.
The inability to access and understand these visualisations can severely limit career choices available to an individual.
Such access is particularly problematic for people who are blind or have visual impairments .
By their very nature, line graphs, maps, bar and pie charts exploit basic aspects of our visual perceptual and cognitive systems to communicate information quickly and efficiently.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To understand further the problems of graph construction using the corkboard technique, we asked a 17 year old blind, male student at the Royal National College  Hereford UK, to carry out some graph transcription exercises.
We asked the student to copy eight line graphs that had been created on raised paper to a grid on a corkboard using the pin and rubber band technique.
Each graph had one data series which contained 3-4 control points.
We video recorded the interaction and photographed the completed graphs.
When carrying out the tasks the participant explored and interacted with the graph using both hands.
To construct, the participant used one hand to mark the origin of the grid and then counted along and up the grid with the other hand to mark a position, before using the origin marking hand to retrieve and insert a pin.
The participant also compared by using a hand to mark a point on the corkboard whilst referring to a point on the tactile paper graph.
There were several cases where the rubber bands became detached, either when trying to explore the graph and re-orientate, or when the band had not been correctly attached.
The participant rarely pushed the pins all the way into the board, making them vulnerable to being pulled out when the rubber bands were applied .
Overall the student correctly completed four out of the eight graphs.
The other four graphs had numerous problems including rubber bands which had not been correctly attached and just formed shapes, as well as instances where a rubber band that had become detached had been placed on an incorrect pin, causing the graph to be meaningless.
Figure 1 shows both of these situations.
Although we worked with only one participant, the findings graphically illustrate the problems of graph construction that have been identified in group discussions with visually impaired users  and how such access is greatly hampered by the relatively primitive technology available.
The other major approach to improving graph and chart accessibility is to use haptic interaction.
Most attempts have looked at using force-feedback technology.
Here the user interacts with a virtual model of the graph via an end-effector, such as a mouse or pen.
As the user moves the end-effector around, its position is tracked and compared to the virtual graph model.
Resistance via motors is then provided to create the illusion of touching a physical object.
Fritz and Barner  created an automatic graph construction tool where a user could input a function via the keyboard and the resulting graph was physically "carved" out of a virtual block which could be explored using a SensAble PHANTOM haptic device .
Yu and Brewster  carried out several studies and developed a number of guidelines for designing both line graphs and bar charts for exploration via a PHANTOM.
Their work showed that users could effectively answer simple questions with the PHANTOM and that accuracy was significantly higher than raised paper graphs.
However, the time taken to complete the tasks was found to be significantly greater.
Additionally, users became confused at intersection points in line graphs where two data series crossed and would often unknowingly switch between data series at these points.
Yu and Brewster  identified no effective way to deal with this problem.
More recent work has looked at allowing users to manipulate the graphs.
McGookin and Brewster  adapted Yu's and Brewster's  technique to allow users to drag bars in a bar graph up and down and thus allow construction.
One limitation of this haptic technology however, is its single point of contact nature.
The user interacting with the graph does so only at one position at a time and cannot easily compare different parts of the graph spatially.
Rather, he or she must try to remember the relevant information or try to relocate it, losing the current position in the graph as this is done.
Lederman and Klatzky  have studied how users haptically interact with physical objects.
They identified several Exploratory Procedures that were used by par-
To overcome the limitations of the graph access and creation tools previously discussed, several researchers have investigated the use of virtual haptics and non-speech audio to provide access to graph-based data.
The earliest of this work, by Mansur , used a pitch based mapping to communicate the y-axis value of a data series, with the x-axis mapped to time.
As the sound was played, the user could gain an impression of how the data series changed over time.
This work has been extended by Smith and Walker , who found that the addition of context through auditory "tick marks", which serve a similar function to the visual grid lines, improved accuracy in understanding the graph.
The work of Mansur, in using a value pitch mapping, lies at the centre of most work in accessing mathematical data through sound and has proven to be robust over several different types of graph.
Flowers and Hauer  have shown that user understanding of pitch value mappings of box plots and scatter plots is comparable to visual representations of the same data.
As graph interpretation relies heavily on all relevant information being concurrently held in short term memory  , this can limit the ability to answer more complex questions, such as those that require comparison between different bars in a bar graph.
These limitations mean that any virtual haptic system is severely impoverished in comparison to the paperbased techniques previously discussed.
To ameliorate these issues, several researchers have proposed ways to augment virtual haptic graphs.
McGookin and Brewster  provided quick overviews of bar graphs by providing a separate auditory view below the x-axis.
Their SoundBar  improved accuracy where multiple bars had to be compared, but was only useful for certain types of questions and added complexity to the interaction.
Wall and Brewster  augmented the haptic graph of Yu and Brewster  with "beacons", small markers that could be spatially set and used to return to previous locations.
However, they found that the beacons were often not used and were found to subjectively increase demands on memory, as participants had to remember where a beacon had been placed.
The limited usefulness and relative increase in complexity that overcoming the problems of single point of contact haptic devices requires, has led to the development of systems that incorporate a static tactile guide to aid exploration.
Wall and Brewster  allowed access to pie charts via a standard graphics tablet.
A compact disc  attached to the tablet represented the pie chart.
As the user moved the tablet pen around the edge of the CD, the segments of the pie chart were sonified using a pitch value mapping.
The tactile chart outline allowed the user to employ his or her other hand and to mark segments for easy return and comparison.
In an evaluation with visually impaired users, the tactile element of the discs afforded better orientation within the graph.
Overall, this prior work has shown that the disadvantages of existing tactile graph access techniques, that they are cumbersome and their inability to be stored, can be overcome by non-speech audio and virtual haptic technologies.
However, what is also clear, is that many of the advantages of physical tactile graph access, two handed interaction, quick overviews, spatial frame of reference and flexibility to employ fingers for marking, have been lost in trying to overcome the problems.
The more recent work of Wall and Brewster  has looked at trying to augment tactile diagrams.
However, this work still constrains the user to interact with the computer based information via a single point of contact, limiting the available interactions and potentially overloading short term memory.
We propose an alternative approach that builds directly from existing tangible technologies whilst incorporating support for computer based data.
Whilst TUIs have become more popular in the last decade, there remains significant disagreement as to the definition of a TUI, with some researchers arguing that conventional mice are examples .
In this paper we consider the definition of Ullmer and Ishii : "user interfaces employing physical objects, instruments, surfaces, and spaces as physical interfaces to digital information.".
Users primarily interact with such systems by manipulating real world objects called physical icons  which are tracked by computer systems causing physical manipulation to change digital state.
We employ this definition as a means of distinguishing between tabletop systems and the virtual haptic and audio solutions previously discussed.
Whilst there are many examples of such interfaces, there is little evaluation of their usefulness .
There are, however, several reasons why we believe that TUIs are a suitable and promising approach for non-visual graph access.
One aspect of a successful mapping they argue, is that the cost of changing or manipulating the state of the tangible user interface should be low.
That is, it should support trial and error activity, much like the construction and manipulation of graphs previously discussed.
Antle, Droumeva and Ha  compared children's performance in completing a jigsaw puzzle when interacting with a tangible system on a digital table and a virtual jigsaw controlled with a mouse.
They found that time taken was lower and completion rates were higher with the TUI than with the mouse.
On analysis, they determined that this was due to the tangible elements of the task leading to better spatial model construction which is again important in graph reasoning .
Whilst TUIs appear to be promising, there is no research that investigates their use for people with visual impairments or the use of tabletop tangible interfaces without vision.
We do not know how such interfaces should be designed and there are several key research questions that must be addressed if TUIs are the be used in non-visual scenarios.
Can such interfaces be designed to support visually impaired users?
How should phicons  be designed to allow non-visual use?
How should functionality be split between tangible and non-tangible elements of the interface?
To investigate these issues we developed a tabletop TUI system that allows users to browse and construct both line and bar graphs non-visually.
Due to the lack of existing nonvisual tangible guidelines, we chose as a starting point, to base our system on the corkboard creation technique.
This has the advantage of being the most unconstrained technique available, being suitable for the creation of many types of graph.
The grid system it employs also forms the basis of fundamental mathematical knowledge of 2D space which is important to understand for mobility in everyday life such as accessing map based information.
During development we employed guidelines generated from tactile diagram research or virtual non-visual graph-based systems wherever these seemed appropriate.
A  tangible grid was constructed from drinking straws and attached to a clear perspex topped table.
As the grid would not be something a user would want to change during construction or browsing, we followed the guidance of Challis and Edwards  in permanently affixing the grid to the table so that it could not be moved.
We replaced the pins of the corkboard technique with phicons.
Two different shapes  were used to represent phicons for two data series .
Each cube was filled with plasticine so that it weighed approximately 110g, in comparison to the polystyrene cones which weighed 10g.
These provide distinctly different textures, shapes and weights that should make discrimination between the phicons easier.
A 4 x 4 cm cardboard square was attached to the base of each phicon allowing it to be snugly held within the grid.
In the corkboard graph construction technique, connecting the control points  is very much "joining up dots", but can cause a great deal of problems in ensuring the correct pins are being connected, or when removing the bands to change the position of the pins later.
In our system the job of connecting the pins was automatically carried out by the computer.
To allow for tracking of the phicons we employed ARToolkit .
ARToolkit tracks fiducial markers  in 3D space with a camera, and more commonly is used to render 3D graphical objects on top of these markers.
Custom software was written in C# to track the markers and determine where in the graph grid they were located.
The markers were tracked using a Logitech Webcam Pro 9000.
We attached the markers to the cardboard base of the phicons, with the camera placed on the floor under the table.
The system maintains a record of where on the table the markers are and what data series they belong to.
Therefore a model of the graph can be maintained.
Based on previous research carried out on both haptic graph browsing  and non-visual sonification of linear data series, we incorporated a sonification strip into the table .
This strip runs along the base of the x-axis and can be controlled using a special phicon .
As the user drags the phicon along the strip and it passes between the major units of the x-axis, the system calculates the appropriate y-value of the data series at that point and converts it to a Musical Instrument Digital Interface  pitch value based on the formula provided by Brown and Brewster .
The notes for each data series were played using a piano timbre  and each data series was spatially panned to the left or right stereo channels to improve separation .
As noted by Brown and Brewster , when identifying crossing points between two data series accuracy is improved if both are concurrently presented.
Conversely, when identifying turning points or gradient of a data series, accuracy is improved if only one is presented.
To allow for both options, we introduced a further area to the left of the y-axis.
Placing a phicon from one or both of the data series in this area caused the corresponding data series to be sonified when the sonification strip phicon was moved.
In this way the user can control which data series are played.
The sonification strip and the control area were physically demarked on the table in the same way as the tangible grid .
The bar graph version of the application was similar to the line graph version; Each column of the grid was treated as a separate bar, a phicon in that column at any row position was treated as the top of the bar.
We changed the sonification strip slightly to include a distinct tone played with a synthesised drum pad  to indicate if the column a user had moved into was set with value 0 .
In addition, the bar graph version only supported one data series , therefore the sonification selection area was not used and the user only had to move the sonification strip phicon to hear the graph.
Because of this, both sets of phicons could be used interchangeably.
We carried out a two part study on Tangible Graph Builder.
The main aim was to identify the usefulness of TUIs for visually impaired users by providing answers to the set of research questions previously outlined.
The first part involved sighted users who were sight deprived, whilst the second involved a group of blind users.
Due to the current problems of graph access technologies, graph knowledge amongst visually impaired users can be variable.
As a fairly small group of visually impaired users was available to us, we wanted to be able to compare the results back to a group of users with consistent graph understanding.
Hence we compared to a group of sight deprived students with at least a high school level of graph knowledge.
Eight sighted  participants performed tasks using Tangible Graph Builder.
All reported normal hearing and normal or corrected to normal vision.
Participants performed four types of task.
All of the tasks were typical of those that might be carried out in graph work at school in the U.K by students in the 14-16 age range .
The questions were based on those from existing work evaluating virtual haptic and auditory graph systems  and allowed flexible use of the sound, referral to the phicons, or a combination of both.
The four task types were: Construct line graphs with two data series: A printed table of control points for two data series was supplied.
Participants were asked to reproduce the resulting line graphs using Tangible Graph Builder.
Browse line graphs with two data series: A line graph with two data series was presented with Tangible Graph Builder.
Participants had to explore the graph and identify the crossing points between the two data series, as well as the number of times each data series changed direction, either from a positive to negative gradient, or a negative to positive gradient.
After these questions had been answered the participants were asked to sketch the graph on a paper grid with the same number of rows and columns as the tangible grid.
Browse bar charts: Participants were given a pre-built bar chart containing 12 bars and had to answer a question about the graph.
The questions and stimuli for this task type were taken from McGookin and Brewster .
Construct bar charts: Participants were given a printed table with values for six bars and asked to construct a bar chart.
Participants had to scale the y-axis values in order to fit the chart into the tangible grid.
Each participant was given a demonstration of Tangible Graph Builder before commencing the tasks.
During this demonstration the participant was allowed to look at the tangible grid and the phicons placed on top of the table.
However, during the experimental tasks a black cloth screen was erected between the participant and the table.
After completing all tasks, participants were interviewed about their experiences using Tangible Graph Builder, with emphasis on the research questions previously outlined.
These data  were analysed based on a framework analysis approach , using the research questions as initial topics, but allowing other topics to emerge.
In the following sections we briefly discuss quantative performance and the major topics to emerge from the framework analysis.
The graph shown in Figure 6 illustrates the accuracy of users on each of the tasks.
Due to participants carrying out a different number of trials for each graph type and task, we express accuracy as a percentage of the total possible score for each task and graph type.
The results obtained are comparable to performance by blindfolded sighted users when performing similar tasks in virtual haptic and auditory graph access software .
Of the two phicon types used to represent data points , seven of the eight participants expressed a preference for the cube phicons.
Several reasons for this were stated, but most commonly was the likelihood of knocking over the lighter cone phicons.
As one participant stated "If you go on the graph and start feeling around, that's when the cones can travel".
Another participant felt that the cubes "were better objects to identify with than the other ones .
The other ones were like papers and I missed them out.
I think the cubes were just bolder.
At times I moved my hands over them  and I just moved them.
The others were a bit difficult to do that to because they were um, stronger.
I think I must have knocked one or two  over.".
The subjective views of participants were confirmed by the quantitative results and video recordings.
Out of the seventy two trials that were performed, there were nine occasions where a cone phicon was dislodged from the grid, compared to one occasion where a cube phicon was dislodged.
Additionally, all of these occurred when the user was accessing or constructing a line graph.
This may be partly due to the increased use of sound in bar graph questions , or that bar gra-
Although participants identified when phicons had been knocked over, it was not immediately recognised if they where only dislodged, even if this was sufficient for the marker to be lost by the tracking system .
Where the phicon had been knocked over, the participant immediately identified this and replaced it in the closest square, even if this was not the grid square where the phicon was originally located.
Although the cubes were preferred by almost all of the participants, there were issues.
Because the cubes were regular, during the graph construction tasks participants sometimes put the cubes down so that the fiducial marker was not on the table top .
This happened on two occasions but was not picked up by participants, primarily due to the lack of sound usage in the construction tasks .
However in cases where the user must browse and modify a preexisting graph, this is likely to be a greater issue.
There were only 4 out of 24 construction trials where the sonification strip was used, compared to 27 out of 48 browsing trials.
In all of the construction trials the participants reported that they used the sonification at the end of the task to confirm that the graph had been correctly created.
Arguably the sonification strip does not add useful functionality in purely construction tasks.
In the line graph browsing tasks the use of the sonification strip formed part of richer strategies.
The sonification strip was more commonly used when accessing line graphs .
The ways in which it was used agree with the results of Brown and Brewster  and their evaluation of the SoundVis system previously discussed.
When trying to find intersection points, participants would sonify both data series together.
When trying to identify features of a single data series participants would explore each individually.
However, participants often would not be able to make a decision on crossing points or turning points purely from the sound.
In such cases they would move along the sonification strip until reaching an area where a crossing might occur and then moving both hands up the graph from that point to explore the phicons.
Users would either mark a phicon with the left hand and then find the next phicon of the same data series with the right hand, or if the user was looking for intersection points, mark a phicon with one hand and feel around for phicons of the other data series with the other hand.
Browsing bar graphs, due to their more regular structure, produced more polarised strategies.
Participants either used the sonification strip , or used a two handed strategy to browse and mark bars .
There were very few cases where a participant used both techniques on a trial.
There was also variation within participants.
Notably one participant who did not attempt to use any sound when browsing the more complex line graphs, immediately started using the sonification strip, and only the sonification strip, when asked to browse bar graphs.
Participants were unable to fully explain the reasons for this.
When using the phicons, participants followed a variant strategy, touching a phicon with the non-dominant hand and spatially comparing it to the next phicon, or using the non-dominant hand to mark candidate bars by touching the phicons.
All of the participants used a two handed exploration strategy to complete all tasks.
The way in which both hands were used varied between the browsing and construction tasks, as did the overall use of the sonification strip, but the strategies broadly followed the techniques observed in our earlier corkboard construction observation.
In construction tasks two main strategies emerged.
In the first, which was used by half of the participants, the users would move relatively.
The non-dominant hand used the previously positioned phicon as a reference point, whilst the user referred to the printed sheet to identify the next phicon and calculate the relative position of that phicon.
He or she would then hold the phicon in the dominant hand and count from the non-dominant hand along the x-axis and then up or down the y-axis to locate the appropriate position.
The non-dominant hand then met the dominant hand to mark position and the participant moved onto the next phicon.
In the other strategy the participant used two hands to count from the origin along the x-axis, holding the phicon in the dominant hand.
The x value was marked with the non-dominant hand and then users counted up with the dominant hand containing the phicon to place it in the correct position in the tangible grid.
In the browsing tasks a number of different strategies emerged.
Participants tended to move between them depending on the task.
An important aspect of any tangible user interface is how to divide the system functionality between phicons that the user can manipulate, and state information that should be communicated by the system to the user either through visual , audio or physical  means.
In our system we have three distinct categories: the tangible grid , phicons  and the line series .
We asked all participants about the division of functionality and if they felt this was appropriate.
No participants raised issues with the division and no relevant events from the user interaction were identified.
We believe therefore that the division of functionality was appropriate.
Tangible Graph Builder used a fiducial visual camera based tracking system to monitor phicon position.
This was primarily motivated to allow for rapid development of the initial system.
Modifications during the initial development  improved tracking accuracy, but there were still instances during the study where markers would either fail to be detected, or be intermittently detected by the system.
Whilst the experimenter stepped in during prolonged periods of intermittent failure and where markers had been totally lost, this was not done initially to observe if users detected the marker loss and how they dealt with it.
There were two types of marker loss that were relevant: failure of a marker in the grid and failure of a marker in the sonification area.
Where a marker detection failure occurred in the sonification selection area, participants became aware of this quickly due to the lack of any sound output from the system and took remedial action .
The participants acquired these strategies from the experimenter during the initial  familiarisation phase.
However, in cases where the markers in the grid failed to be detected, or were intermittently detected, the system redrew the graph which caused the sonification to change.
In such cases the participants attempted to take no corrective action in-spite of the sonification strip providing highly inconsistent sounds.
In the post study interviews where intermittent detection was most prevalent, participants expressed their lack of confidence in the sonification, their answers and the reliability of the sound.
As one participant said of using the sonification strip to browse a line graph: "but then it went screwy, I don't think I heard the same thing twice".
In visual tangible interfaces it is generally straightforward to communicate loss of detection, but non-visually it is much harder.
We discuss this further in the future work section.
Another issue surrounding tracking accuracy occurred with marker "jitter": cases where the detected position of the marker varied between frames of the camera.
This usually caused no problems, as the marker would jitter within the tangible grid square it was placed.
We deliberately chose each grid square to be 4 x 4cm to ensure that the jitter would not be an issue.
However, this still arose with the sonification strip phicon.
The sonification strip would play the musical note that represented the current y value of the sonified data series when it moved between x-axis grid squares.
Users would often move the sonification phicon slowly to hear and count each note .
If they paused near the transition between grid squares, the marker could jitter to each side, playing a note each time.
One participant described this issues as it related to browsing a bar graph: "I knew each sound corresponded to a bar, but sometimes I would listen to two sounds, two pitches that played very fast.
So in order to increase accuracy and assign a pitch to a bar, I was counting as I was moving a bar ".
Participants also handled this by moving the phicon back to the origin and trying again.
Whilst both of these issues may be reduced or eliminated by more accurate tracking technology, they provide useful insight into the issues of marker detection and the coping strategies employed.
To confirm the results obtained, we carried out the study again with four blind users  with no residual sight.
Two were congenitally blind, whilst the other two were late blind and had been sighted when taught graphs in school.
Participants completed the same study as the sight deprived group with the following variations.
Participants were initially introduced to the system by free exploration.
Participants explored the table and had features explained as and when they came into contact.
As the participants were blind rather than visually impaired, the black screen between participant and table was not used.
Any printed materials were read out by the experimenter on the request of the participant.
As there was no effective way to reproduce the line graphs on paper, participants were only asked for the turning and crossing points in the line graph browsing tasks.
Summary accuracy results are shown in Figure 8.
The results show overall good performance in all tasks, with the exception of the identification of turning points in the line graph browsing tasks.
Reasons for this are discussed in the following sections.
In other areas performance exceeded that of the sight deprived group, with no errors at all in the bar graph construction tasks.
We noted no significant differences between the congenitally blind and late blind participants.
Strategies employed to explore the graph were similar to the sight deprived group, with participants using two handed exploration strategies with one hand to manipulate the graph and the other to mark context within the graph.
In browsing tasks participants again used both hands to explore and mark features in the graph.
When carrying out the line graph browsing tasks the distribution of strategies changed.
Two of the participants did not use the sonification strip at all, whilst the other two primarily used the sonification strip with a small amount of physical exploration with the phicons.
When asked, the participants who had only used the phicons for exploration said that they had been trained on using touch and that the tactile was obvious.
However both immediately opted for sound when browsing the bar graphs.
The use of two hands was mentioned as an important aid to completing the tasks, that at times made it unnecessary to try to use the sonification strip: "In terms of the question when it was a case of how many times the item  cross over each other.
I found it easier .
That may have been partly due to not knowing if I got reliable feedback or not.
I trusted my mental image with what I felt with my hands".
However, two participants, those that had used the sonification strip extensively during the line graph browsing tasks, felt that the sounds were fine if the marker tracking could be made more accurate.
The issues identified with the phicons by the sight deprived group were also present.
However, there were less occasions when phicons were knocked or dislodged by participants.
On four trials a participant dislodged a phicon from the grid.
On all occasions this was a cone phicon which the participant identified but replaced in the wrong grid square.
All of the participants expressed that the cube phicons were preferred, as the cones were too light.
As one participant said: "The blocks are nice and solid, the cones being lighter are easier to tip over and knock out of the slot they were in.".
Another participant mentioned that: "In a thing this size you tend to have a sweep around so you need something fairly solid.".
Overall, one participant commented on the phicons as being: "nice that they just snug fit.
I think you want something that just fits really snugly.".
In addition to phicons being knocked, there was one occasion where the participant put a cube phicon down with the marker facing upwards requiring intervention by the experimenter to correct.
In addition to soliciting comments about issues with Tangible Graph Builder, we explicitly asked participants if there were any features that could be added which would make completing the tasks easier.
Primarily comments addressed contextual feedback.
Participants mentioned that especially in large grids, the ability to query what the value of a grid square was, or the current position of a phicon, would be important to complete the tasks.
In addition to confirming the results from the sight deprived users, we wanted to gain more qualitative feedback on how the use of "real" tangible user interfaces compares to virtual haptic feedback  and a Mansur style soundgraph .
To this end, after participants had completed the tasks with Tangible Graph Builder and their views had been elicited, we asked them to complete bar and line graph browsing, as well as bar graph construction tasks using two previously written and evaluated systems .
The tasks that we asked participants to complete were of the same complexity as those carried out with Tangible Graph Builder.
However, we excluded the line graph construction tasks as the soundgraph software  only supported graph browsing and not graph manipulation.
Data collection was based on interviews conducted after all systems had been used.
SoundVis, as evaluated by Brown and Brewster , provides access to soundgraphs via the numeric keypad of a standard computer keyboard.
Users use the keypad to move left and right in the graph.
SoundVis can sonify up to two data series at a time and users can switch between serial and parallel presentation of two data series, as well as switch between the data series using other keys on the keypad.
Each data series is rendered using a general MIDI piano timbre  using a pitch value mapping and stereo panned to a different speaker.
SoundBar Builder combines two evaluated techniques  to allow construction and overviews of bar graphs.
Bars are modelled as recessed grooves that can be explored using the PHANTOM.
Below the bars and the x-axis is a SoundBar.
This acts in the same way as the sonification strip from Tangible Graph Builder; as the user moves the PHANTOM pen along the strip, the bar immediately above is sonified using the same pitch mapping as the sonification strip.
SoundBar Builder makes extensive use of speech, primarily to stop users from getting lost.
Touching any feature in the graph and then pressing the button on the PHANTOM pen, yields speech feedback on that feature .
A screenshot of the graph model is shown in Figure 2.
The strategies used to explore the graphs in SoundBar Builder and SoundVis were the same as those that have been previously identified in the individual evaluations of these systems .
Due to space constraints we concentrate here on the qualitative differences between the three approaches.
All of the participants said that Tangible Graph Builder provided a useful two handed interaction: "More than one point of contact gives you the ability to see where things are relative to each other which you don't get with the PHANTOM.
All participants expressed that when exploring a graph  they would try to obtain an overview of the area first: "The PHANTOM does give you a tangible thing, its probably slightly more work in the mental memory processes to remember how things are relatively.
You have to get into the bar and find out if it is higher or lower than the next one.
There is quite a methodical process required there.
The tactile  gives you the ability to fairly quickly move and get an indication of where the lines were.".
When using the PHANTOM version participants could easily become disorientated: "In the tangible one, it is easier for me to locate things in space.
In the PHANTOM one I sensed that the degree ...
I would be more likely to make a mistake trying to find the points".
Another important topic to emerge from user discussions was the confidence that participants felt about the system and as such their answers.
All of the users felt the PHANTOM to be more reliable than Tangible Graph Builder, primarily down to issues of marker loss.
However the combination of different ways of accessing the graph in both SoundBar Builder and Tangible Graph Builder was an important factor in improving confidence: "The good thing with the tangible one and the PHANTOM one, is that you are integrating senses.
That allows you to rely more in your answers.".
Two of the participants felt that SoundVis provided a more accurate representation of the change of shape of a line graph and made identifying turning points easier: "You do get a feel for the shape of the curve  ... which you don't get very easily off that ".
However, as SoundVis contained a greater number of divisions on the xaxis  in comparison to the 9 divisions on the Tangible Graph Builder sonification strip , this may be able to be rectified with more units on the sonification strip.
In any non-visual tangible user interface, there are three types of data.
In extension of the guideline of Challis and Edwards , fixed information should be represented by immovable physical objects, directly manipulated data should be represented by phicons, whilst indirectly changed data should be presented via sound or tactile stimuli.
Participants were explicitly invited to comment on this issue during the interview phase and all felt that the distinction employed did not adversely affect their performance, therefore we believe that it is an appropriate way to divide functionality.
In any tangible system phicons can fail to be detected.
Even if the system is reliable, the user may put a phicon in the wrong place, or in a way that means it is not detected.
In cases where intermittent phicon detection failure caused the sonification to change, users were unsure of their answers and had no way to test the detection status of a phicon.
Whilst is it relatively easy to indicate that a phicon is not detected via visual means, it is harder to do so non-visually in a way that does not annoy the user.
However, such awareness is important and should be provided.
We originally designed our phicons to be haptically different, so that they could be quickly discriminated using as many Exploratory Procedures as possible .
However, the cone phicons were more often knocked over and least preferred by the participants.
Phicons should therefore be hard to accidentally move.
Varying the weight, as shown by the preference for the cube phicons, is an effective way to do this.
The cube phicons at 110g are a good starting point.
Although the cube phicons were preferred, there were occasions, due to their regular shape, where they were placed with the fiducial marker on an incorrect side.
Phicons should be irregular so they can only be placed on the table one way.
An effective means of doing this, as was suggested by one of the visually impaired users, may be to attach embossed shapes on top of the phicons so "up" can be determined.
Tangible Graph Builder has provided much useful guidance in developing non-visual tangible user interfaces, however there are several aspects of its design that still require development.
From the results of both evaluations we need to improve the tracking detection of our system as well as provide information about the status of each phicon.
We propose to insert a small Arduino  - a programmable microcontroller - into each phicon.
Via a bluetooth connection, our system will be able to communicate with the phicon and inform it if it is, or is not, being detected.
The microcontroller can then provide feedback to the user.
For example, each phicon could play a short musical sound when it was detected and lost by the camera.
If either sound was heard when the phicon had not been explicitly moved, the user would know of a problem.
To aid finding the problematic phicon a peltier heat pump  could be employed so phicons that were not detected could be cooled.
This would allow feedback to be provided in an unobtrusive yet useful way.
An additional aspect is to confirm our guidelines in other tangible scenarios.
Overviews of maps and geo-data present similar problems for visually impaired users, indeed a grid is the basis of most map systems, and we plan to investigate the role of TUIs there.
Graph Builder presents a first step at augmenting those technologies to be more useable, whilst incorporating the benefits of computer-based support.
In doing so, we have moved away from current research approaches, such as virtual haptics and sonifications controlled via a keyboard which, whilst overcoming many of the problems of the paper-based and corkboard techniques, restrict the exploration strategies users would naturally employ.
Our evaluation with both sight deprived and blind users has shown the benefits this approach can bring.
Participants were able to complete graph construction and browsing tasks by employing a range of twohanded exploration strategies.
We believe the application of TUI technology to provide non-visual access to graphs is a valuable approach, that will empower users to access information which has up to now been largely inaccessible.
