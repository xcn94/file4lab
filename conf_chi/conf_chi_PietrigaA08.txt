Focus + context techniques such as fisheye lenses are used to navigate and manipulate objects in multi-scale worlds.
They provide in-place magnification of a region without requiring users to zoom the whole representation and consequently lose context.
Their adoption is however hindered by usability problems mostly due to the nature of the transition between focus and context.
Existing transitions are often based on a physical metaphor , and are almost always achieved through a single dimension: space.
We investigate how other dimensions, namely time and translucence, can be used to achieve more efficient transitions.
We present an extension to Carpendale's framework for unifying presentation space accommodating these new dimensions.
We define new lenses in that space, called Sigma lenses, and compare them to existing lenses through experiments based on a generic task: focus targeting.
Results show that one new lens, the S PEED -C OUPLED B LENDING lens, significantly outperforms all others.
Many techniques can be used in combination with classical pan & zoom to navigate large multi-scale worlds.
Among them, a range of bifocal display techniques have been designed, which can be broadly categorized as either overview + detail or focus + context techniques.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
While overview + detail techniques are generally favored and have been shown to perform well in some situations , there are cases where they show their limits: for instance, when navigating a map of a densely populated region such as the Greater London area to look for particular localities, overview + detail techniques can only use a few pixels to display each of them in the context view.
On the contrary, focus + context techniques can convey additional information in the context view, such as the localities' names, thus providing users with more contextual information that can guide navigation.
They have also been shown to perform efficiently in other situations, e.g., for large steering tasks  or when selecting small targets with a stylus .
Even though they have been studied for some time, the adoption of focus + context techniques remains limited, mostly due to comprehension and low-level interaction problems  related to how the transition between the context and the magnified focus region is achieved.
Many of the transitions described in the literature are inspired by the physical world and are presented through metaphors such as magnifying glasses, rubber sheets , and more generally surface deformations .
For instance simple magnifying glasses  create occlusion of the immediate context adjacent to the magnified region ; graphical fisheyes , also known as distortion lenses , make it difficult to acquire targets , especially for high magnification factors.
To cancel the negative effects of distortion associated with fisheyes, Gutwin proposed Speed-coupled flattening lenses , introducing time as a dimension to transition between focus and context.
The comparison of these time-based lenses with plain fisheye lenses demonstrated that the performance of lens-based techniques can be improved by using dimensions other than space to control the transition between focus and context.
In addition to space and time, we argue that other dimensions readily available in the electronic world can be used to provide more efficient transitions between focus and context.
In this paper we introduce a generalization of Carpendale's framework for unifying presentation space .
This generalization encompasses transitions based on two orthogonal dimensions: space and translucence , which can be combined with a third dimension: time .
This opens up a large design space, called the Sigma lens design space, in which we identify interesting points.
We report on the results of an evaluation of five lenses on the generic task of focus targeting, a basic motor task involved in many highlevel navigation tasks.
The main finding of these evaluations is that one new lens, the S PEED -C OUPLED B LENDING lens, significantly outperforms all other types of lenses for that task.
Focus + context techniques are mainly differentiated by the way they transition between the focus and context regions.
Techniques such as the DragMag  and Manhattan lenses  display the focus as an inset which is offset from the corresponding context region so as not to occlude the local context adjacent to that region.
In this particular case, there is no actual transition between focus and context, which are simply connected through lines serving as visual cues.
Techniques that do not offset the focus region provide an in situ magnification that sits on top of the corresponding context region .
They have to use some type of transition in order to avoid occlusion of the adjacent context.
This is almost always achieved by distorting the representation, so as to smoothly integrate the focus into the context.
The distortion can affect the entire representation: Graphical Fisheyes , the Rubber Sheet , the Document Lens , the Perspective Wall .
Or it can be restricted to a specific region, in which case they are called constrained lenses .
These have the advantage of distorting only a limited region around the focus, leaving most of the context untouched.
They have been shown to work better than full screen lenses for some tasks , and should be favored when the focus has to be relocated often, as they reduce the amount of visual changes during lens movements .
Various improvements to these techniques have been proposed, such as ways to achieve higher magnification , and visual cues that can help in comprehending distortion .
In almost all cases, however, the transition between focus and context is achieved through one single dimension: space.
Magic Lens filters, part of the See-Through Interface , are powerful generic lenses that are used to modify the rendering of objects seen through them.
However, to our knowledge, they have not been used to specifically address the problem of smoothly transitioning between focus and context, whether through space, time, or translucence.
Lieberman used translucence in Powers of Ten Thousands , a bifocal display technique that makes the focus and context views share the same physical screen space, by using multiple translucent layers.
But as with the DragMag, there is no actual transition between focus and context, which are overlaid on top of one another.
Even though it has been shown to be usable in exploratory studies , this type of representation based on transparent or translucent layers is cognitively demanding, causing visual interferences that are the source of serious legibility problems, and requiring additional mental effort from the user to relate focus and context.
Translucence remains, however, an interesting dimension which has been used successfully to reclaim some screen real-estate, either in combination with other filters such as in multiblending  or by making the translucence level dynamically vary as a function of cursor movements .
As mentioned earlier, cursor movements have been used in a different context, closer to our problem, for controlling the magnification factor of speed-coupled flattening lenses  over time, with the effect of increasing focus targeting performance compared to the equivalent static fisheye lenses.
Another technique, Speed-dependent automatic zooming , couples zoom level in a window with scroll rate, zooming-out as speed increases.
We believe that new types of lenses can be created by more systematically combining the above dimensions, namely space, translucence and time, in order to provide more efficient transitions between focus and context in multi-scale interfaces.
The basic concepts for describing spatial distortion between focus and context have been defined in Carpendale's framework for unifying presentation space .
In this section we reformulate these concepts in a slightly different, but equivalent way in order to accommodate our generalization of transitions between focus and context.
This formulation is based on space-scale diagrams  and uses the associated terminology.
Basic knowledge about these diagrams is assumed.
We consider the focus and context regions of any lens-based representation as separate viewing windows in a space-scale diagram.
The final rendered viewing window is a composition of points from both windows.
All constrained lenses, no matter how they transition between focus and context, have the following properties: * M M : the maximum magnification in the focus region , * RI : the radius of the flat-top region, which we call inner radius, * RO : the radius of the lens at its base , which we call outer radius, *  : the coordinates of the lens' center.
Figures 2 illustrates these definitions using a distortion lens applied to a scene made of a series of equal-size rectangles that form a regular color spectrum.
The context viewing window corresponds to what is seen in the absence of any lens.
Points A and B represent the boundaries of the constrained lens within the context viewing window, at a distance RO from the lens' center C. The focus viewing window is then a flat magnification by a factor of M M of the region delimited by A and B.
RI controls the size of the lens' flat-top.
If RI = RO , the lens is a mere magnifier lens  as illustrated in Figure 1-a.
If RI is zero, the flat-top is reduced to a single point at the center of the lens, which is then the only point at full magnification.
The final viewing window obtained at rendering time can be seen as a combination of the two abstract windows introduced above: the rendering of the focus window is integrated, after some transformation, in the context window.
The most common transformation consists in displacing all points in the focus window to achieve a smooth transition between focus and context through spatial distortion .
This type of transformation can be defined through a drop-off function, such as a Gaussian , which models the magnification profile of the lens.
Associated with a distance function d, the drop-off function is defined as: Gscale :   s with s a scaling factor.
Gscale is usually a monotonically decreasing function with a range of .
This is not a strong requirement; other functions may however introduce discontinuities in the spatial transition.
Digital image compositing and more particularly alpha blending represents another, yet unexplored, method for transitioning between the focus and context regions of a constrained lens.
As with spatial distortion, the final viewing window obtained at rendering time is a combination of the two abstract viewing windows: here, points of the focus window are composited with points of the context window.
For instance, using gradually increasing translucence, it is possible to smoothly blend the focus viewing window into the context, thus achieving a transition without resorting to distortion .
The continuity between focus and context is realized through compositing only.
As with scale for distortion lenses, the translucence profile can be defined by a drop-off function that maps a translucence level to a point  located at a distance d from the lens center: Galpha :    with  an alpha blending value in , F T being the lowest translucence level used in the lens' flat-top.
Note that it does not necessarily have to be 1.0 , though it will often be close to it.
Drop-off function Galpha is usually a monotonically decreasing function.
Again, this is not a requirement, but other types of functions may introduce discontinuities in the blending gradient.
Most lenses are either radial  or orthogonal .
A Gaussian function is often used to define drop-off function Gscale , as it provides one of the smoothest visual transitions between focus and context.
Figures 2 and 1-b illustrate Gaussian distortion lenses.
Figure 3 shows how translucence is used to transition between focus and context in what we call a B LENDING lens.
This dimension offers an alternate way to smoothly transition between focus and context without resorting to spatial distortion, thus eliminating the drawbacks specifically associated with the latter.
As we will discuss in the evaluation section, this transition type introduces problems of its own.
Spatial transitions and transitions based on translucence can be combined in a single lens, each with their own drop-off and distance functions.
Additionally, several lens properties can be made time-dependent.
This makes for a rather complex expression for computing the rendering of a point  seen through the lens, which reflects the richness of our new design space.
Table 1 gives a summary of the properties of both existing and new lenses within this design space.
The first three lenses already exist in the literature.
The M AGNIFYING G LASS, illustrated in Figure 1-a, consists only of a flat-top  which occludes the immediate surroundings of the magnified region.
F ISHEYE denotes the common graphical fisheye lens.
Here we use a Gaussian drop-off function to transition, through space only, between focus and context .
The S PEED -C OUPLED F LATTENING lens is a variation on the one introduced by Gutwin , applied here to constrained lenses.
It uses a simple interpolated low-pass filter inspired by the one of trailing widgets  as a time-based function to control the magnification factor based on the lens' velocity and acceleration.
The last two techniques are new contributions identified while exploring our design space.
The B LENDING lens, illustrated in Figures 1-c and 3, can be seen as the simplest example of a translucence-based transition: it is like a M AGNIFYING G LASS that gradually blends into the context.
Smoothness of transition is achieved without resorting to spatial distortion: context pixels gradually fade out as we get closer to the lens' center, while focus pixels gradually fade in.
It shares all of its properties except that its F T depends on the lens' movements.
When still, the lens looks like a M AGNIFYING G LASS.
The same type of low-pass filter as that governing the behavior of the S PEED -C OUPLED F LATTENING lens is used.
Figure 1-d shows a screenshot of a S PEED -C OUPLED B LENDING lens moving at slow speed.
The transition functions described in the previous sections make it possible to create a broad range of lenses.
However, as is the case with most lenses reported in the literature, the properties of these lenses are defined statically.
One notable exception is the Speed-coupled flattening lens  which uses the lens' dynamics  to automatically control magnification: basically, M M decreases toward 1.0 as the speed of the lens  increases, therefore "flattening" the lens into the context, and increases back to its original value as the lens comes to a full stop.
Speed-coupled flattening lenses have been demonstrated to outperform their static counterpart, and represent a first step in the direction of using time-dependent transitions to improve the usability of lenses.
The magnification factor of a lens  is an obvious parameter to control over time.
There is no reason however to limit the use of the lens' dynamics to this one alone.
We note F any time-based function returning a numerical value that can be used to dynamically change one or more of the above-mentioned lens properties.
In the following we focus on one particular function: the lens' velocity and acceleration over time.
A small inner circle can be noticed inside the lens at ,  and .
This circle identifies, at the scale of the context, what region is magnified in the flat-top.
The visibility of this translucent circle is controlled by 1 - F T : invisible when the lens stands still, it becomes more and more apparent as the lens moves faster, and conversely.
This indicator was added as a result of a pilot study: we discovered that feedback, in the context view, of the position and size of the region to be magnified helped targeting objects more efficiently.
This inherent instability makes it less convenient than its S PEED -C OUPLED B LENDING counterpart, but it is still of great help when targeting an object.
The latter could actually be further extended to include other rendering techniques to achieve focus-context transitions, such as those based on multiblending , to highlight particular features of objects in the transition area.
These are however still too computationally expensive to achieve acceptable frame rates on most personal computers, and are left as future work.
To test the limits of each lens, we included factors up to 14x.
We used a Dell Precision 380 equipped with a 3 GHz Pentium D processor, an NVidia Quadro FX4500 graphics card, a 1600 x 1200 LCD monitor  and a Dell optical mouse.
The program was written in Java 1.6 using the open source ZVTM toolkit  which offers a wide range of distortion lenses and could easily be extended to support translucenceand time-based transitions.
The application was limited to a 1400 x 1200 window with a black padding of 100 pixels in order to accommodate instruction messages and simulate screen real-estate that would usually be taken by control and information widgets.
We conducted an experiment to compare the performance and limits of the three existing and two new lenses described in the previous section.
Participants were asked to perform a simple task, namely focus targeting, which consists in putting a given target in the flat-top of the lens.
Focus targeting is one of the building blocks of many higher-level navigation tasks such as searching .
Focus targeting performance was evaluated at five different magnification factors .
Our focus targeting task consisted in acquiring a target in the flat-top of the lens as quick as possible.
In our experimental setting, the lens was centered on the mouse cursor.
The task ended when the participant clicked the left mouse button, provided that the target was fully contained within the flat-top.
As focus targeting consists not only in correctly positioning the lens, but also in looking at the magnified target, additional conditions were imposed on some lenses to guarantee sufficient target visibility.
If all conditions were met when the participant clicked the mouse button for the first time, the targeting was counted as a hit, otherwise, as a miss.
Each trial consisted in performing 24 successive focus targeting tasks.
As illustrated in Figure 5, the targets were laid out in a circular manner.
Translucence-based transitions used in B LENDING lenses have their own problems, with a negative impact on targeting performance, but these might not be as strong as that commonly associated with distortion.
Each focus targeting task can be divided into two phases: in the first phase, the user moves the lens quickly to reach the target's vicinity, while in the second phase, she moves it slowly to precisely position the target in the focus.
In the first phase, the user is not interested in information provided in the focus region since she is trying to reach a distant object in the context as quick as possible.
Here, we hypothesize that no matter the transition dimensions involved, providing a detailed view during the first phase is of limited value and has a negative effect on performance, leading to the conclusion that smoothly and automatically neutralizing the focus and transition regions during this phase, and then restoring them, can help the user.
We decided to have only one target visible at a time, as we noticed during a pilot experiment in which all targets were visible that some participants were often taking advantage of the layout pattern to acquire the object set as the current target by positioning the lens relative to that object's siblings.
Our experiment was a 5 x 5 within-participant design: each participant had to perform several trials using each of the five lenses  with five different magnification factors .
We grouped trials into five blocks, one per lens, so as not to disturb participants with too many changes between lenses.
To avoid a non-controlled effect of order, we used a Latin square to compute five different orders of presentation for lenses and assigned two participants per order.
Trials within a block were presented in a random order after a training phase containing 3 trials , allowing participants to get familiar with a given lens before empirical measures were collected.
The 24 targeting tasks of a trial had to be performed in a row, but participants were allowed to rest between trials.
The first targeting task of each trial was ignored.
A total of 11500 actual focus targeting tasks were thus taken into account in the analysis.
The experimenter first introduced the task, and then each lens immediately before the corresponding block, and made sure that participants did understand how each one worked and how best to operate it.
We drew the following predictions based on each lens' properties, the results of previous studies and a theoretical analysis of the motor movements involved in focus targeting.
From a pure motor perspective, the difficulty of a focus targeting task can be evaluated as a view pointing task in a fixed-scale interface .
We can thus use Formula  in Figure 6 to quantify the difficulty of moving the lens' flat-top, of size Wf ocus , to a position where it will contain the target, of size Wtarget , initially located at a distance D from the lens' center.
Formula  computes the Index of Difficulty, ID, of our focus targeting task.
The lens' position in the context window is controlled in the visual and motor space of that window.
As M M increases, the size of Wf ocus decreases, making the task more difficult.
M AGNIFYING G LASS and S PEED -C OUPLED B LENDING lenses are made of a flattop only: Wf ocus = Wlens = 200, while other lenses have to accommodate the transition within the same overall area: Wf ocus = Wlens /2 = 100 in our implementation.
M AGNIFYING G LASS and S PEED -C OUPLED B LENDING thus feature a larger flat-top than other lenses with the same overall size, consequently making focus targeting easier from a motor perspective: ID ranges from 3.2 to 6.3 for M AGNIFYING G LASS and S PEED -C OUPLED B LENDING while it ranges from 4.2 to 8 for F ISHEYE, S PEED -C OUPLED F LATTENING and B LENDING.
This reasoning however does not take into account non-motor aspects of the task which also depend on the type of lens used.
For instance, occlusion caused by the always-opaque M AG NIFYING G LASS should increasingly hinder performance in the second phase of the task  as M M gets bigger: Wf ocus becomes smaller while Wlens remains constant, making the occlusion zone between focus and context on the path to the target larger, along with the chances of losing track of the target.
Altogether, these three hypotheses only provide a partial order of performance between the five lenses.
One strong expectation is that the S PEED -C OUPLED B LENDING lens will perform efficiently as it addresses many issues: it does not distort the representation, the dynamically controlled translucent flat-top reduces occlusion problems, and its large size makes the task easier from a motor perspective.
As the mean number of errors for each lens and the total number of errors are low , we focus the following analyses on hits only.
We verified that there was no effect of lens presentation order on time and observed that learning effects were not significant for each lens.
Figure 7 illustrates these results.
As expected, S PEED -C OUPLED B LENDING performs better than all other lenses.
Other predictions are only partially supported by the measures we collected.
First, H1 is not supported.
Eliminating distortion by switching from a spatial transition to a smooth translucence-based transition does not seem to provide an advantage.
H2 is partially supported:  smoothly neutralizing and restoring the focus of a M AGNIFYING G LASS by making it translucent  does improve performance ; but  flattening a fisheye  does not yield a significant improvement over F ISHEYE .
This last result is surprising since the study reported in  showed that S PEED -C OUPLED F LATTENING outperfoms F ISHEYE for a distortion level of 5 .
This inconsistency can be explained by taking a closer look at implementation details.
First, we implemented S PEED -C OUPLED F LATTENING as a constrained lens while it was implemented as a full-screen lens by Gutwin.
In full-screen lenses, distortion affects the whole representation, which thus benefits more from the neutralization effect than constrained lenses that only affect a limited area.
Second, as we require that M M  60% of max.
Finally, H3 is supported: S PEED -C OUPLED B LENDING, with its large flat-top, outperforms all other lenses starting at M M = 4.
Conversely the performance of M AGNIFYING G LASS goes down rapidly as M M gets higher.
It becomes the worst lens starting at M M = 6, due to the earlier-mentioned negative effects of occlusion that make precise positioning difficult.
It is interesting to note that for the lowest value of M M , M AGNIFYING G LASS outperforms all other lenses.
Occlusion caused by the lens' opacity still causes the user to temporarily loose track of the target.
But the occlusion zone is small at such low magnification.
The negative impact of occlusion on performance is thus not significant compared to the positive impact of the larger flat top.
Compared to S PEED C OUPLED B LENDING, M AGNIFYING G LASS also has the advantage of not requiring the user to wait several hundred milliseconds for the flat top to become opaque enough.
In the particular case of very low magnification factors , M AGNIFYING G LASS should thus be considered by interface designers.
Experiment 1 compared lenses with the same size .
We found that S PEED -C OUPLED B LENDING lenses outperform S PEED -C OUPLED F LATTENING lenses, and attributed this performance gain  to the large flat-top of the S PEED -C OUPLED B LENDING lens which makes focus targeting easier from a motor perspective, and  to the absence of distortion and reduction of occlusion effects through the coupling of focus translucence with lens speed.
Experiment 2 aimed at better understanding the results of the previous experiment by identifying the contribution of both properties to this performance gain.
Apparatus was the same as before.
Tukey post hoc tests revealed the following lens performance order: S PEED -C. B LENDING > S PEED -C. B LENDINGsmall > S PEED C. F LATTENING, as illustrated in Figure 8.
These results show that even at the same level of motor difficulty , the S PEED -C OUPLED B LENDING lens still performs better than the S PEED -C OUPLED F LATTENING lens.
The focus targeting task was exactly the same as the one used in Experiment 1.
To reduce the length of this experiment, we picked two representative magnification factors: M M  {8, 12}.
This experiment was thus a 3 x 2 withinparticipant design.
We again grouped trials by lens type.
We used a Latin square to compute three different presentation orders for lenses and assigned two participants per order.
Each participant performed three Lens blocks.
For a given lens presentation order, one participant saw trials in order M M = 8 then M M = 12, while the other one saw trials in order M M = 12 then M M = 8.
Translucence can affect targeting performance , especially when targets are superimposed on a complex background such as a map or photograph.
As the simple abstract world we used in the first two experiments might have hidden negative effects of translucence on lenses, we conducted a third experiment to check whether our comparative lens performance ordering was still valid when targeting objects that blend into a realistic background.
Apparatus was the same as before.
The task was essentially the same as before, except for the fact that the 24 targets were laid out on a satellite photograph , and could either be filled with a fully opaque red color  or with a translucent red , in which case they blended into the background and were less easily identifiable.
The satellite photograph was a 7000x5000 pixels portion of NASA's Blue Marble Next Generation world map , providing appropriate levels of detail in both the focus and context regions.
To limit the length of this ex-
This experiment was thus a 4 Lens x 2 M M x 2 Opacity within-participant design.
Trials were again grouped by lens type.
Participants performed four blocks which were presented in four varying orders computed through a Latin square.
Each block was made of four trials , one per randomly distributed Opacity x M M condition, and was preceded with a training phase of two trials .
We compared two such new lenses with three representatives of the first category using a focus targeting task.
Empirical data revealed that our new S PEED C OUPLED B LENDING lens outperforms all other lenses.
These results encourage us to further investigate the use of non-spatial dimensions to transition between focus and context.
First and foremost, the exploration of our design space has revealed several potentially interesting new lenses, based on innovative combinations of space and translucence, on the coupling of lens speed with properties such as its radii, or on the use of time-based functions other than lens speed.
Secondly, we have seen that, depending on the lens and task studied, non-motor aspects can have a significant influence on performance, e.g., flat-top size and legibility, occlusion and search .
We thus plan to formally evaluate lenses based on a wider range of tasks, including high-level cognitive ones.
Results were consistent with that of previous experiments, and matched participants' subjective preferences.
Our initial performance ordering was preserved.
The only difference was that S PEED -C OUPLED F LATTENING significantly outperformed F ISHEYE .
We tentatively attribute this higher significance to the more disturbing effects of distortion during lens movements on a complex background, to be confirmed by further evaluations.
Regarding the specific effect of target visibility, we found a simple effect of Opacity on time , and an interaction effect of T echnique x Opacity on time , confirming that lens performance does depend on this factor.
However, Tukey HSD post hoc tests revealed that conditions Opacity = 0.5 and Opacity = 1 were in two different groups only for the B LENDING lens.
This result is not unexpected as the B LENDING lens can be prone to visual interference between focus and context in the transition region depending on the nature of the representation, especially when non-contrasted objects are targeted.
No matter how aesthetically pleasing , the B LENDING lens suffers from its earlier-mentioned lack of reliance on a familiar physical metaphor, and proneness to visual interference in the transition region.
The S PEED -C OUPLED B LENDING lens, however, does not suffer from these problems, as its use of translucence is very different: it can be seen as a magnifying glass whose content smoothly fades out to prevent occlusion at focus targeting time.
P. Baudisch and C. Gutwin.
Multiblending: displaying overlapping windows simultaneously without the drawbacks of alpha blending.
Toolglass and magic lenses: the see-through interface.
Computer graphics and interactive techniques, pages 73-80.
M. S. T. Carpendale, D. J. Cowperthwaite, and F. D. Fracchia.
3-dimensional pliable surfaces: for the effective presentation of visual information.
M. S. T. Carpendale, D. J. Cowperthwaite, and F. D. Fracchia.
M. S. T. Carpendale and C. Montagnese.
A framework for unifying presentation space.
S. Carpendale, J. Ligh, and E. Pattison.
Achieving higher magnification in context.
The usability of transparent overview layers.
In CHI 98 conference summary on Human factors in computing systems, pages 301-302.
C. Forlines, D. Vogel, and R. Balakrishnan.
Hybridpointing: fluid switching between absolute and relative pointing with a direct input device.
G. W. Furnas and B.
Y. Guiard and M. Beaudouin-Lafon.
Target acquisition in multiscale electronic worlds.
Improving focus targeting in interactive fisheye views.
The effects of dynamic transparency on targeting performance.
C. Gutwin and C. Fedak.
A comparison of fisheye lenses for interactive layout tasks.
C. Gutwin and A. Skopik.
Fisheyes are good for large steering tasks.
Transparent layered user interfaces: an evaluation of a display design to enhance focused and divided attention.
B. Bederson, and C. Plaisant.
Navigation patterns and usability of zoomable user interfaces with and without an overview.
T. Igarashi and K. Hinckley.
Speed-dependent automatic zooming for browsing large documents.
9241-9 Ergonomic requirements for office work with visual display terminals -Part 9: Requirements for non-keyboard input devices.
International Organization for Standardization, 2000.
T. A. Keahey and E. L. Robertson.
Powers of ten thousand: navigating in large information spaces.
The perspective wall: detail and context smoothly integrated.
C. North and B. Shneiderman.
Snap-together visualization: a user interface for coordinating visualizations via relational schemata.
A Toolkit for Addressing HCI Issues in Visual Language Environments.
E. Pietriga, C. Appert, and M. Beaudouin-Lafon.
Pointing and Beyond: an Operationalization and Preliminary Evaluation of Multi-scale Searching.
C. Plaisant, D. Carr, and B. Shneiderman.
Image-browser taxonomy and guidelines for designers.
T. Porter and T. Duff.
G. Ramos, A. Cockburn, R. Balakrishnan, and M. Beaudouin-Lafon.
Pointing lenses: facilitating stylus input through visual-and motor-space magnification.
G. G. Robertson and J. D. Mackinlay.
M. Sarkar and M. H. Brown.
Stretching the rubber sheet: a metaphor for viewing large layouts on small screens.
R. Stockli, E. Vermote, N. Saleous, R. Simmon, and D. Herring.
The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS.
Published by the NASA Earth Observ., 2005.
C. Ware and M. Lewis.
In CHI '95 conference companion, Human Factors in Computing Systems, pages 407-408.
