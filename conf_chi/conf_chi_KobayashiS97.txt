Browsing audio data is not as easy as browsing printed documents because of the temporal nature of sound.
This paper presents a browsing environment that provides a spatial interface for temporal navigation of audio data, taking advantage of human abilities of simultaneous listening and memory of spatial location.
The motion of the sound sources maps temporal position within the audio recording onto spatial location, so that listeners can use their memory of spatial location to find a specific topic.
This paper describes the iterative design approach toward the audio browsing system, including the development of user interface devices.
The iterative design approach toward implementation is reported next; the audio presentation is designed to help form spatial memory and to enhance selective listening, and new interaction methods rae developed to reduce errors and enable fine grain control of audio playback.
Audio Notebook  is an enhanced paper notebook, which allows a user to capture and access an audio recording of a lecture or meeting in conjunction with notes written on paper.
Users can access the recorded audio by flipping pages or by pointing to a location in the notes.
With the Audio Notebook, users often remember the mapping of physicat location in their notes to the desired audio information.
We recognize the size and structure of the document, and use our viswd spatial memory to recall and search for specific topics.
Browsing audio is not so easy.
When browsing an audio recording, we must repeatedly play and skip portions; without playing, we cannot pmeive the sound or its contents.
We must hear all of the audio stream to reliably capture all its topics.
The goal of this paper is to create a tool to "browse" audio efficiently, by using the human ability to spatially access information.
The "cocktail party effect" is the foundation on which this paper relies; we have the ability to selectively attend to one sound source in the presence of other sounds and background noise, and the ability to listen to a background channel.
By taking advantage of the cocktail paty effect, and by introducing the idea of spatial mapping of audio, we mated an audio browsing environment in which one can browse audio data by switching focus between multiple sound sources, and can use spatial memory to access information.
Filochat  co-indexes speech recording to pen-strokes of handwritten notes taken with a digital notebook.
Users can access a portion of audio data that is associated with a specific note by gesturing at the note.
Filochat allows users to access audio data directly by using spatial memory of written notes.
Unlike Audio Notebook or Filochat, this paper focuses on the creation of audio-only browsing environments.
Taking advantage of the ornni-present and omni-directional nature of our hearing, this paper implements a system that utilizes hearing as another channel of input which is available for listening even when you are busy writing or driving.
Audio Notebook and Filochat are relevant to this work since they utilize spatial memory to access audio recordings.
While they have visuat marks on notes to help remember location, this paper takes on the more challenging task of using spatial memory in an audio only environment.
Cohen and Ludwig  used acoustic processing to give audio "windows" status as foreground/background auditory prominence.
The virtual meeting room developed at AT&T Bell laboratories  used spatialized audio to provi& information about the connectivity, presence, focus and activity of the participants using sampled sounds such as keyboard clicks as well as speech.
Pitt and Edwards  examined the use of stereo audio for manipulating objects through mouse movements for blind users.
Objects in the space make sounds simultaneously, and the intensity of the sounds reflects the dkance to the cursor.
Users can ` This research was completed at the Media Laboratory while the first author was a student there.
PAPERS The Speaker orbits the user's head as it plays the audio &i@ and so creates a map between time and space.
When the user wants to re-play a topic that he/she ~ he/she indicates the position where the topic was presented by pointing in that direction, Another Speaker is created at the point and begins playing from the point in time of audio data presented there .
The original Speaker continues playing after the new Speaker is _ so the user hears multiple portions of the rtxording simultaneously from the Speakers.
The original Speaker demases its loudness until the user returns his/her attention to it by indicating the current position of the original Speaker.
The user can jump ahead by indicating the appropriate position ahead of the original Speaker.
A new Speaker is created there, playing the appropriate audio data for that position in the sound file, and the original Speaker again continues to play and decrease loudness.
Though the user jumps ahead, hehhe can hear the skipped audio data from the original Speaker which is running after the new one, and when he/she finds something interesting from the original Speaker, hekhe can switch back to the original one by indicating it.
AudioStreamer  creates an audio-only browsing environment that enhances the listener's ability to browse audio data by taking advantage of the "cocktail party effect."
It presents three audio data streams at three distinct fixed locations in a virtual audio space.
The spatial armngement of the three sound sources facilitates the separation of the simultaneously presented multiple audio da~ and allows users to attend to one of them selectively.
AudioStreamer enhances our ability to selectively attend to the source of greatest interest by making it acoustically prominent when the user leans toward one particular sound source.
It also augments our ability to perceive events in the nonattended audio channels by auditorially alerting users to salient events on those channels.
Usem of AudioStreamer can use their spatial memory for audio navigation, such as a topic which was heard on the "left channel."
AudioStreamer showed the potential for simultaneous listening for audio browsing.
Motivated by AudioStreamer, this paper implements an alternative form of spatialized simultaneous listening for more efficient temporal navigation of a single audio wording.
The major differences between AudioStreamer and this paper are  the number of audio data streams, and  the location of sound source.
By playing a single audio stream by means of moving sound sources, the system of this paper maps time to space, while AudioStreamer maps three audio streams to three locations by playing three audio streams through three fixed location sound sources.
Figure 1 illustrates the concept of the auditory space created by the system.
The objects in the figure are invisible audio objects.
The basic object of the Soundscape is a "Speaker," a moving sound source orbiting the listener's head.
There can be multiple Speakers simultaneously playing different portions of the same audio data stream, although there is one Speaker when the system starts playing.
When the system starts, a Speaker is created at some point in an orbit around the user.
Simultaneous presentation of multiple portions of a single audio recording is one of the key ideas of this paper.
Instead of fast-fomvarding or rewinding, the user can browse the audio data by switching attention between multiple sound sources, each of which plays different portions of the same recording, analogous to visual browsing in that we move our focus around the document.
Even when the user is concentrating on one sound source, he/she can hear other portions of the audio data from other sound sources in the background, and helshe can switch to another sound source upon finding a more interesting topic in it.
Spatial presentation of audio data is the other key idea of this paper.
In contrast to the lack of temporal attributes, the visual attribute of spatial location is commonly and automatically encoded to the memory of events.
Whether it has a red or imagined context of space, it is fiwquently recalled and intimately associated with the recognition of the events, and enhances the memory of the event  .
The spatial presentation proposed in this paper allows the use of spatial attributes of our memory to compensate for By associating the the weakness of temporal recall.
When the user pointed to a location where no Speakers exist, the user's input is interpreted as the "cnxnz new Speuke#' command which creates a new Speaker to play the audio recording from the point that corresponds to the location.
There can be at most four Speakers at the same time.
When the user reouests a fifih Soeaker.
By creating Speakers, users can play the desind audio based on their spatial memory, and by switching focus between multide Soeakers.
The Sparcstation and the storage device work as the audio server, which plays multiple digitized audio signals.
The Sparcstation has two stereo ports, so the system can have at most 4 monaural outputs.
To implement the browsing system described in the previous section, we took an iterative design approach; w started with a rough implementation of the idea, and evolved the system based on feedback of the implemented system.
This section describes the initial implementation of the iterative design, and the problems found.
We initially tested three models of Speaker motion:  mono-directional straight line,  bi-directionrd straight line, and  a circular orbit.
The orbital motion was chosen because  it can present a long audio recoding continuously,  it provides a two dimensional space for accessing Speakers, and  the disadvantage for users who cannot tell front from back can be reduced by suggesting that the Speakers are moving clockwise.
Two Crystal River Beachtron audio cards, installed in the audio spathdization server PC, receive the four audio signals from the Sparcstation.
The Beachtron cards spatialize the audio signals, locating monaural audio signals at the specified location in virtual acoustic space.
The Polhemus position sensor is mounted on the headst% and measures the location and direction of the user's head.
For locating sound, we naturally use the cue of the change of audio stimuli as we move our heads.
By actively changing the virtual audio environment according to the motion of listener's head, the sense of space of the virtual audio can be enhanced .
The information captmed by the Polhemus sensors is sent to the audio spatialization server, so it can change the relative position of the sound sources.
Another cause may be that the speed of the Speaker was inappropriate.
The speed of 6 degrees per second was chosen because at this speed the Speakers seem to move, and there is enough spatial resolution to access multiple events within a topic.
However, it seemed too fast to remember the location of events and topics.
Further evaluation of the manner and the speed of Speakers' motion seemed to be essential to make the system usable.
Although we have the ability to listen to a sound selectively, when adding the Speakers in order to play other portions of audio, it becomes had to hear one sound selectively.
Selective listening seemed to be lxuder in the virtual audio space than in the natural audio space because of the less than perfect spatial localization.
Another factor is that the multiple Speakers may be playing the same talker's voice, since they all play the same recording.
Difference of voice, which is one of the factors that contribute to the ability of selective listening , is small.
A study about the way we selectively listen to one sound source among multiple sounds was needed to provide the basis to build a human interface to enhance the selective listening in the virtual audio space.
The Macintosh computer receives the user's input through interface devices connected to the ADB bus, to which various devices are available in the market.
In this initial implementation, the touchpad interface  and the knob interface  me connected.
The Macintosh is connected to the Sparcstation via the serial interface.
For users who do not perceive the audio spatially, it is impossible to remember the spatial location of topics.
Even for users who perceive the audio spatially, but imperfectly, the error in locating sound sources results in memory of the wrong location.
Since all users use a common HRTF , which should vary slightly between each user because each of us has differently shaped ears, there always is a gap between the intended and perceived position of the sound.
It is necessary to bridge that gap.
Memory of the spatial locations of audio events is one of the key issues of this paper.
By playing an audio recording through moving Speakers whose location is a function of time, a mapping between spatial location and time is formcxl, and users can employ spatial location to navigate through the temporal audio.
We expected that users could remember the location of topics  or events  in the audio recordiig; such memories of location ae essential to use the spatial location for audio navigation.
However, in experimental use of the browsing system in which Speakers move at the speed of 6 degrees per second, it seemed hard for users to remember the locations of topics and events in the audio recording.
Positions seemed to become vague because the Speaker moved while they wem being presented.
One cause of this vagueness of memory about the location may be the motion of the sound sources.
Our memory about the location of topics in the audio recording has poor resolution.
We usually memorize the location in quadrants or 12ths of a circle, such as saying "a topic in left front," but never say "the topic at 38 degrees from the front."
When pointing to a location to access the audio corresponding the location, we may be able to point close to the correct location, but it is almost impossible to It is necessary to estimate the pinpoint the position.
Errors also occur when pointing to a location by using the pointing device.
Even if the user has an accurate memory of location of the sound sources, an error may occur when he/she transfers the location in the space of memory, which is ideally same as the space of audio, onto the location in the space of interface device.
The ordinary length of single topic in the news program used in this With the slow continuous experiment was 30 seconds.
The slow continuous motion  was chosen as the motion of Speakers of the browsing system.
Presentation of audio was redesigned to solve the first two problems of the initial system.
The mapping of time to space has been redesigned to solve Problem I: Diftlculties in remembering topic locations, and the slight head motion was utilized to solve Problem II: Difficulties in selectively listening to virtually spatialized audio, enhancing the human ability of selective listening.
Using the initial system, it seemed difficult to remember efficiently the topics and their locations.
Two factors might contribute to the difficulties: the motion itself, and the If the motion itself is the inadequate speed of motion.
If speed is the problem, slower speed lets users memorize better.
In the initial implementation, it was hard to listen to one of the multiple sound sources, though we have the ability to listen selectively in the natural environment.
This is certainly in part due to the fact that each Speaker may talking in the same announcer's voice.
To enhance the ability of selective listening in the virtual acoustic space, we first observed our behavior while listening to a sound, and then developed an interface based on natural behavior.
Four subjects were asked to listen to a 5 minute recording of a radio news program being played through a Speaker that moves in one of the three motions.
Each subject performed three sessions with three motions in a random order.
After each session, subjects were asked to list all the topics they remembered, and the location of the topics.
In order to find the natural behavior that could be used as a clue for the system to know the user's interest, we did a brief observational study of listening to a sound in the presence of background noise.
To simulate the situation of the system, we placed three3 loudspeakers around the subjects, and played three audio streams of conversation.
Subjects were asked to listen to one sound source ad understand the contents.
The behavior was video taped by a camera in front of the subjects.
Some subjects participated in an additional subsequent experiment, in which binaural virtual audio was used instead of the three loudspeakers.
Similar experiments have been done to observe the H motion when locating a sound source in space  .
Strategic motions to find the sound sources were observed in those experiments.
This experiment focused on listener motions for selective listening, for better reception of the desixed sound.
In the first experiment, some subjects moved their bodies radicdly, and others did not.
Selective listening is performed by the combination of both physical movement and internal filtering within our brain.
Some subjects moved their bodies actively to hear better, and others could listen selectively without moving.
Slight adjusting head motions were common even among the subjects who did not move much.
They adjusted their head location and direction repeatedly to find the best location and direction.
Leaning toward the speaker was often Though observed in the adjusting motion .
In the experiment with virtual audio, the subjects tended to not move their heads.
If the audio spatialization worked properly for the subjects, adjusting their hed location should be beneficial for getting better reception of the desired audio.
After the experiments, subjects said that they did not 3 The hardware supported 4 output channels, and we xeserved one for the audiocursor, which will be described in the next section, leaving three max for Speakers.
With the slow continuous motion , all subjects remembered more topics and their locations than with other motions.
Even subjects who did the session with slower motion f~st and the session with fast motion next remembered the location of topics better in the slower motion.
A subject who tied had to remember the location of topics could tell locations of topics which sometimes span 180 degrees, but only about the topics presented at the beginning and the end of the session, reflecting the characteristics of short term memory.
The slow continuous motion was also the motion most preferred.
Discrete motion  did Furthermore, the sudden sometimes made it difficult especially in multi Speaker not make for better memory.
The slower speed motion, which was chosen as described in the previous section, maps longer portions of the audio recording to a unit space; as the result the resolution of pointing decreases.
In order to enable tine grain control of audio, the system employs a "grab-and-move" interface, with which users can adjust the location interactively after hearing the audio which is of the wrong location.
Like the "pointing" interface of the initial implementation, users request the system to play a portion of audio by pointing to the location that corresponds to the audio.
When there is a Speaker at the location, the system puts the Speaker under the user's control, which is the "grabbed" state.
If the audio that the grabbed Speaker begins playing is different from what hehhe expected, the user can move the grabbed Speaker to adjust the point to play.
While the Speaker is moved, it plays small segments of the audio of the location, so the user hear the contents of the audio.
When there is no Speaker at the location the user pointed to, the system creates a new Speaker there, and starts playing the audio recording from the corresponding point.
The system has a table of times which are probable boundaries of topics.
The preprocessor, which was developed for Newscomm , generates the table based on acoustic cues such as the long pause, change of talker, or emphasized voice.
When the system decides the position to play from, it chooses a time closest to the pointed location from the boundary table.
This is to enable more efficient navigation by guessing a more salient point from which to play.
We learned from these observations that people move their heads to help selectively listen.
This cue was dded to the system, using head motion to enhance the human ability of selective listening.
The system measures the direction of leaning with the Polhemus sensor attached to the headset.
The system changes the loudness of each Speaker according to the angular distance between the angle of the Speaker's location and the angle of the direction of leaning.
The change of the loudness is proportional to the angular distance.
So the closer the user leans his/her head toward a Speaker, the louder the Speaker plays.
Since the change of loudness is a continuous function of the angular distance between the direction of leaning and the location of the Speaker, users can adjust their leaning direction by responding to the feedback from the system.
Such adjusting motion induced by the feedback from the system is similar to the natural motion while listening.
The system makes a Speaker at most 8 dB louder than others when it is close to the direction of leaning, an exaggerated feedback, which never happens in the natural environment.
This exa~erated feedback makes the benefit of leaning motion clear to the user, artificially enhances selective listening.
For most users testing the browsing system, audio spatialization was less than perfect.
To enable precise interaction with the objects in the virtual audio space, a means to bridge the gap is necessary.
The "audio cursor" is an audio object in the virtual audio space of the system.
It continuously plays the sound of a vibrating spring in a tube , a noise with a distinctive rhythm, while it is turned on by the user.
It provides location feedback, so the audio cursor moves within the virtual audio space as the user operates the interface device.
Before "grabbing" an audio object, the user moves the audio cursor to the location of the audio object, and acoustically overlays the audio cursor on the audio object.
The touchpad interface and the knob interface were modified to adopt the grab-and-move interaction and audio cursor.
The user can move the audio cursor by moving his/her finger on the touchpad or by rotating the knob, grab a Speaker by pressing the touchpad/knob, and move the Speaker by moving hisfher finger pressing down the touchpad surface or by rotating the knob pressing down it.
It was difficult for users of the initial system to point to the correct location to access the desired portion of audio.
To improve the accumcy and the resolution to access the desired da@ three new interaction methods were developed.
The point-by-hand interface is a hand gesture interface, with which users can access their desired data by directly pointing to the location where they hear the audio.
The interface device to detect the hand gesture is built with the Fish sensor, which is a non-contact sensor based on the interaction of a person with an electric field .
As shown in Figure 7, with a transmitter on the chair on which the user sits, and four receivers hang over the user, the Fish ean detect the distance between each sensor and the user's hand as the intensity of electric field.
After a calibration session, which is necessary to adapt to the various sizes and shapes of user's body, the system can compute the x-y coordinates and the height of user's hand.
With the point-by-hand interface, the user turns on the audio cursor by raising a hand, and moves the audio cursor by moving the hand.
To grab a Spmker, the user moves the audio cursor to the location of the Speaker, and stretches the arm like grabbing an apple on a branch.
The grabbed Speaker is kept grabbed until he/she lowers the hand.
In contrast to the initial implementation where users could not remember the location of audio events, most users reported that they could use their spatial memory for audio navigation with the relined system.
When the Speakers wem moving at an adequate speed to form memory of the topics, the space seemed to help users to memorize the topics.
By observing subjects, we are led to believe that the association between the topics and spatial locations helps to transfer the memory of topics to the long term memory.
The spatial presentation provides an environment to organize information, with the mapping which associates the contents of the topics to spatial locations.
This association aids recall of the story topics.
This memory effeet similar to the "Simonides memory palace" is made available in the audio only environment by this system.
It imitates the interactive process between listener and the audio space; as the listener moves his/her head, the reception of the sound source changes, and then the listener adjusts his/her head repeatedly to get better reception.
For many users, it was a natural iterative process, and they could comfortably use the interface to listen selectively.
With this interfaee, users can naturally and quickly switch their attention among multiple sound sources.
They reported that both interfaces were easy and accurate for navigating audio.
The large "point-by-hand" interface was preferd because it bad the scale closer to the scale of the path of the Speaker.
We expected that the small interface would place higher cognitive load on users because of difficulties with the crossspace mapping between the large virtual audio space and the small interface space.
However, most users did not find it hard to use the small interface device, because they wm familiar with controlling small devices such as a mouse.
To use the "point-by-hand" interface, users bad to learn the height to raise their hands to control the audio cursor or to grab a Speaker.
However, for those who were used to the operation of the interface, it was an easy and direct interface.
By moving the audio cursor, they could learn the correlation between locations in the virtual audio space and locations in the space of interfaee devices.
Whh the "point-by-hand" interface, the audio cursor sounds at the location close to the user's hand.
It produces an illusion of moving the audio cursor by their hand, and enhances the sense of space of the virtual audio space.
The typical length of topics may differ by the type of the audio recording, such as radio news, recordings of lectures, or audio books of novels.
It is desirable to change the speed of Speakers based on the type of the audio recording, or to &velop more adaptive mapping that maps each topic in an audio recording to the same amount of space by changing the speed according to the length of the topic.
We thank Jordan Slott who implemented the Network Audio Server , and Atty Mullins who implemented the AudioStreamer which gave us the motivation and technical basis of this work.
We also thank several research associates of the MIT Media Lab for testing the system, and the anonymous reviewers for their constructive critiques.
Arons, B., SpeechSkimmen Interactively Skimming Recorded Speech, procs.
Cherry, E. C., Some experiments on the recognition of speech, with one and two ears, Journal of the Acoustic Society of America, Volume 251953.
Cohen, M. and Ludwig, F. L., Multidimensional Audio Window Management, International Journal of ManMachine Studies, Vol.
Hudson, S. E. and Smith, I., Electronic Mail Previews Using Non-Speech Audio, procs.
King, W. J. and Weghorst, S. J., Ear Tracking: Visualizing Auditory Localization Strategies, procs.
Some users reported that it was difficult to notice salient topics spoken by a non-attended Speaker.
In this system, all Speakers are presented at the same loudness unless the user leans toward a Speaker.
Users tend to move their heads and The M switch around the Speakem once in a while.
Although they could patrol other Speakers by hopping around, users sometimes miss interesting events in the background channel because of the temporal nature of audio.
Approaches developed in AudioStreamer , which arouse the user's attention at prominent events, should be combined with this browsing system.
By mapping time to space, and simultaneously presenting multiple portions of an audio recording, the system enables users to browse audio data spatially.
Although further work to implement adaptive mapping is necessary, this paper suggested the approximate guideline of mapping that a topic should be mapped to a unit area of our memory of sound location, which is generally a quadrant or a 12th of a circle.
This paper also covered methods of interactive access to the system.
The grab-and-move interface enables fine grain control of audio, and compensates for the small spatial resolution of our memory of sound locations.
The audio cursor compensates for localization error, which largely depends on the individual listening characteristics, and it also enables precise access of audio objects by acoustic overlay of The point-by-hand interface the cursor and the object.
Along with the audio cursor, the "point-by-hand" interface creates the feeling that the user is touching the audio object, and increases the spatial experience of the virtual audio space.
The head interface contributes to enhance the ability of selective listening in the virtual audio space.
The implemented system proved that spatial memory of We expect audio events is usable for audio browsing.
