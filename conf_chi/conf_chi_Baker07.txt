We present a machine-learned model that can automatically detect when a student using an intelligent tutoring system is off-task, i.e., engaged in behavior which does not involve the system or a learning task.
We show that this model can both accurately identify each student's prevalence of off-task behavior and can distinguish off-task behavior from when the student is talking to the teacher or another student about the subject matter.
We use this model in combination with motivational and attitudinal instruments, developing a profile of the attitudes and motivations associated with offtask behavior, and compare this profile to the attitudes and motivations associated with other behaviors in intelligent tutoring systems.
We discuss how the model of off-task behavior can be used within interactive learning environments which respond to when students are off-task.
One such type of behavior that may affect students' learning is off-task behavior, where a student completely disengages from the learning environment and task to engage in an unrelated behavior.
Examples of off-task behavior include talking to other students about unrelated subjects , disrupting other students , and surfing the web .
It has been hypothesized that off-task behavior is associated with poorer learning , but this hypothesis has only been studied to a limited degree within learning environments.
However, a later meta-analysis by the same research group  found a statistically significant negative correlation between offtask behavior and learning.
Hence, it may be possible to make Cognitive Tutors - and other types of interactive learning environments - more educationally effective, by detecting and responding to off-task behavior.
It is worth noting that off-task behavior occurs in many types of interactive systems beyond just educational software.
Such systems might also be more effective if they could detect when their user is not paying attention to the task at hand.
Detecting whether a student is off-task, in a classroom setting, is likely to be a challenging task.
In a highly instrumented setting, with microphones, gaze trackers, or fMRI machines, it might be relatively easy to determine whether a student is off-task.
However, such equipment is not available to most schools; for a system to be widely useful, it must detect off-task behavior using data only from students' actions within the software.
It has been found that recognizing a user's intentions solely from his or her actions within a system can be quite challenging ; however, off-task behavior detection need not be perfect in order to be useful.
In existing learning environment-based school curricula, the responsibility lies entirely with the teacher to detect and respond to when students are off-task.
Teachers cannot observe and interact with every student at the same time.
By contrast, an off-task behavior detector built into the learning environment can observe every student at every moment.
However, the vast majority of this past work has focused specifically on how students choose to act within the software.
A student's behavior outside of a system may also affect how well the student learns from the software.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In this paper, we present a machine-learned model which can determine whether a student is off-task, using only data from students' actions within the software - the model uses no audio or video data.
We compare our model to a model which simply treats idle time as off-task, and show that the machine-learned model is more accurate.
Then, we analyze the features that make up the model, in order to understand the model better.
Next we examine data from attitudinal and motivational surveys, in order to see what factors are associated with the choice to spend more or less time off-task.
We also compare these factors to the factors associated with the choice to game the system .
Gaming the system in Cognitive Tutors consists of behaviors such as systematic guessing and persistent overuse of hints, and has also been shown to be significantly associated with poorer learning .
We conclude with a discussion of potential ways that interactive systems can respond to a student going offtask, considering in particular the challenge of responding in a way that does not reduce off-task behavior at the cost of an increase in gaming the system.
Figure 1: A screenshot from a Cognitive Tutor lesson.
The five studies shared the following general design.
Each student in each of the five studies first viewed conceptual instruction on the upcoming tutor lesson, delivered via a PowerPoint presentation with voiceover and some simple animations.
After viewing conceptual instruction, each student used the tutor for around 80 minutes .
Data about learning was collected using pre- and post-tests given before and after the students used the Cognitive Tutor.
Two essentially isomorphic problems were constructed for the tests, for each lesson.
Each problem was used as a pre-test for half of the students, and as a post-test for the other half.
The problems were designed to exercise the key skills involved in the lesson , and were graded in terms of how many of the skills a student successfully demonstrated.
The test items used in each study are given in .
Each student's behavior was systematically observed a number of times  during the course of multiple class periods, by one of three observers.
Off-task behavior included off-task conversation , off-task solitary behavior , and inactivity .
Gaming the system was not treated as a type of off-task behavior; within the observations, it was a separate category.
Data from five studies, conducted between 2003-2005, was used in our investigation of students' off-task behavior as they used Cognitive Tutor software.
Each of the studies presented in this paper was conducted in mathematics classrooms using Cognitive Tutor software, a popular type of interactive learning environment now used by around half a million students a year in the USA.
Cognitive Tutor curricula combine conceptual instruction delivered by a teacher with problem-solving where each student works one-on-one with a cognitive tutoring system which chooses exercises and feedback based on a running model of which skills the student possesses .
A screen shot of a Cognitive Tutor is shown in Figure 1.
Each study was conducted in the Pittsburgh suburbs, within classrooms that had used Cognitive Tutors within their regular curriculum for several months.
None of the studies involved gifted or special needs students.
Three of the studies involved a tutor lesson on scatterplots; the other two studies involved tutor lessons on percents and geometry.
Across studies, most of the observations were carried out by a single observer.
However, an inter-rater reliability session was carried out in 2004.
In this session, two observers classified the same student at the same time.
Inter-rater agreement as to whether a behavior was off-task, gaming the system, or other categories of behavior was reasonably high - Cohen's   = 0.74.
In addition, within two of the studies, motivational and attitudinal questionnaires were given to increase understanding of why students choose to game the system.
In this paper, we will use these questionnaires to help us understand why students decide to engage in off-task behavior, and to compare between the motivations associated with off-task behavior and gaming the system.
A final source of data that we will use to understand offtask behavior is data from student log files as the students used the tutoring software.
Across the five studies, 429 students performed between 50 and 500 actions in the tutor in each lesson, for a total of 128,887 tutor actions .
For each student action recorded in the log files, a set of 26 features describing that student action were distilled.
Time taken, considered in three fashions o How many seconds the action took.
Latent Response Models  were used as the statistical basis for all of the detectors of off-task behavior discussed in this paper.
Latent Response Models have the advantage of easily and naturally integrating multiple data sources, at different grain sizes, into a single model.
In addition, they were used as the basis of successful detectors of gaming behavior, within the same data .
A detector of off-task behavior, in the framework used here , has one observable level and two hidden  levels.
The detector's assessments for each student can then be compared to the observed proportions of time each student spent off-task, OT0...OTn.
The proportion of time each student spends off-task is assessed as follows: First, the detector makes a  assessment as to whether each individual student action  is off-task.
From these assessments, OT'0...OT'n are derived by taking the percentage of actions which are assessed to be off-task, for each student.
An action is assessed to be off-task or not, by a function on parameters composed of the features drawn from each action's characteristics.
Each parameter in a candidate model is either a linear effect on one feature , a quadratic effect on one feature , or an interaction effect on two features .
An assessment SAm as to whether action m is off-task is computed as SAm = 0 X0 + 1 X1 + 2 X2 + ... + n Xn, where i is a parameter value and Xi is the data value for the corresponding feature, for this action, in the log files.
Each assessment SAm is then thresholded using a step function, such that if SAm  0.5, SA'm = 0, otherwise SA'm = 1.
We can then assess a model's goodness of fit by calculating the correlation between OT'0...OT'n , and the original observed data OT0...OTn.
Considering each action's time in the context of the distribution of time students take on the relevant problem step does not appear to perform substantially better than considering time in an absolute fashion.
However, it may be that off-task behavior manifests itself in a more complex fashion than this within the tutoring environment.
In the next section, we will consider whether a model trained using a fuller set of features can detect offtask behavior better than a model based just on each individual action's time.
Within this  modeling approach, the simplest and most straightforward way to determine whether a student is off-task is probably to set a cut-off on how much time an action should take and treat all actions that take longer than that cut-off as off-task.
Interestingly, that prior work viewed time in the opposite fashion than would be appropriate in this situation, looking for actions shorter than a time cut-off, rather than actions longer than a time cut-off.
If we use a single-parameter model which determines if an action is off-task, using only the time taken for that action, it fits well to the data, achieving a correlation of 0.47 .
According to this model's best-fitting parameter value, actions which take longer than 80 seconds are off-task.
This model is also not overfit; a 10-fold student-by-student cross-validation achieves an average correlation of 0.44 across test sets.
Alternatively, it may be that we can get a better fit by taking the average time for each problem step into account .
Hence, we can set up a single-parameter model which determines whether an action is off-task, using the time taken for that action, expressed in terms of the number of standard deviations the action's time was faster or slower than the mean time taken by all students on the relevant problem step, across problems.
This model also fits well to the data, achieving a correlation of 0.46.
According to this model's best-fitting parameter value, actions which take more than 3.8 standard deviations longer than normal are off-task.
This model is also not overfit; a 10-fold crossvalidation achieves an average correlation of 0.45 across test sets.
Within the model structure described above, there is a very large space of potential models that may potentially describe student behavior .
A combination of Fast Correlation-Based Filtering 1 and Forward Selection  was used in order to efficiently search this space of models, as follows: First, a set of single-parameter models were selected, such that: 1.
Each single-parameter model was at least 60% as good as the best single-parameter model found .
If two parameters had a closer correlation than 0.7 to each other, only the better-fitting singleparameter model was used.
Once a set of single-parameter models was obtained, each model was expanded, by repeatedly adding the potential parameter that most improved the linear correlation between the model's assessments and the original data, using Iterative Gradient Descent  to find the best value for each candidate parameter.
Parameters were added to the model until adding a parameter worsened the model's performance in a student-by-student 10-fold crossvalidation.
10-fold cross-validation is equivalent to doing a training set/test set validation ten times.
Pseudocode of this algorithm can be found in .
The best-fitting multiple-parameter model fits well to the data, achieving a correlation of 0.62.
The model is mildly overfit; a 10-fold cross-validation achieves an average correlation of 0.55 across test sets.
However, while there is some decrease in performance in cross-validation, the cross-validated performance of this model is substantially better than the single-parameter time-only models .
Overall, then, the multiple-parameter model is effective at determining how much each student is off-task.
In addition, this model is effective at determining how much each student is off-task, relative to other students.
If the observers found that student A was off-task more often than student B, the multiple-parameter model agreed 83% of the time.
The third parameter identifies specific situations where offtask behavior is more or less likely.
A student is less likely to go off-task when they are inputting a string, and know the step well .
A student is more likely to go off-task when they are inputting a string, and have already made an error.
Adding this parameter to the model adds 0.023 to the model's cross-validated correlation.
The fourth through six parameters together add only 0.009 to the model's cross-validated correlation.
The fourth parameter indicates that repeated help-requests are not offtask behavior, regardless of how fast or slow they are.
The fifth parameter indicates that two or more errors or help requests in a row are associated with off-task behaviors.
Because the fourth parameter is already in the model, this parameter likely focuses on errors, suggesting that some level of carelessness may be associated with off-task behavior.
The sixth parameter, making many errors on skills students generally know before starting the current tutor lesson, also seems to be indicate a general pattern of carelessness.
Overall, then, off-task behavior occurs in the tutor not just as slow actions, but as co-occurrence of very slow and very fast actions.
In terms of student motivation, off-task behavior appears to be associated with careless actions, and possibly also with avoiding help .
This pattern of behavior, though it has some commonalities with the knowledge-engineered time-only model of off-task behavior, represents off-task behavior in a more subtle fashion than the time-only model, and thus adds to our understanding of off-task behavior in a way that model cannot.
The best-fitting multiple-parameter model is made up of six parameters.
We will discuss these parameters in the order they were selected by the model; in the framework used here, each parameter after the first parameter must be understood in the context of the parameters already selected.
The full model is given in Table 1.
The first parameter involves very fast actions immediately before or after very slow actions.
This represents the fact that consistent very slow actions may indicate being offtask, but may also indicate careful thought or even asking the teacher for help.
Careful thought or asking for help would probably not lead the student to work extremely quickly right before or after a long, thoughtful action.
Hence, slow actions right before or after fast actions is more indicative of off-task behavior than slow actions alone.
Taken alone, this parameter, when 10-fold crossvalidated, achieves a correlation of 0.483 to the frequency of off-task behavior; hence, it already performs better than a model which labels all actions longer than a cut-off as offtask.
The second parameter indicates that if the current action is extremely slow or extremely fast, the evidence that it is an off-task action is even stronger.
This feature is somewhat similar to the single-feature models considered above.
Interpretation OT: Very fast actions immediately before or after very F1 timelast3SD timelast5SD -0.08 slow actions 0.039 F2 timeSD timeSD 0.013 OT: Extremely fast actions or extremely slow actions OT: Less likely on well-known string-input steps 0.023 F3 string pknowretro -0.36 OT: More likely when inputting a string after error 0.004 F4 notfirstattempt recent8help -0.38 Not OT: Asking for a lot of help OT: Two errors or help-requests in a row 0.004 Not OT: Errors or help requests on skills the student F5 notright pknowretro -0.16 has already mastered generallyOT: Indicated by many errors on skills students 0.001 F6 pctwrong known 0.04 generally know prior to starting this lesson Table 1.
The model of off-task behavior .
In all cases, param1 is multipled by param2, and then multipled by value.
One important goal for a model of off-task behavior is that it should effectively distinguish off-task behavior from other types of behavior that occur outside of the system - for example, on-task conversation .
A sophisticated system should not respond in the same way to a student asking a peer or the teacher for help, as it would to a student going off-task.
However, there is some risk that a system may not be able to distinguish between these categories of behavior just from log files of the student's behavior within the tutor.
Fortunately, data is available to investigate whether the model of off-task behavior can distinguish these behavioral categories.
Talking to the teacher or another student about the material was one of the categories of behaviors coded within the original observations, in each of the studies.
Since there is no correlation between these two categories of behavior in the observational data, r= -0.04, this is evidence that the model does not completely distinguish between these categories of behavior.
However, the correlation between observed ontask conversation and the model of off-task behavior is much lower than the cross-validated correlation between the model of off-task behavior and the observed off-task behavior , t=6.85,p<0.001, for a test of the significance of the difference between two correlation coefficients for correlated samples.
Hence, the model of off-task behavior does appear to successfully distinguish between these two categories of behavior, but does not achieve complete success in doing so.
Interestingly, the model of off-task behavior that relies only upon a time cutoff  appears to do a worse job of distinguishing between on-task conversation and off-task behavior, than the full model of off-task behavior does.
This model's correlation to on-task conversation is marginally significantly higher than the full model's correlation to ontask conversation, t=1.84, p=0.07.
This suggests that more sophisticated models of off-task behavior not only capture those behaviors better, but are more successful at discerning the difference between off-task behavior and other behaviors which involve idle time, such as on-task conversation.
This may be because the machine-learned model takes behavioral correlates  into account.
Data from two self-report questionnaires was used to study the relationship between students' motivations and attitudes, and their frequency of off-task behavior.
All items on both questionnaires were drawn from existing motivational inventories or from items used across many prior studies with students from the relevant age group, and were adapted minimally .
Both questionnaires were given to students along with their unit pre-tests, before they worked through a Cognitive Tutor lesson on scatterplots  or percents .
All items were given as 6-point Likert scales, except for a small number of multiple-choice and true-false items.
In order to analyze the relationship between student motivations/attitudes and off-task behavior, we correlated students' responses on the questionnaires to their frequency of off-task behavior, as assessed by the model of off-task behavior presented in this paper.
It is advantageous to use the model's assessments of off-task behavior rather than the classroom observations, because the model of off-task behavior's assessments are more precise than the classroom observations.
2-3 researchers can only obtain a small number of observations of each student's behavior, and thus the estimations of each student's frequency of off-task behavior have high variance.
By contrast, the model, with access to predictions about every student action, can make considerably more precise predictions.
We also compare the relationship between off-task behavior and student responses to the relationship between gaming the system and student responses, in order to better understand the relationship between these two categories of behavior.
We focus this discussion on "harmful" gaming, which occurs on steps the student finds difficult.
The first questionnaire, given in Spring 2004, is discussed in complete detail in .
If you had your choice, what kind of problems would you like best?
The second questionnaire, given in Spring 2004, is discussed in complete detail in .
This questionnaire consisted of items measuring: * If the student believes that computers in general, and the tutor in specific, are not very useful.
Relationships between the categories in the second questionnaire, and off-task behavior, as assessed by the model.
Statistically significant relationships  are in boldface; marginally significant relationships  are in italics.
Another possibility is that the students' relationship with their teacher may influence this choice.
It may be that students who feel positively towards their teachers, and want their teacher to approve of them, game the system rather than engaging in more noticeable behaviors such as talking off-task or surfing the web .
By contrast, students who feel more negatively towards their teacher may have less desire to avoid being seen off-task.
Another possibility is that students systematically differ in whether they prefer gaming the system or going off-task, for reasons that are not explicitly attitudinal or motivational.
One possibility is that students learn over time that their teachers or parents respond better to one of these behaviors than the other, and adopt the behavior which they have previously found more successful, when working in the Cognitive Tutor.
It is also possible that personality factors such as extraversion play a role - for example, more extroverted students may prefer to talk to their neighbors than interact with the system when they are unmotivated.
The similarity between the attitudes and characteristics associated with off-task behavior and gaming the system is striking - especially when the lack of correlation between the behaviors themselves is taken into account.
In the long term, we will understand both behaviors better when we can identify what factors differentiate between the students who engage in each type of behavior.
Overall, off-task behavior is associated with disliking computers, disliking mathematics, passive-aggressiveness, and not being educationally self-driven.
This pattern is quite similar to the pattern of attitudes in students who game the system in a fashion associated with poorer learning.
Those students dislike computers, dislike the tutoring software, dislike mathematics, and are not educationally self-driven.
It is somewhat curious that passive-aggressiveness is associated with off-task behavior, rather than gaming the system.
Gaming the system would seem, at some level, to be related to "doing a bad job on a task I don't want to do" - however, gaming can also be seen an attempt to succeed in an undesirable task without having to put full effort into that task, rather than an attempt to intentionally perform poorly or work more slowly.
One possible explanation for the overall commonalities in the attitudes associated with off-task behavior and harmful gaming is that the same students engage in both behaviors - i.e.
However, the two behaviors are, if anything, negatively correlated with each other.
Across the five studies, the frequency of harmful gaming and off-task behavior in each student's actions  are negatively correlated, F=8.22, p<0.01, r= -0.14.
The negative correlation between the two behaviors, combined with the similarity in the motivations and attitudes associated with the two behaviors, suggests that the choices to game the system or go off-task arise from relatively similar motivations but that some other factor leads students to choose between these two approaches.
One possibility is that this factor may be the degree to which the students perceive the current tutor lesson as difficult.
Knowing which student characteristics and attitudes are associated with off-task behavior is a good start towards developing systems that can respond appropriately when a student is off-task.
One important implication of our results is that off-task behavior is likely more than just evidence that a system is badly designed; instead, it is likely to be associated with deeper motivational problems.
In particular, redesigning systems to respond immediately and in a heavy-handed way - for example, by making a loud noise when a student is off-task - are likely to be counterproductive.
Re-designing systems in this fashion may actually lead students to game the system in order to avoid the system's intervention.
For example, a student might learn to type in an answer - any answer - every 20 seconds so that the system thinks he or she is actively working.
In addition, heavy-handed solutions are likely to irritate a student who is off-task, and irritate students even more when the model is incorrect and the student was not off-task .
Instead, it may be more appropriate to respond to off-task behavior with more long-term oriented, non-heavy-handed solutions.
Alternatively, it may be possible to increase challenge when students go off-task, or to give rewards to students who correctly complete problems quickly without gaming the system.
Rather than reducing off-task behavior by increasing gaming behavior, such an approach may even be able to remediate both off-task behavior and gaming the system at the same time, an important step towards interactive learning environments that can respond sensitively to the full spectrum of ways students choose to interact with them.
We then analyzed what student attitudes, motivations, and characteristics are associated with off-task behavior, using the detector in combination with questionnaire data.
We determined that off-task behavior is associated with disliking computers, disliking mathematics, passiveaggressiveness, and lack of educational self-drive.
These student attitudes and characteristics are very similar to the attitudes and characteristics found in earlier research to be associated with gaming the system - an especially surprising result in the light of the negative correlation between gaming the system and off-task behavior.
One possibility is that the two behaviors are different responses to the same motivation.
A student's decision of which behavior to use may interact with the student's prior learning experiences, specifics of the learning situation , their relationship with the teacher, or personality characteristics not measured in the questionnaires.
Future work will be needed to determine why some students choose to go off-task, while others choose to game the system.
Understanding the answer to this question may enable the development of systems that can respond appropriately to both of these student behaviors.
I would like to thank Ido Roll, Albert Corbett, Ken Koedinger, and Angela Wagner for the significant role they played in the original design, administration, and analysis of the studies re-analyzed here, as well as their helpful comments and suggestions on this paper.
I would also like to thank Jason Walonoski and the anonymous reviewers for their very helpful comments and suggestions.
In this paper, we have presented a model that can automatically detect, with reasonable effectiveness, when a student is off-task in a Cognitive Tutor.
This model does not rely upon sophisticated instrumentation which is unavailable in most school computer labs, such as microphones, eye-trackers, or fMRI - it relies only upon data about students' actions within the tutoring system.
We have shown that this model is more accurate than a simpler approach which treats all actions longer than a certain cutoff as off-task, both at determining each student's frequency of off-task behavior, and in distinguishing offtask behavior from on-task conversation, a category of behavior which - like off-task behavior - involves idle time.
The methods used to develop this model may be relevant for detecting off-task behavior in other types of interactive systems; idle time alone is generally likely to be less accurate than detecting idle time in combination with behavioral correlates.
Toward tutoring help seeking: Applying cognitive modeling to meta-cognitive skills.
Proceedings of the 7th International Conference on Intelligent Tutoring Systems , 227-239.
Anderson, J.R., Corbett, A.T., Koedinger, K.R., Pelletier, R. Cognitive Tutors: Lessons Learned.
Amershi, S., and Conati, C. Automatic Recognition of Learner Groups in Exploratory Learning Environments.
Proceedings of the 8th International Conference on Intelligent Tutoring Systems , 463-472.
Carnegie Mellon University Technical Report CMU-HCII-05-104.
Detecting Student Misuse of Intelligent Tutoring Systems.
Proceedings of the 7th International Conference on Intelligent Tutoring Systems , 531-540.
The Relationship Between Gaming the System and Learning in Cognitive Tutor Classrooms.
Off-Task Behavior in the Cognitive Tutor Classroom: When Students "Game the System".
Do Performance Goals Lead Students to Game the System?
Proceedings of the 12th International Conference on Artificial Intelligence in Education , 57-64.
Engagement tracing: using response times to model student disengagement.
Proceedings of the 12th International Conference on Artificial Intelligence in Education , 88-95.
Boyd, S., and Vandenberghe, L. Convex Optimization.
Cambridge University Press, Cambridge, UK, 2004.
A Model For School Learning.
A coefficient of agreement for nominal scales.
The effects of a self-management program in reducing off-task behavior.
16. de Vicente, A., and Pain H. Informing the detection of the students' motivational state: an empirical study.
Proceedings of the 6th International Conference on Intelligent Tutoring Systems , 933-943.
Accounting Student Acceptance of Tax Preparation Software.
Integrating Affect Sensors in an Intelligent Tutoring System.
Workshop at 2005 International Conference on Intelligent User Interfaces,7-13.
Measure Development: The Children's Attitudes Towards Technology Scale .
Development of a Shorter, More Reliable, and More Valid Measure of Test Motivation.
Paper presented at the 1980 annual meeting of the National Council on Measurement in Education.
Knezek, G., Christensen, R. Computer Attitudes Questionnaire .
Denton, TX: Texas Center for Educational Technology.
Maris, E. Psychometric Latent Response Models.
Praise for Intelligence Can Undermine Children's Motivation and Performance.
Murray, R.C., and vanLehn, K. Effects of Dissuading Unnecessary Help Requests While Providing Proactive Help.
Proceedings of the 12th International Conference on Artificial Intelligence in Education , 887-889.
Parker, G., Hadzi-Pavlovic, D. A Question of Style: Refining the Dimensions of Personality Disorder Style.
The Statistical Sleuth: A Course in Methods of Data Analysis.
Anxiety in Elementary School Children: A Report of Research.
Selwyn, N. Students' Attitudes Towards Computers: Validation of a Computer Attitude Scale for 16-19 Education.
Suchman, L. Plans and Situated Actions: The Problem of Human-Machine Communication.
Cambridge University Press, Cambridge, UK, 1987.
Yu, L., and Liu, H. Feature selection for highdimensional data: a fast correlation-based filter solution.
Gender Implications of Visual Cognition in Electronic Games.
