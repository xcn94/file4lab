Eric Larson1, Gabe Cohn1, Sidhant Gupta2, Xiaofeng Ren3, Beverly Harrison3, Dieter Fox2,3, Shwetak N. Patel1,2 1 3 Electrical Eng., 2Computer Science & Eng.
Intel Labs Seattle UbiComp Lab, DUB Group, Univ.
We present HeatWave, a system that uses digital thermal imaging cameras to detect, track, and support user interaction on arbitrary surfaces.
Thermal sensing has had limited examination in the HCI research community and is generally under-explored outside of law enforcement and energy auditing applications.
We examine the role of thermal imaging as a new sensing solution for enhancing user surface interaction.
In particular, we demonstrate how thermal imaging in combination with existing computer vision techniques can make segmentation and detection of routine interaction techniques possible in real-time, and can be used to complement or simplify algorithms for traditional RGB and depth cameras.
Example interactions include  distinguishing hovering above a surface from touch events,  shape-based gestures similar to ink strokes,  pressure based gestures, and  multi-finger gestures.
We close by discussing the practicality of thermal sensing for naturalistic user interaction and opportunities for future work.
This has also largely been due to the decreasing costs, versatility, size and portability of modern cameras.
Traditional  cameras have seen considerable use in the HCI community for detecting hand gestures, touch points, and object recognition .
The introduction of depth cameras or pixel-mixed devices  provides a mechanism for 3D reconstruction and depth segmentation for user interfaces .
However, the use of RGB and depth cameras in HCI is limited by the type of information that can be extracted from a scene, and the speed at which information can be extracted.
For instance, inaccuracies or gaps in gesture detection often result if hand motion is too fast .
Thermal imaging, which provides a pixel-level thermograph of anything that is in its field of view , has largely been under-explored in the user interface community.
Recent maturation and advances in solid-state imaging technology and embedded systems have made thermal imaging more practical for consumer use in terms of size, cost and software access to video data.
In this paper, we critically examine the role of thermal imaging as a new sensing solution for enhancing user surface interaction.
In particular, we demonstrate how thermal imaging and well known computer vision techniques can make segmenting and detecting certain routine interaction techniques possible in real-time and complement or simplify algorithms for traditional RGB and depth cameras.
Example interactions include  distinguishing surface touch or target selection from hovering over surface,  shape-
Human-computer interface design has significant interest in natural interaction-i.e., systems that do not rely upon mediated interaction through devices such as a mouse, keyboard, or stylus.
This has in part been reflected by the popularity in touch screens and surface-based systems .
In an attempt to avoid instrumentation on users, objects, and surfaces ,
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We demonstrate these in two prototype applications:  a pressure-aware drawing application that supports multi-touch and multi-user interactions,  a marking menu application using thermal traces as the "ink stroke" menu selection method .
Both examples demonstrate the feasibility of real-time thermal traces for UI design.
In the following sections, we briefly discuss the technique of thermal imaging, how it differs from standard IR-based cameras, and the advantages gained by using thermal imaging to complement traditional RGB and depth cameras for surface user interactions.
Next, we discuss the related work in surface interaction and thermal imaging, followed by details of our real-time computer vision approaches for extracting meaningful information from our thermal camera, and a collection of interaction techniques they support.
Finally, we propose some challenges and future applications of thermal imaging that extend beyond surface interaction.
Thermal imaging provides several distinct features that address some of the challenges faced by traditional RGB and depth cameras, and enables new applications which are difficult using traditional imaging technologies.
Additionally, we believe that thermal imaging can be combined with RGB and depth to provide more robust systems for surface interactions in a variety of natural settings.
First, images produced by thermal cameras are independent of illumination and are far less susceptible to changes in light intensity than traditional RGB cameras or many IRbased depth cameras.
RGB cameras do not work well in low-light scenarios, and obviously fail in complete darkness.
Since thermal sensing works independently of the visible light spectrum, it works equally well in low- and nolight situations as it would under normal indoor lighting.
Furthermore, thermal sensing works in direct sunlight, where some IR depth cameras do not work because their own IR illumination source is washed out by the sun.
Additionally, thermal sensing is not confounded by constantly changing light sources and therefore can be used with projected systems without any special considerations.
Second, thermal sensing can detect unique features including short-lived heat transfer from one object to another, which are undetectable with traditional RGB and depth cameras.
These features can be used to support a variety of user interaction techniques .
For example, using the heat transferred from a user's hand to the surface, multiple touch points can easily be extracted as well as complicated gesture shapes.
Moreover, the amount of heat transfer between the finger and the surface can indicate the pressure with which the user touched or grasped the object or surface.
Lastly, since the transferred heat dissipates over time, a history of a user's interactions is captured in the form of residual heat traces even after the interaction is done.
Third, thermal imaging provides a distinct mechanism for easily segmenting hands  from the background and is independent of scene complexity, colors, and textures .
Traditional RGB sensing and algorithms rely upon computer vision techniques such as background subtraction, color and texture matching, contour detection, and/or optical flow to find target objects of interest.
However, many of these features fluctuate due to changes in illumination, pose or position, and color .
These fluctuations often make it difficult to reliably segment a person from a scene, an object from the hand that grasps it, or gestures from a scene or surface.
Thermal imaging is a technique for passively constructing a high resolution heat map of objects appearing in a scene without using an external illumination source.
This is accomplished by measuring the quantity of far-infrared  radiation emitted by any object.
Plank's law describes that the wavelength of the peak of electromagnetic radiation from any object is inversely proportional to its absolute temperature.
Objects that we interact with on a daily basis are near room temperature and radiate mostly in the F-IR spectrum .
Furthermore, the quantity of thermal, or black-body radiation, emitted by an object is directly proportional to the fourth power of its absolute temperature, as given by the Stefan-Boltzmann law .
Therefore, by measuring the quantity of radiation emitted in the F-IR spectrum, a thermal sensor can produce a thermographic image of anything in its field of view .
It is important to note that thermal imaging differs from the more well-known "IR imaging" techniques used in the HCI community.
Infrared light detection and night vision devices use what is called reflected infrared and operate in the near-infrared  spectrum.
These approaches require an illumination source in order to reconstruct an image.
NIR is employed in some fairly recent interactive tabletop surfaces  and depth cameras.
Figure 2 shows the visible and infrared spectra and differentiates N-IR from FIR.
Note that we do not utilize N-IR in the present work.
Earlier sensors found in thermal imaging cameras employed a gas filled lens and required refrigeration sources.
Advances in semiconductor technology have enabled the development of arrays of silicon-based bandgap detectors and photo-resistive detectors, which allow for 2D imaging planes similar to traditional CCD cameras.
Thermal cameras are becoming popular for home energy auditing applications, which has created a demand for portable thermal cameras that continue to reduce in size and cost.
Since thermal sensing only needs a single frame for segmentation , pose and motion issues are less significant than in RGB or depth based systems.
The application deals primarily with interactive painting on a special paper, but does not investigate the problem of hand segmentation, tracking, or gesture recognition.
The work of Oka et al.
They use the trajectory of extracted finger tips as input to a hidden Markov model to identify one of six different gestures.
The method used does not detect finger contact with the surface, but instead makes use of the fingertip motion .
The authors note that their system lags when more than one hand is used, and that their methodology may not scale well beyond tracking a few finger tips.
Even so, they are able to use extracted fingertips and in-air gestures to drive an overhead projected user interface.
Such a system, combined with the system we propose, could provide a pervasive vocabulary for in-air and touch gesturing.
Beyond tracking fingertips, previous work in RGB and depth imaging has attempted to identify touch pressure .
With thermal imaging one can use not only the shape deformation of the finger but also the size and heat of the touch spot left behind to infer the pressure a user exerts on a surface .
We organize the related work into three broad sections:  surface and gesture interaction which uses instrumented surfaces, involving traditional, depth, and thermal cameras,  prior work using traditional IR sensing in the HCI community, and  the use of thermal imaging in other areas of research.
Camera-based gesture and touch detection in prior work can be roughly categorized by the type of camera sensor used: RGB, depth, and thermal.
To enable multi-touch interfaces using RGB cameras  there has been substantial work in image segmentation that tracks and identifies various body parts .
Typically these systems use skin color matching, edge or contour detection, and motion tracking to segment fingers and hands .
More recently, N-IR depth cameras have also been applied alone or in concert with RGB for gesture detection and tracking .
Using thermal imaging, some of the initial interaction work comes from Iwai and Sato .
They use a behind-thesurface thermal camera and rear projection for drawing upon a translucent paper surface.
Aside from depth cameras, IR sensing  is a popular technique for surface and gesture-based interaction .
The advantage of N-IR imaging is the ability to use commodity cameras as the sensor.
IR-based sensing typically requires an external illumination source, which dictates its range.
A number of projects in the HCI community have used IR for tabletop interaction by detecting hand gestures using an under mounted camera and illumination source .
Others have employed a similar approach on vertical semi-transparent surfaces .
IR sensing has also been used for fingertip detection and gaze tracking because the retro-reflective properties of these objects allow IR-filtered cameras to easily discern their appearance .
Others have used structured IR light pat-
As we have pointed out, IR imaging differs from thermal imaging; however, many of the computer vision techniques used in IR-based solutions parallel those that would be used in thermal imaging with minimal modification.
The added value of thermal imaging is its ability to passively discern objects in view without an illumination source in addition to the hysteretic information it offers.
Thermal imaging has largely been used for military and surveillance applications , where the heat signature produced by the human body is used to track individuals in arbitrary environments and conditions.
Thermal imaging has also been used in hospitals and border crossings to identify individuals with a fever within a crowd of people using face temperatures .
The miniaturization and decreasing costs of thermal imaging cameras have more recently enabled a number of civilian applications.
For example, in energy audits air filtration and insulation problems can be quickly identified by scanning the indoor space.
Home inspectors have also extended the use of thermal imaging to look for hot spots near the electrical infrastructure to uncover potential arcing problems or overloaded circuits.
Thermal scans of circuit boards can identify potential failure points from heat dissipation problems.
The automotive industry has also used thermal imaging for similar applications, especially when temperature analysis needs to be conducted at a safe distance.
Beyond its commercial use, researchers have looked at using thermal imaging for a number of affective computing applications, such as detecting and classifying anxiety based on the minute changes in the thermal signature of one's face .
Using well known techniques from the medical literature, changes in anxiety can be correlated to the blood flow on various parts of the face, such as at the forehead and cheeks.
Others have extended the use of thermal imaging to infer emotional states exhibited by individuals and have used that information to enhance a user's gaming experience by altering a game's difficulty based on these sensed parameters .
Thermal imaging has also been used for illumination-invariant facial recognition .
For our experimentation, we used the RazIR NANO, which contains an un-cooled Focal Plane Array  microbolometer sensor with 160x120 pixel resolution .
The thermal sensor is tuned for wavelengths in the IR spectrum between 8 and 14 m, and captures data at a maximum rate of 30 frames per second.
As an artifact of the sensor, the thermal values captured from an object of fixed temperature will drift slightly over time, and therefore a periodic recalibration is required.
We developed software to remove the effects of this drift in real-time, as described later.
Although the RazIR NANO thermal camera has an onscreen user interface and can perform some signal processing on-board, we have done all processing on an external computer using a data feed over USB.
The data collected from the feed represents the raw values from the camera's analog-to-digital converter.
Our algorithms operate in real-time on the 8 most significant bits of the raw data.
Thermal imaging has many potential applications when used independently or in conjunction with other sensors such as RGB and depth.
In this work, we have focused on developing software for one of the more interesting and unique aspect of thermal imaging: the detection and extraction of heat traces.
Heat traces are the residual heat left behind on a surface due to the heating of that surface by another warmer object, such as a human hand.
Since traditional RGB and depth cameras cannot see signals like heat traces, there has been no other work in developing software to extract such features.
This section describes our approach, using Open CV  on streaming thermal imaging data and demonstrates how these features can be robustly extracted in real-time using standard computer vision algorithms.
Figure 4 shows the step by step processing of the algorithms with callout images for each process.
There are currently a variety of thermal cameras commercially available, and their cost varies based on the required thermal sensitivity and resolution.
For instance, at the time of publication, thermal imaging cameras with super-cooled, sealed components that "see through walls" such as those used by law enforcement, cost just under 100,000 USD.
HVAC auditing and general purpose thermal cameras are currently around 5,000 USD-which was the price of depth cameras less than one year ago .
As thermal sensing also gains popularity, the capabilities of these devices will surely increase while the costs decrease.
The raw thermal images returned from our camera are fairly noisy from the thermal and scattering noise around the micro-bolometer sensors in the camera.
To suppress this noise we apply smoothing in both the spatial and temporal domains.
We first apply a 5 pixel by 5 pixel median filter within each video frame.
Then, for each pixel, we apply a 5 frame low-pass, finite impulse response  filter to smooth the signal in time.
In order to accurately detect heat traces, it is extremely important to model the background signal level-there is slight drift in the hardware sensor over time and surface temperatures may change over time.
For these reasons, we compute the mean background image dynamically  whenever we detect that a human hand is not present in the image.
A heat trace is created when an object warmer than the background surface heats the surface enough to leave evidence of its presence.
Over time, this heat trace will disappear and the surface returns to the background temperature.
Figure 4 shows smoothed data in which the background has been subtracted and identifies the region corresponding to the heat decay.
When the finger simply hovers over the surface, there is no heat transfer or decay, but when the finger touches the surface the heat decay is very distinctive.
We constrain our search space by "ANDing" together hand segmentations within the past one second of video  and subtracting the current hand segmentation.
In this way, we only look for heat traces in pixel locations where the hand has recently traveled.
This reduces the search space significantly, and thus drastically decreases computational complexity.
We frame the detection of heat traces as a Bayesian estimation problem.
In particular, we observe the likelihood of a pixel being part of a heat trace given three features: smoothed temperature, temporal derivative, and background subtracted temperature.
The temporally smoothed temperature and derivative are calculated over five frames using FIR filters.
This 5-frame buffering results in a system latency of 166 ms , which can be considered real-time for most interactive applications.
The likelihood distributions are assumed i.i.d.
That is, each feature is modeled well using a distribution with a single tail and the product of all these distributions is a good model of the overall posterior heat trace probability.
Prior distributions are assumed to be uniform.
This is ideal in thermal sensing because a hand's temperature is almost always distinct from the background, even when the background temperature is not uniform or has temporarily changed due to touching.
This type of thresholding is widely used in computer vision applications and is highly optimized.
For our image resolution, the operation takes a fraction of a millisecond.
An example of segmentation is shown in Figure 4.
We use the segmented image of the hand together with well known morphology operations to extract multiple fingertips in a scene.
We first apply a thinning operation to the binary segmented image, resulting in a skeletization of the fingers.
We then iteratively apply a hit-and-miss transform to the image with a rotating, 3x3 structuring element of endpoints at each possible angle.
The result tells us the endpoints of each finger.
The simplicity of this extraction technique is made possible by the robust segmentation that thermal sensing provides.
However, we found endpoint extraction to be efficient and robust, especially because we only use the extracted fingertips for refining the search space in which we look for heat traces.
This type of extraction is difficult using depth cameras because "thinning" is sensitive to the outline of the segmented object.
Each feature variance and mean threshold, f and f, are selected empirically using histograms of collected heat traces.
When the probability of a pixel being a heat trace, P, surpasses a global threshold, we classify the pixel as a heat trace.
We found this single model to work well for a variety of surfaces and users .
In addition, we allow the system to be calibrated and adapted to each user or surface.
Unlike the Bayesian system, the calibrated system attempts to classify heat traces into more than one class based on the pressure with which the user pressed on the surface .
The calibration process consists of three stages.
During each stage the user "draws" a line on the surface at increasing pressures.
We save features  around the trajectory of the moving fingertip using the Bayesian estimate to extract pixels that are part of a heat trace.
In this way, the Bayesian estimate is used to bootstrap the training of a more complicated classifier.
This is done for each pressure the user wishes to calibrate.
For example, these steps can be repeated for "light", "moderate", and "hard" pressure stroke calibration.
We then train the system to identify the pressure of each heat trace using a C4.5 tree classifier , as implemented by Weka  .
We found a number of algorithms implemented in Weka to perform well on a test set of 20,000 detected heat trace pixels.
We chose the tree classifier because it provided 96% accuracy on a 10-fold cross validation of the test set and runs quickly enough to assess pressure in real-time.
The test set had five classes: High, medium, and low pressure, hand and background.
In addition to pressure, this method can be used to calibrate to different surfaces.
To test this hypothesis, we calibrated to four different materials and asked a user to trace a fixed projected "S" eight times on each of the four materials .
After the experiment, we superimposed the detected traces on top of each other for each material, resulting in a set of images where brightness denotes how often a heat trace was detected on the material surface.
Drawing speed, temperature of the finger, hand and surface were not controlled .
The superimposed image results are shown in Figure 5.
Brightness denotes classification accuracy: bright yellow is 100% correct and completely black  is 0% detection.
There were no false positives detected.
Notice that paper , table top laminate , and wooden surfaces have easily detectable traces, and that plastic is the most difficult surface to detect traces upon.
Based on this initial evaluation, we hypothesize that less heat is transferred to the plastic than other materials.
To test the usefulness of thermal features as driving input for user interfaces, we built two prototype applications.
Each application uses an overhead projection system and an overhead-mounted thermal camera to transform an arbitrary surface into a multi-touch user interface .
The system ports easily to different tables and other flat surfaces.
Conversion from camera coordinates and projected coordinates is achieved using a four point calibrated homographic transform .
No other instrumentation is necessary and the camera and projector can be placed at many different orientations to the surface and each other.
The first application is a multi-user and multi-touch drawing application that displays arbitrary gestures made by the users, and alters the brightness of displayed colors based on the pressure with which each user draws using three pressure levels.
The second application uses line gestures for image manipulation.
Images are chosen using marking menus , then once the images are displayed they can be translated, rotated, and scaled using thermal lines.
The two applications are designed to demonstrate that thermal traces can be used as a plausible substitute for multi-touch screens and can drive typical user interfaces in real-time with naturalistic interactions.
Images from interactions with each application can be seen in Figure 6.
Although heat trace detection can be used to detect arbitrary shapes and gesture patterns, it is useful to detect when heat traces are collinear.
We focus on lines because detected lines can be used for chording style gestures and as input to many applications such as marking menus.
To detect line gestures we buffer detected heat traces into a single image for the past one second.
We then apply a binary Hough transform  to the buffered image to reveal heat traces that are collinear.
The buffered image provides a binary history of where a user has placed ink strokes with their hands and can be used with other transformations to fit arbitrary curves and circles, not just lines.
For the drawing application no additional feature processing is necessary--it uses the unaltered heat trace locations and their corresponding pressure as the sole driving inputs.
The touch positions are projected onto the surface, with their brightness representing applied pressure.
The marking menu and image editing application uses the extracted fingertip locations and heat traces  as driving input.
The steadiness of the fingertip is used to detect finger-down events.
When a detected fingertip has been stationary for 500 ms, a marking menu is displayed at the fingertip location.
All of these algorithms could be tuned to work quite well in specific situations, but none of them were able to work well over a wide range of scenarios .
The Bayesian approach described in this paper appears to be highly robust, and works well for all of the scenarios that we have tested.
In addition, the approach can be used to as a bootstrap for more complicated classifiers like the C4.5 trees implemented in this paper.
The angle of the detected lines controls which sub-menu is displayed and the subsequent selections.
As with any marking menu, the user can draw a line and pause to bring up another menu or, alternatively, can draw multiple lines in a single motion to navigate several menus at once.
Once the user selects an image to open, they can move and scale the image using a combination of one, two, and three finger gestures.
The interactions are: a single line drawn on the image translates the image; two fingers moving inward or outward from the image scales it; and three fingers in a sweeping motion dismisses the image from the surface.
We found these inputs and extraction methods to be easily implemented and the applications were straightforward additions to the system.
The prototype applications provide proof of concept for systems that detect and use arbitrary gestures--using nothing more than a binary Hough transforms on detected heat traces and simple fingertip tracking.
During our examination of thermal imaging, we determined that the residual heat traces after touching a surface provide significant value for differentiating between hovering and touching, which is clearly a challenge for top mounted RGB cameras and can be problematic with depth cameras .
However, two factors impact the decay rate and hence ease of detection for heat traces: material type and dwell time.
Decay rate varies greatly depending upon the surface material  but can be addressed using the calibration sequence presented.
Wooden and drywall surfaces exhibit the slowest decay rates due to their thermal properties and, thus, are the easiest materials to classify heat traces upon.
On the other hand, metal surfaces exhibit the fastest thermal dissipation, with traces typically disappearing after only a few frames.
This confounds our algorithm in many scenarios and we still consider metal surface heat trace extraction to be an open problem.
Additionally, the dwell time  impacts the amount of heat transfer and the size of the heated region .
For many gesture based interactions, dwell time is not significant; however, one can easily imagine gestural vocabularies or situations where this would need to be taken into account.
Our interaction techniques thus far have been fairly independent of dwell time - that is, most gestures only require the user to press the surface quickly.
However, extracting reliable pressure estimates becomes more difficult with gestures that have a large variance in dwell time.
Although thermal imaging provides a number of advantages over traditional RGB and depth imaging solutions, there are clearly some differences and challenges.
Many of the challenges with thermal imaging, such as reflection and occlusions have analogous problems in RGB and depth imaging, but there are also challenges that are unique to thermal imaging, such as surface temperature and material type.
Additionally, applications driven by heat traces must be designed to minimize the effects of these limitations, but still provide an intuituve interface.
Unlike RGB and depth cameras, the computer vision algorithms for thermal imaging must account for residual heat traces that may linger for a significant amount of time.
Therefore, continually computing/updating the background model is important to avoid false positives.
Similarly, it is possible for the surface to heat up due to the interaction and thus make it harder to segment out the hand if the surface temperature nears that of the hand, but we found this to be extremely rare in our experimentation where the surface was at about 20 C. Note that maintaining a background model is only important for detecting heat traces.
Similar to RGB and depth, occlusion is an issue for thermal imaging.
Since our system relies on the heat traces left by finger contact, these traces are only visible when the finger moves away from the contact points .
An angle mounted camera helps alleviate problems with occlusions when the hand covers the heat traces.
Furthermore, there is an unavoidable delay in the detection of lift  events, and it is not possible to detect touch-down  events.
Note that this is distinct from the algorithm processing lag of 166 ms. An interesting solution to extracting touch-down and touchup events with an overhead system is to use a metallic surface.
Metallic surfaces tend to reflect F-IR waves .
These reflections could be used to indicate hover distance or indicate when a surface is touched .
There is a tradeoff, of course, between the reliability of extracting heat traces from polished surfaces and touch-down extraction because the reflection may be directly over the heat trace.
This tradeoff is in contrast to depth cameras, which are known to have significant problems with shiny or reflective surfaces.
Our marking menu application was ideally suited for thermal line input and we were able to drive menus with selections as narrow as 15o fairly easily.
Moreover, translation and scaling of images is easily interpreted from the detected lines in near real-time.
The longest delay comes from line detection because our algorithm requires buffering of detected heat traces.
The buffering process adds an additional 200 ms delay onto the delay in heat trace detection, resulting in an average delay of about 400 ms between drawing a line and reaction by the interface.
This delay was insignificant for our example application but could be limiting for applications that require faster driving inputs, such as gaming.
For image manipulation, one limitation is when the user tries to move an image towards the camera because the hand moves directly over the heat trace path.
One outcome of this is that interface feedback may appear "jerky" and delayed since we can only process segments of the heat trace that are unoccluded.
We have anecdotally noticed that drawing with an index finger alone tends to naturally offset the hand orientation and thus the heat trace is less often occluded.
More occlusion seems to occur with multi-finger chording-style manipulations.
More investigation is required but this does suggest that gesture design choices can be optimized to reduce potential problems while preserving reasonably intuitive interaction.
In our ongoing tests of multi-touch thermal drawing interfaces, it is rare that parts of the hand are not segmented correctly or heat traces are not detected.
One exception occurs when someone with very cold hands uses the system.
For instance, someone who was holding a cold drink moments before interacting.
When this occurs, the finger tips are about the same temperature as the table top and segmentation severs a portion of the finger tip.
Because we use segmentation to constrain the search space for heat traces, some heat traces are missed.
Also, after long periods of being in contact with the table top  the finger tips begin to cool down and the table top begins to heat up, potentially confounding heat trace identification.
The heating of the surface is magnified when many users interact.
We found that letting the surface cool for about 10-20 seconds is sufficient to reset the background model and to let the fingertips return to natural temperature .
This suggests for applications that require sustained and continuous finger strokes from the user  that thermal imaging alone may not be appropriate as an input method.
Lastly, we found occlusion was not a factor for drawing applications because users almost always pull their hands away from the surface after marking a line or curve to see the visual representation, revealing the heat trace.
In general, varying lighting conditions did not pose any problems in extracting gestures.
For instance, we observed similar system algorithm behavior across indoor, outdoor, and dark environments.
This is encouraging since thermal imaging may be a viable option for use in arbitrary environments.
Even more encouraging is that images projected on a surface with a digital projector also do not interfere with hand/finger segmentation.
This is because the IR emitted from the projector's light source is in the N-IR band, and does not extend to the F-IR spectrum.
This paper describes our initial exploration of thermal imaging as a means for detecting gestural input on surfaces to support interaction.
We have additionally started investigating a wider range of possible applications, some of which are briefly described below.
At present, we have collected data to illustrate the viability of each of these ideas.
We have found that multiple users have different thermal gradients on their hands .
Although these gradients can change over long time intervals , we believe the overall hand "heat signature" may remain unchanged for the duration of a session.
Using these thermal differences among the hands of multiple users, we believe that we can uniquely identify several users within an interactive session.
This would allow customized multi-user interaction on arbitrary surfaces without the need for instrumenting the surface or the users.
In addition, this algorithm would not use the angle of approach, enabling users to move freely around the surface.
We imagine a system in which the user would simply touch the surface  and the system would detect and measure the spatial and temporal properties of the surface.
Using a database of such parameters, the surface material could potentially be determined.
This would provide mobile surface interaction systems with additional context regarding which surface the user is interacting with.
The concept of heat trace detection also has interesting security ramifications.
For instance, thermal heat trace detection can be used to view a password or bank PIN that a user types on a keyboard or keypad.
A residual heat trace is left behind on the keys even after the password has been entered.
Figure 7d shows the heat traces on the keys of an actual bank ATM.
Colleagues in our lab are conducting research in personal robotics where a roving robot with a robotic arm can pick up, move or manipulate objects.
A key element of this work is to understand how the robot can best grip everyday objects.
Thermal imaging could help train robots about grip methods by identifying the exact positions of contact between an object and a human hand.
The human hand leaves heat traces on objects, and these traces clearly show all of the points of contact between the object and the hand .
We plan to apply the thermal image grip patterns as a means of training robots how to grasp arbitrary objects.
Similarly to the grip patterns discussed above, thermal imaging can be used to determine the points of contact between objects and any part of the human body.
Figure 7c shows the heat pattern left in a chair as a result of a person sitting.
This information can be used to determine elements of posture and provide feedback whenever a user stands up.
Additionally, points of contact could be used to determine which chairs, household areas, and objects are used most within a space or even when these are used .
We have described how thermal imaging technology can complement or augment more traditional RGB or depth cameras for surface gesture interaction.
Thermal imaging is more robust under circumstances where RGB or depth would fail and thus could provide more robust solutions for the variability that occurs in natural settings.
We have demonstrated that well-known computer vision techniques can provide good models for extracting the heat traces that human interaction with surfaces  leaves behind in real-time.
We also demonstrated this approach on a variety of different surfaces and offered a technique for surface calibration.
We have demonstrated several traditional user interface techniques can be driven in real-time based on thermal heat trace input.
Finally, we have outlined a number of interesting new opportunities beyond gesturebased surface interactions where thermal imaging provides unique data to enable new applications.
Since thermal imaging makes human bodies and body parts easily distinguishable from scene elements, we can easily detect and track airborne gestures and movements.
We plan to continue this work, developing real-time algorithms to robustly segment and track people and airborne gestures on non-planar surfaces, arbitrary objects, and in scenes where the user stands in front of the camera.
Our current implementation is designed to work on surfaces that are made from a variety of materials.
We believe that thermal imaging can be used to identify the surface material based solely on its thermal properties.
For example, each material will dissipate heat at a different rate, and the spatial spread of the initial heating point will also be different.
Felzenszwalb, P., McAllester, D., and Ramanan, D. A Discriminatively Trained, Multiscale, Deformable Part Model.
Hilliges, O., Izadi, S., Wilson, A.D., Hodges, S., GarciaMendoza, A., Butz, A. Interactions in the air: adding further depth to interactive tabletops.
Iwai, D. and Sato, K. "Heat Sensation in Image Creation with Thermal Vision", ACM SIGCHI International Conference on Advances in Computer Entertainment Technology , pp.213-216, Jun.
Going beyond the display: a surface technology with an electronically switchable diffuser.
Kane, S., Avrahami, D., Wobbrock, J.O., Harrison, B., Rea, A.D., Philipose, M., LaMarca, A. Bonfire: a nomadic systems for hybrid laptop-tabletop interaction.
Automated Facial Expression Classification and affect interpretation using infrared measurement of facial skin temperature variations.
Khan, M.M., Ward, R.D., and Ingleby, M. Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature.
Kittel, C. and Kroemer, H. Thermal Physics.
Multiscale Fusion of Visible and Thermal IR Images for Illumination-Invariant Face Recognition.
International Journal of Computer Vision, 2007.
Kurtenbach, G. The Design and Evaluation of Marking Menus, Ph.D. Thesis, Department of Computer Science, University of Toronto.
Lee, J., Hudson, S., Summet, J., and Dietz, P. Moveable interactive projected displays using projector based tracking.
Letessier, J. and Berard, F. Visual tracking of bare fingers for interactive surfaces.
Manresa, C., Varona, J., Mas, R. and Perales, F. Hand tracking and gesture recognition for humancomputer interaction.
Electronic Letters on Computer Vision and Image Analysis, 5 , 2005, pp.
IEEE Computer Graphics and Applications.
Otsu, N, "A threshold selection method from gray-level histograms".
Computer Vision and Image Understanding, 2007.
Puri, C., Olson, L., Pavlidis, I., Levine, J., and Starren, J. StressCam: non-contact measurement of users' emotional states through thermal imaging.
Quinlan, J. R. C4.5: Programs for Machine Learning.
Rekimoto, J. SmartSkin: an infrastructure for freehand manipulation on interactive surfaces.
Saletan, W. Heat Check: Swine flu, body heat, and airport scanners.
Sato, Y., Kobayashi, Y., and Koike, H. "Fast tracking of hands and fingertips in infrared images for augmented desk interface," Proc.
2000 IEEE International Conference on Automatic Face and Gesture Recognition , pp.
Tanriverdi, V. and Jacob R.J.K.
Interacting with eye movements in virtual environments.
Wellner, P. Interacting with paper on the DigitalDesk.
Wilson, A. D. PlayAnywhere: a compact interactive tabletop projection-vision system.
Witten, I. and Frank, E.: "Data Mining: Practical machine learning tools and techniques", 2nd Edition, Morgan Kaufmann, San Francisco .
An Effective Surveillance System Using Thermal Camera.
Wu, M. and Balakrishnan, R. Multi-finger and whole hand gestural interaction techniques for multiuser tabletop displays.
Yun, C., Shastri, D., Pavlidis, I., and Deng, Z. O' game, can you feel my frustration?
