Determining who is interacting with a multi-user interactive touch display is challenging.
We describe a technique for associating multi-touch interactions to individual users and their accelerometer-equipped mobile devices.
Real-time device accelerometer data and depth camera-based body tracking are compared to associate each phone with a particular user, while body tracking and touch contacts positions are compared to associate a touch contact with a specific user.
It is then possible to associate touch contacts with devices, allowing for more seamless device-display multi-user interactions.
We detail the technique and present a user study to validate and demonstrate a content exchange application using this approach.
We present ShakeID, a technique for associating a specific user's touch contacts on an interactive display to a mobile device held by the user.
It exploits the combination of the phone's on-board sensors and touch screen sensing to perform this association.
Previous work has explored several different methods for uniquely identifying multiple users sharing an interactive display.
The DiamondTouch table  identifies four unique users by capacitively coupling touch inputs through users to receivers in the environment.
The territory-based approach  divides the surface into multiple territories, each assigned to a single user.
The Medusa table  uses 138 proximity sensors to map touch points to specific users.
On mobile phones, PhoneTouch  detects "bump" events from the phone accelerometer and touch is used to perform user identification.
ShakeID differs from these techniques in that it only requires the user to hold the phone while touching the display, rather than bringing the phone in physical contact with the display.
By using Kinect tracking data, our method also has the potential to provide additional capabilities, e.g.
Assuming each user is holding a smartphone or other portable device that can sense its own movement, ShakeID matches the motion sensed by the device to motion observed by a Microsoft Kinect camera pointed at the users standing in front of the touch display .
By comparing the motion of each phone in the scene with the motion of each user, the system can associate each phone to a specific user's hand.
Increasingly, interactive multi-touch displays are large enough to support multiple simultaneous users, either working individually or collaboratively.
In many multi-user scenarios it may be desirable to accurately associate touch input with particular users.
Identifying which user is touching an interactive surface in a collaborative setting enables personalization, access control, and score-keeping.
Such capabilities could enable useful shared interactive displays for walk-up use in conference rooms and office hallways, for example.
Other interesting applications for interactive displays incorporate smaller devices such as mobile phones.
Such devices are personal and private, and so complement a larger display.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
ShakeID first associates each phone with a particular user's left or right hand and then similarly associates touch contacts on the touch screen with the multiple users' hands.
The combination of these two steps allows touches to be associated with phones and users.
Below we describe each of the steps in detail:
Touches are thus associated to users and users are associated with devices they hold.
For example, if two users touch a display simultaneously in different locations to grab content, ShakeID can associate each touch to a specific user and transfer the correct content to each user's personal device.
As far as we know this is the first attempt to fuse Kinect, mobile device inertial sensing, and multi-touch interactive displays.
Our contributions are the ShakeID method for user association and an initial user study that applies the technique to enable sharing and content exchange between phones and a touch display.
The study shows that ShakeID is easily learned and requires minimal feedback.
In the remainder of the paper we describe how ShakeID works and then present the user study.
To associate the phone with a user's hand, we continuously correlate phone acceleration for each phone connected to the system with the accelerations of all hands tracked by the Kinect.
Data captured from the 3-axis accelerometer in the phone is sent wirelessly to the display system continuously.
Meanwhile, a Kalman filter is used to estimate acceleration of hand position over time.
The matching algorithm is as follows: For every phone !, and observed hand , 3-axis accelerometer data  !!
However, accelerometer and hand accelerations cannot be directly compared because the orientation of the phone is unknown, and the phone's accelerometers include acceleration due to gravity.
We address both problems by searching the space of all possible phone orientations by generating a uniformly distributed set of points on the unit sphere.
ShakeID uses a two-step process.
The first step associates personal "private" smartphones to users holding them, while the users interact with the single shared "public" display.
The second step associates touches on the shared display to users who performed those touches.
We assume that the smartphones have been previously paired to the system and focus on identifying the device - there are many existing ways of establishing this pairing .
Figure 1 shows the arrangement of the system.
We implemented ShakeID using the Microsoft Kinect for Windows SDK to track the hands of multiple users, the Microsoft Surface 2.0 SDK for the multi-touch display and two Windows Phone smartphones.
We placed the Kinect sensor within 1.5m distance of a vertical touch display.
Physical layout is critical, since the Kinect camera has a limited 0.8-4 meter range.
The Kinect SDK provides a skeletal tracking capability which can be used to track the left and right hands of two simultaneous users in the view of the Kinect camera.
The Kinect was positioned so that it could capture valid skeleton data and see the users' hands the entire time users interacted with the shared display.
In the second step, the algorithm associates touch contacts on the display to users' hand positions.
Furthermore, if users understand that the system uses hand motion to perform user association, they may initiate a simple, natural motion such as shaking, to trigger an association.
In any case, the nature of the feedback provided to users when association changes will be important, so users understand and are confident of the associations being inferred by the system.
Another limitation arises when people are standing too close for the Kinect to correctly compute skeleton data.
This situation is less likely for large displays suitable for multiple users.
Because the Kinect SDK currently only provides active skeletal tracking with joint data for up to two people, the current implementation is limited to two simultaneous users.
Our approach should work if more users can be tracked simultaneously, although accuracy could decrease.
User study content exchange display by tapping the desired location on the display.
To copy from display to phone this process is reversed.
To clearly indicate when the system fails to associate the correct device with the user, each phone is associated with a particular color.
As Figure 3 shows, white shapes on the shared display are colored by copying shapes from the phone to the display: white shapes on the display copied to the phone become the phone's color.
We randomly selected which phone we gave to each user and the system determined which one the user was holding.
Parallel Use: Participants worked side-by-side, representing scenarios in which individuals work in parallel around a shared display.
Each participant conducted 20 PhoneToDisplay copies and 20 DisplayToPhone copies.
To simulate movement that might happen during collaboration in real-life scenarios, participants were asked to switch sides halfway through the task.
Collaborative Use: To simulate collaborative actions where participants share and discuss content through a shared display, participants copied shapes between them .
They repeated this for 12 shapes, with each participant originating the sharing 6 times.
As before participants switched sides halfway through the task.
During the study we recorded several parameters including instances when the association was incorrect  and the length of time between a tap on the display and phone.
We measured tap delays to show overall system response time and to capture the use of different interaction techniques, e.g.
To evaluate ShakeID, we implemented a system for content sharing between phones and devices using ShakeID.
We conducted a user study with 7 pairs of participants .
Each study consisted of two people performing a set of content sharing tasks.
Participants were members of our organization with no prior knowledge of ShakeID.
Each received a $10 incentive.
Based on our experience piloting the study, we added visual feedback indicating the position of phones at all times .
During each session the participants completed the following tasks: Training: Standing alone, each participant copied 20 shapes from their phone to the matching empty spot on the display  and then copied 20 shapes from the display to their phone .
To copy a shape from phone to display, the participant tapped the shape on the phone.
During the Parallel and Collaborative tasks, 94% and 92% accuracies were observed, respectively.
These errors primarily occurred when the hand holding the phone hand moved out of the field of the view of the Kinect and so accurate position data for the hand was not available.
If an incorrect association was made, participants often shook or waved the phone to re-associate.
Average delay between two taps among all users was 0.80 seconds.
Ten participants used one hand to tap the two devices, while 4 participants used two hands simultaneously resulting in almost instantaneous actions of 0.16 seconds on average.
After the study, we surveyed participants' mental and physical demand using the NASA TLX questions.
On a 7point scale with 1 = "very low" and 7= "very high" participants reported low mental demand , low physical demand , and that ShakeID was easy to learn  and use .
Initially we hoped that ShakeID could successfully associate input to users unaware of how it worked.
We conducted pilot studies with a "walk-up" condition where users performed the tasks with no knowledge that the association relied on movement of the phone with 8 people.
We found that for some users it worked well since they naturally gestured with their hand holding the phone , while others  did not move the hand holding the phone while approaching the system.
ShakeID cross-correlates acceleration data from smartphones that people carry together with hand acceleration captured through Kinect to perform user identification.
To validate the accuracy of this approach we conducted a 14 person user study and showed accuracy rates of 92% and higher.
We anticipate a number of extensions to our basic approach for future work.
While our current approach matches acceleration data directly, it may be beneficial to instead match on features derived from the acceleration.
Matching with orientation invariant features such as points of maxima would remove the need to search over all orientations.
This search would also be required less frequently if the mobile device sensors allow for the calculation of absolute orientation, such as those that include 3-axis magnetometers and gyros.
This will likely improve the reliability of the matching process.
Our present work matches the phone to the users' hands only.
But the technique may work when matching to body parts other than hands.
For example, it may be possible to successfully associate a device to a user if it is in the user's pants pocket by matching to users' hip joints data from the Kinect SDK.
There may be additional ways to take advantage of the device sensors.
For example, the orientation recovered by the matching process, combined with frame to frame sensor updates, can be used to provide fast and accurate hand orientation information that is not provided by the Kinect SDK.
Furthermore, low-latency, high frame rate hand position data could be derived by combining the body tracking position data with the  device acceleration data using a Kalman filter.
Our current implementation does not address situations where a malicious user might imitate another user's motion to gain control of their device.
Determining how easy this is in practice and designing mechanisms to prevent this is important.
For example, assigning more weight to the skeleton which holds the phone first or most may mitigate this concern.
Maunder, A. J., Marsden, G., and Harper, R. SnapAndGrab: accessing and sharing contextual multimedia content using Bluetooth enabled camera phones and large situated displays.
Hutama, H., Song, P., Fu, C. and Goh, W. B. Distinguishing multiple smart-phone interactions on a multi-touch wall display using tilt correlation.
Morris, M. R., Fisher, D., and Wigdor, D. Search on surfaces: Exploring the potential of interactive tabletops for collaborative search tasks.
Schmidt, D., Chehimi, F., Rukzio, E., and Gellersen, H., PhoneTouch: a technique for direct phone interaction on surfaces, Proc.UIST2010.
Enhancing multi-user interaction with multi-touch tabletop displays using hand tracking.
Dietz, P.H., and Leigh, D. DiamondTouch: A multi-user touch technology.
Scott, S. D., Carpendale, M. S. T., and Inkpen, K. M. Territoriality in collaborative tabletop workspaces.
Rukzio, E., Schmidt, A., and Hussmann, H. An analysis of the usage of mobile phones for personalized interactions with ubiquitous public displays.
Workshop on Ubiquitous Display Environments in conjunction with UbiComp 2004, Nottingham, UK, 2004.
Grimes, A., Tarasewich, P., and Campbell, C. Keeping information private in the mobile environment.
Position paper presented at First Intl.
Workshop on Social Implications of Ubiquitous Computing at CHI 2005.
Using Mobile Phones to Spontaneously Authenticate and Interact with MultiTouch Surfaces.
AVI: Workshop on designing multitouch interaction techniques for coupled private and public displays PPD, 2008.
Seewoonauth, K., Rukzio, E. , Hardy, R., Holleis, P. Touch & connect and touch & select: interacting with a computer by touching it with a mobile phone, Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services, September 15-18, 2009.
Michelle Annett, Tovi Grossman, Daniel Wigdor, and George Fitzmaurice.
Medusa: a proximity-aware multi-touch tabletop.
