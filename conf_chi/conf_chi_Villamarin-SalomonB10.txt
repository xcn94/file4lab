Users have a strong tendency toward dismissing security dialogs unthinkingly.
Prior research has shown that users' responses to security dialogs become significantly more thoughtful when dialogs are polymorphic, and that further improvements can be obtained when dialogs are also audited and auditors penalize users who give unreasonable responses.
We contribute an Operant Conditioning model that fits these observations, and, inspired by the model, propose Security-Reinforcing Applications .
SRAs seek to reward users' secure behavior, instead of penalizing insecure behavior.
User studies show that SRAs improve users' secure behaviors and that behaviors strengthened in this way do not extinguish after a period of several weeks in which users do not interact with SRAs.
Moreover, inspired by Social Learning theory, we propose Vicarious Security Reinforcement .
A user study shows that VSR accelerates SRA benefits.
This paper advances the notion that this problem is also a behavioral one.
Users acquire the habit of ignoring security dialogs partly because they find more rewarding to do so.
Like other undesirable habits, learning that this habit may be harmful, and even conceptually understanding why it may be harmful, can be insufficient to quit it.
If ignoring security dialogs is at least partly a behavioral problem, then it ought to respond to behavioral interventions.
We show that this is indeed true.
In particular, we introduce Security-Reinforcing Applications , a novel class of applications that can reliably deliver rewards when users accept justified risks or reject unjustified ones.
Deploying SRAs, system administrators can manipulate users' reward matrix such that users find more advantageous to heed security dialogs and make more prudent decisions.
We also contribute Vicarious Security Reinforcement , a form of training that is well suited for SRAs.
SRA and VSR are intended for social contexts  where some individuals  are tasked with supervising and positively affecting the behavior of others.
We report user studies that show that SRAs are effective and continue to be effective even after users have not interacted with them for more than a month, and that VSR significantly accelerates learning of desired security behaviors in SRA users.
The next section provides an overview of psychological theories inspiring SRAs and VSR, respectively Operant Conditioning and Social Learning.
We then argue why users may find it more rewarding to ignore security dialogs, and reinterpret behaviorally two previous interventions, polymorphic and audited dialogs .
Next, we describe the techniques we designed based on aforementioned theories.
Afterward, we present our methodology for evaluating these techniques, and experimental results.
We then discuss related work and present our conclusions.
Human Factors, Security The designer of security dialogs faces a difficult problem: users tend to ignore such dialogs and accept risks imprudently.
Earlier security warnings often used language users didn't understand and delegated to users decisions they were ill-prepared to make.
Researchers demonstrated that dialogs that instead disclose threats in plain language and strongly suggest a preferred course of action can lead to significantly more prudent user decisions .
Dialogs in recent versions of applications such as Internet Explorer and Firefox have been accordingly modified .
Researchers have also found that user training before application use, possibly employing games , or training embedded in the application itself, especially in the form of cartoons , can also help users make more prudent security decisions.
A behavior's strength is measured by how often it's emitted .
Consequences that strengthen a behavior are called reinforcers .
Reinforcement is called positive or negative, respectively when it presents something pleasing or withdraws something displeasing to the individual.
Consequences that weaken a behavior are called punishments.
They present something displeasing or withdraw something pleasing to the individual.
Stimuli present in the environment only immediately before behaviors that are reinforced are called antecedents .
Antecedents cue such behaviors , making them more likely to occur.
Behaviors can be weakened by removing from the environment the respective antecedents, instead of or in addition to punishing them.
Behaviors can alternatively be weakened by extinction, i.e., ignoring them and making sure they are not reinforced.
The schedule of reinforcement has large impact on how resistant a behavior is to extinction .
Continuous reinforcement occurs every time the individual exhibits the desired behavior.
It is appropriate only for new or infrequent behaviors.
Continuous reinforcement of high-frequency behaviors leads to early satiation and quick extinction when reinforcement is absent.
On the contrary, intermittent reinforcement occurs only some of the times the individual exhibits the desired behavior .
It can preclude satiation in high-frequency or stable behaviors.
An intermittent schedule is called fixed rate, variable rate, fixed interval, or variable interval, respectively, if the desired behavior must be observed a fixed or variable number of times or a fixed or variable time interval must pass before the desired behavior is reinforced.
Behaviors reinforced according to variable rate or variable interval schedules tend to be particularly resistant to extinction.
Social Learning  theory extends OC by noting that individuals can also acquire and maintain behaviors by observing their consequences in others .
This is known as observational learning , modeling, or vicarious learning.
OL can be quicker or less costly than an equivalent personal experience.
A modeling intervention may conclude with reinforcement of the model if the behavior is desired.
This process is called vicarious reinforcement.
OL is governed by four sub-processes: attention, retention, reproduction and motivation .
These are further discussed in section "Vicarious Security Reinforcement."
If a risk that is object of the dialog materializes, security is compromised.
However, often security breaches are not immediately apparent, or causal links between them and the user's dismissal of a security dialog are unclear.
In any case, users rarely are held accountable for security breaches.
Thus, security breaches are usually ineffective as punishments for ignoring security dialogs.
On the contrary, when a user heeds a security dialog and abandons his goal, he gets no reinforcement for his decision.
OC predicts that lack of reinforcement tends to extinguish the user's behavior of heeding security dialogs.
Worse, in some cases the user may be punished for abandoning his goal.
A net result of these perverse incentives is that users learn to ignore security dialogs.
In previous research , we proposed two techniques to improve users' behaviors.
First, we showed that users make significantly more prudent security decisions when presented with polymorphic instead of fixed dialogs.
Polymorphic dialogs have intentionally variable form to make it harder for users to dismiss them.
This result can be explained by OC.
Fixed dialogs can be interpreted as antecedents of users' dismissive behaviors.
Polymorphic dialogs weaken those behaviors by removing their antecedents.
Second, we showed that users make even more prudent security decisions when logs of users' responses to security dialogs are available to auditors, and the latter suspend or fine users who respond inappropriately.
However, suspensions or fines can leave users confused or upset.
These results can also be explained by OC.
Audited dialogs weaken the behavior of dismissing security dialogs by enabling reliable detection of such behavior and delivery of consequent punishments .
However, punishments do not per se teach the desired behaviors.
Without other interventions to achieve the latter, many individuals find punishment unfair.
Interventions with better effectiveness and user acceptance than those of audited dialogs would be highly desirable.
SL and OC suggest that modeling and reinforcing desired behaviors and extinguishing undesired behaviors, in preference to punishing the latter, could achieve desired effects.
Embedding such interventions in computer applications enables immediate feedback, maximizing intervention effectiveness.
We accordingly designed SRAs and VSR.
This section discusses from a behavioral viewpoint why users ignore security dialogs, and the effectiveness of two previous interventions, polymorphic and audited dialogs.
Many users ignore security dialogs because they find it more rewarding to do so.
Users typically view an application as a tool used to achieve some goal.
Securely using the application rarely is a conscious or high-priority part of that goal.
When a user dismisses a security dialog and accomplishes his goal, the latter accomplishment usually rein-
A security-reinforcing application  is a computer application that can reinforce its users' secure behaviors .
An organization can initiate such reinforcement manually or automatically.
In the former case, special entities  possess the privilege of instructing the application to apply reinforcement.
For instance, an SRA may be configured to reward employees automatically each time they reject three risks flagged as unjustified.
We define secure behavior as either rejection of unjustified risks or acceptance of justified risks.
Insecure behaviors are defined as acceptance of unjustified risks  and rejection of justified risks .
A security risk is justified if its acceptance is allowed by a security policy of the organization that a user is member of.
In this paper, the policy we use is that a risk may be accepted only if  it is necessary to do a user's primary tasks,  there are no other, less risky, alternatives to accomplish such tasks, and  there are no available means to mitigate the risk.
Otherwise, accepting a security risk is unjustified.
For example, in the case of email, a UR may be an email message containing an attachment that is unexpected, from an unknown sender, unnecessary to the user's job-related tasks, or of a type that may spread infections .
In this case, the user may mitigate the risk by, e.g., asking the sender to retransmit the attachment in a less risky file format .
A JR may be represented by an email that  the user was expecting and contains an attachment useful to complete a work-related task, or  was sent by a known member of the user's organization, with wording not appearing out of character for such sender, and explaining clearly why the recipient needs the attachment for her work.
Existing computer applications typically are not SRAs.
However, SRAs could be advantageous in a wide variety of domains.
In the case of email, for instance, companies could designate a security auditor who may send employees email messages intentionally including JRs or URs.
The auditor would disguise her messages to look like other email messages.
The auditor would instruct the SRA to reward the user for rejecting URs and accepting JRs, according to a reinforcement schedule.
Security auditors would include in these messages a special email header line that they would sign with a private or secret key that attackers cannot obtain.
The SRA verifies the auditor's header line using the corresponding public or secret key, and hides it from users.
By selectively rewarding the employees' secure behaviors, the auditor can increase the likelihood of secure behaviors, as predicted by OC.
More specifically, our hypothesis is: H1.
When users interact with SRAs, they have lower UR acceptance than when they interact with conventional applications, whereas their JR acceptance and time required to complete tasks remain similar.
A combination of praise and prizes is an effective positive reinforcer in a security-reinforcing application.
A SRA can deliver different types of rewards to users after they emit secure behaviors.
For instance, praise rewards can be easily presented as congratulatory messages.
A prize reward can be delivered, e.g., by announcing that a bonus will be added to the employee's paycheck, or by showing a coupon code redeemable in authorized online merchants.
Figure 1 shows a praise reward that an email client could be configured to show to users when they reject a UR.
To help users who don't know what kinds of risk their organization deem acceptable, the software would provide a "" link.
If the user clicks on that link the software presents an explanation .
It's important that the user not simply learn to avoid all risks.
Had the user accepted a justifiable risk, the software would present a dialog similar to figure 3.
The dialog in figure 1 also announces that monetary rewards can be forthcoming if the user keeps handling her email securely.
The user can get more information about the latter by clicking on the "" link .
Figure 5 shows a notification of a prize reward.
Security auditors who use SRAs can measure if a reward is reinforcing, and adjust it accordingly, by performing a direct test.
If the frequency of a desired behavior increases when the presentation of a stimulus is made contingent upon the behavior, then the stimulus is considered reinforcing.
Prizes and praise are generalized reinforcers  that are commonly used to strengthen a wide range of behaviors necessary to maintain productivity.
Thus, it is plausible that they can be also effective in strengthening secure behaviors, though this has not been experimentally tested before.
Security auditors that employ SRAs need guidance on when to provide reinforcement.
In general, reinforcement can be given continuously or intermittently.
Auditors can arrange to provide reinforcement continuously during an initial learning phase, to promote user's acquisition of new behaviors.
However, continuous reinforcement cannot be provided long-term.
In production, only a small percentage of messages received by a user could be realistically expected to be tagged by auditors for reinforcement.
Only intermittent reinforcement can be maintained long-term.
Previous results from OC suggest that behaviors intermittently reinforced are resistant to extinction.
However, this has not been verified in software applications.
Intermittent reinforcement schedules are effective in a security-reinforcing application.
Little is known about what rewards would work well in a software environment such as SRAs.
It is not possible to know a priori if a particular stimulus will be reinforcing for a user under specific circumstances.
Auditors cannot simply ask users either, as self-reporting may be unreliable, especially if contingencies are complex .
Users may not use SRAs during, e.g., weekends or vacations.
Thus, security auditors cannot provide reinforcement every day or even every month.
If users' secure behavior extinguishes during these absences, security auditors would need users to go through a learning phase after they return.
We hypothesize that this will not be usually necessary: H4.
After a user's secure behaviors have been strengthened by interacting with a security-reinforcing application using intermittent reinforcement schedules, those behaviors remain strong after a period of several weeks during which the user interacts only with conventional applications.
It may not be initially apparent to users why security auditors reward some decisions and not others.
If users find an SRA's rewards unpredictable or unfair, they may reject the SRA, even if the SRA objectively improves security.
To help users understand what is rewarded , all SRA's notifications include links that users can click to obtain plain-language explanations.
During the initial learning phase, SRAs can also display notifications explaining what is not rewarded .
Users can ignore these notifications, and SRAs never penalize users for insecure behaviors.
For testing our hypotheses, we extended the email client Mozilla Thunderbird 1.5 to convert it into a SRA, as described next.
First, the application uses the same polymorphic dialogs as , to eliminate the discriminative stimulus of insecure behaviors which compete with secure behaviors .
A SRA with polymorphic dialogs asks the user to provide context information necessary for a security decision, and then suggests an appropriate course of action .
Second, we incorporated the praise and prize dialogs shown in figures 1 and 5.
The praise dialog is shown non-modally and embedded as part of the application's chrome .
The dialog in figure 6 is also shown this way.
We did so to allow users to continue interacting with the program without having to explicitly dismiss the dialog first .
The prize notification is shown as a floating balloon above the application's status bar.
Both dialogs disappear whenever the user selects another message.
Figure 11 shows an instance when both praise and prize rewards are given to the user at the same time.
However, in general, each reward could be presented alone according to a reinforcement schedule.
The tight integration of the reinforcing stimuli with the email client's chrome makes it difficult for attackers to imitate such stimuli.
Third, we implemented the continuous and fixed-ratio schedules of reinforcement, with the ability of presenting either praise or prize rewards just described.
An arbitrary number of schedules can be active at the same time forming a combined schedule.
When the requirements of the active schedule are met, the appropriate stimuli are displayed immediately.
In this section we first describe our rationale for complementing SRAs with vicarious security reinforcement  and our hypothesis about the latter.
We then describe the design of a VSR intervention that we evaluated.
SRAs can be effective in strengthening secure behaviors.
However, when interacting with SRAs, users need to actually experience a situation in which they will be reinforced after securely handling a security risk.
Thus, s/he may accept several URs or reject several JRs before s/he receives a reward.
There are at least two undesirable implications of this.
First, it may take some time for a user to understand the association between secure behavior and reward.
Second, given the sheer number of risky situations affecting security, a user may get reinforced for securely handling some of them, but may miss others.
A possible solution could be to include in instruction manuals or help messages rules for discriminating between types of risk, and the consequences of accepting and rejecting instances of each type.
Vicarious security reinforcement  can model secure behaviors and present their desirable consequences without waiting for users to emit fortuitously such behaviors and stumble upon their consequences.
This use of vicarious reinforcement for strengthening secure behaviors is new.
It is also worthwhile since faster improvement of security behaviors may help users avoid unnecessary errors.
Specifically, our hypothesis is: H5.
While learning to use an SRA, users who previously had VSR training have lower UR acceptance than and similar JR acceptance and time to complete tasks as do users who did not have such training.
In this section, we describe the design of our VSR intervention in which secure behavior of a model is reinforced.
We describe the features included in our intervention grouped by the four sub-processes that govern vicarious learning.
The produced intervention can be watched at .
Three aspects have been identified as influential in getting and maintaining observers' attention: the model, the observers' characteristics, and the modeling display .
First, regarding the models, there are two types frequently used: coping models and mastery models .
The former is a model whose initial behavior is flawed, but that gradually improves to the desired level of performance.
The latter is a model that acts flawlessly from the beginning.
Given that very proficient security behavior from a person  often has negative connotations , we chose to use a coping model.
Other recommendations in the relevant literature about models and their characteristics, are that several different models be utilized, and that at least one "high status" model be included.
We heeded such advice as follows.
We used two extra models acting as co-workers of the main model.
When interacting with the latter, they emphasized the desirability of behaving securely.
This was also intended to convey the idea that secure behavior can be socially acceptable .
Also, we included a model portraying the coping model's boss.
The latter's status is distinguished by more formal clothes.
Second, the characteristics of the observer must be taken into account.
Hence, we tested our intervention only with people having no computer-technical background but who had work experience, and who use or have used an employer-assigned email account to complete their work-related tasks.
We conjectured that people with the selected profile would be more predisposed to empathize with the model, and thus to pay attention to him and his behavior.
Third, there are several ways to display a VSR intervention such as live performances and videos.
Since we could schedule only one person at a time, we chose to portray our intervention using a video, which is easily reusable.
Experts  argue that, for maximizing a vicarious-learning intervention's effectiveness, the modeling display should portray behaviors to be modeled:  vividly, and in a detailed way,  from least to most difficult,  beginning with a little stumbling, followed by self-correction, and with a strong finish,  with enough frequency and redundancy to facilitate retention,  keeping the inclusion of non-target behaviors to a minimum,  with a length of between 5 and 20 minutes, among others.
Based on these criteria, we implemented the intervention as a video with 4 scenes, and running time of approximately 10.5 minutes.
Scene 1 first introduces Jack Smith, the main model in the video, in his work environment .
Then, it shows him receiving an assignment from his boss, who  hands Jack printed information useful to complete the tasks assigned,  states that other information will be sent by email, and  presses Jack to complete the task as soon as possible.
Scenes 2 to 4 each shows the model handling risks of increasing difficulty.
In scenes 2 and 3 Jack handles URs, while in the last scene he handles a JR. To make the model's behavior appear respectively detailed and vivid, he "thinks aloud" when trying to determine whether a risk is justified, and gesticulates accordingly.
In scenes 2 and 3, at first Jack appears to fall for the ploy in the emails, and he is seen about to open the attached file.
However, he realizes that the emails possess suspicious characteristics, verbalizes them, and rejects that risk.
Lastly, in scene 4, Jack is initially wary about the JR in his inbox because it was sent by somebody who doesn't work in his company, and who he doesn't remember.
However, after reading the email, he recalls that he was expecting such email based on information given earlier by his boss, and finally accepts it.
We included a JR to avoid having subjects simply learn to reject any risk regardless of it being justified or not.
Several studies  have shown that the inclusion of a list of "learning points" about the main ideas presented in a modeling intervention  enhances observers' retention.
We implemented that suggestion by showing,
This section presents the methodology we used to test our hypotheses.
We performed a user study, called the SRA study, to test hypotheses 1-4, and another user study, called the VSRA study, to test hypothesis 5.
No user participated in both studies.
Each study used a within-subjects design, as recommended for reinforcement experiments .
Each subject role-played an employee in two similar scenarios, A and B, under a single study's different conditions.
The first condition in each study was the same .
Its goal was to measure each subject's performance when using conventional security dialogs.
The control condition used a randomly selected scenario and the unmodified Mozilla Thunderbird 1.5 email application, while subsequent conditions used the other scenario and the SRA email application.
Note that the control condition did not teach anything that might affect the subject's performance in subsequent conditions because  before the study, selected subjects were already familiar with email programs and conventional security dialogs, and  subsequent conditions use a different scenario and our modified security dialogs.
In each subsequent condition, we compared the subject's performance following our interventions to the respective performance under the control condition.
The SRA study had four conditions: control, learning, maintenance, and extinction.
The learning condition differs from the subsequent conditions by offering more frequent reinforcement.
Continuous or very frequent reinforcement is often necessary for acquisition of new behaviors, according to OC, but in the long term makes those behaviors more susceptible to extinction.
Moreover, very frequent reinforcement usually cannot be maintained in production.
Accordingly, the maintenance condition offers less frequent reinforcement.
Subjects progressed from learning to maintenance condition when their measured ability to discriminate JRs and URs was considered adequate.
Proficiency is required before the maintenance condition because maintenance reinforcement could be insufficient for acquiring new behaviors.
Subjects performed under the first three conditions in a single session.
The extinction condition differs from the maintenance condition only in that it was performed in another session five weeks later.
Between sessions, subjects did not use SRAs.
The purpose of the extinction condition was to test whether acquired security behaviors would extinguish after long periods without reinforcement .
The learning condition used a combined schedule of reinforcement.
Its component schedules were  continuous with praise reward, and  fixed ratio with a prize reward  every other secure behavior emission.
The dialog in figure 6 was shown only during learning.
As explained earlier, we do this to help users understand what behaviors are not rewarded .
The clues were shown and narrated one by one.
Several clues were shown in all three summaries, thus providing the redundancy that facilitates learning .
Observers must be able to enact the behavior modeled in a vicarious intervention.
In our experiments, the tasks assigned didn't require more skills than handling emails using an email client, opening attachments, and editing documents using Microsoft Word.
Our eligibility criteria during recruitment ensured that subjects already had these abilities.
Social Learning theory draws a distinction between acquisition and behavior since observers will not apply everything they learn .
To ensure enactment of modeled behaviors, it's necessary to make desired consequences contingent upon such behaviors.
We incorporated this important point in our intervention, as explained next.
The model receives the praise and prize rewards implemented for the SRA .
These rewards are presented every time the model behaves securely, namely, after rejecting an UR in the scenes 2 and 3, and accepting a JR in the fourth scene.
In addition, at the end of scene 2 after receiving the rewards, the model invites two coworkers, a female and a male, to see such on-screen rewards .
The former expresses surprise and satisfaction for the company's new practice of rewarding employees for managing their email securely, and asks the latter if he also considers such practice "cool."
The male coworker model agrees, and mentions that he was rewarded earlier too.
Then he states that he'll definitively be handling his email account more carefully.
The boss model, who overheard part of the conversation when transiting through the hallway, enters into Jack's office and congratulates him for behaving securely .
He does the same with the male coworker, and states he is sure the female coworker will behave securely too.
Before leaving, he encourages the models to keep up the good work.
After discussing with Jack how to use the rewards they will get for behaving securely, the coworkers leave.
Each prize reward consisted of $0.70.
The VSRA study differed from the SRA study in only two ways.
First, between the control and learning conditions, subjects watched our VSR intervention.
Second, there was no extinction condition, because the study's goal was simply to measure any speed up in learning caused by VSR.
We used metrics from signal detection theory  to quantify subjects' performance.
In a signal-detection task, a certain event is classified as signal and a subject has to detect if the signal is present.
Noise trials are those in which the signal is absent.
The hit rate  is the proportion of trials in which the signal is correctly identified as present.
The false alarm rate  is the proportion of trials in which the signal is incorrectly identified as present.
A measure of detectability, known as sensitivity, is defined as d'=z- z, where z is the inverse of the normal distribution function.
In our user studies, the signals were JR email messages, while the noise were UR email messages.
We defined a hit to be user acceptance of a JR , and a false alarm to be user acceptance of a UR .
They had to have work experience of at least one year in organizations that assigned them an email account which they had to use for job-related purposes.
They had to have experience with desktop email applications, not just webmail.
Finally, they could not hold or be currently pursuing a degree in computer science or electric engineering.
The latter criterion was intended to avoid testing people who were already security proficient.
Figure 12 presents the criteria we used for subjects to pass between conditions in a study.
Only subjects whose sensitivity was d' during the control condition were selected for participating in the learning condition.
Remaining subjects' security behavior was deemed as already strong, and unlikely to significantly benefit from our reinforcement interventions.
If a subject's sensitivity was d'> after handling the risks in the Learning-I set, the SRA pushed the entire Maintenance set into her inbox and activated the corresponding combined schedule.
However, if the subject's sensitivity was d', the SRA kept pushing subsets si  Learning-II into the subject's inbox and waited for her to handle the risks in those subsets.
The SRA only pushed subset si+1 if the subject's sensitivity was still d' after handling the risks in her inbox.
Otherwise, the subject was switched to maintenance condition.
The number of risks in the pushed subsets s1, s2, and s3 was respectively 4, 4, and 2.
Each subset contained an equal number of JRs and URs.
If, after processing the entire Learning -I and -II sets, the subject's sensitivity hadn't exceeded the cutoff , her participation was terminated to limit the session's length.
Subjects who progressed to maintenance were eligible for another session to test if their secure behaviors extinguished.
We used the same scenarios in random order in the two studies.
In scenario A, an employee is selecting applicants for a job at her company.
In scenario B, an employee needs to process customers' insurance claims .
In both cases, the role-played characters work for fictitious companies and know specific people inside them.
We created 4 sets of emails per scenario.
Each set consisted of 10 emails, half of which represented JRs and the rest URs.
We will refer to these sets as Learning-I, Learning-II, Maintenance, and Extinction.
There were two learning sets because some users may require more practice and reinforcement to learn to distinguish justified and unjustified risks.
Each email in these sets contained a Word attachment.
The arrangement of risks in each set is shown in table 1.
We created these emails inspired on messages received in our email accounts  and emails in the Enron corpus  .
Each email contains a header that identifies the type of risk it represents, and which is signed by a security auditor.
We advertised the study with flyers around our university's campus, and with electronic posts in online websites.
We announced that the study was related to email clients' usability, not security.
Once interested people contacted us, we asked them to fill out a short web-based questionnaire to determine their eligibility.
In the first session of the SRA study, and only session of the VSRA study, subjects received a handout that briefly described the scenario they were about to role-play, and were given the opportunity to ask questions about it.
We told subjects that the main objective of the study was to evaluate the usability of email programs when used in a corporate setting.
We didn't tell subjects that we were studying security of email clients because we didn't want to prime them to security concerns before the control condition.
We asked them to behave as close as possible as they would if they were at work, considering the scenario they were about to role-play.
We explicitly instructed subjects not to request information from us regarding what to do with the emails they were processing.
We then had subjects sit at a desk in our laboratory, which we told them to be the office of the role-played employee.
The desk was equipped with a laptop , a pen, and a phone in case the person wanted to make calls.
Subjects were told they were allowed to call the fictitious company's technical support referred to in the handout, or to any other phone number they desired in relation to the experiment.
After finishing the scenarios, subjects who interacted with the SRA were asked to complete an exit survey.
Then, during debriefing, we asked them to share with us some insights about their decisions of accepting or rejecting specific risks.
They were also encouraged to provide feedback about our interventions.
We didn't tell subjects in the SRA study whether they had qualified for a second session.
Four weeks after the first session, we asked only those subjects who proceeded to maintenance in the SRA study to come for a second laboratory session during the subsequent week.
When they came back, they received the handout of the last scenario they role-played.
After they read it, we emphasized again that subjects should behave as closely as possible as they would do at work considering the role-played employee.
After processing the extinction set, subjects were asked to complete the same exit survey of the first session.
Only in the VSRA study, just before qualified subjects proceeded to the learning condition, we told them that they were going to watch a video, and that it was up to them to decide what to do, when role-playing the described scenario, with the information presented.
They could either apply the information given in the video or ignore it if that was what they would do if they were at work.
To evaluate retention, after watching the VSR video, subjects took an on-screen quiz consisting of 4 questions.
The quiz was previously unannounced to avoid biasing subjects to pay more attention than might otherwise be the case.
Before starting the quiz, a message box was shown instructing users that, while taking the quiz, they should imagine they were Jack Smith, the model just observed in the video, and providing Jack's employer name and email address.
Each question showed a snapshot of an email message, gave context information related to that email, and asked the user to identify whether it is a JR or a UR.
After a subject answered each question, a message box was shown telling her whether the answer she picked was correct and why.
If the answer was correct, the subject was also congratulated.
Once a subject finished the quiz, a short video  was shown explaining subjects that they shouldn't worry if they didn't remember all the rules shown in the VSR video, because they'd be interacting with an email program that uses context-sensitive guidance with polymorphic dialogs  to help users apply such rules.
Then, a short video presenting a brief overview of such guiding interface was shown.
Finally, subjects role-played the other scenario with our SRA.
A total of 37 people participated in our studies, but 13 of them did only the control condition because their behavior was deemed already secure enough .
We do not consider their results any further.
Of remaining subjects, 12 participated in the SRA study , and 12 in the VSRA study .
We scheduled an equal number of subjects of each gender, but absenteeism was higher among males.
Most of these subjects had two or more years of work experience .
Tables 2 and 3 respectively show summary statistics of subjects' performance in the SRA and VSRA studies.
To compare each reinforcement condition to the respective study's control condition, we calculated p-values with Wilcoxon's signed-ranks test.
This non-parametric test is appropriate for comparing averages of related samples of any size without assuming that the averages have any particular distribution .
We used a one-sided test to compare UR acceptance and two-sided tests to compare JR acceptance and time to complete tasks, because we expected relationships as specified in hypotheses 1-4.
One of the subjects in the SRA study didn't progress past the learning condition because the subject's behavior improvement was insufficient.
The other 11 subjects were invited for a second session, and 7 of them did so after about 40 days.
We first describe results of the SRA study.
However, this improvement was not part of our hypotheses, and didn't reach statistical significance at the sample size considered.
Also, the persistence of improvements in the maintenance and extinction conditions can be attributed to the use of intermittent reinforcement schedules, which make behavior resistant to extinction.
Compared to the control condition, subjects spent less time completing tasks in the learning , maintenance , and extinction  conditions.
These reductions were medium from control to learning , and large from control to maintenance  and control to extinction .
In the SRA conditions, the reduction in task completion time was because subjects spent little or no time reviewing the attachments of UR emails.
These results confirm hypotheses 1-4.
We now present results of the VSRA study.
All subjects correctly answered all post-VSR quiz questions.
We found that subjects had equivalent JR acceptance in learning  and maintenance  conditions as in the respective control condition.
Moreover, there was a statistically significant and large reduction in time spent completing the assigned tasks when subjects used the SRA, during the learning  and maintenance  conditions than in the respective control.
To test hypothesis 5, we compared results of the SRA and the VSRA experiments.
We could do such comparison because the tasks assigned to subjects in both cases were the same .
To make fair comparisons, we subtracted the rates obtained in the SRA-Learning, SRA-Maintenance, VSRA-Learning, and VSRA-Maintenance conditions from the rates in their respective control conditions.
This was done to avoid possible biases because of a priori differences between groups .
We compared these differences with Mann-Whitney Control Learning Maintenance Extinction # subjects mean std.
Times to complete tasks were compared directly without any adjustment.
We did a one-sided test for comparing acceptance of unjustified risks and two-sided tests for comparing acceptance of justified risks and times.
We computed effect sizes using pooled standard deviations.
We found that there was no significant difference in acceptance of JRs or time to complete assigned tasks between subjects who interacted with the SRA with or without previous VSR training.
Additionally, there was a significant and large improvement in rejection of URs in subjects in the VSRA-Learning condition relative to subjects in the SRA-Learning condition , but a non-significant difference between subjects in the SRA-Maintenance and VSRA-Maintenance conditions .
These results verify hypothesis 5.
Subjects' opinions about the interventions were uniformly positive.
This is reflected in the scores  they gave in the exit survey that they took after the first session of the SRA study , and after the only session in the VSRA study.
A two-sided Mann-Whitney test found no significant difference between the scores of the two experiments.
PG sends users special phishing email messages with links to a website with cartoons that teach users how to avoid falling for phishing attacks.
Unlike PG, SRAs embed rewards in the client such that it can deliver rewards immediately after Control # subjects mean std.
This could give an advantage to SRAs because OC suggests that the behavior learning effect is much stronger when rewards are immediate.
When used to educate about organization-specific security policies and targeted attacks, PG and SRAs are likely to require similar supervisory effort .
However, to the extent that PG seeks to educate only about generic threats, it has the advantage that it can benefit individuals without supervision.
Another difference is that users need to learn security concepts from the PG site and then remember and apply them unaided.
Unlike PG, SRAs embed an organization's security policy in the application, guide users, and require only that users provide truthful context information.
By reducing cognitive load, SRA may facilitate decisions involving complex policies.
PG-trained users might be quicker applying simpler policies.
Considering such tradeoffs, an organization might use both PG  and SRAs .
They then designed two different warnings with an overall better effectiveness .
They concluded that warnings alone are insufficient to deter users from behaving insecurely.
Their findings are consistent with our results in , where we found that polymorphic dialogs alone, although effective, need to be complemented with punishment or reinforcement to achieve larger improvements.
We evaluated employing reinforcement for strengthening secure behaviors through security-reinforcing applications  and vicarious security reinforcement .
SRAs reward users for accepting justified risks  and rejecting unjustified risks .
We tested a SRA in the context of email where a security auditor sends to end-users email messages with JRs and URs.
The reinforcers used were praise and prize rewards.
In a user study, users who interacted with a SRA behaved significantly more securely than when they interacted with a conventional application, and there was no adverse effect on time needed to complete tasks.
Subjects were first conditioned using continuous reinforcement, and then their behavior was maintained with intermittent reinforcement.
The strengthened secure behaviors didn't extinguish after a period of 40 days in which users didn't interact with SRAs.
In another user study, before using the SRA, users observed a model being reinforced for secure behavior .
These users improved their security behavior faster than did the first study's users .
Sasse, "Users are not the enemy.
Why users compromise computer security mechanisms and how to take remedial measures," Communications of the ACM, vol.
A. Bandura, Social learning theory, Prentice-Hall, 1977.
Goldstein, and M. Sorcher, Changing supervisor behavior, Pergamon Press, 1974.
B. Klimt, and Y. Yang, "Introducing the Enron corpus," in Proc.
B.F. Skinner, "Operant behavior," American Psychologist, vol.
B.F. Skinner, Science and human behavior, Macmillan Pub Co, 1953.
Ferster, and B.F. Skinner, Schedules of reinforcement, Appleton-Century-Crofts, 1957.
Saari, "Application of sociallearning theory to training supervisors through behavioral modeling," Journal of Applied Psychology, vol.
H. Xia, and J.C. Brustoloni, "Hardening Web browsers against man-in-the-middle and eavesdropping attacks," in proc.
Pierce, Rewards and intrinsic motivation: Resolving the controversy, Bergin & Garvey, 2002 11.
J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, & L. Cranor, "Crying Wolf: An Empirical Study of SSL Warning Effectiveness," in Proc.
J.C. Brustoloni, and R. Villamarin-Salomon, "Improving security decisions with polymorphic and audited dialogs," in Proc.
Sasse, and I. Flechais, "Usable Security: Why do we need it?
How do we get it," in Security and Usability: Designing Secure Systems That People Can Use, L. Cranor, and S. Garfinkel eds., O'Reilly, 2005, pp.
Creelman, Detection theory: A user's guide, Cambridge University Press, 1991.
P. Kumaraguru, Y. Rhee, S. Sheng, S. Hasan, A. Acquisti, L.F. Cranor, and J. Hong, "Getting users to pay attention to anti-phishing education: evaluation of retention and transfer," in Proc.
APWG's annual eCrime researchers summit, 2007, pp.
Decker, "The enhancement of behavior modeling training of supervisory skills by the inclusion of retention processes," Personnel psychology, vol.
Dowrick, Practical guide to using video in the behavioral sciences, Wiley New York, 1991.
Miltenberger, Behavior modification: Principles and procedures, Cole Publishing Company, 1997.
S. Egelman, L.F. Cranor, and J. Hong, "You've Been Warned: An Empirical Study of the Effectiveness of Web Browser Phishing Warnings," in Proc.
S. Sheng, B. Magnien, P. Kumaraguru, A. Acquisti, L.F. Cranor, J. Hong, and E. Nunge, "Anti-Phishing Phil: the design and evaluation of a game that teaches people not to fall for phish," in Proc.
