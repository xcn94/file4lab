The vast majority of work on understanding and supporting the gesture creation process has focused on professional designers.
In contrast, gesture customization by end users-- which may offer better memorability, efficiency and accessibility than pre-defined gestures--has received little attention.
To understand the end-user gesture creation process, we conducted a study where 20 participants were asked to:  exhaustively create new gestures for an openended use case;  exhaustively create new gestures for 12 specific use cases;  judge the saliency of different touchscreen gesture features.
Our findings showed that even when asked to create novel gestures, participants tended to focus on the familiar.
Misconceptions about the gesture recognizer's abilities were also evident, and in some cases constrained the range of gestures that participants created.
Finally, as a calibration point for future research, we used a simple gesture recognizer  to analyze recognition accuracy of the participants' custom gesture sets: accuracy was 68-88% on average, depending on the amount of training and the customization scenario.
We conclude with implications for the design of a mixed-initiative approach to support custom gesture creation.
Gesture sets created by two participants in Task 2.
Each set includes one gesture for each of 12 actions, selected as the "best" gestures following a brainstorming phase.
Gesture traces are in yellow.
When appropriate, red boxes provided a target for the gesture .
The examples show varying number of touches, hands and other characteristics.
However, in either case, the primary focus is on aiding professional designers create more effective gesture sets.
In this paper, we investigate the feasibility of end-user gesture creation by studying how typical users create gestures and characterizing the challenges encountered therein.
Self-defined gestures have many potential advantages, including improved memorability over predefined gestures .
Gesture customization may also improve touchscreen accessibility for people with physical disabilities, for whom there is a need to replace standard multitouch gestures  with alternatives that are accessible for each user .
Finally, Ouyang and Li  have proposed personal gestural shortcuts as a means of more efficiently accessing information on a smartphone.
Despite these motivations, little is known about the process of end-user gesture customization.
Are typical users able to generate novel gestures?
How distinguishable are those gestures from the user's point of view and from the machine recognizer's point of view?
Given that gesture creation can be difficult even for professional designers , assessing its feasibility for typical users could yield complex challenges.
To understand the end-user gesture creation process, we conducted a study with 20 participants who completed three tasks:  exhaustively creating new gestures for an openended use case;  exhaustively creating new gestures for 12 specific use cases ;  judging the saliency of different touchscreen gesture features.
The first two tasks were exploratory, designed to provide insight into the gesture creation process when users are asked to come up with new,
With the rapid emergence of touchscreen devices, gesturebased interactions such as pinch-to-zoom have become ubiquitous from smartphones to tablets to interactive tabletops.
Commensurate with this growth has been an equal interest from the HCI research community in how to design good gestures that are easy to discover, execute, and remember.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The second task also allowed us to explore the quality of usercreated gesture sets from a gesture recognition standpoint.
The third task was designed to uncover implications for how systems could better support users in creating personal gestures, for example, by having the software recommend new gestures or modifications to gestures based on recognition performance.
Our findings reveal that while there was a tendency to generate familiar gestures, participants applied subtle mechanisms to distinguish between gestures.
Participants also often accounted for perceived abilities of the recognition system, although their understanding of that system was not always accurate.
Simulations with a simple gesture recognizer  showed that the recognition rate of individual participants' gesture sets was 76-88% on average depending on the number of training examples, and lower, at 68% for a specific customization scenario.
Although the $N recognizer has several limitations, this accuracy level offers a calibration point for future research and motivates the need for mechanisms to better support end users in creating good quality gestures.
The primary contributions of this paper are:  a characterization of the thought processes and challenges encountered when end users are asked to create novel, distinct gestures,  an assessment of the potential recognition accuracy of end-user created gesture sets,  and implications for the design of a mixed-initiative system to support end users in more effectively creating custom gestures.
While our focus is on touchscreen gestures, many of the challenges and implications should apply more broadly, for example, to 3-dimensional gesture interaction.
The key distinction between these studies and our own is that their goal is to create a gesture set that can be intuitive and guessable by a wide range of users.
In contrast, our focus is on creating personal gestures for only one user .
In addition, for a typical user-defined gesture study there is no assessment of the quality of an individual user's gestures in terms of both user perception and recognizer accuracy.
Indeed, a primary goal of the userdefined gesture method is: "to consider the types of surface gestures people make without regard for recognition or technical concerns" .
In contrast, our ultimate goal is to support gesture customization that results in high recognition accuracy while also minimizing user burden in understanding the complexities of the recognizer.
Most of these papers either do not include a user evaluation or include an evaluation where the user is provided with a set of gestures on which to train the system, rather than creating their own gestures.
As such, these studies do not address the main goal of our work, to explore the process of gesture creation.
One exception is from Ouyang and Li , who used crowdsourced gesture creation and recognition to allow users to employ new gestural shortcuts for applications without having to predefine those shortcuts.
The approach shows promise: in a controlled laboratory study it produced the correct match within the 8 most likely matches 77% of the time a user entered a new shortcut.
Personalized gestures have also been used in computer music applications , although this type of artistic expression may be substantially different from everyday device use.
Finally, personalized gestures have been used in place of passwords for secure device access .
Wobbrock, Morris and Wilson , asked participants to create gestures for 27 individual actions on an interactive tabletop.
The results were combined into a single set that contained the most popular of the user-defined gestures, with multiple options included for some actions .
In follow-up work, the same authors showed that a different set of participants preferred the user-defined gesture set to an expert set that the authors themselves had created .
Subsequently, a number of studies have been inspired by this method, including gestures for communicating between two devices , 3-dimensional gestures for mobile phones , flexible paper displays for mobile phones , 3dimensional "touch free" interaction , gestures for blind vs. sighted people , and text input .
Several tools have been proposed to help professional designers create gestures more easily.
MAGIC , for example, is a tool to create and test motion gestures.
Proton++  allows developers to specify gestures as regular expressions, thus easing gesture creation.
Gesture Coder  also supports programming gestures by demonstration.
Some more integrated user interface design tools also support gestures .
Most relevant to our work is that of Long et al.
Their system includes a component to model perceived gesture similarity  and to warn the designer when two gestures will be perceived by users as being too similar .
Given that twelve years have passed since the original work, it is worth reexamining many of the findings now that mobile device users have much more exposure to gestural input.
Our work also differs in that it focuses on end users, as compared to the understandably complex system Long et al.
More recently, several tools have supported more accurate gesture articulation, by making the gesture recognition process more transparent to users.
Kristensson and Denby  present an incremental recognition algorithm to enable continuous visual feedback during gesture articulation, which aids users in understanding the recognition process.
OctoPocus  also reveals the low-level recognition process, reducing gesture articulation errors by previewing possible gesture matches and varying the path thickness based on the likelihood of each match.
Finally, Fluid Sketches  incrementally recognizes each gesture as it is drawn and transforms it to an ideal shape , thus providing feedback about the recognition progress and the appearance of the final shape.
While these examples address the interpretation gap between the user and the gesture recognizer during gesture articulation, our study focuses on this gap during gesture creation.
To display gesture animations for the third task, we used a laptop with an Intel Core i5 processor, Windows 7, and an external monitor .
A screenshot of the tablet software is shown in Figure 2.
The interface included three sections: the gesture creation area took up most of the screen, the current task instruction appeared at the top, and a scrollable history panel listed previously entered gestures for the current task.
Gestures could be drawn in the gesture area simply by touching the screen with fingers or hands.
Real-time feedback was provided by a yellow trace .
Finally, a Clear button allowed users to clear and restart the current gesture, while a Save button added the current gesture to the history panel.
Gestures could also be deleted from the history panel in case participants had accidentally saved unwanted gestures.
The system did not actually recognize drawn gestures but recorded all touch events to a log file, including time-stamped  coordinates and the touch identification number provided by the operating system.
The main premise underlying this research is that a good gesture set needs to be easy to use, easy to remember, and easy to recognize.
To understand how users conceptualize and meet these criteria when asked to create gestures, we conducted a controlled lab study with three tasks.
For the first two tasks, participants created an exhaustive set of gestures on a touchscreen tablet either for open-ended use or for a specific action.
In the third task, we extended previous work on gesture feature saliency by Long et al.
Combined, the tasks objectively and subjectively explore a user's understanding of and ability to create distinct gestures.
These goals are in contrast to user-defined gesture studies , which focus on guessability and creation of individual gestures without consideration for an entire set; the results are then handed to a professional designer.
For customization, the user's ability to create distinct gestures will directly impact the effectiveness of the interface.
Twenty participants  were recruited through on-campus mailing lists.
All participants were volunteers and were compensated for their time.
Eight of the participants have majors outside of the engineering field.
One participant was left-handed, one was ambidextrous, and the others were right-handed.
All participants were experienced touchscreen users.
Each owned at least one touchscreen device, such as an e-reader, smartphone, or tablet.
With the exception of one person who rarely used a touchscreen device, all participants reported daily use.
Participants completed a single one-hour session consisting of:  open-ended gesture creation,  gesture creation for specific actions, and  judging feature saliency.
For the first two tasks, the touchscreen device was laid flat on a table in front of the participant.
No gesture recognition feedback was provided to participants.
Sessions were video recorded and the think-aloud protocol was used throughout all tasks.
Care was taken throughout the study to reduce bias.
For instance, the experimenter did not demonstrate or provide example gestures while communicating instructions .
Before beginning the first task, participants completed a background questionnaire to collect demographic information and previous touchscreen experience.
Physical Object Manipulation Actions Zoom In*: Enlarge an image to display "up-close" details in the viewable screen area of an object Zoom Out*: Decrease the size of an image to display more in the viewable screen area of an object Rotate*: Move an object clockwise in a circle around its one or more pivot points Navigation Actions Previous: Go back to the page/screen that came immediately before the current one Next: Go forward to the page/screen that comes immediately after the current one Editing Actions Select Single*: Choose an object for a particular purpose Select Group*: Choose a group of objects for a particular purpose Cut: Delete an object/file/app completely so that its copy can be inserted to elsewhere  Copy: Reproduce an object/file in another location Paste: Insert an object/file that is copied from elsewhere to the current location Shortcut Actions Call Mom: Call your mom  Launch Browser: Launch your favorite web browser 
Permutations for each of the 9 gesture features participants evaluated in Task 3:  Orientation,  Aspect Ratio,  Scale,  Pattern Repetition,  Curviness,  Speed,  Stroke Count,  Finger Count, and  Stroke Order.
The base gesture to which modifications were applied is highlighted within each set for illustration only; participants were not shown the highlight.
Small circles represented finger touches and animated to indicate how the gesture was articulated.
For  and , separate strokes are shown in different colors.
Participants were then asked to create  as many gestures as possible that met the following requirements: easy to draw, easy to remember, and different enough from one another that they could be used for different actions.
Participants were also told they could use any number of fingers, strokes and hands.
We were primarily interested in whether participants would be able to create a range of gestures that fulfilled the requirements and to what degree they would fixate on gestures they already knew .
The task ended when the participant indicated that they could not think of additional gestures.
Finally, 7-point Likert scales were used to elicit preference ratings for the number of touches, strokes and hands.
In contrast to Task 1, participants were also told novel gestures were preferred; we encouraged them not to use tap, swipe, and two-finger pinch-to-zoom gestures commonly found on touchscreen devices.
For each action, participants were given as much time as they wished and were asked to indicate when they could think of no more gestures.
After all 12 actions had been completed, participants were asked to identify which had been the easiest and hardest.
Finally, participants composed a custom set of 12 gestures , with the same requirements as before: easy to draw and remember, distinguishable.
If upon reflection the participant felt he/she had not already created a good gesture for a particular action, a new gesture could be added.
After completing the set, participants rated their satisfaction with it on a 7-point Likert scale.
To later explore the quality of the custom sets from a machine recognition perspective, participants also provided five additional training examples for each gesture in the final set.
The second task provided a more realistic context of creating gestures for 12 specific actions.
Participants first brainstormed multiple distinct gestures for each action before composing a final custom set of gestures, one per action.
The custom sets allowed us to assess how users conceptualize distinct gestures across actions and to conduct a preliminary evaluation of the quality of the sets from a gesture recognition point of view.
To include a variety of actions, we considered the categories in Table 1.
For brainstorming, the 12 actions were presented in random order and participants were asked to create as many gestures as they could for each; see Figure 1 for example gestures.
Similarly to Task 1, we asked that the gestures be easy to draw and easy to remember.
If we find that typical end users do have difficulty in creating good quality gestures, it may ultimately be useful for a mixed-initiative system  to recommend to users how to improve their gestures.
For example, an effective recommendation may be to distinguish between two similar gestures by adding another finger to one of them.
To inform such recommendations and to more explicitly investigate which characteristics users rely on to distinguish between gestures, we asked participants to rate the distinctness of 9 features of finger-based, multi-touch gestures.
We selected 9 gesture recognition features to evaluate, as shown in Figure 3.
Six of these features are adapted from the popular Rubine's gesture recognizer .
For example, consider the difficulty of manipulating these four features:  sine and  cosine of the initial angle of the gesture, and  sine and  cosine of the angle between the first and last points of the gesture.
Instead, we collapsed these four features into a single, more userfriendly Orientation feature.
In total, we defined 6 features in this way based on Rubine's set: Orientation, Aspect Ratio, Scale, Pattern Repetition , Curviness and Speed.
Since Rubine's recognizer was not designed for multi-touch or multi-stroke input, we additionally included Finger Count, Stroke Count, and Stroke Order.
To isolate the influence of each of the nine features on subjective assessments of gesture distinctiveness, we selected a single base gesture  and modified it to create five permutations for each feature; see Figure 3.
Although the base gesture was chosen from the predefined gesture set used by Long et al.
For each feature, participants were presented with the five gesture permutations on screen.
Red circles representing finger touches animated along each gesture permutation to show how it would be articulated .
During the study, all participants mimicked the animations to some extent with their own fingers.
To represent varying stroke orders, we used different colors and explained this representation to participants.
The order of presentation for the features was randomized.
Again, subjective questionnaires were given.
Within each feature's set of permuted gestures, participants were asked to judge the overall similarity of the gestures using a 7-point Likert scale and to identify the most and least similar pairs.
After all features had been presented, participants were shown all nine at once and asked to rank across the features from most to least distinguishable.
In Task 1, we were primarily interested in the quantity and types of gestures generated when participants were given few constraints.
We were also interested in studying their reasoning process during the task.
As shown in Table 2, gestures were much more likely to be one-handed than two-handed, have a single stroke as compared to multiple strokes, and use a single touch as opposed to multiple concurrent touches.
No gestures were both multi-touch and multi-stroke.
Although this task was completely open-ended, participants tended to create gestures based on specific, familiar actions.
Using the rationale expressed in the think-aloud data, we categorized each drawn gesture as being specific to a particular action or not.
On average, participants verbalized specific actions for 58.2% of gestures .
Almost all participants  added action-specific gestures such as zoom-in/out, scroll-up/down, ok/cancel, go to previous page/next page.
Eight participants did not want to create gestures for random purposes .
Gestures that were not mapped to specific actions included numbers, letters, and geometric shapes .
Those participants who created more gestures in total also created a larger proportion of gestures that were not tied to specific actions; there was a moderate positive correlation between these two measures .
The challenge of functional fixedness  was evident, where people get "fixed" on the known functions of an object and have difficulty coming up with new ways of using it.
We used a combination of analysis methods including simulations with gesture recognition algorithms, statistical tests, and qualitative analyses.
Due to a technical issue, time-stamped touch events were lost for three participants in Task 2; we note the affected analyses in the Results section, where N = 17 instead of 20.
We specify which statistical tests were used throughout the results.
In general, we applied parametric t-tests where appropriate  and non-parametric tests such as Friedman and Wilcoxon signed rank tests otherwise .
All post-hoc pairwise comparisons were protected against Type I error using a Holm-Bonferroni adjustment .
Finally, we performed a qualitative analysis on the think-aloud data from the video recordings and other responses.
Comments were iteratively categorized into themes by a single researcher around how participants conceptualized the gesture creation process.
While Task 1 explored gesture creation with few constraints, Task 2 allowed us to evaluate a similar brainstorming process but where participants are given specific scenarios.
In Task 2 we also asked participants to compose a set of "best" custom gestures, one per scenario.
In total, 747 gestures were created, many more than the 244 seen in Task 1.
Below, we highlight differences in gestures created across actions before turning to our primary questions of how users create and conceptualize distinct gestures and what recognition rates are achieved for the custom gestures sets when using a basic gesture recognizer .
Participants created 3.1 gestures per action on average ; see Figure 4.
To understand how different actions impact gesture brainstorming, we calculated the average number of gestures created for each action type outlined in Table 1.
Post-hoc pairwise comparisons using Wilcoxon signed ranks tested showed that the shortcut actions resulted in significantly fewer gestures than did the navigation and physical object manipulation actions .
In terms of subjective difficulty in creating gestures, 14 participants chose a shortcut action as the most difficult .
In contrast, Next and Previous received the most votes for being easy .
Recall that a primary goal of this study was to understand what process typical users would follow in trying to create distinct gestures.
During the brainstorming phase of Task 2, participants considered the distinctness of gestures both within an action prompt and between action prompts.
For an example of the latter, one participant commented after adding a circle gesture for Paste: "This one can be confused with zoom since it also has a circle" .
Another participant justified an `e' gesture for Cut by saying: "I used different  letter `E' for the web browser context" .
We observed two general strategies to distinguish between gestures during the brainstorming phase of Task 2:  variations on a single gesture and  multiple scenarios for the same action.
Variations of a gesture included changing the number of fingers, direction of the stroke, duration of the gesture, or adding an additional touch for mode switching.
For example, one participant explained, "This gesture is different because the other one it's just one click while this one is press and hold" .
As an example of the second strategy, one participant added different gestures for Copy when considering an object versus a paragraph.
Another participant added gestures for selecting only a few objects versus selecting many objects within the Select Group action.
The first strategy tended to be used when creating multiple gestures for the same action, while the second strategy was often used for creating gestures for different actions.
While composing their sets of 12 custom gestures, participants were asked an open-ended question about why they included each gesture.
Responses were subsequently pooled together across all participants and iteratively grouped into categories.
The results are shown in Figure 5.
The top two reasons were intuitiveness  and ease and/or simplicity .
Interestingly, accuracy of input was cited by 14 participants as a reason for selecting at least one gesture; half of these participants had technical backgrounds, while half did not.
Although we had not mentioned machine recognition accuracy, it was still considered.
For example, one participant selected a multistroke gesture with the comment: "Adding one more stroke seems more accurate for computers" .
Another participant  wanted to have pop-up menus for editing actions to minimize the confusion for the device.
Finally, despite asking participants to create novel gestures, there was still a tendency to stick to familiar ones.
During the gesture set creation process, participants were allowed to add new gestures if they were inspired to do so.
Nine participants added one new gesture each at this point, the majority because they had a new idea , one person because she felt the existing gestures were too similar, one person due to comfort, and one person due to a better understanding of the action.
This behavior suggests that participants were able to reflect critically on the gesture creation process.
After composing the custom gesture sets, participants were asked to rate their satisfaction with the set on a scale from 1  to 7 .
While responses were generally positive , only two participants were fully satisfied.
The remaining participants felt their set could be improved but did not know how to do so, suggesting that recommendations from a mixed-initiative system to support gesture creation may be useful.
Since our focus is on personalization, training and testing was per participant.
While the first analysis is useful in understanding the potential recognition accuracy of custom gesture sets, the second analysis addresses a common customization scenario: users often perform a batch of customizations at once based on triggers like software upgrades, job changes, or social pressure .
The gesture examples initially created during this customization period may not necessarily be representative of more experienced use later.
To emphasize this point, the initial examples for gestures that ultimately ended up in the custom sets took participants on average 1225 ms to articulate compared to only 726 ms for the later repetitions  = 3.411, p = .004.
Figure 6 shows results from the 5-fold cross validation.
As expected, recognition accuracy increases with the number of training examples.
To mitigate the effort required to customize gestures, it would be ideal to have users provide only one training example; however, with one example average accuracy was only 76.4% .
For the second analysis, training on the initial gesture examples, average accuracy drops to 67.8% .
This is a significant decrease compared to the 5-fold cross validation results for a single training instance  = 3.20, p = .006.
Overall, there is room for improvement with end-user gesture creation, and the effect may be particularly pronounced in a batch customization scenario.
To assess the quality of the custom gesture sets from a gesture recognizer point of view, we calculated recognition accuracy using the $N recognizer  with Protractor matching method .
The $N recognizer is meant for quick prototyping of shape-based gestures and is thus limited.
However, it allows for gesture creation by example with only a few training examples, which is necessary for gesture customization.
As such, these results provide a calibration point for future research.
We conducted two analyses using the five extra examples  participants had created for each gesture in their custom set.
First, for each participant we created five random training sets from the extra examples  and ran a 5-fold cross validation.
To create an accuracy response curve, we varied the amount of training data and tested on the remaining sets: from 1 to 4 training examples, covering all combinations.
Second, for each gesture in the custom set, we trained the recognizer on the initial instance the participant had created during brainstorming and we tested on the five later repetitions.
In this third task, we investigated what features people use to subjectively distinguish between gestures.
The intent is to derive implications for future gesture customization systems that can provide end users with feedback on how to improve their gestures.
An effective recommendation would take into account both the recognizer's needs and the user's ability to make the required change.
In general, features where the interpretation could vary greatly from one person to the next  were considered to be less distinct than more objective features .
Related to this point, one participant commented: "Even if the same person is performing the gesture, it might not have the same speed and size" .
Post-hoc pairwise comparisons on the ranking data using Wilcoxon signed ranks tests showed that Speed, Scale, and Pattern Repetition were considered to be significantly less distinct than Finger Count, Stroke Count, and Stroke Order.
Speed was also less distinct than Orientation, Curviness, and Aspect Ratio.
Finger Count and Aspect Ratio were also significantly different.
Aspect Ratio has been previously identified as an important feature for perceived gesture saliency by Long et al.
Responses relating to Stroke Count also highlighted the potential challenge of relying on strokes to distinguish between gestures.
Among other issues, three participants felt it was easy to add one more stroke by mistake or to adjust their pre-drawn gestures.
Participants also considered physical effort in comparing gesture pairs, a dimension that we had not captured explicitly in our recognizer-based feature set.
However, when considering user perception of gestures, effort is important.
Five participants commented on differing levels of effort when comparing pairs within the Stroke Order permutations, while two participants also commented on this issue with the Curviness permutations.
Within each feature set, we asked participants to identify similar and dissimilar gesture pairs.
Many of the responses were predictable, such as all participants finding the slowest vs. fastest pair to be the most dissimilar.
Features for which there was lower agreement included Aspect Ratio, where participants were divided on whether width or height was more important , and Orientation, where participants were again divided, this time between 180 degree differences being the most dissimilar vs. ~90 degree differences .
In terms of Finger Count, 18 participants picked gestures with only a one-finger difference as the most similar pair.
For the most dissimilar pair, most participants selected 1 vs. 4 fingers or 1 vs. 5 fingers.
More interestingly, 3 participants felt moving from 1 finger to 2 fingers was the most distinguishable and another participant expressed that 1 finger is different from any multi-touch gesture.
Perception was also nuanced when the strokes between gestures varied.
Overall, participants were able to brainstorm a reasonable number of gestures both for the open-ended scenario and for more specific actions.
Participants employed two general strategies for generating distinguishable gestures:  varying a single gesture in multiple ways and  considering specific contexts of use .
The think-aloud data and the gestures that were created, however, highlighted the difficulty that participants had in conceiving of gestures beyond those they already use.
Indeed, despite having asked participants to create novel gestures, familiarity was the third most popular reason provided when selecting gestures for the custom sets.
Participants may have been limited in their ability to generate new gestures based on their understanding of the machine recognizer or touch technology's capabilities.
We had expected that users would create gestures beyond the recognizer's classification ability.
We did see examples of this problem, such as using two fingers on different hands vs. two fingers on one hand--a distinction most mainstream touchscreen technology does not support.
However, we had not predicted the opposite effect: that users' conceptions of the recognizer would actually constrain what gestures were created.
To uncover these misconceptions, we had explicitly chosen not to provide participants with gesture recognition feedback; however, future work should examine how the misconceptions change with feedback.
Of the nine gesture features, finger count and stroke count were seen to be the most important for distinguishing between gestures.
In constrast, participants considered features that were likely to be inconsistently interpreted across people to be less distinguishable.
For example, speed was not considered to be a distinguishing feature because one person's "fast" could be different from the next person's.
Interestingly, while Long et al.
Moreover, our results also differ from Wobbrock et al.
This latter difference may reflect the participants' differing levels of touchscreen experience in the studies: almost all of our participants used a touchscreen device daily, whereas Wobbrock et al.
Participants were not completely satisfied with their gesture sets despite being given the opportunity to add new gestures while composing their final sets.
This finding and the need, at least in some contexts, to increase machine recognition accuracy of the customized sets suggest that a mixedinitiative approach  may be useful in supporting enduser gesture creation.
Such approaches have been used for customization of graphical user interface elements  , but not, to our knowledge, for more generative customization tasks such as gesture creation.
One mixed-initiative approach, for example, could be to monitor the gesture customization process and, when an ambiguous gesture is detected, recommend specific modifications that the user can make to improve the gesture.
These recommendations would go one step beyond Long et al.
The recommendations would need to balance distinguishability for the gesture recognizer with Task 3's findings about user preference and perceived ability to manipulate features.
Our results are thus directly applicable: features that users feel they are able to control precisely  should be prioritized over features that are more open to interpretation .
Rather than recommending modifications to gestures that the user creates him/herself, another option is to allow the user to customize by selecting a gesture from a list of candidates that the system provides; again, populating the list of candidates would need to balance predicted user preference and distinguishability for the gesture recognizer.
However, such an approach also needs to consider that participants often could not  generate gestures for arbitrary contexts .
To address this issue, crowdsourcing the candidate gestures  may result in inherently more meaningful gestures than candidates generated by a fully automated system.
Finally, to further inform the design of mixed-initiative support for end-user gesture creation, more nuanced comparison of the differences between user perception and gesture recognizers capabilities may be useful.
As with any single lab study, ours has limitations.
First, the results may have differed had we recruited a different population; our participants were split between technical and non-technical backgrounds, although it should be noted that an equal number of each group considered the gesture recognizer's accuracy without prompting.
Second, while we attempted to cover a variety of actions in the study, a larger investigation or field study may uncover additional factors influencing the gesture creation process.
Third, participants were asked to create gestures that other people may use.
While our findings do have implications for the creation of more highly personalized gestures, future work should more explicitly explore differences between the two scenarios.
While the gesture recognition analysis for Task 3 explores the potential recognition rates for the custom gesture sets, that analysis is also limited by two factors:  users were not provided with recognition feedback for the gestures they created;  the $N recognizer  itself is meant for quick prototyping of shape-based gestural interfaces and is not effective for arbitrary gestures such as two-finger pinchto-zoom.
We chose the $N recognizer because it offers high recognition accuracy with few training examples; however, to effectively support end users in creating arbitrary gestures, future work is needed to identify, and potentially to develop, a more appropriate recognizer.
We have reported on a study to understand how end users create customized gesture sets.
Our results show that when asked to generate distinct gestures and even novel gestures, users tend to focus on the familiar.
In general, participants had misconceptions about the recognizer's abilities that even led them to overly constrain the types of gestures they created.
We also gained insight into the strategies that people use while creating distinct gestures, such as modifying an existing gesture in multiple ways.
Finally, our findings have implications for mixed-initiative approaches to better support end users in customizing gestures.
While our focus has been on touchscreen interaction, the implications should apply to other types of gestural interaction, such as 3D depth sensing.
