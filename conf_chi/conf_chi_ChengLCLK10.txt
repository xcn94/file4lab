This work describes a novel approach to utilizing everyday objects of users as additional, auxiliary, and instant tabletop controllers.
Based on this approach, a prototype platform, called iCon, is developed to explore the possible design.
Field studies and user studies reveal that utilizing everyday objects such as auxiliary input devices might be appropriate under a multi-task scenario.
User studies further demonstrate that daily objects can generally be applied in low precision circumstances, low engagement with selected objects, and medium-to-high frequency of use.
The proposed approach allows users to interact with computers while not altering their original work environments.
Individuals normally use handy everyday objects to explain or represent physical scenarios, e.g., a pencil, as a symbolic representation of traffic conditions.
Board games such as Agricola1 also utilize the instant, available everyday objects for representing insufficient properties.
However, using everyday objects as input controllers might be contentious because each everyday object has its own function, and is not intentionally designed as an input device or a controller, such as a keyboard and mouse, thus calling into question the adequacy of using those everyday objects.
Therefore, this work initially performs two comprehensive field studies to analyze work environments of users to explore the possibilities of the design.
Findings from the field studies indicate that some everyday objects are always on desktops of users with available space.
Therefore, this work presents an easily deployed solution by utilizing original environmental settings of users to setup a webcam as a detector to transform the everyday objects into the controllers by pasting pattern stickers on them, as shown in Figure 1.
Everyday objects may not be designed for input usage, making them inadequate as major input devices, e.g.
However, the naturally constrained affordance of everyday objects makes them suitable as auxiliary input devices.
By exploring possible mappings of gestures of the everyday objects with suitable computer operations in working scenarios, we conclude that everyday objects can be used in a multi-task work environment.
However, binding everyday objects as controllers has its limitations, and not all everyday objects are suitable for arbitrary binding.
Many tangible user interfaces have been developed  since Ishii and Ullmer proposed the notion of Tangible Bits  by utilizing graspable physical objects to bridge the gap between physical environment and cyberspace.
Additionally, advances in surface computing technologies  explain the considerable attention that tabletop interaction has received in research and industrial practice.
Based on the results of Carvey et al.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In sum, this work presents a novel approach to utilizing the everyday objects of users as additional, auxiliary, and instant tabletop controllers.
Based on the proposed prototype platform, iCon application, feasible designs are explored.
Evaluation results of the user studies demonstrate the feasibility of utilizing the bound everyday objects.
The pioneering work of Ullmer and Ishii  developed tangible user interfaces  by utilizing phicons to allow users to manipulate virtual graphical user interface  elements through specifically designed physical objects.
While considering the lack of tactile feedback in a multi-touch table, Weiss et al et al.
However, the specifically designed physical objects might not be easily available compared to the surrounded everyday objects on the tables To utilize everyday objects in a computing environment, RubberShark , WebStickers  and Mir:ror2 attempted to bind shortcuts of information related to the everyday objects.
Based on different technologies, WebStickers  utilizes the barcodes, and Mir:ror uses RFID.
To augment information related to everyday objects, Itiro  designed InfoBinder by using LED lights.
For instance, a telephone can be augmented with a virtual phonebook.
TaPuMa  also utilizes everyday objects of users to display the contextual information.
However, regardless of whether binding shortcut commands or providing augmented information, above works did not utilize the natural affordances of everyday objects to explore the possible design for controlling.
According to Henderson and Feiner , the natural affordances of physical objects can provide passive haptic that eases the gesture input.
That work used augmented reality  technology to change the surface of physical objects as Opportunistic Controls.
A similar work, CamSpace3 , detects the original color and shape of objects to utilize the objects as the game controllers.
However, above works all require that users closely pay heavy attention on the controllers.
Patten and Ishii  also confirmed this point that compared with GUI, through physical space, TUI can provide a spatial relationship to help users to determine what a TUI represented.
This work thus attempted to use everyday objects on a desktop computer with such benefits.
However, according to Grossman et al.
Moreover, keys on the keyboard remain fixed, incapable of moving or rotating.
Nevertheless, although possibly overlapping the design of hotkey, that of iCon does not full cover it.
Despite the convenience in using hotkeys under some scenarios, some operations enhanced by the physical space or the characteristics of physical objects, can be performed by iCon, because iCon can be moved, dragged, and rotated.
Despite performing extremely well under certain scenarios, specifically designed input devices, such as SpaceNavigator4 or Optimus Maximus keyboard5 are not easily available and may be expensive.
Moreover, iCon does not compete with those specifically designed input devices, and in contrast, iCon might collaborate with them.
Before the proposed system was designed, two field studies were conducted to observe the role of everyday objects in the working space of users, and how the users interact with them.
Under practical circumstances, the proposed design can be made more practical.
The first field study collected snapshots of desktops of users to understand what everyday objects are on their tables, and to know how they arrange the objects on the tables.
Although these snapshots can facilitate an analysis of user desktops, exactly how they interact with the everyday objects remains unknown, necessitating on-site interviews to further understand the interaction.
Participants and Method The field study gathered the snapshots of 33 persons  from the desktops of their workspace.
The participants were requested to take two pictures  without rearranging their desktops.
The photos were then analyzed, as well as objects on their desktops counted.
Figure 2 displays one of the pictures.
Following analysis of the gathered photos, the following observations were made.
Findings Surprisingly, analyzing the snapshots of participants revealed no one working with an empty table in their working space.
Among the everyday objects on the tables that surrounded individuals included stationeries, souvenirs, and accessories.
Analysis results further indicated that most participants  tend to place temporary everyday objects, such as cell phones or water bottles, on their non-dominant hand-side of the desktops.
Except for those temporary everyday objects, most everyday objects are placed on the dominant hand-side without frequent movement.
Without intentional arrangement, some space is still found on the non-dominating handside of the working tables of most participants .
Additionally, more than half of the participants  have desk lamps on their tables to help them to work in poorly lit areas or help them concentrate on their work.
This section discusses the general design of the proposed system and guidelines, which is elicited from the field studies.
Our taxonomy of possible gestures is then illustrated, followed by illustration of our principles of control gesture mapping.
Next, the proposed time-to-live  mechanism is introduced, which could prevent most of the conflicts between controls and everyday behaviors.
Finally, feasible scenarios with everyday object-controllers are proposed.
Participants and Method The on-site interviews involved selecting 6 representative participants from the previous 33 participants , including a professor and two students in design school, two students in an engineering school, and a CEO from a mobile service company.
For each participant, an hour interview was performed in his/her own office or working space.
They were asked to illustrate their everyday objects on their working table and were also asked several typical questions, such as how they used their everyday objects, whether those everyday objects had special meanings or purposes, how they organized their everyday objects in their work environments.
Following the questions, the participants were instructed to perform daily habits while arriving or leaving their workplaces under normal circumstances.
Interview contents were photographed and documented.
Findings The interviews have three major findings.
First, most participants  have everyday rituals that they performed regularly when arriving to their work place before starting to work.
For instance, participants would take out some of their belongings, such as wallets and cell phones, from their handbags or pockets and place them on the desktops and then take them away before leaving, ensuring that their desktops would never be empty and always occupied by several everyday objects nearby.
Second, most participants  placed something useful on their working tables.
For instance, they placed their tools, reference materials, or water bottles within hand reach while working.
Third, all participants tended to classify their everyday objects, and placed homogeneous objects together to recall their location and function more easily.
This phenomenon reveals that individuals tend to utilize their spatial memory with their desktop environments.
The design goal attempts to utilize the original setting of user desktops to help them perform computer operations easily.
Although iCon allows users to bind the operations with their everyday objects, the natural defects of the everyday objects still exist.
However, readily available everyday objects are not like an adequately designed mouse or other specific devices that can achieve high-precision control.
The original shape and function of the everyday objects are not designed for control, implying that the shape cannot be adapted to the binding operations in order to achieve a satisfactory control performance.
Moreover, constrained to the affordances of everyday objects, users can perform only restricted gestures with the everyday objects.
Nevertheless, the cons accompany pros.
Owing to the restricted gestures, the iCon system can be easily learned.
Additionally, readily available everyday objects and an inexpensive setting enable the iCon system to perform without location constraints.
According to observations in our field studies, we found that no one's desktop is empty.
Despite the inability use them in high-precision work, everyday objects can still be auxiliary tabletop input gadgets to help users in their work environment.
Therefore, potential designs are explored below considering the merits and limitations of everyday objects.
This section introduces the fundamental controls and gestures that users perform on everyday objects.
The fundamental controls are categorized into binary and circular controls.
The potential mappings are listed for each control.
For instance, clicking is an intuitive and well-known binary control.
Clicking with everyday objects can be done in several ways.
Users can pat the everyday object once , or lift and put down the everyday object within a limited time frame once .
Either way depends on how our system is implemented.
Hence, two possible operations are single-click and double-click.
For binary manipulations, the most suitable mapping is toggle control and single command operation.
Two control types can be combined into one, with the everyday object functioning as compound controls.
For instance, merging play/pause  and previous/next songs operations  does not only make the everyday object desirable to use, but also reduce the number of iCon on a desktop.
Single command operations include bookmarking current webpage, returning to homepage in web browsing, and saving current documents.
For instance, in a photobrowsing scenario, the user can perform multiple zoom-in and zoom-out in consecutive operations.
This subcategory consists of rotation and drag operations.
In rotation, clockwise or counter-clockwise rotation can be performed with everyday objects  & .
The fact that the everyday objects are not designed for the control purpose explains why users are not expected to use everyday objects in order to handle high precision work; this would also be infeasible for users whom perform tasks requiring close engagement with everyday objects.
Consequently, the rotation tick is made every 30  45 degrees, and the users are provided with auditory feedback when each tick is performed.
Drag is another major operation in this subcategory, which includes three types that users can perform with everyday objects: vertical, horizontal, and free  & .
Vertical or horizontal drag is concern only with movement of one-axis, and is relatively easy to perform and recognize.
In contrast, although free drag allows users to drag in any direction, low engagement with the everyday object during operations makes it difficult for the system to recognize user gestures.
Moreover, the more complex free drag requires additional mental effort, explaining why a high tolerance for casual use is required when designing the free drag.
For consecutive operations, appropriate controls can include previous/next songs and volume-up/down in a mu-
The original design for an everyday object is not used in the bound operations.
For instance, a mug is used for drinking.
Hence, for users that utilize a mug as a music play/pause controller, while drinking, they may also trigger the play/pause event, which is an undesired situation.
Therefore, in this work, the notion from the TTL mechanism of computer networking is adopted.
Whereas an iCon controller disappears from the table, the TTL timer is triggered.
A situation in which the iCon controller returns to the table within threshold time t1 is recognized as a simple click; otherwise, no operation is triggered.
Nevertheless, a situation in which the iCon controller leaves the table and exceeds the threshold time t2 fires the iCon controller.
Via this mechanism, this work provides the flexibility of using everyday objects as controllers and also maintaining its original functions.
Inappropriate Unsuitable Bindings iCon focuses on providing supplementary or additional assistance for temporal needs.
Hence, an everyday object as a controller is fuzzy and imprecise, and is inappropriate for high-precision manipulations such as drawing a picture or editing an image or video.
Using a specific-design controller might be preferred under this circumstance.
Moreover, although the TTL mechanism designed in this work can discriminate between the binding operations and the original purpose for use, users might still have the opportunity to unintentionally trigger binding operations.
Therefore, operations that might cause unrecoverable results, e.g., delete operation or e-mail sending, are inappropriate to be bound with the everyday objects.
Finally, a daily object that is readily available and can be easily bound with computer operations occupies a part of user working areas.
Therefore, we recommended that the too-low-frequency-to-use operations are inappropriate for binding with everyday objects, e.g., printscreen and disk-defragment.
Appropriate Scenarios Using iCon in appropriate scenarios can enhance the experience and efficiency of daily tasks.
Possible scenarios are described as follows * Multi-task Scenarios While operating with the computers, we often switch multiple tasks back and forth.
However, the context-switch is distractive and time consuming.
While performing iCon under such circumstances, users do not need to switch their foreground attentions to control background activities.
The benefits of everyday objects are tangible, movable, graspable, and can be sensed with our peripheral awareness.
Hence, carefully mapping the computer controls to gestures of everyday objects based on their natural affordances can provide a more intuitive means of interacting with a computer.
For instance, while binding photo manipulations with everyday objects, users can switch and rotate photographs by simply rotating the bound everyday object, and can also zoom-in/out photographs with push and pull gestures.
Under an eagle-eye view, clicking on a user-defined iCon is similar to clicking a button, by patting on the pattern.
The metaphor is sufficiently intuitive for users to learn how to use.
For the rotation and drag operations, users can simply rotate and drag the defined iCon directly.
The eagle-eye view installation is easily deployed, despite certain restrictions.
First, the detection area is limited and users may not clearly know the exact boundary of the detectable area, thus making some big-move gestures inappropriate.
Second, some everyday objects may be excluded from the usage since the pattern can be detected only when the pattern sticker is on the top-surface of the object.
For instance, mugs without lids can not have the pattern stickers attached to them.
Finally, some gestures may be difficult to perform under an eagle-eye view.
For instance, users performing rotation operations may conceal the pattern while operating, possibly triggering a click event if the iCon is also bound with a binary operation.
Hence, given the above restrictions, another possible implementation, i.e.
Under-Desk Installation Although not all everyday objects have top-surfaces, most everyday objects have bottom-surfaces.
Therefore, the underdesk installation is in contrast with the eagle-eye view one.
The camera view is upwards to detect the patterns on the desktop surface.
To detect the pattern, in this work, the table surface was replaced with a translucent one, and the illuminator was placed under the table to increase the pattern recognition rate.
Figures 4  and  show the setting.
Hence, the fiducials are sufficiently large.
The concept is adopted from the real desktop metaphor and the patterns are treated as coasters.
The operations and gestures are almost the same as the eagleeye view setting, except for the click operation.
While performing the click operation, users must lift their everyday objects slightly off the table surface; the objects are then placed down within a time frame to follow the above-mentioned TTL mechanism.
The metaphor of the click behavior is similar to performing a stamp operation, which is also sufficiently intuitive for users to learn.
With this installation, the entire table surface area is detectable.
Therefore, users do not need to be concerned with whether the performed gestures are within an active region, thus enhancing the experience of users.
Despite two different settings, the fundamental hardware and software solution are similar.
Transforming an everyday object into a user-defined controller requires using a webcam as the detector and attaching a pattern sticker on the everyday object.
Pattern stickers can identify everyday objects, making them detectable and distinguishable.
The reacTIVision library  is selected in this work to detect and recognize pattern stickers by using Amoeba.
Despite their limited number, patterns are sufficient for use in our scenario.
Given the limited area of a user desktop, the number of everyday objects that a user can place on a table is limited as well.
As for the pattern stickers, removable material is chosen to avoid our stickers from damaging everyday objects of users.
Eagle-Eye View Installation According to the field studies, most individuals have lamps on their desktops, allowing us to perform an inexpensive installation by attaching a webcam on the lamp.
Figures 4  and  display the setting.
A Logitech S5500 webcam is used for the prototype, and can provide 15 fps and 1280 x 960 resolution.
Therefore, the webcam is used to discriminate the patterns with the size of 2 x 2 cm2 within the distance of a half meter, which is sufficient under normal desktop circumstances.
Software Architecture Figure 5 shows the overview of the software architecture of the iCon application.
The system needs a pattern detection engine to recognize the movements of everyday objects.
Through the TUIO protocol, the message is passed to the iCon application.
Via the JNA library, the paired operations can be passed to the paired Windows applications.
In doing so, users can control the everyday objects to perform the Windows operations.
Five manipulations provided in the iCon application are PC control, music control, photo browsing, document processing, and web browsing.
Each manipulation provides predefined bound operations for users to choose.
Notably, the users can only bind the active operations.
For instance, the music control operations are only available when a music player is running.
The icon of each operation is well defined to allow users to easily understand how to perform related gestures to issue commands.
A simple example of the binding and releasing operations of music control is illustrated as follows.
First, the user selects an everyday object and, then, pastes the pattern sticker onto it.
The bound iCon is placed in the detecting area, and a colored bubble appears on the GUI to represent the detected position of iCon.
The user then moves the iCon to the desired music operation, and halts for a while.
Thereafter, the selected everyday object can control desktop operations.
Also, the control is easily released from the everyday object by clicking the represented colored bubble on the GUI to confirm the release operation.
The overall procedure is intuitive and easily learned.
This work attempts to determine how everyday objects can be used as tabletop controllers by performing two formal user studies of quantitative and qualitative evaluations, respectively.
During the quantitative evaluation, users are tested in a context-switch scenario; they are then allowed to experience free binding as the qualitative evaluation.
Analysis results indicate that in addition to feeling comfortable with graspable cylinders such as bottles, mugs, or glue sticks, all participants preferred to move everyday objects with a hand without too much resistance or too much efforts, implying that the chosen objects should be easily movable.
Thus, in this work, everyday objects with a cylinder shape are selected for our quantitative evaluation based on the feedback and findings.
Five paid volunteer participants  were recruited to participate in the pilot studies, in which many everyday objects were prepared with different affordances, including boxes, sticks, cylinders with different radii, and flat CD cases.
The setting of the eagle-eye view prototype installation included a desktop PC equipped with an Intel Core 2 Duo 2GHz CPU, 2GB RAM, and a 22 LCD display with 1920 x 1080 resolution.
The setting of the under-desk prototype installation included a laptop PC with an Intel Core 2 Duo 2GHz CPU, 4GB RAM, a Microsoft VX-1000 webcam beneath the table, and a 17 LCD display with 1280x1024 resolution.
Because most participants were familiar with Microsoft Windows OS, both installations were running Microsoft Windows XP.
Additionally, the prepared applications for experimentation were Microsoft Windows Media Player, Google Picasa Picture Viewer, Adobe Acrobat Reader, and Mozilla Firefox.
The participants were asked to perform the task twice, with and without iCon as a supplementary input controller.
The factors were controlled by preparing two articles, A and B, which are of similar length and related content.
To confound the variations, a user was instructed to read article A, followed by article B.
For the next user, the reading order was changed, and so on so forth.
The number of interrupted commands was equal to 12 typed words and 12 music control commands.
The typed words and performed controls were the same in the two reading articles, and the commands were randomly spread in the two articles.
The number of single-choice questions was also the same.
Before the user studies were performed, participants were instructed how to use the iCon controllers and allowed to perform some operations with the iCon controllers with one example document to eliminate unfamiliarity.
The test was started after each participant fully understood the test procedures.
The quantitative test consisted of two rounds: with a mouse and keyboard as input devices and another one with a mouse and keyboard, but also two everyday objects as supplementary input controllers placed around their non-dominant hands.
The bound operations included document page up/down and music play/pause/previous/next.
Finally, the total time required complete the reading in the two rounds was recorded.
Results Figures 7  summarizes the quantitative results.
Hence, using everyday objects as assistant controllers is more efficient than without them while under the context switching scenario.
Above results support our hypothesis.
Discussion The most attention that we consumes while using the conventional desktop computer is by our eyes.
Hence, the ability of users to utilize the affordances of a physical environment and the physical everyday object allows them to offload attention from visual engagement to physical world perceptions in order to perform multi-tasks simultaneously owing to their ability to touch and use a physical object without determining whether it is located near them and they are familiar with its shape.
This finding reveals that everyday objects can be designed as auxiliary input controllers to increase the productivity of users.
In a multi-task scenario, users can retrieve a readily available everyday object and then bind the background operations with it to eliminate the focus switching time, thus allowing them to concentrate on their foreground work.
Moreover, "affordances are normally context dependent".
As Kirsh argued , affordances of an object can be changed by altering its context, implying that an iCon can have at least two affordances in the proposed design.
Twenty two paid participants were recruited through a standard recruitment process.
The participants consisted of 11 males and 11 females, ranging between 22 and 29 years old.
Their varied educational backgrounds included engineering, management, and design.
All participants had used a computer for at least 8 years, and spent at least 5 hours daily on their computers.
Test I - Quantitative Evaluation Hypothesis As is well known, switching between tasks generally interrupts the original work task, causing inefficiency.
Therefore, we believe that the tangible benefit of everyday objects can be utilized by manipulating them as auxiliary controls to help users eliminate the interruption time while working.
Hence, we posit that the overall processing time by using iCon is lower than without using them.
Hypothesis: Using everyday objects as assistant controllers is more efficient than without them while under a context switching scenario.
Experimental Design and Method A multi-task scenario was designed.
In this scenario, the participants were instructed to read an article.
Meanwhile, background music was playing using the Microsoft Windows Media Player on the same computer.
During the evaluation, they were asked to complete the reading.
While reading the article, they randomly received requests to type some words by using the keyboard or control the music player by either the mouse or everyday object.
The commands were randomly spread in the reading document and were marked as different colors for them to easily discriminate between the commands.
The typing and music controlling requests were highlighted as black and red colors, respectively.
To prevent the participants from seeking the commands in the article instead of reading it, single-choice questions were embedded in the article to keep their attention focused on the reading content.
The participants can only continue reading if their answers are correct in those question spots.
The questions do not require the participants to infer anymore.
For instance, a sample question is "Who invented the first desktop system?
All tasks and interviews were recorded and reviewed to elicit the future design of the iCon system.
The questionnaire contained five questions.
In the first two questions, the participants were queried whether multi-task and photo-browsing are appropriate scenarios to have everyday objects involved as the input controllers.
The options range from very agree to very disagree on the Likert 5 scale.
In the third question, the participants were instructed to rate what level of the precision, engagement, and frequency of use, that they thought is appropriate to involve the everyday objects as the controllers in their desktop environments.
Finally, the participants were queried as to why they liked or disliked the iCon application and how the system could be improved.
Results According to Figures 7 , nearly all participants agreed that iCon is appropriate in the multi-task scenario, which correlates with the quantitative results.
With respect to Q2, more than half of them agreed that iCon can be used in photo browsing.
Furthermore, 5 participants emphasized that using iCon in the photo-browsing scenario improves the interactions of peer sharing, allowing many users to perform multiple-controls simultaneously.
Also, 5 of them believed that browsing photographs in such an intuitive way is unique.
However, 5 participants felt frustrated by memorizing different bound operations, and also hoped to have their customized gestures as they want.
Analysis results also demonstrate that the most appropriate circumstances that iCon can be designed with are low precision operations, low engagement with the objects, and medium-high frequency of use.
Above results are reasonable and identical with our design concerns.
Discussion Based on interviews with participants after they experienced the design, useful and encouraging feedback was gathered.
Some participants  felt that iCon is interesting, inexpensive, and easily learned.
Additionally, most participants  enjoyed the ready availability of using the everyday objects as input controllers.
Nevertheless, most of them  requested gesture customization features for the bound controls, while some of them  would prefer that everyday objects interact with each other to provide more possible operations.
The requested are all interesting and applicable, and the possible designs will be tested in the near future.
However, providing a more powerful function with everyday objects, which might further complicate the iCon application, might not be as intuitive an approach to use.
This is obviously a trade-off.
Nevertheless, making iCon more powerful while maintaining its easy-to-use characteristics is a challenging task.
Hence, the placed location might be important to a user who wants to use an everyday object as a controller.
Moreover, the designed control gestures are important, which should not conflict with the gestures of the original usage.
The placed location and the used object can not be determined arbitrarily.
The proposed platform enables users to utilize their environment and adapt to their surrounding daily objects.
We believe that individuals will take advantage of the provided tools with spatial intelligence.
Test II - Qualitative Evaluation Method In the qualitative evaluation, participants were instructed to bring at least three everyday objects from their working space.
They were then encouraged by attempting to bind each operation with the everyday objects that they brought to explore possible mappings.
The participants were then instructed to compare the experiences by browsing photographs with a mouse and keyboard and by using everyday objects instead.
The participants all experienced the possible mappings with their everyday objects in two different settings.
During the testing, they were encouraged to show and freely discuss their ideas.
Given the inability to change the affordances of everyday objects, while selecting the binding objects, a relatively appropriate one must be identified.
Moreover, some everyday objects can not be appropriately bound under any situation, such as a mug with water inside, objects too large , objects to small, valuable objects, fragile objects, objects to heavy, and objects too light.
Fortunately, according to field study observations, several appropriate everyday objects are always on user desktops for instant and temporal use, such as wallets and bottles.
Since the setting heavily relies on the fixed camera to detect the iCons position, the allowed gestures are constrained and limited.
Although possible, the setting would be more complex and no longer an inexpensive solution.
Finally, in contrast with adequately designed or custom designed devices with mechanical knobs or scroller that can provide haptic feed-back, an everyday object is designed for its original purpose rather than transient bound operations.
However, visual and auditory feedback can still be used as compensation.
We also believe that iCon cannot only function as a controller, but also a container to store a temporal task and information .
A previous work demonstrated that the most important function for desktop organization is to enable users to recall their tasks or important things.
Therefore, storing information into everyday objects on the tabletop, and deeming everyday objects as "memos" might be a viable solution to helping users to organize their daily work.
However, prototype testing must be performed to confirm the benefits.
User studies revealed that utilizing everyday objects on the desktop allows users to perform multi-user interaction.
Additionally, controlling different everyday objects on the table enables users to collaborate on a task.
For instance, users may browse photographs together, and one might control the Next/Previous iCon and the other one might control the Zoom iCon.
We believe that the collaborative interactions with iCon should be explored in the future.
As the physical world and virtual reality emerge, we believe that everyday objects and computational devices can form a new ecology.
This work can provide a foundation for intersecting augmented reality, tangible interfaces, and graphical interfaces.
Augmented reality allows us to design several augmented functions to suggest how users can interact with iCon   or allow users to perform gestures in order to augment the functions of iCon .
Nevertheless, comparing everyday objects with well-designed input devices reveals the defects of everyday objects.
However, the merits and limitations are obvious.
Utilizing the defects of everyday objects allows the nearly "useless" everyday objects on the tabletop to become useful auxiliary input devices without altering the original environments of users.
However, to confirm and analyze the possible usages, iCon must be deployed and tested actual work environments of users, as well as observations and subsequent recording made of how users interact with the "transformed" everyday objects.
Everyday objects on the tabletop can be more useful in terms of exploring further usages in the future.
This work has demonstrated the feasibility of utilizing everyday objects as additional, auxiliary, and instant desktop controllers.
Its merits and limitations are compared with those of the proposed iCon system.
The proposed iCon system attempts to assist users in altering everyday objects as auxiliary and transient input devices instantly.
Rather than changing work environments of users, iCon attempts to utilize their original environments.
Among of the benefits of iCon include easiness, tangibility, ability to customize the physical form of the operations, which are interesting and inexpensive.
Especially for customization, iCon allows users to select the objects that they think the most meaningful or most appropriate to themselves.
However, iCon as an auxiliary device has certain limitations and should not be able to compete with other specific design input devices, like a mouse and keyboard.
However, iCon can be combined with them to enhance work productivity of users.
To increase the effectiveness, power and ease of use of iCon, providing more customized features might be an approach, such as user-defined gestures, which could allow users to freely define their gestures and combine them with the selected operations.
The self-designed gestures of users might be more intuitive and can be performed more easily and appropriately with their selected everyday objects.
This is owing to that users can adapt their gestures to their affordances of everyday objects, which could not be easily changed.
Bricks: laying the foundations for graspable user interfaces.
T. Grossman, P. Dragicevic, and R. Balakrishnan.
Strategies for accelerating on-line learning of hotkeys.
Low-cost multi-touch sensing through frustrated total internal reflection.
S. J. Henderson and S. Feiner.
Opportunistic controls: leveraging natural affordances as tangible user interfaces for augmented reality.
S. T. Iqbal and E. Horvitz.
Disruption and recovery of computing tasks: field study, analysis, and directions.
The tangible user interface and its evolution.
H. Ishii and B. Ullmer.
Tangible bits: towards seamless interfaces between people, bits and atoms.
Reality-based interaction: a framework for post-WIMP interfaces.
M. Kaltenbrunner and R. Bencina.
In Proceedings of 2007 International Conference on Tangible and Embedded Interaction, pages 69-74, 2007.
The intelligent use of space.
P. Ljungstrand, J. Redstr om, and L. E. Holmquist.
WebStickers: using physical tokens to access, manage and share bookmarks to the web.
How do people organize their desks?
TaPuMa: tangible public map for information acquirement through the things we carry.
In Proceedings of 2008 International Conference on Ambient Media and Systems, pages 1-5, 2008.
Things that make us smart: defending human attributes in the age of the machine.
Q. Pan, G. Reitmayr, and T. Drummond.
Interactive model reconstruction with user guidance.
In Proceedings of 2009 IEEE International Symposium on Mixed and Augmented Reality, pages 209-210, 2009.
J. Patten and H. Ishii.
A comparison of spatial organization strategies in graphical and tangible user interfaces.
I. Rosenberg and K. Perlin.
The UnMousePad: an interpolating multi-touch force-sensing input pad.
InfoBinder: A pointing device for a virtual desktop system.
N. Streitz, T. Prante, C. M uller-Tomfelde, P. Tandler, and C. Magerkurth.
Roomware c : the second generation.
B. Ullmer and H. Ishii.
The metaDESK: models and prototypes for tangible user interfaces.
SLAP widgets: bridging the gap between virtual and physical controls on tabletops.
Interaction and presentation techniques for shake menus in tangible augmented reality.
In Proceedings of 2009 IEEE International Symposium on Mixed and Augmented Reality, pages 39-48, 2009.
Visual hints for tangible gestures in augmented reality.
In Proceedings of 2007 IEEE International Symposium on Mixed and Augmented Reality, pages 1-4, 2007.
User-defined gestures for surface computing.
