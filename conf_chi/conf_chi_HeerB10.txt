Understanding perception is critical to effective visualization design.
With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation.
In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments.
We replicate previous studies of spatial encoding and luminance contrast and compare our results.
We also conduct new experiments on rectangular area perception  and on chart size and gridline spacing.
Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization design.
Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies.
Crowdsourced experiments may also substantially reduce both the cost and time to result.
Unfortunately, crowdsourcing introduces new concerns to be addressed before it is credible.
Some concerns, such as ecological validity, subject motivation and expertise, apply to any study and have been previously investigated ; others, such as display configuration and viewing environment, are specific to visual perception.
Crowdsourced perception experiments lack control over many experimental conditions, including display type and size, lighting, and subjects' viewing distance and angle.
This loss of control inevitably limits the scope of experiments that reliably can be run.
However, there likely remains a substantial subclass of perception experiments for which crowdsourcing can provide reliable empirical data to inform visualization design.
In this work, we investigate if crowdsourced experiments insensitive to environmental context are an adequate tool for graphical perception research.
We assess the feasibility of using Amazon's Mechanical Turk to evaluate visualizations and then use these methods to gain new insights into visualization design.
We make three primary contributions: * We replicate prior laboratory studies on spatial data encodings and luminance contrast using crowdsourcing techniques.
Our new results match previous work, are consistent with theoretical predictions , and suggest that crowdsourcing is viable for testing graphical perception.
We conduct experiments investigating area judgments, chart size and gridline spacing.
The results provide novel insights for optimizing display parameters.
For example, we find that qualification tasks and verifiable questions help ensure high-quality responses and that experimenters can accelerate the time to results by increasing the compensation level.
Although we focus on evaluating visualizations, we believe these latter results generalize to a variety of crowdsourced studies.
Such services are increasingly attractive as a scalable, lowcost means of conducting user studies.
Micro-task markets lower the cost of recruiting participants, offering researchers almost immediate access to hundreds  of users.
Similarly, by reducing the burden of participation, the subject pool is greatly increased and diversified .
The reduced cost structure of crowdsourced evaluations is particularly attractive in visualization, where the design space of possible visual encodings is large and perceptually interconnected .
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Assessing the impact of visual encodings on graphical perception enables designers to optimize their visualizations and is vital to the design of automatic presentation software .
Inspired by Bertin's  systematic treatment of visual variables, researchers in cartography , statistics , and computer science  have derived perceptually-motivated rankings of the effectiveness of variables such as position, length, area, and color for encoding quantitative data.
Some have further tested their predictions via human subjects experiments.
For example, subjects in Cleveland & McGill's  seminal study were shown charts and asked to compare the values of two marks by estimating what percentage the smaller value was of the larger.
This accuracy measure was then used to test and refine the ranking of visual variables.
Many researchers have applied experimental methods to graphical perception tasks, for example to test differences across chart types , shape discrimination in scatter plots , and the effects of viewing angle and perspective distortion .
These studies measure how an individual visual encoding variable affects the accuracy and/or response time of estimating values of the underlying data.
Researchers have also investigated interactions between visual variables .
Viewers decode separable dimensions such as position and shape largely independently, while perception of integral dimensions such as color hue and saturation are correlated .
For example, a redundant encoding using integral dimensions may incur performance improvements  or deficits .
The interaction of visual variables complicates our characterization of the design space, as extrapolating the results from studies of isolated visual variables is unreliable.
Graphical perception is also affected by other design parameters and data characteristics, including contrast effects , plotting density , and changes to chart size , scale , or aspect ratio .
Such contextual cues need not be purely visual; some studies suggest that environmental context  or textual prompts priming specific visual metaphors  may also affect the decoding of visualized data.
The above considerations reinforce the need for empirical assessment of visualizations to validate theory, replicate prior results, and evaluate real-world applications.
We aim to establish the viability of crowdsourcing as a low-cost adjunct to laboratory experiments.
Moreover, as visualizations become increasingly prominent online , web-based experimentation may improve ecological validity by reaching a diverse population of subjects and display configurations.
In this work, we investigate the viability of crowdsourcing graphical perception experiments.
To do so, we conducted a series of experiments on Amazon's Mechanical Turk , a popular micro-task market.
On MTurk, requesters post jobs  for consideration by a pool of workers colloquially referred to as Turkers.
Each HIT has an associated reward--typically a micro-payment of $0.01 to $0.10--and a set number of assignments--the maximum number of Turkers who can perform the task.
HITs may also require one or more qualifications, such as having 95% or better HIT acceptance or successfully completing a quiz.
Workers discover HITs through a keyword search interface that supports task previews and from which workers can elect to complete any number of tasks.
The requester pays the workers for completed tasks, but retains the ability to reject responses deemed invalid.
At any time MTurk has thousands of active HITs; at the time of writing the number was 97,212.
MTurk provides a convenient labor pool and deployment mechanism for conducting formal experiments.
For a factorial design, each cell of the experiment can be published as an individual HIT and the number of responses per cell can be controlled by throttling the number of assignments.
Qualification tasks may optionally be used to enforce practice trials and careful reading of experimental procedures.
The standard MTurk interface provides a markup language supporting the presentation of text, images, movies, and formbased responses; however, experimenters can include interactive stimuli by serving up their own web pages that are then presented on the MTurk site within an embedded frame.
Recent research has investigated the use of MTurk for crowdsourcing labor, including user studies.
Turker ratings correlated with those of Wikipedia administrators when the tasks included verifiable questions and were designed such that completing them meaningfully is as easy as not.
Mason & Watts  studied the effect of compensation level for image sorting and word puzzle tasks.
They found that raising the reward for each HIT increased the quantity of individual responses but not the quality  of the work performed.
The implication is that paying more results in faster, though not better, results.
Mechanical Turk has also been applied to perception experiments.
They collected 275,000 gauge measurements from 550 Turkers, which they used to evaluate rendering techniques.
Compensation and collection time were not reported, and the study did not validate the use of MTurk via comparison to results collected in a laboratory.
The web is increasingly being used for experimentation and research.
For example, by silently presenting different interfaces to randomized subsets of users, companies study the impact of changes on user behavior through log analysis.
Our second goal was to conduct additional experiments that demonstrate the use of Mechanical Turk for generating new insights.
Our third goal was to analyze data from across our experiments to characterize the use of Mechanical Turk as an experimental platform.
In the following four sections, we describe our experiments and focus on details specific to visualization.
Results of a more general nature are visited in our performance and cost analysis; for example, we delay discussion of response time results.
Our experiments were initially launched with a limited number of assignments  to serve as a pilot.
Upon completion of the trial assignments and verification of the results, the number of assignments was increased.
We first replicated Cleveland & McGill's seminal study  on Mechanical Turk.
Their study was among the first to rank visual variables empirically by their effectiveness for conveying quantitative values.
It also has influenced the design of automated presentation techniques  and been successfully extended by others .
As such, it is a natural experiment to replicate to assess crowdsourcing.
Seven judgment types, each corresponding to a visual encoding  were tested.
The first five correspond to Cleveland & McGill's original position-length experiment; types 1 through 3 use position encoding along a common scale , while 4 and 5 use length encoding.
Type 6 uses angle  and type 7 uses circular area .
Ten charts were constructed at a resolution of 380x380 pixels, for a total of 70 trials .
We mimicked the number, values and aesthetics of the original charts as closely as possible.
For each chart, N=50 subjects were instructed first to identify the smaller of two marked values, and then "make a quick visual judgment" to estimate what percentage the smaller was of the larger.
The first question served broadly to verify responses; only 14 out of 3,481 were incorrect .
Subjects were paid $0.05 per judgment.
To participate in the experiment, subjects first had to complete a qualification test consisting of two labeled example charts and three test charts.
The test questions had the same format as the experiment trials, but with multiple choice rather than free text responses; only one choice was correct, while the others were grossly wrong.
The qualification thus did not filter inaccurate subjects--which would bias the responses--but ensured that subjects understood the instructions.
A pilot run of the experiment omitted this qualification and over 10% of the responses were unusable.
We discuss this observation in more detail later in the paper.
In the original experiment, Cleveland & McGill gave each subject a packet with all fifty charts on individual sheets.
Lengthy tasks are ill-suited to Mechanical Turk; they are more susceptible to "gaming" since the reward is higher, and subjects cannot save drafts, raising the possibility of lost data due to session timeout or connectivity error.
We instead assigned each chart as an individual task.
Since the vast majority  of subjects accepted all tasks in sequence, the experiment adhered to the original within-subjects format.
To analyze responses, we replicated Cleveland & McGill's data exploration, using their log absolute error measure of accuracy: log2 .
We first computed the midmeans of log absolute errors1 for each chart .
The new results are similar  to the originals: the rough shape and ranking of judgment types by accuracy  are preserved, supporting the validity of the crowdsourced study.
Next we computed the log absolute error means and 95% confidence intervals for each judgment type using bootstrapping .
The ranking of types by accuracy is consistent between the two experiments .
Types 1 and 2 are closer in the crowdsourced study; this may be a result of a smaller display mitigating the effect of distance.
Types 4 and 5 are more accurate than in the original study, but position encoding still significantly outperformed length encoding.
We also introduced two new judgment types to evaluate angle and circular area encodings.
By designing judgment types 6 and 7 to adhere to the same format as the others, the results should be more apt for comparison.
Indeed, the new results match expectations: psychophysical theory  predicts area to perform worse than angle, and both to be significantly worse than position.
Theory also suggests that angle should perform worse than length, but the results do not support this.
Cleveland & McGill also did not find angle to perform worse than length, but as stated their position-angle results are not directly comparable to their position-length results.
After successfully replicating Cleveland & McGill's results, we further extended the experiment to more judgment types.
We sought to compare our circular area judgment  results with rectangular area judgments arising in visualizations such as cartograms  and treemaps .
We hypothesized that, on average, subjects would perform similarly to the circular case, but that performance would be impacted by varying the aspect ratios of the compared shapes.
Based on prior results , we were confident that extreme variations in aspect ratio would hamper area judgments.
We also wanted to assess if other differences, such as the presence of additional distracting elements, might bias estimation.
We systematically varied area and proportional difference across replications.
We modified the squarified treemap layout to ensure that the size and aspect ratio of marked rectangles matched exactly across display conditions; other rectangle areas were determined randomly.
As a qualification task, we used multiple-choice versions of two trial stimuli, one for each display condition.
For each trial , we requested N=24 assignments.
We also reduced the reward per HIT to $0.02.
We chose this number in an attempt to match the U.S. national minimum wage .
We again used Cleveland & McGill's proportional judgment task: subjects were asked to identify which of two rectangles  was the smaller and then estimate the percentage the smaller was of the larger by making a "quick visual judgment."
We used a 2  x 9  factorial design with 6 replications for a total of 108 unique trials .
To facilitate comparison across studies, we used Cleveland & McGill's log absolute error measure.
We omitted 16 responses , for which the subject's estimate differed from the true difference by more than 40%.
Midmeans for each display type are included in Figure 3.
We see a dependence on the true proportions: judgments become easier towards the extremes of the scale .
Confidence intervals are shown in Figure 4.
The results confirm our hypothesis that, on average, the accuracy of rectangular area judgments matches that of circular area judgments.
We found a significant  effect of aspect ratio on judgment accuracy, as shown in Figure 5.
Somewhat surprisingly, comparisons of rectangles with aspect ratio 1 exhibited the worst performance, a result robust across both the rectangle and treemap display conditions.
This finding suggests that viewers actually benefit from the inability of a squarified treemap algorithm to perfectly optimize the rectangles to 1:1 aspect ratios.
The result is consistent with the hypothesis that viewers use 1D length comparisons to help estimate area: comparing the lengths of sides as a proxy for area leads to maximal error when comparing squares.
Additional experimentation is needed to form an accurate perceptual model.
We found no significant difference between the rectangle  and treemap  conditions, suggesting that other elements in a treemap display do not interfere with judgment accuracy.
That said, we might extend the study to comprehensively test for interference effects by including rectangles of varying color intensity.
However, as we lack control over subjects' display configuration, we must first establish the reliability of crowdsourced studies involving luminance contrast.
We take up this issue in our next experiment.
We asked users to parameterize the display of chart gridlines drawn over a plotting area.
In task L, we asked subjects, "Adjust the grid so that it is as light as possible while still being usably perceptible."
In task D, we instructed them, "Adjust the grid strength to meet your best judgment of how obvious it can be before it becomes too intrusive and sits in front of the image; some users have called this a `fence'."
As the experiment requires interactivity, we could not use the standard MTurk markup to create our HITs.
Instead, we hosted a Flash application, presented to subjects in an embedded frame.
The interface consisted of a chart display and alpha adjustment controls.
By hosting the task ourselves, we were also able to use custom JavaScript to collect display configuration data, an option unavailable in the standard MTurk interface.
As a qualification task, subjects were asked to adjust a sample display so that the grid was fully transparent  or fully opaque , thereby ensuring that the subject could successfully run our Flash applet and adjust the grid contrast.
We also considered eliciting additional display configuration information , either by asking explicitly or with a calibration task.
While a number of devices for facilitating user-provided perceptual estimates of monitor gamma exist, they are unreliable.
For example, many LCD monitors are direction sensitive, with changes of viewing angle of just a few degrees causing a significant shift in perceived contrast.
However, a rough estimate of gamma can be made using the web browser's "User-Agent" field to infer the operating system: most PC systems use a gamma of 2.2 while Mac OS X  uses 1.8.
The previous experiments examined spatial encodings using black and white images.
We now turn to a different set of perceptual tasks: separation and layering via luminance contrast.
To do so, we replicated an alpha contrast experiment by Stone & Bartram  in which subjects configure the alpha  of scatter plot gridlines across variations of background darkness and plot density.
The experiment seeks to bound the range of acceptable luminance contrast settings for visual reference elements such as gridlines.
The results can inform smart defaults for the presentation of reference elements within display software.
As this experiment involves careful calibration of luminance contrast within visualization displays, a successful replication would help establish the utility of crowd-sourced experiments for a broader range of perception tasks.
We expect monitor display settings and lighting conditions to affect the outcome of this task.
While we lose control over such details when crowdsourcing, we might simultaneously gain a more representative sample of web users' displays: results may exhibit higher variance, but with means suitable for a larger user population.
Accordingly, the goals of this replication were to  compare our crowdsourced results with those gained in the laboratory and  determine which display configuration details we can unobtrusively collect and assess to what degree they impact the results.
Our results corroborate Stone & Bartram's recommendation of alpha = 0.2 as a "safe" default.
We also examined the effect of display configuration on alpha values in task L.  We found a weak positive correlation  = 0.07, p < 0.01 between alpha values and screen resolution .
Thus as the resolution increased, users tended to make the  gridlines slightly darker.
Unsurprisingly, we also found a negative correlation  = -0.176, p < 0.01 between alpha values and monitor color depth : subjects tended to select lighter alphas on displays with greater color resolution, presumably due to better contrast.
The darker alpha values for Mac OS X prior to 10.6  versus other operating systems  are consistent with a more "washed-out" monitor gamma of 1.8, indicating that the User-Agent field provides some predictive power.
We used a 5  x 4  factorial design with 3 replications, resulting in 60 trials  per task.
Figures 6 and 7 illustrate these conditions.
Each plot was sized at 450x300 pixels, and displayed within a frame 700 pixels tall.
The background of the frame was varied with the trial and sized to fill the majority of a standard laptop display.
For each trial, we recorded the alpha value, time to completion, and the subject's screen resolution, color depth, and browser type , as reported by JavaScript.
We posted 60 HITs each for tasks L and D with N=24 assignments.
Our next experiment focuses on a design variable that is difficult to control in a crowdsourced study: visualization size.
While pixel size can easily be varied, the subjects' physical display size, resolution, and viewing distance can not be measured reliably.
Still, by canvassing a diversity of web users, we might determine pixel-based settings to optimize presentation.
Our goal was to assess the use of crowdsourcing for experiments involving variations in chart sizing.
We investigated the effects of chart size and gridline spacing on the accuracy of value comparisons in a chart.
The experiment design was inspired by Heer et al.
However, they did not investigate the effect of further increasing chart height or introducing gridlines.
In this experiment, we sought to determine optimized sizing and spacing parameters for web-based display.
We analyzed 60x24 = 1,440 responses to task L, and 1,126 responses to task D. The missing responses  to task D were due to the expiration of our HITs on MTurk; we describe the reason why later.
Our results are shown in Figure 8, juxtaposed with the results of Stone & Bartram.
Alpha values in task L are higher in our experiment.
Stone & Bartram note surprise at how low their values are; we surmise that crowdsourced results may be more representative of web users than a single laboratory display.
The results confirm our hypothesis that accuracy plateaus as chart heights increase, and suggest little benefit for increasing chart height beyond 80 pixels when using a 0-100 scale.
This size roughly coincides with the point at which the pixel and data resolutions match.
Error increased steeply in charts with a height of 40 pixels and gridline spacing of 10 units.
Presumably the dense packing of gridlines impedes accurate tracing to their labels.
The results suggest that gridlines be separated by at least 8 pixels.
Subjects were instructed to be as accurate as possible while making a "quick visual judgment."
We used a 2  x 3  x 4  factorial design with 3 replications, for a total of 72 trials .
Either a bar chart or a line chart was shown with a height of 40, 80, or 160 pixels; gridlines were drawn at intervals of 10, 20, 50, or 100 units .
Each chart consisted of 12 values distributed on a range of 0-100 units.
Each value was labeled A-L and values D and I were always the compared values .
As a qualification task, we used multiple-choice variants of two trial stimuli: one bar chart and one line chart, each 80 pixels tall.
For each experimental trial, we recorded estimation error as |judged difference - true difference|.
We chose this error measure to facilitate comparison of our results with those of Heer et al.
We requested N=24 assignments and paid $0.02 per HIT.
We subsequently conducted a second experimental run, denoted as 3B.
The extended experiment used chart heights of 160 and 320 for a total of 48 HITs.
We again requested N=24 assignments, but raised the reward to $0.04 per HIT.
A total of 186 different Turkers participated in our experiments.
Experiment 1A was launched in June 2009 as four simultaneously deployed collections of HITs grouped by judgment type.
Participation across HIT groups was highly overlapping: of the 82 Turkers participating, 93%  contributed to multiple HIT groups and over half  contributed to all four.
Experiment 1A consisted of a total of 70 HITs, so completing all HITs in a single session was easily achieved.
The remainder of our experiments launched in September 2009 as five HIT groups, one each for experiments 1B, 2L, 2D, 3A, and 3B.
HIT totals per group ranged from 48 to 108.
These experiments netted 117 subjects.
In our analyses we treat all experiment 1A runs as one group, as they match single HIT groups in the remaining experiments.
Figure 12 shows the cumulative distribution of Turkers by the number of experiments to which they contributed.
Across experiments, 31% of Turkers  contributed to two or more experiments, and 15%  contributed to three or more.
Only 1 Turker participated in all experiments and only 7% of Turkers  who participated in experiment 1A later participated in any of the other studies.
In summary, there was substantial variability in the subject pool across experiments and very little overlap in studies separated by 3 months.
For any given study, an average  1 3 of subjects also participated in another experiment.
We found that the combined use of  qualification tasks to ensure subject understanding, and  clearly worded tasks with verifiable answers, encourages accurate crowdsourced results.
Trial runs of Experiment 1 omitted the qualification task, and over 10% of the responses were unusable.
We attribute this degradation in quality to confusion rather than "gaming" of the system.
The use of verifiable answers  serves to dissuade gaming, as wildly incorrect answers can be rejected outright, stripping Turkers of their pay.
There is little incentive for crafting subtly incorrect answers; one might as well perform the task.
Do Turkers randomly sample tasks across HIT groups, or do they plow through every task in a group?
Given the overhead of learning a new task, it would make economic sense to complete related tasks in batch, and the Mechanical Turk interface facilitates this process.
However, we found that the number of trials completed by a subject varied substantially.
An "average" Turker completed 62 HITs  across all experiments--roughly one full study.
However, as Figure 13 illustrates, the distribution of study completion rates is bi-modal.
The histogram groups Turkers by their average HIT completion rate, which we calculate as the weighted percentage of HITs completed within participating HIT groups.
Thus, if a Turker never participated in experiment 1B, the lack of HITs for that group is not factored into the average.
To analyze participation, we fit the data using Gaussian mixture models.
A three cluster model provides the best fit according to AIC and BIC selection measures.
The model confirms that Turkers cluster around low and high rates of completion.
One cluster centers at a 10% completion rate, representing Turkers who sample only a few HITs in a group.
The other localized cluster centers above 95% and represents Turkers who complete nearly all HITs in a consecutive streak.
It is these "streakers" who do the lion's share of the work: almost half of all trials  were completed by the 52 Turkers with an average completion rate of 95% or higher.
It is difficult to state definitively the implications of these results for study design.
Certainly, these patterns do not result in strict between-subjects or within-subjects designs.
However, in terms of user attention, these results suggest an interesting cross-slice of task behaviors.
Real-world interfaces often have both dedicated and sporadic users, and it is possible that Turker completion patterns reflect similar distinctions.
Further study is needed to evaluate these distinctions and also to assess how participation varies by task.
Although we found crowdsourcing to provide high-quality responses, the standard MTurk interface makes it difficult to collect fine-grained timing data.
In a laboratory setting, we estimate that the trials in our experiments take a few seconds on average.
In our crowdsourced studies, however, the average timing data was significantly higher.
Rather than a few seconds per trial, the median response time was 42s .
We observed a minimum time of 5 seconds, yet many responses took multiple minutes.
There is simply not enough control: it is unclear how much time is due to page loading, scrolling, user inattention, and response submission.
Despite these limitations, significant effects due to time may still be found in the data.
However, due to the inordinately high means and large variation, we forego making any predictions or recommendations based on such results.
If fine-grained timing is needed, experimenters should implement their own task interface and present it in MTurk as an embedded frame.
One option is to maintain the typical micro-task format, but include "ready-set-go" phases at the beginning of each task and record response times using JavaScript.
Another option is to use a "macro-task" format by batching a number of trials into a single HIT with higher compensation.
While such a format might enforce within-subjects participation, pacing, and timing accuracy more similar to a lab study, it violates standard usage.
Further study is needed to assess how such "macro-tasks" impact the performance and scalability of crowdsourced experiments.
Given the variety of completion rates, does the quality of Turker results vary?
Overall, we found the quality of Turkers' responses to be high: rejected outliers constituted only 0.75% of responses.
Though crowdsourced responses exhibited higher variance, our replicated studies  match prior results and imply identical design recommendations.
Note the steeper slope, and thus faster completion rate, for tasks with higher rewards.
One complicating factor is that the low-reward studies were launched on a holiday; however, the pattern holds even if timing values are shifted by one day.
We note that submissions in Experiment 2D lag those of 2L; this resulted in HITs for 2D expiring prior to completion.
We attribute the lag to a naming error: the HIT titles for tasks 2L and 2D included the words "Part 1" and "Part 2", respectively.
Turkers took us at our word and allocated more effort on "Part 1".
Experimenters should take care to avoid such mistakes when studies need not be performed in sequence.
We analyzed the elapsed time from experiment launch to HIT completion across all studies .
Our separate runs of Experiment 3A and 3B-- priced at $0.02 and $0.04 respectively--also allowed us to inspect the affect of reward on result accuracy.
We analyzed HITs with a chart height of 160 pixels, which we intentionally overlapped across runs.
On the other hand, reward did not affect the time spent completing an individual HIT  = 0.08, p = 0.778, only the total rate of HIT completion.
The difference does not alter the design implications of experiment 3.
Our results corroborate those of Mason & Watts : paying more does not substantially affect the quality of results, but does increase the rate of HIT completion.
By raising the reward, experimenters can decrease the time to results.
The results from Mechanical Turk demonstrate that crowdsourced graphical perception studies can be viable.
We successfully replicated prior experiments on proportional judgments of spatial encodings  and alpha contrast adjustment of chart gridlines , with our crowdsourced results providing a good match and identical design guidelines to prior work.
The increased variation of our results compared to previous results may be compensated by the platform's scalability: for the same cost, many more subjects can participate.
We also found that operating system and monitor details reported by JavaScript, though supporting only incomplete and approximate inference of subjects' display configuration, can be predictive of results and so should be recorded if possible.
The results also demonstrate the use of Mechanical Turk to gain new insights into visualization design.
Our rectangular area judgment experiment  revealed that comparison of rectangles with aspect ratios of 1 led to higher estimation error than other aspect ratio combinations.
This result suggests that the "squarified" optimization objective of leading treemap algorithms  may rest on tenuous perceptual footing, and that viewers benefit from the inability of the algorithm to achieve its objective.
Future work may lead to improved layout algorithms.
Our chart height and gridline spacing experiment  suggests optimized parameters for displaying charts on the web: gridlines should be spaced at least 8 pixels apart and increasing chart heights beyond 80 pixels provides little accuracy benefit on a 0-100 scale.
Our results help characterize the use of Mechanical Turk for conducting web-based experiments.
Experimenters can expect significant subject overlap when running simultaneous studies, and unreliable response times when using the standard HIT interface.
By using qualification tasks and verifiable questions, one can increase the likelihood of highquality responses.
As higher rewards led to faster completion rates with little substantive difference in response quality, experimenters can use payment level to influence study completion time.
To facilitate replication, we recommend that experimenters describe qualification tasks and compensation rate when publishing the results of crowdsourced studies.
Finally, we identified benefits for crowdsourcing over laboratory experiments.
We found that crowdsourcing can provide up to an order of magnitude cost reduction.
Such savings could be reinvested in more subjects or more conditions.
For constant dollars, we might run better experiments.
We realized a faster time to completion.
This is separate from cost and can also be used to enrich experimental design, especially when experiments are run in stages.
We can also gain access to wider populations .
Many experiments are done on college undergraduates due to the difficulty of recruiting wider populations.
We believe crowdsourcing will be particularly useful in combination with other methods.
There is something wrong with every methodological technique, which can often be compensated by combining techniques.
Small-scale traditional laboratory experiments can be paired with Mechanical Turk experiments with overlapped conditions.
In this way the results of laboratory experiments and crowdsourced exper-
The total expenditure for our crowdsourced experiments was $367.77.
Had we instead run five laboratory studies , using the same number of subjects  and paying a typical compensation of $15, the cost would have been $2,190.
Thus our crowdsourced studies realized a cost savings factor of 6.
Had we run all crowdsourced experiments with a $0.02 reward, this increases to a factor of 9 and thus order of magnitude savings are possible.
However, experimenters should also consider the equitable treatment of Turkers.
Our own misestimation of the average response time led us to compensate Turkers at decidedly less than minimum wage.
Crowdsourcing also provides opportunities beyond simple cost-cutting.
Mechanical Turk largely eliminates recruiting effort, makes it easy to extend or modify a study, and automates administration.
Moreover, crowdsourcing can scale to large samples that would otherwise be prohibitively large , greatly expanding the space of feasible study designs.
Future research is needed to develop better tools for crowdsourced experimentation.
The facilities for conducting user studies on Mechanical Turk are still rudimentary.
Dynamic task generation and easier access control would help researchers conduct adaptive studies, enforce between-subjects designs, and prevent subject overlap across experiments.
Already, tools such as Turkit  are being developed to close this gap.
We believe these tools have an important role to play beyond simplifying study administration.
By collecting and aggregating statistics of Turker performance, these tools might provide a means of tracking a dynamic market place, helping researchers make more informed estimates of participation, time to completion, and appropriate compensation.
By integrating crowdsourcing tools with web-based experiment design tools , an entire class of user studies may be subject to cheap, scalable web-based design and deployment.
Moreover, by archiving and disseminating HIT definitions, such tools might also greatly facilitate study replication, comparison, or modification.
In this spirit, all materials used for the studies in this paper can be downloaded from http://hci.stanford.edu/gp/chi10.zip.
Of course, crowdsourcing is far from a panacea.
Some studies, particularly those dependent on physical or environmental context  are simply ill-suited to the web.
Crowdsourcing results might also be insensitive to factors such as color blindness or limited visual acuity.
Despite these limitations, it is clear that crowdsourcing offers a cheap and scalable way to conduct a valuable range of graphical perception experiments.
The time is ripe for investigating more subtle aspects of visualization design.
