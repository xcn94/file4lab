A large body of work asserts that interactive tabletops are well suited for group work, and numerous studies have examined these devices in educational contexts.
However, few of the described systems support simulations for collaborative learning, and none of them explicitly address immersion.
We present SimMed, a system allowing medical students to collaboratively diagnose and treat a virtual patient using an interactive tabletop.
The hybrid user interface combines elements of virtual reality with multitouch input.
The paper delineates the development process of the system and rationale behind a range of interface design decisions.
Thereby, the role of realism in gaining procedural knowledge is discussed - in particular, the interplay between realism, immersion and training goals.
We implemented several medical test cases and evaluated our approach with a user study that suggests the great potential of the system.
Results show a high level of immersion, cooperation and engagement by the students.
New didactic methods include learning in small groups based on complex medical scenarios  as well as simulations using mannequins or actors as patients.
Full-scale mannequins of patients can be used to simulate medical scenarios.
In contrast to the classic apprenticeship model, the environment is safe and risks to patients are avoided.
Scenarios are immersive and not limited to the diseases of actual patients currently available.
However, full-scale simulators are very expensive, often costing well in excess of e100.000.
This severely limits the time that students can spend with them.
We developed the SimMed system for medical education in an effort to overcome these limitations.
By using an interactive multitouch tabletop to display a simulated patient , we have created an immersive environment which supports a large variety of learning scenarios with substan-
Medical education is a complex field with some unique challenges.
Traditional medical education relies heavily on lectures to teach factual knowledge.
In the traditional model, practical skills are later learned by watching professionals and treating patients under supervision.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Realtime diagnosis and treatment of the virtual patient is possible using simulated instruments operated by multitouch interaction.
The simulated patient can show skin changes and be animated to show realistic bodily and facial movement.
By its nature, the setup allows scenarios to be repeated easily and to be changed and configured dynamically.
SimMed is a hybrid system: Similar to mannequins and full virtual reality  systems, it's goal is a realtime learning experience that is immersive and supports acquiring procedural knowledge.
In contrast to full VR systems, the level of realism is limited.
On the output side, rendering quality and perspective is limited by the medium used.
On the input side, the system only allows touch input on a flat surface and does not provide haptic feedback.
However, the laboratory study with 18 medical students we present shows that SimMed achieves the goals of providing an immersive, highly engaging group learning experience despite these limitations.
We report on the mechanisms used to achieve this: On the one hand, a clear theoretical foundation, on the other hand, the highly interdisciplinary and iterative development process leading to several carefully chosen interface design decisions.
The remaining paper is structured as follows.
After providing a background on medical education and looking at related work, we discuss design considerations and outline the underlying interdisciplinary development process.
The SimMed system is introduced in the subsequent section, followed by an account of its realization and a description of the conducted user study.
The paper is concluded with a discussion and an outlook to future work.
Cases are largely text-based, sometimes with additional audio and video material that shows real patients.
Learning is not team-based and the environment is not immersive.
A review of virtual patient studies concludes that "effects in comparison with non-computer instruction are on average small" .
In contrast, problem-based learning  is a group learning technique that uses realistic cases from medical practice as basis for student-centered learning.
The students identify what facts they need to learn to 'solve' a case in an initial meeting, learn the facts using external sources and meet again to discuss results .
PBL has been shown to increase student engagement and help foster essential social and problem solving skills .
Often, paper sketches of patients are used as learning aids, causing the line between PBL and simulationbased learning to blur.
However, procedural and sensorimotor skills are not the primary focus of PBL.
Simulation, by its very nature, comes closest to the actual practice of medicine.
In addition to full-scale mannequins of the human body, we can distinguish: * Part-task trainers: Simulators of body parts that allow the practice of specific sensorimotor skills.
These vary in complexity from the aforementioned orange to complex surgery simulations.
These are visually immersive but inherently limited in their support for team interaction.
Gaba provides an extensive taxonomy of simulation used in medical education in .
A variety of teaching methods are used in medical education.
Besides the traditional lectures and apprenticeship methods, medical schools are increasingly using software training programs, PBL and a wide variety of simulators to educate students.
These teaching methods cover different learning goals, including : * Declarative knowledge: Knowledge of facts, e.g.
In general, declarative knowledge is not sufficient to treat a patient.
One cost-effective method of teaching procedural knowledge is the use of virtual patients.
The first mannequins built specifically for medical education appeared in the early 1960s.
They were simple puppets that supported training in artificial respiration .
Vital signs can be monitored using replicated medical equipment.
Simulator sessions are generally led by competent medical professionals with teaching and acting skills .
The instructor is responsible for explaining the setting and conducting a meaningful debriefing.
She is also responsible for enhancing the setting by simulating everything that the mannequin is not capable of.
This includes comments on the state of the patient, speaking with the patient's voice, playing additional people, and even moving the mannequin to simulate cramps, seizures or other muscle activity.
Also, teamwork can be trained well .
However, the costs involved are very high: In addition to the costs of the simulator itself, complete rooms full of medical equipment and a competent instructor must be available.
Dieckmann's investigations of the two concepts of realism and simulation competence as they apply to mannequin-based medical education  provide a theoretical foundation on which we can base the design of immersive applications for procedural learning.
Dieckmann makes the point that realism is not a goal in itself: "The purpose of a simulation scenario is to create an experience episode from which participants can learn.
It is irrelevant whether the scenario is realistic as long as it serves this function" .
He concludes that the aspects that need to be realistic depend on the specific learning goal.
Following his reasoning, the simulation needs to provide realistic haptic feedback only if sensorimotor skills are the learning objective.
In the case of procedural learning, "the procedure needs to be performed and trained as it would be in clinical practice" .
He further points out that there are a number of positive aspects of departures from realism: For instance, simulations can dilate or accelerate time or omit irrelevant aspects of a situation.
In fact, the whole point of a simulation is that it is much safer than the real situation .
Dieckmann further introduces the concept of simulation competence.
Further, a study by Antle et al.
The presented system supports experiential and constructivist learning; not surprisingly, many of the observations are similar to ours.
However, there are significant differences in the target audiences  and subject matters .
Only a few studies have focused on adult learning with tabletops .
Of these, Shaer et al.
None of the prior work focuses on providing an immersive learning experience.
Moreover, we did not find any tabletop system that has procedural learning as an educational goal.
In designing SimMed, we applied Dieckmann's theories on realism - originally developed for mannequin-based simulation - to a software program running on a tabletop.
Our hypothesis was that immersion and successful acquisition of procedural knowledge could be achieved without full haptic or VR-level visual realism if the semantics of actions and procedures were preserved.
Research questions to be answered during development were therefore: How is immersion and learning affected by these limitations?
In particular: What level of rendering quality is needed to produce the learning effects desired?
What user interface aspects have an effect on immersion?
What mechanisms can we use to support collaboration and group work?
Our goal was to create a very cost-effective alternative to full scale mannequin-based simulation in many scenarios.
In addition, the system should easily simulate many additional aspects of the situation: Skin images and images of the insides of the mouth and ear of a patient should be integrated, body movements and facial expressions simulated.
External medical equipment should be simulated as well, obviating the need for additional gear.
In contrast to first-person VR, this approach should also preserve the team-building aspects of the situation .
Numerous studies have found that tabletops support collaboration  and in particular collaborative learning very well.
A great majority of these studies has focused on either children's learning in the classroom  or museum settings .
The study is interesting in our context because some of the concepts are similar to our system.
There is a strong focus on triggering externalization of thinking, and the use of planning and reflection phases corresponds with SimMed's design as well.
SimMed was built by an interdisciplinary group composed of medical doctors, psychologists, software developers, interaction designers and 3D artists.
The general vision of the project was clear to everyone from the outset.
However, we quickly found that the diverse backgrounds hindered mutual understanding and that it would be essential to build common ground upon which discussion could take place.
For this reason, software developers and interaction designers spent several days accompanying doctors in an emergency ward as well as observing simulation sessions.
We also adopted a highly iterative development methodology.
To foster discussion and conduct initial verification of user interface concepts, we used life-sized paper prototypes and scripted UI mockups.
Early paper prototypes did not include UI elements.
Instead, they were simple paper replicas of elements present in the situation to be simulated .
Domain experts could use these as explanatory aids: Since the goal was to simulate the procedures involved, we asked physicians to perform the tasks using the prototypes.
We developed UI concepts based on what we had seen, scripted mockups that would run on the interactive table and did adhoc user observations in quick iterations.
In accordance with the envisioned hybrid approach, we sought abstractions appropriate to a touch device while preserving the sequence and semantics of the actions performed.
As we progressed, some elements were changed repeatedly and drastically - see figure 3 for an example.
One other interesting phenomenon in initial iterations was confusion between symptoms of the patient and simulation or rendering artifacts.
In one early prototype, the patient image was bald, simply because the 3D artist had not finished that part of the model .
This immediately prompted reactions of the form: "Does she have Leukemia?".
This phenomenon disappeared in the course of the project as UI designers and 3D artists learned to anticipate medical perception.
By the end of this phase, the mockups had evolved to a UI prototype  that allowed for a sufficient set of examinations, including taking a blood sample, to be performed using a static 2D image of a healthy patient .
The SimMed system consists of a 50" interactive tabletop with an additional 40" vertical screen.
At the heart of SimMed is the simulated virtual patient .
A group of three to five students, assisted by a tutor, is given the task to diagnose and treat the patient.
The configuration of people around the table is predefined: Students stand on the sides of the table while the tutor stands at the head, opposite the vertical screen .
The main goals are to teach procedural skills  and teamwork.
In addition to the main phase that involves the virtual patient, the application supports an introductory anamnesis video as well as a planning and a debriefing phase.
Following the introductory video, users can place notes with action items on a timeline in the planning phase.
In the debriefing phase, a screencast of the user actions during the simulation is shown on the secondary screen.
Playback of the screencast is controlled by the tutor, thus allowing her to precisely go through the actions of the students and comment accordingly.
The planning and debriefing phases were inspired by PBL, where similar concepts are used to help externalization.
In addition to numerous informal tests with prospective users and verification of the medical correctness with clinical experts, we conducted a formal videotaped study using the UI prototype mentioned above .
In the study, students were asked to individually use SimMed to conduct an examination and explain what they were seeing and doing .
They were asked to describe the UI elements and patient images as well as the inferences on patient state.
None of the students had prior experience with the system.
The videos were evaluated, recurring usability issues were quantified and appropriate changes to the interface made.
Following the study, the next iteration within the system development cycle was started.
A second vertical monitor was added to solve perspective issues, and over time, the focus shifted from building a general medical simulation to implementing specific scenarios.
The following section will introduce the finally developed system and its user interface design as well as describe how immersion was achieved.
SimMed allows the students to carry out a wide variety of diagnostic steps and treatments.
Temperature can be measured, blood pressure and pulse taken, auscultation  performed; skin, ear, mouth and eyes can be examined.
A medical monitor can be attached that continuously displays the patient's vital signs .
Students can take blood and mucus samples in several varieties and send them away for testing.
In addition, the patient can be treated using various medicaments, e.g.
Available medications and dosages depend on the simulated scenario.
Note that there is a large potential for conflicts in applying the first and second principles, in particular, when 2D UI elements interact with the 3D model.
One of the major challenges of the project was finding creative ways to resolve these conflicts; our solutions are detailed below.
Initially, students only see the patient and a variety of dock elements on the interactive surface .
These dock elements represent items in an idle state and can be dragged along the side of the table.
There were two reasons for this setup.
On one hand, it forces a certain level of organization in the table setup and keeps the center  relatively unobscured.
On the other hand, we wanted to enforce cooperation to a certain extent - we assumed students would have to ask to obtain access to a dock element in another's personal zone.
Doctors examine a patient in a variety of ways, for instance, by feeling pulse and applying pressure on the skin.
In keeping with our principle to mimic real actions wherever possible, users should touch the appropriate spot on the patient to initiate these examinations.
However, we have a classic modal input issue here: How should the user indicate the examination to be performed?
The table has no user recognition, so selecting an examination mode 
We solved this issue using dragging menus : A touch on the patient opens a context menu.
Items can be dragged from the menu and initiate actions when certain active points  are reached.
For example, there are active points for pulse at the wrists and neck of the patient.
In this way, the input mode is simply attached to the dragging finger.
The action is is initiated on proximity to the active point and immediately stopped when the finger is released.
Dragging menus and the corresponding actions are transient and short-lived: The menu disappears as soon as anything is dragged out, and the complete action is terminated when the dragging finger is removed from the table.
Active points are not indicated on the user interface if the location can be inferred using basic medical knowledge.
For instance, students should know where they can feel the pulse, so they need to drag the icon to the active point before there is a reaction.
We applied this concept of active points to the variety of instruments used in examination and treatment as well.
In the simplest case, an instrument is dragged from its resting place  to an active point.
Conceptually, SimMed consists of a base system and a variety of scenarios that simulate different patients and clinical conditions.
Items that can be configured on a per-scenario basis include detail images and videos, vital signs, skin texture and animations, as well as available treatments.
We implemented three scenarios in the course of the project.
In the current research prototype, all scenarios are based on a simulated 18-month old child.
Scenario themes were chosen based on educational and project-related goals: * Healthy: A child without any illness, initially implemented to test user interface concepts.
Students can practice standard examinations using this scenario.
We also use it to introduce the SimMed system and UI concepts to new users.
The patient arrives in an emergency room with reduced vital signs due to internal bleeding.
Students must quickly initiate correct action to avoid permanent disabilities or loss of life.
Medical conditions and treatments given can change the patient constitution in the course of the simulation.
We implemented simplistic scenario-specific vital signs simulations that ensure the corresponding reactions.
Our design of the SimMed user interface was based on several principles.
First and foremost, we used Dieckmann's theory that "salient characteristics of the task to be trained should be preserved" as a basis.
In our case, that meant basing UI actions on the corresponding clinical actions as much as feasible.
Second, we based many aspects of the user interface on well-known 2D touch interface principles to avoid long training times.
Among the elements used are conventional buttons, lists boxes and drop-down lists, all of which are used like the iPhone and/or Android elements familiar to most medical students.
Items representing pieces of paper  can be dragged and rotated using standard two-finger multitouch gestures and thus also fall into this category.
A third principle was that there would be no global changes of UI mode during the simulation; concur-
There is a significant difference to the dragging menu-based examinations here: The simple examinations are activated by proximity to an active point.
In contrast, instrument-based actions are activated on release and deactivated by dragging them away.
Instrument UI feedback takes into account the hybrid 2D/3D design of the system: While idle or being dragged, a 2D icon of the instrument is used as interface element.
When activated, we switch to a 3D model that is attached to the patient.
This smooth transition from an abstract icon to a realistic rendering of an instrument is depicted in figure 7.
This simple principle applies to all instruments we implemented; there are around twenty.
Interactions with instruments can be combined to form complex, realistic procedures.
For instance, to take a blood sample, a sequence of five instruments needs to be used: The vein is not accessible before a tourniquet has been applied, etc.
Again, this preserves the procedural aspects of the action while ignoring the physical aspects - finding and accessing a vein is a complex sensorimotoric skill that needs to be trained using other methods, but the sequence of actions is something that can be trained very well using SimMed.
A real blood test culminates in filling out a complicated form to choose from more than a hundred possible individual tests.
Cost and speed requirements make it important to select only the pertinent items.
The form, along with one or several vials of blood, is then sent to the laboratory.
Since displaying it in its entirety would have completely filled the screen, we substituted a two-level menu in its place.
The tutor has access to several additional buttons that allow her to switch to a different application phase or pause the simulation.
This is an exception to the 'no global mode changes' principle postulated above and reflects the tutor's role as mediator of the session.
In a lo-fi substitute to real user recognition, we placed this tutor menu in the personal area of the tutor.
The expectation was that social protocols would prevent others from accessing the menu .
The second monitor also plays an important role in this context.
It can display elements that would be out of place on the table.
Members of the patient's family or simulated staff as well as wall-mounted items like a medical monitor fall into this category.
These elements are displayed life-sized and in correct perspective, thus enhancing the effect of immersion.
The general idea to do this was taken from Ryall et al.
The anatomy has a high degree of realism.
In addition, we incorporated realistic body and facial animations.
Textures and animations are scenario-specific and convey information about the situation at hand.
They change in accordance with changes in the patient's vital signs .
For instance, we slowly close the patient's eyes and open her mouth as blood pressure decreases.
On the other hand, the patient skin is not rendered with maximum realism.
To determine if rendering improvements would have an effect, we showed different images to physicians.
The images showed life-sized patients with a skin rash on the table.
We asked for discernible symptoms using the following images: * Photo: A photo as baseline for the maximum achievable rendering quality, * Simple rendering: A 3D rendering using realtime off-theshelf methods  and * Toon rendering: A 3D rendering using a cel shader, mimicking anatomy textbooks that use drawings of salient features instead of realistic images.
The result was clear: First, the toon rendering was perceived as distracting and unrealistic.
Second, the only clearly discernable information in all three cases is that there is a rash - physicians use a second close examination to determine the type of rash.
We placed a high priority on generating an immersive environment.
A large part of this emphasis manifests itself in the interaction principles detailed above - like the seamless and immediate touch measurements by using the dragging menu or the smooth transitions in the rendering of applied tools.
In addition, there are hardware setup and display aspects of the system that we believe have a significant effect.
On the hardware side, it appears that one of the most important aspects is the ability to display a life-sized patient.
Instead, we added the ability to view a detail photo in a focus lens.
The result can be seen in figure 9.
Subtle visual clues are essential as aids in diagnosis in many other cases as well.
The appropriate actions are initiated using the aforementioned dragging menus and instruments.
However, there is another issue: Avoiding global mode changes means that the rendered patient is fixed in perspective.
Detail is limited by the display resolution, but at this point, the users need to see a very realistic image.
Displaying the detailed images in the form of 2D photo and video insets  solves this in an elegant way and makes it very easy to change the media depending on the needs of the scenario.
After initial discussion and informal user tests, we viewed this departure from realism as positive, since it ensures that discussions are based on common understanding, thus fostering teamwork.
A special case occurs in the meningococcemia scenario.
Here, the skin changes in the course of the scenario.
Initially very small spots  grow significantly, indicating fast progression of the infection.
Since the change take place within 5-10 minutes, it is however often missed by physicians occupied otherwise.
We wanted to recreate the potential for this dangerous type of oversight in the simulation.
Hence, while the spots are not realistic, the growth is recognizable.
The detail images reflect the changes as well.
A custom libavg plugin written in C++ integrates the 3D scene into libavg's 2D scene graph.
The plugin provides a highlevel Python interface to load and display models and set camera positions and lights.
It also allows control of poses and animations.
Another advantage of this setup is that we have a clear separation of 3D model, individually configurable aspects of the model and vital signs.
This allows the system to scale: once a suitable library of animations and poses is present, we can mix and match them to quickly support new, realistic scenarios.
To learn more about the effectiveness and the use of SimMed, we conducted an observational study in a laboratory setting.
Among others, our goals were to gain insights in the learning effects involved and to verify our thoughts on immersion and realism.
The study involved five teams with a total of 18 medical students in the sixth semester.
Meningococcemia was used as scenario.
Each team was given a hands-on introduction to using SimMed with our healthy child scenario.
They then went through the cycle of SimMed phases  twice, giving them an opportunity to act on insights in the second iteration.
Total time for the study was 45 minutes per team.
All students had extensive experience with PBL and group work; the curriculum in that semester had included lectures on meningococcemia in children.
It could therefore be assumed that the students had declarative knowledge concerning symptoms and required treatment of this disease.
All participants signed an informed consent and ethical clearance to conduct the study was gained from the ethics committee of the Charit e. Tutors were present but only intervened in the case of major user interface issues.
We also took notes during the study itself.
A second quantitative study to determine the learning effect was conducted in parallel.
The results will be reported separately.
SimMed runs on an off-the-shelf Core i7 920 computer with a mid-range NVidia GeForce 260 GTX graphics card, using Ubuntu as operating system.
The table is a direct illumination-based  Archimedes Session Desk.
All application-level logic is scripted in Python.
In evaluating the four hours of video material, we recorded a number of events for each group.
One person analyzed system usage, verbal comments and other actions of the participants.
Other research team members provided informal plausibility checking based on direct observations of the users.
They also assisted in reviewing critical parts of the videos.
Based on the goals of the study, we defined four event categories as basis for analysing the videos: Diagnosis and Treatment, Immersion, User Interface Issues and Social Aspects.
Events assigned to the categories immersion and user interface issues helped us approach the research question: 'How is immersion affected by the limitations of the touch user interface?'
Since SimMed is meant to facilitate group learning, we were also interested in looking at the social aspects of the users' interactions.
In particular, there are a number of features in the system designed to support collaboration.
The first event category - diagnosis and treatment - included the time of important diagnostic steps and all treatments given for both iterations.
In addition, we determined an ideal sequence of actions by asking two experienced pediatricians to use SimMed and questioning them as they went through the scenario.
During the scenario, participants were expected to act the part of medical personnel in an emergency.
As a measure of immersion, we recorded instances where participants' verbalizations did not reflect this.
This included talking about the user interface  and generally making any comment not within the scope of the simulated situation.
Instances of interesting role-playing were also noted.
With regard to the level of immersion attained, we observed high engagement by all groups.
Well over 90% of the verbalizations made by the students were in the context of the simulated situation.
Most instances of verbalizations outside of this context were related to resolving UI issues.
In some instances, even major UI issues were resolved in the context of the simulation.
As an example, consider one situation where an IV cannula was removed by a UI malfunction.
The following conversation followed: IV cannula removed, patient is bleeding slightly.
Oh my, I've pulled the cannula out.
B: Here, I've got a swab.
To C: Can you place a new one on the other side?
C proceeds to place a new IV cannula, while B cleans up the old spot and places a bandaid.
There were frequent emotional responses to the patient, most often at the start of a scenario .
In particular, differences in face and body pose between the different scenarios were noticed and commented upon, so we can assume that this was relevant for immersion.
We did not observe issues regarding the less-than-realistic skin rendering we adopted.
In the given scenario, the realistic detail images were essential in determining the diagnosis and referred to multiple times by most groups.
In particular, all groups eventually noticed the simulated growth of skin spots .
A positive sign was that it caused uncertainty: "Is it me or is the skin getting worse?"
In the first iteration, four of five groups recognized the disease and saved the patient.
The last group failed to recognize the disease during the scenario but made the correct diagnosis in the debriefing.
This group then proceeded to save the patient in the second iteration.
We compared the recorded diagnosis and treatment data with the aforementioned ideal sequence of actions.
This revealed clear shortcomings of most groups' actions in the first iteration that disappeared in the second iteration.
The model sequence is as follows: Meningococcemia is hinted at immediately by the general bad state of the patient and the skin rash.
Attaching a medical monitor confirms low blood pressure and a very fast heart beat, while a skin examination shows petechiae, making the diagnosis very probable.
At this point, the priority should be to stabilize the patient by intravenously applying fluids.
This prevents the blood pressure from dropping to dangerously low levels.
Only after stabilization are additional tests done and is medication administered to confirm the suspicion and actually cure the disease.
In an ideal case, there is a delay of about one minute until the first IV fluids are applied.
The study groups took between four and nine minutes in the first iteration, with one group not applying fluids at all.
More importantly, the video recordings show that the priorities were not clear.
A majority of groups wanted a clear diagnosis before acting, and when they did act, most groups misjudged the dosis by a large factor and gave insufficient fluids.
The consequences - decreasing blood pressure and possible permanent disabilities - were noticed too late.
In the second iteration, all groups had reevaluated priorities and stabilized much earlier.
Even the slowest group applied IV fluids within less than two minutes of scenario start.
This is a prime instance of learning through experience, as there was no tutor involvement in any of the cases.
Due to the iterative development process and the formative study performed during development, we had a high confidence in the performance of the individual UI elements.
This was largely verified in the user study: After introduction to the system and practice with the healthy child scenario, only a few UI issues remained.
Even complex procedures like blood tests or placing of IV lines were handled well in the large majority of cases.
Still, we observed some interesting behavior in using the interactive elements: * The ability to drag dock elements - introduced in an effort to help communication - was frequently ignored.
Users often reached over to use elements they needed .
In two groups, they even periodically switched places when things got too awkward physically.
Our observations did not confirm this fear; the distinction seemed clear in practice.
Observations confirmed that searches in the menus took a comparatively long time and often occupied more than one person.
This menu had been added well after the formative study, so we did not have a sufficient number of iterations to improve it.
We also observed several cases of low-key conflict resolution through simply working in parallel.
In one case, a team member realized the severity of the situation before the others and tried to convince them to interrupt diagnostic work and give medication.
When this was largely ignored, she proceeded to act, constantly keeping the others aware of her status and seeking confirmation .
The other two team members continued diagnostic work for several minutes before joining the first member again.
In general, the groups worked very well together socially.
The video review shows that all students were involved verbally and through interactions; no major inequalities of participation were visible.
We had feared fewer opportunities for interaction for the students at the foot end of the child - this was not apparent either.
The UI support for interactions in parallel  was fully used and proved essential to collaboration.
Many groups had phases where two or three different interactions were being carried out at once .
In contrast to other reports , personal areas were largely ignored.
Reaching across other people to touch a UI element or actions in extreme proximity to other students was common and seemed to be socially accepted .
One possible reason for this is the perceived urgency of the situation.
Another is that physicians treating a patient in a group often need to work in close proximity to get things done physically - a behavior that may have been imitated in the simulation session.
We were intrigued by the uncommonly high engagement of the test subjects.
The observations support the theory that procedural learning requires only a certain level of realism in the simulation, and that we achieved this level.
The guiding concept - only those elements of the simulation that are relevant for the skills to be gained need to be realistic - served us very well.
In particular, a high level of immersion was achieved despite clear limits in realism when compared to classic VR: The touch interface worked well as a substitute for direct patient contact in this context, and the less-thanrealistic rendering was not a hindering factor.
Moreover, the changing treatment priorities we observed suggest a significant learning effect.
The major causes for loss of immersion were UI issues.
This was to be expected: In these cases, the system itself comes to the foreground, shadowing the simulated situation.
Therefore, we suspect that systems with less-than-intuitive user interfaces will not be able to achieve the effects we have described.
The difficulties users had with the blood test form were interesting.
It was one of the few UI elements for which we departed from the layout of the physical item, and this may have been the cause of the issues.
In a future version, we would like to experiment with using a focus-context approach as alternative, with a minimized version of the original form layout as context.
There are also several possible confounding factors involved in the study.
The subjects were young digital natives, so fascination with new technology might have played a role.
All participants were from the same university and from one semester, which might have skewed the results.
One aspect of this is that the students had all been involved in a curriculum with a very high ratio of PBL.
They all had three years of experience in self-driven group learning using problem-solving techniques.
In this paper we introduced the interactive SimMed tabletop application as a new way of medical education.
In an iterative design process with a highly interdisciplinary group of experts we developed a flexible system capable of simulating multiple medical cases.
Its aim is the teaching of procedural skills in a collaborative and student-centered fashion.
Interface elements like dragging menus, docks on opposite sides,
By means of an iterative development process as well as a set of clear guiding principles, we were able to achieve a high degree of immersion and engagement, making the system ready for real usage in medical education.
To learn more about the use and potential of SimMed, we conducted a qualitative study with 18 medical students.
We observed very high engagement and immersion as well as positive social aspects.
Further, the observations suggest a significant learning effect.
One issue with the scalability of the current software is that new scenarios need to be scripted in Python.
This currently takes some work and requires a programmer.
Putting the system into general use would require a much larger selection of scenarios.
In the future, we will work on significantly reducing the authoring time per scenario, possibly to the point at which physicians can build their own scenarios using a CMSlike interface.
Besides continuing work on system scalability, we will evaluate the quantitative data gathered to ascertain the level of learning involved.
We would also like to test the system with groups that have less experience in group work to see if the results change.
In addition, it would be very interesting to build systems based on similar principles for other educational domains to compare and contrast the effects involved.
We would like to thank all contributors to the SimMed project, in particular Maria Kaschny, Katharina Loderst adt, Joachim Plener, Anna Sch afers, Thomas Schott and Kevin Thiel, all participants of the study and Ricardo Langner for valuable help with the video.
The paper benefited greatly from the constructive criticism of several reviewers - thank you.
Work on SimMed was funded in part by the European Regional Development Fund ERDF.
