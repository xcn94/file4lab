I have attached five papers for your reading pleasure.
Privacy as Part of the App Decision-Making Process contains the final work of my doctoral thesis.
It explores how we understand consumers selecting applications in the Google Play store and how we can design better interventions to support their privacy decision making.
Immaculacy: A Game of Privacy lays the foundation for building an educational and fun narrative game around privacy concepts.
It was included in the CHIPlay 2014 Student Game Design Contest.
Standardizing Privacy Notices: An Online Study of the Nutrition Label Approach is my  soon classic paper on how we redesign better privacy policies for the internet.
I Would Like To..., I Shouldn't..., I Wish I...: Exploring Behavior-Change Goals for Social Networking Sites explores the behavior-change goals and reasons where people are not entirely happy with their SNS lives and is forthcoming at CSCW 2015. and finally, Guess again : Measuring password strength by simulating password-cracking algorithms is our papers describing how to measure password strength against various  privacy polices.
It was published at Oakland  2012.
Smartphones have unprecedented access to sensitive personal information.
While users report having privacy concerns, they may not actively consider privacy while downloading apps from smartphone application marketplaces.
Currently, Android users have only the Android permissions display, which appears after they have selected an app to download, to help them understand how applications access their information.
We investigate how permissions and privacy could play a more active role in app-selection decisions.
We designed a short "Privacy Facts" display, which we tested in a 20-participant lab study and a 366-participant online experiment.
We found that by bringing privacy information to the user when they were making the decision and by presenting it in a clearer fashion, we could assist users in choosing applications that request fewer permissions.
Our research aims to provide an alternative permissions and privacy display that would better serve users.
Specifically, we address the following research question: Can we affect users' selection decisions by adding permissions/privacy information to the main app screen?
To answer this question, we created a simplified privacy checklist that fits on the main application display screen.
We then tested it in two studies: a 20-participant laboratory exercise and a 366-participant Mechanical Turk study.
In each study we asked our participants to role-play selecting applications for a friend who has just gotten their first Android phone.
Participants were assigned to use either our new privacy checklist or the current permissions display found in the Android market.
Our results suggest that our privacy checklist display does affect users' app selection decisions, especially when they are choosing between otherwise similar apps.
We also found that both the timing of the privacy information display and the content of the display may impact the extent to which users pay attention to the information.
In the past five years Android and iOS, the two now-largest smartphone operating systems, have transformed phones from devices with which to call others into true pocket computers.
This has largely been accomplished through smartphone applications, often small, task-focused, executables that users can install on their phones from software markets.
However, with each application a user downloads they may be sharing new types of information with additional app developers and third parties.
Easy access to hundreds of thousands of applications from a diverse and global set of developers and the large amount of personal and sensitive data stored on smartphones multiply the privacy risks.
In Google Play, the current Android application marketplace, users are shown a series of "permissions" only after they have elected to download an application.
Previous research suggests that users are likely to ignore the permissions display because it appears after they have decided to download a particular app .
As of May 2012, Android has had over 15 billion application downloads, and over 500,000 applications, with both these numbers continuing to grow at an increasing rate .
Applications are not pre-screened for quality.
Android app rating and recommendation site AppBrain reports that 33% of the applications in the Android Market are rated "low quality" by users.
Additionally, a 2011 Juniper Networks report found "a 472% increase in Android malware samples" between July and November 2011 .
Similar studies from McAfee , Kaspersky Lab , and Symantec are all reporting continued exploits.
The types and quality of this malware vary widely, ranging from attacks that collect user data , to attacks that delete user data or send premium SMS messages.
To combat malicious applications Google internally developed a malware blocking tool codenamed Bouncer.
While Android has only existed publicly since 2008, a significant amount of work has been conducted on studying the Android permissions/security model.
Much of this work focuses on creating theoretical formalizations of how Android security works or presents improvements to system security, and is largely out of scope.
However, neither of these papers explore end-users understanding of permissions.
There is also a growing body of work on the complexity of the current permissions schemes users must deal with.
Researchers have discovered novel attack vectors for applications to make permission requests that are not reported to users .
Others who have looked at Android permissions have attempted to cluster applications that require similar permissions to simplify the current scheme  or have attempted a comparison of the differences between modern smartphone permission systems .
Moving away from permissions, the work of King et al.
This broader work, which included interviews with both iPhone and Android users, highlighted difficulties in recognizing the difference between applications and websites, personal risk assessments of possible privacy faults, and how users select applications in the application marketplaces .
Research in privacy policies, financial privacy notices, and access control have all similarly shown that privacy-related concepts and terms are often not well understood by users expected to make privacy decisions .
No work we are currently aware of has proposed and tested alternative permissions displays, or other ways to help users select applications in Google Play, or other application markets, as we do here.
This section details how Google Play currently presents privacy information and other information to consumers to help them select new applications to download to their Android smartphone.
We then discuss the privacy facts display we designed to make privacy- and security-related information more central to users' selections.
Google Play users are presented with a number of ways to search and browse for new applications.
This screen provides users with a long list of information about each application.
This includes , a series of navigational items, application information, screenshots, a series of market-assigned labels , free-test descriptions, a series of reviews, and a series of other types of applications that users may have viewed or chosen.
The current market application display screen is very long, yet completely lacks privacy information.
Privacy/security information appears on the above screens only when it is mentioned in free-form text by developers or when it appears in text reviews .
Market-provided  privacy/security information appears only on the secondary screen shown after a user has clicked the download button.
This is followed by a very large accept button, which is followed  by a list of grouped permissions.
Only some permissions are shown initially, followed by a "See all" toggle that expands to display the remainder of the permissions an application requests.
Each of these permission groups can be selected to see a pop-up window that contains the definitions for each of the permissions in the selected group.
Android permissions are a system controlled by the Android OS to allow applications to request access to system functionality through an XML manifest.
As these permissions are shown to the user at install time, this system as a whole forms a Computer-Supported Access Control  system, as defined by Stevens and Wulf .
The majority of work done on user expectations related to this Android access control system has been done by our own group at Carnegie Mellon  and two separate teams at Berkeley.
Felt and her colleagues have published a series of papers on the Android permission model, and how users understand it.
They found that most users do not pay attention to the permissions screens at install time  and that only three percent of their surveyed users had a good understanding of what the permissions were actually asking for access to .
They also performed a large risk-assessment survey of users' attitudes towards possible security and privacy risks, and possible consequences of permission abuses .
These results influenced our selection of items to include in a privacy checklist.
Felt also performed work detailing other possible methods for asking for permission, with a set of guidelines for presenting these privacy and security decisions to users .
We posit that by the time a user selects to move forward by tapping the Download button, they have already made their purchase decision.
We will see that this is true within our interview study below.
For privacy information to be a salient part of the decision process, it must be presented to the user earlier in the process.
Privacy information could be included in the long list of other application aspects on the standard application screen.
Instead the current market places permissions on a secondary screen.
While some might argue that placing permissions on their own screen draws users' attention to them, our results suggest that it actually does a disservice to users because they are unable to consider permissions as they consider other app characteristics.
The items are each displayed at the standard size, with the headers in capital text in a lighter font color.
Location-- The display is shown immediately after the Description section  and always immediately before the Reviews section.
This means when participants first see each app screen there is no visual difference from the market as it is currently displayed, as the Privacy Facts section appears below the fold .
Permission mapping-- For this display we strayed from the current Android permissions by: * Including types of information being collected that fall outside of the scope of the current permission model .
The final selection of the checklist items we used was strongly influenced by the work of Felt et al.
We created a series of several possible locations and distinct styles of display in an ideation round.
The display has several features: Information-- The display has two areas of information.
The first with the header "THIS APP COLLECTS YOUR," describes eight types of information the app may collect: Personal information, contacts, location, calendars, credit card/financial, diet/nutrition, health/medical, and photos.
The second header specifies "THIS APP USES" and lists advertising and analytics.
Each of these ten items has a checkbox next to it, indicating use.
Display Style-- The display is 270 pixels tall and the full width of the device .
We wanted this display to include both, for a more holistic privacy summary.
Also, by including an item like photos, we create a display that is more in line with users' expectations .
A more complex form of this display could include information that explains how these permissions are used, what they are used for, or how frequently they are used.
We will discuss two phases of experiments: a 20-participant laboratory exercise and interview study, and an online 366participant MTurk app comparison survey.
In our studies we ask participants to actively consider how and why they download applications in the market, complete our application selection task, and then discuss that experience.
In both studies, the core of the experiment was an application selection task using different market designs that vary in how privacy information is presented.
Our study design was based on a similar study run by a team of researchers at Berkeley.
The researchers had participants decide whether to install applications on a computer to see whether people read license agreements at install time.
Their users evaluated the software tools as complete packages, based on brand, design, functionality, and also End User License Agreements .
Similarly, we seek to understand whether people read the permissions display or our updated privacy facts display when installing software on an Android smartphone, and whether we can manipulate their decisions through improved design and information.
To test the privacy facts display, and explore our research question, we conducted a series of semi-structured laboratory exercises in July 2012 with 20 participants.
This was a between-subjects design.
For the main application selection task ten participants saw the privacy facts checklist, and the other ten saw the current Android permissions display.
We performed exploratory follow-up interviews seeking broad understanding of participants' interactions with their smartphones as well as diving deeply into issues surrounding the display of permissions, understanding of the terms in the checklist/permissions display, the safety of Google Play, and possible harms of information sharing.
We recruited participants through flyers and local Craigslist postings.
Each candidate filled out a short pre-survey online before the exercise, which allowed us to confirm they used an Android-enabled smartphone.
We performed the study in an on-campus lab and audio recorded the interviews.
Participants were assigned randomly to conditions .
They were paid $20 for successful completion of the interview, in the form of their choice of Target, Starbucks, or Barnes & Noble gift cards.
The main task asked participants to select one application from each of six pairs of applications we presented in our "custom Android market."
We presented two applications for each of the six categories .
All of the applications we used were real applications that could be found and downloaded in the market.
Their names, screenshots, descriptions, features, ratings, and reviews were all authentic.
However, we picked most applications in the 1,000 to 10,000 download range, such that the applications would not have been seen or used by most participants.
We displayed three text reviews per application, one 2- or 3-star, one 4-star, and one 5-star review.
In four of the comparisons we tested applications that were roughly equivalent .
In each of these four cases participants were presented with two applications with different permissions requests, detailed in Table 2.
In each of these choices one of the applications requested less access to permissions and personal information .
We also tested two special-case comparisons, to begin to explore the effects of rating and brand.
In the flight-tracking comparison, we modified one of the applications , to have an average rating of 3-stars.
All of the other applications in all categories had 4-star average ratings.
We conducted an online survey, a 366-participant MTurk test of the same application selection task used in the laboratory study.
We again used a betweensubjects design, but with three conditions.
Participants saw one of: the privacy facts checklist ; the current android permissions display ; or the current android permissions display style and terms, presented in the application display screen with additional terms to cover categories from the privacy facts display .
In each case they were asked to pick six from the same 12 applications that our participants in the lab study were given, and then were asked to write a short sentence explaining their choice.
For successful completion of the survey turkers were paid $0.30.
We used MTurk's user filtering system  and required English speakers and Android users.
The survey was front loaded with questions about the turker's Android device to discourage users who did not use Android phones.
We manually inspected free-response questions to check for participants who were answering randomly, but removed no participants in that stage, only filtering  users who had not used the Android market.
The Privacy Facts display appears to have influenced participants in two of the four standard comparisons and in both of the special comparisons.
Full selection percentages can be found in the first two columns of Table 3 .
In two of the four standard comparisons  participants who saw the privacy facts display were, on average, more likely to pick the application that requested fewer permissions.
In Document scanning, only one participant in each condition did not pick DroidScan Lite .
In the diet application choice, no participants in the Android condition picked Doc's Diet Diary , while three with the Privacy Facts display did.
In both the two special comparisons more of the participants who saw the privacy facts display picked the lowrequesting app.3 Participants placed substantial weight on the design and perceived simplicity of using the application.
Participants continued to surprise us with ever more idiosyncratic reasons for selecting certain applications.
One participant preferred applications with simplistic names, saying "I like to download the apps that have a name that I can easily find.
So Calorie Counter, I know where that is gonna be on my phone.
I don't have to be like, oh, what is this called."
Participants reported wanting to try the apps out, often saying they would download many and see which was the best .
One said "And I might try things out and see...
I just kind of see how well it works, because some things are more glitchy."
As shown in Table 1, 25% of our 20 participants were male and 75% were female.
Participants were between 20 and 44 years old, with an average of 28; 30% were undergraduates.
All of our participants had downloaded Android applications from the market and were neutral or satisfied with the Google Play experience.
Only two participants explicitly mentioned privacy information in their application selection decisions, both in the privacy facts checklist condition.
One participant, said, "If this one is offering the same thing and they want less of your information, I would go with the one that wants less of your information."
This comment shows her awareness of the privacy information, but also that the functionality must be matched between apps.
Overall, the entire laboratory exercise ranged from 29 minutes to 59 minutes .
Participants spent between 3 minutes and 47 seconds to 25 minutes and 6 seconds on the application selection task.
There was no statistically significant difference between conditions , although participants who saw the privacy facts checklist took on average 50 seconds more  to complete the task.
Across all participants in the Android permissions condition, the permissions screen was used by participants for about half the selection decisions.
Four participants decided which applications they would select without ever looking at any permissions screens.
Another four participants looked at permissions for all the applications they selected.
A6 looked at both Twitter applications permissions, but did not look at the permissions for either of the flight applications.
A9 looked at only the permissions for the Twitter application she selected and no other applications.
Across all 31 permission screen views, participants spent between 1 and 11 seconds looking at the Android permissions display.
On average they viewed the permissions display for 3.19 seconds , including page load time, a minuscule amount compared to time spent on the applications display screen.
Possible hidden costs also impacted application selection.
Several participants noted that while the music streaming applications were free , they might have to purchase a subscription, or be unable to access certain functionality after a trial period ended.
Participants generally wanted to avoid applications where features would expire or that would require later costs, but more importantly they expected the details of these arrangements to be extremely clear in the descriptions.
Most participants reported not seeing much about Android in the news, and most of what they did see being comparisons between Apple's iOS and Android.
When we asked about reports of malicious apps, or apps doing unintended things, participants said they had not heard about this.
Many believed that it could be hypothetically possible.
One participant said "Like, I have wondered, oh could an app be a virus," another "I've heard about viruses, that they can actually shut your computer or phone down.
To test whether the terms we selected for the Privacy Facts display were understandable, we asked participants to explain what each term meant.
While most were very clear, Personal Information and Analytics were the two that participants had the most trouble with.
Personal Information answers were often too broad, encompassing things we did not intend.
For example, one participant defined it as "That would mean like... interactions within the phone, Gmail, Messaging, Calling different people."
Participants generally preferred the checklist and its terminology.
One participant said, " very straightforward to me.
And that is something I noticed, I was thinking, Oh this is cool, is this what they are doing now.
That is why I didn't say anything about it.
I can immediately go: No, Yes, No, Yes."
In our online study, the application selection task was conducted on MTurk through a participant's computer, not a smartphone.
Participants saw the applications presented at smartphone size, side-by-side in iframes.
Participants selected the application they thought was better for their friend, provided a short text reason, and then rated each of the two presented applications on the likelihood that they would personally acquire it.
This treatment was designed to separate the location of the privacy information from its format.
This condition tested whether it was only the existence of any privacy information on the application screen that changed behavior, or the checklist format and position.
We used the graphic design of the permissions display from the current Google Play store; however, we modified the labels to present the same information as our Privacy Facts display .
Application selections in the laboratory and online studies.
The application that requested access to fewer permissions  is always displayed on top.
Statistics for the online study are comparisons to the base Android display.
The right-most column shows the significance between the checklist and the inline permissions.
Differences in bold, Fisher's Exact.
Comparisons with the Android display were planned contrasts.
The final comparison between the permissions inline and privacy facts display is Holm-corrected with an adjusted alpha of 0.01667.
Of our 366 MTurk participants 59% were male and 41% were female .
Our participants were between 18 and 63 years old, with an average of 28.
All of our participants had experience downloading Android applications from the market .
In three of the four standard comparisons, significantly more privacy facts participants than Android participants chose the low-requesting app.
Only for the document scanner did more participants in the standard Android condition choose the low-requesting app, and this difference was not significant.
For the Twitter choice, nearly three-quarters of the Android display participants chose Plume .
One participant captured many of the common reasons for making this choice, reflecting, "Plume has 35,000 more reviews, which suggests to me that this is the more popular, more frequently used application.
The description includes a list of everything you can do with the app and those all seem like useful features."
However when presented with the privacy facts checklist, the two applications were selected at almost the same rate, with slightly more selecting Twidroyd.
Here participants noted and cited the permissions information.
One stated, "I picked the one that respects privacy more.
The other gets too much personal info."
Another participant wrote, "Plume collects too many personal facts."
For the special comparisons, rating and brand recognition outweighed privacy.
For the flight tracking choice, more participants chose iFlights  over Flight Tracker.
Although participants thought iFlights "sounds like an iPhone port," many believed it had a cleaner UI, but the top reason given was the rating difference.
Flight Tracker's 3-stars seems to have outweighed all other factors.
For the streaming music choice, Spotify  had much higher brand recognition .
In the Android permissions display condition over half of the people  who selected Spotify explicitly stated that they had already heard it was very good or that they or friends use Spotify.
One participant said "Spotify is pretty popular and I have never heard of Rdio."
Spotify collected much more information than Rdio.
As shown in in Table 3, the permissions inline display, while in the same place and often more space-consuming than the checklist, did not have as large an effect on users' decisions.
In only one of the four standard comparisons, the nutrition application, was this change significant, and in most cases it underperformed the checklist display .
This suggests that in addition to moving privacy information to the application display screen, it is important to present that information in a holistic, clear, and simple way if it is to impact users' app selections.
Information or info were mentioned by 49 people in the privacy facts checklist condition, but by only six participants using the Android display.
Based on these responses privacy and personal information seem to have factored more strongly into the decisions of those who saw the privacy facts checklist.
Similar to our lab study, many participants, when directly asked, said they did not notice the privacy facts checklist.
Of the 125 participants who were shown the privacy facts checklist, 49  reported in a free-text response having not noticed or paid any attention to the display.
Both those people who did and those who did not notice the display provided reasons for why they ignored it, or believed it was not necessary: * "I noticed the Privacy Facts but it really didn't influence me that much.
I feel like with social networking it's so much easier to get contacts, photos, or information of someone."
I tend to pay more attention to ratings and usefulness then anything else."
It's not the most important factor.
I don't keep a bunch of vital personal info on my phone, so no worries.
I think people who do are really stupid."
There were also users who found the privacy facts display helpful and made their decisions based on it: * "Yes.
I believe the privacy information is helpful.
It would only bother me if I saw something that didn't make sense for the app to use.
However, I am not terribly concerned about privacy."
It only influenced me if it seemed to be the only thing to distinguish between the two apps."
I want to know exactly what is happening to and with my data from that program when I use it.
It was useful though I wish some apps would go into greater detail."
Participants who both used and didn't use the display still had misconceptions about companies, sharing information, and the market.
Many assumed that all applications collect the same information.
One participant who didn't look at the display said she did not because, "I assume they always say the same thing...." Participants also continued to believe external forces protect them.
One said, "Yes I saw the privacy facts.
That didn't really affect my decision as companies are required to protect consumer's information and companies don't really wanna get sued for breach of security so I am not worried about all that."
Another stated the continued belief that the market is internally well-regulated, "I think it is trustworthy, I would assume google play keeps a tight leash on that stuff."
Finally, one participant gave an answer that applies quite broadly, and mirrors work by Staddon et al.
We also asked our online participants to rank a series of factors in their personal application-selection process from "Not important" to "Very important."
The results of this are presented in Figure 2.
Permissions ranked 8th , just below two metrics of popularity and just above the size of the application.
80% of participants said ratings and reviews were important or very important, compared with only 43% who said that permissions were important or very important.
This result seems to align with how often participants across our tests tended to ignore permissions.
Our short checklist display had items that consumers were able to explain in most cases.
Analytics and Personal Information were the most problematic.
Participants were generally correct when defining Analytics, but often created more invasive definitions that were not intended.
Personal Information was more difficult, as it was too vague and many participants listed other types of data that they then realized were covered by another item on the list.
We will continue to further refine the terms and types of information that is most important to people.
One more significant design flaw with the display was that participants do not view permissions displays in the same way as they view privacy policies.
They see this information only as items the phone can take, not things that they personally input.
While we believe a complete privacy display should cover both user-provided information that is stored  and automatically collected information like location, this was not explained well by the current design.
Our lab study has many more female participants than male participants, and due to random condition assignment they were not evenly distributed.
We note this as a potential limitation, though our results from the two studies are aligned, and we did not see such a similar gender imbalance in our online study.
Mechanical Turk also has its own set of limitations and biases, which we attempted to counter through a careful survey design.
While we compared our two survey phases, they did not follow identical methodologies.
Our lab study was more realistic, with users using actual cell phones, when on MTurk users saw the applications side by side, and could make direct and visual comparisons.
While the reasoning and behavior given seems similar, it is possible that our online survey users had an easier time making decisions, not due to our improved permissions display, but due to the side-by-side display.
The only evidence we have to counter this is the permissions-inplace display did not perform as well as the privacy facts display, implying that the side-by-side display alone is not responsible for all the improvements we saw.
Finally, we tested only 12 applications in the studies described above .
We picked applications that seemed similar, functional, and would be unrecognized, but we would like to expand this work in the future to consider larger application datasets.
All of our participants had never seen a privacy facts display before, but were immediately able to make comparisons when specifically instructed to do so after the selection task.
However, some simply did not believe privacy information was important or relevant to their decision.
Some said it would depend on how much their friend  cared about his or her own privacy.
These results are similar to those seen in other labeling efforts.
Consumers who care more about privacy, whether they have had a credit card stolen or have started receiving spam text-messages, are more likely to take advantage of labeling information.
Even if the impact is not drastic, we see the privacy information on the main screen having an affect on selection behavior.
Our goal was to better understand how users select Android applications, and to make privacy and permission information a salient part of that process.
We found that users did not use the current permissions display.
By moving privacy/permissions information onto the main screen and presenting it clearly and simply we could affect user decisions in cases where applications were similar.
Users mostly appreciated the new privacy facts display, said they would use it to help make their decisions or at least glance at it, and found comparing applications in the market to be a difficult task where better displays would assist them.
Participants in our studies reported being familiar with permissions displays and being aware that there are differences between applications.
While this may seem unimportant or obvious, leveraging the awareness of privacy differences means creating interfaces, like checklists, that help consumers identify and compare differences should benefit users who want to make privacy-preserving decisions.
Participants reported that they did not, in most cases, read the information in the displays, and they did not select the permission groupings to see more details or try to better understand the terms.
Even when the display was moved to the main screen, it does not have the impact of the privacy facts display.
Participants continued to report not being concerned with data sharing generally, partially due to a belief that companies are following laws and a strong belief that Android/Google is watching out for their safety as a consumer.
While this is accurate in a very general sense, the specifics are quite far off from reality.
Correcting the ubiquitous idea of Google Play as a safe, protected marketplace, must necessarily be changed if consumers are to protect themselves through understanding privacy and security in their decision-making process.
From both the lab and online studies we found that participants continued to report that other characteristics of applications are as important or more important than permissions, including: cost, functionality, design, simplicity, rating, number of ratings, reviews, downloads, size, and others.
Continuing to understand how much privacy can compete and offset other aspects is important future work as consumers battle with a crowded and complex market.
When asked why an application was collecting a type of information, participants most often stated they did not know, but would occasionally venture possibilities.
The short answer, is yes--the privacy information on the application display screen affected user behavior.
In laboratory responses and our online test we saw behavioral differences as well as differences in quality and tone of responses relating to private information.
We also found most people do not consider permissions when downloading new applications.
Even when instructed to download applications, most users made decisions without ever pushing the button that would take them to the permissions display.
Both our lab participants and our online participants also self-reported that they were aware of the display, but did not look at it.
This was confirmed by our lab study participants who, when they did fully "download" applications, spent a median time of 2 seconds on the permissions display.
While this was expected based on other research and our own earlier work, we now have evidence that the permissions are, at least partially, disregarded due to their position in the application selection process.
In online testing we found that having a privacy display would in cases of equivalent applications change user application selection, and do so more strongly than simply moving the permissions display to the main screen.
Finally, participants overwhelmingly trusted the application in both the privacy facts display and the permissions display.
The question of trusting the information was one most had never considered, and actually gave some participants pause as they realized for the first time that this information might not be accurate.
Again, users believe this information is correct, is being verified, and will assume they misunderstand something before they would believe the displays are incorrect.
Mistakes in the permissions are not recognized, even when directly discussed.
Users will assume they themselves are wrong, not the policy.
In Proceedings of the 27th international conference on Human factors in computing systems, ACM , 319-328.
Enck, W., Gilbert, P., Chun, B., Cox, L., Jung, J., McDaniel, P., and Sheth, A. Taintdroid: an information-flow tracking system for realtime privacy monitoring on smartphones.
In In Proceedings of the 9th USENIX conference on Operating systems design and implementation  .
Felt, A., Chin, E., Hanna, S., Song, D., and Wagner, D. Android permissions demystified.
In In Proceedings of the 18th ACM conference on Computer and communications security  .
Felt, A. P., Egelman, S., and Wagner, D. I've got 99 problems, but vibration ain't one: A survey of smartphone users' concerns.
In 2nd Annual ACM CCS Workshop on Security and Privacy in Smartphones and Mobile Devices  .
Felt, A. P., Ha, E., Egelman, S., Haney, A., Chin, E., and Wagner, D. Android permissions: User attention, comprehension, and behavior.
Stopping spyware at the gate: A user study of privacy, notice and spyware.
In In Proceedings of the 5th Symposium on Usable Privacy and Security  .
Mobile malware development continues to rise, android leads the way, 2011. http://globalthreatcenter.com/?p=2492.
Kelley, P., Consolvo, S., Cranor, L., Jung, J., Sadeh, N., and Wetherall, D. A conundrum of permissions: Installing applications on an android smartphone.
In Financial Cryptography and Data Security, vol.
In Proceedings of the 2009 Symposium On Usable Privacy and Security  .
Kleimann Communication Group Inc. Evolution of a prototype financial privacy notice., February 2006.
Smartphones have unprecedented access to sensitive personal information.
While users are aware of this, generally, they may not be considering privacy when they select applications to download in the application marketplace.
Currently, users have only the Android permissions displays to help them make these application selection decisions, screens which are placed after the main decision occurs, and are not easily understood.
We sought to investigate how we could make permissions and privacy play a true part in these decisions.
We created a short "Privacy Facts" display, which we then tested in 20 in-lab exercises and an online test of 366 participants.
We found that bringing information to the user when they are making the decision and by presenting it in a clearer fashion, we can assist users in making more privacy-protecting decisions.
We acknowledge our colleagues at Carnegie Mellon University, and would like to thank Alessandro Acquisti and Sunny Consolvo.
This work was supported by the National Science Foundation under Grant DGE-0903659 .
Au, K., Zhou, Y., Huang, Z., Gill, P., and Lie, D. Short paper: a look at smartphone permission models.
In Proceedings of the 1st ACM workshop on Security and privacy in smartphones and mobile devices  .
A methodology for empirical analysis of permission-based security models and its application to android.
In In Proceedings of the 17th ACM conference on Computer and communications security  .
Barrera, D., Clark, J., McCarney, D., and van Oorschot, P. C. Understanding and improving app installation security mechanisms through empirical analysis of android.
In 2nd Annual ACM CCS Workshop on Security and Privacy in Smartphones and Mobile Devices  .
With the intent of addressing growing concerns regarding online privacy, Immaculacy is an interactive story that immerses the player in a slightly dystopian world littered with privacy issues.
Events unfold in the narrative based on hidden scores kept during gameplay and calculated based on specific decisions made by the player.
Ultimately, we hope to create an engaging environment that helps players consider the decisions they are making in their own lives.
We give the player experience with many privacy issues through their explorations of a world of hyper surveillance and connectivity.
Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author.
Copyright is held by the owner/author.
The continuous evolution in the pervasiveness and connectedness of technology is a subject of growing concern.
Databases, smartphones, tablets, laptops, wearables are all united through a single global network--the Internet.
Consequently, new threats have emerged which are capable of harming people professionally, financially, personally, and even physically.
With medical, educational, financial, and other personal data available online, exploited vulnerabilities have the potential to devastate large numbers of people, with a single attack.
Because of this, it is easy to see how privacy and security are among top concerns for many people around the world.
Immaculacy is a game that takes the player to a nearfuture society, New Washington, with an overbearing city government.
The New Washington government keeps constant watch on its citizens through various means of surveillance, in the name of maintaining the "immaculacy."
The player will be exposed to many consequences that can arise from this type of governing.
There will be instances of citizens being unfairly accused of wrongdoings and sometimes becoming rebellious.
Some people will abuse the power or access that they are given.
Such abuse, among other vulnerabilities, can result in sensitive information ending up in malicious hands and may also put innocent people in danger.
In many cases, the player will find him or herself in the middle of sensitive situations and will be forced to make decisions that may have major consequences in the future.
This interactive story takes the player on a journey, through a world in which personal information is in constant jeopardy.
The player is placed in the role of an eighteen-year-old girl, Sydney Carlisle.
Sydney is a high-school graduate who grew up in the small town of Danville.
When her friend is given a full ride to New Washington University, the pressure builds on Sydney to join her in the peculiar city.
Upon arriving in New Washington, Sydney realizes that the watchful eye of the city government is just one of many threats.
In order to survive in her new home she must learn how to protect her information and be cautious when trusting others.
Because the intent behind Immaculacy is to communicate both the seriousness and the range of attacks that can be used against Internet users, the target audience is very broad.
In general, any smartphone owner or person who uses the Internet on a regular basis can benefit from and enjoy this game.
Events unfold in Immaculacy based on two types of hidden scores: SECURITY SCORES Data Leaking: Determined by how much sensitive information the player gives when questioned by any NPC.
City Suspicion: Determined by how much information the player withholds when questioned by city officials.
PERSONAL SCORES Individual NPC Scores: Determined by how the player treats the individual NPC, in addition to other NPCs that are somehow related to the individual NPC's story arc.
Karma Score: Determined by overall treatment of other characters either directly or indirectly.
These scores affect the main character's: * relationships with other characters; * extent of government surveillance; * vulnerability to criminals; * employment and community status.
Sydney, the character controlled by the player, will have the ability to interact with people, devices, and items in the world through a narrative interface.
Certain interactions will present the player with decisions that are tracked and maintained by the game using hidden score metrics.
The primary novel quality associated with Immaculacy is the nature of the game within its own genre.
Rather than creating a game revolving around quizzing players on their knowledge of privacy issues, this dialoguebased role-playing game is designed to present the player with realistic situations in which Sydney's privacy, and sometimes safety, can be compromised.
Furthermore, this game is intended for the average Internet user and is not tailored to a player working within a specific field.
Several other games exist that use a more story or situation based approach to teach a player about security concepts.
In contrast to Immaculacy, these games are intended to train personnel who are or will be working in an information assurance related field.
The games presented in  and  consist of simulations, which were used to train personnel who are or will be working in a cyber-security, or information assurance related field.
Similarly, the game introduced in  is a card game and is used in a similar fashion to train people for a privacy related job position.
Each of these games presents the player with a variety of scenarios that a professional in the information assurance field may encounter.
Depending on the way a player handles a certain encounter; they are met with the appropriate consequence in game.
User studies for many of these games have shown some success in training computer users in privacy and security related topics.
We have created a privacy themed mobile game for the average Internet and smartphone user.
With this game, our goal is to raise awareness of different types of Internet related threats and the impact they can have on someone's life.
Through an interactive story, we have simulated many real situations concerning privacy topics.
We hope to give the user some experience with these threats and issues as a result of playing Immaculacy.
The studies conducted in  and  not only reveal concerns that users have with different privacy issues, but they also show where there are still large vulnerabilities for average users.
The online games presented in  and  educate the average Internet user on several threats that they may encounter on the Internet.
The game in  is intended to help users identify phishing attacks by studying web addresses, whereas  focuses more on general security like virtual and physical handling of sensitive data.
Both games however, do present the information similarly, utilizing a quiz-based format where players are asked questions that have a right or wrong answer.
Similarly, the study conducted in  included user testing with privacy policy visualizations to optimize the appearance and layout of the visualization for average users.
We hope to build on the studies in  and  by addressing some of the issues discussed in these works within the story for Immaculacy.
We will be conducting upcoming playtests on a demo of the first part of the game.
From these tests, we hope to learn more about privacy concerns of Internet users and whether Immaculacy is effective in helping players address and consider such concerns.
We also plan to use a similar approach to the user studies in  and  by using the tests that we will be conducting to assist in determining our approach in implementing the full version of the game.
In Proceedings of the 10th Annual Workshop on Network and Systems Support for Games, IEEE Press , 7.
In Proceeding of the 44th ACM technical symposium on Computer science education, ACM , 729.
Information revelation and internet privacy concerns on social network sites: a case study of Facebook.
A "Nutrition Label" for Privacy.
Symposium On Usable Privacy and Security  .
With the security threats facing Internet and smartphone users today, Immaculacy is a game about Internet privacy that gives players the opportunity to play through events that they could potentially encounter on their connected devices.
By assessing various studies and evaluating known threats, we have created a storyline that provides its players with topics and situations to consider when revealing personal information online.
While Immaculacy is currently in the very early stages of development, we aim to further build on the foundation that we have created.
Ultimately, hope to provide an enjoyable, informative, and thought provoking experience.
A video of the gameplay demo can be found at: exit.cs.unm.edu/immaculacy.
Earlier work has shown that consumers cannot effectively find information in privacy policies and that they do not enjoy using them.
In our previous research on nutrition labeling and other similar consumer information design processes we developed a standardized table format for privacy policies.
We compared this standardized format, and two short variants  with the current status quo: full text natural language policies and layered policies.
We conducted an online user study of 789 participants to test if these three more intentionally designed, standardized privacy policy formats, assisted by consumer education, can benefit consumers.
Our results show that providing standardized privacy policy presentations can have significant positive effects on accuracy and speed of information finding and reader enjoyment with privacy policies.
For background, we discuss work highlighting the problems with current online privacy policies, a standards-based technology aimed at solving them, and a user interface designed to combat these problems.
We also introduce layered privacy notices, a policy format that has gained some traction with large companies.
We conclude with an in-depth explanation of the work completed toward standardizing financial privacy notices, a multi-year project that closely matches our own design and testing processes.
Consumer testing has shown privacy policies are unusable.
An online survey of over 700 participants that tested policies from six different companies, in three currently existing formats, found that "participants were not able to reliably understand companies' privacy practices with any of the formats" and that "all formats and polices were similarly disliked" .
In the United States, Internet privacy remains almost entirely unregulated, which means consumers who wish to find websites with privacy-protective practices must be able to read and understand privacy policies.
Policies should ideally have usable and accessible information, but are commonly long, textual explanations of data practices, most frequently written by lawyers to protect companies against legal action.
We used an iterative, user-centered design process to develop a more compelling and informative privacy policy format.
We conducted a large online user study to evaluate three variants of our privacy policy format as well as two formats commonly used by large corporate websites today.
In the next section, we detail some related work on the drawbacks of current privacy policies and describe other efforts to design better policy formats.
We then explain each of the five formats we tested, followed by accuracy, comparison, timing, and enjoyability results from our participants.
We then discuss the implications of this work with some future directions.
Reading current online privacy policies is both challenging and time consuming.
It has been estimated that if every Internet user read the privacy policies for each site they visited, over the course of a year, this lost time would account to about 781 billion dollars .
It is admittedly unrealistic to expect users to read and understand the privacy policy of every site they visit as people can rarely find information in even a single policy.
Most policies are written at a level that is suitable for consumers with a college-level education and use specific domain terminology that consumers are frequently unfamiliar with .
Rarely is a policy written such that consumers have a clear understanding of where and when their data is collected, how and by whom it will be used, if it will be shared outside of the entity that collected it, and for how long and in what form it will be stored.
Even worse, it is unlikely consumers will even read a single policy given a widespread consumer belief that there are no choices when it comes to privacy: consumers believe they do not have the ability to limit or control companies' use of their information .
Researchers at the CyLab Usable Privacy and Security  Laboratory proposed a privacy "nutrition label" to assist consumer understanding of privacy policies .
The nutrition label approach to privacy was supported by studies of the design and consumer acceptance of nutrition labeling programs .
This tabular privacy format1 was designed to enhance user understanding of privacy practices, increase the speed of information finding, and facilitate policy comparisons.
1 The tabular format can be filled in automatically if a site uses a W3C privacy standard called The Platform for Privacy Preferences  , which was designed to create a standardized, machine-readable privacy policy.
Other work has been attempted to create usable displays of P3P policies .
Layered privacy notices, popularized by the law firm Hunton & Williams , were created to provide users with a high-level summary of a privacy policy.
The design is intended to be a "standardized" format; however, the only standard components are a tabular page layout and mandatory text for the section headers.
Other design details and the text of each section is left at the discretion of each company.
Additionally, the amount of information to include in a layered notice is left up to each company -- with layered notices requiring consumers to click through to the full text, natural language policy, to learn more.
Three of these formats are standardized and were created by our lab using an iterative design approach.
Of these, two are tabular and one is textual.
Two explicitly describe absent information and one presents it in the context of the policy.
Each of these five formats is immediately followed by a list of 16 definitions of privacy terms, consistent across the formats.
These definitions define the row and column headers in the table conditions and the text tokens in the short natural language condition.
They also assist with understanding the terminology used in the survey questions.
The Gramm-Leach-Bliley Act , passed in 1999, contains the Financial Privacy Rule, which requires that financial institutions disclose their privacy policy "at the time of establishing a consumer relationship...and not less than annually" .
Financial institutions must comply with requirements on what they disclose, but their disclosures may be in any format.
In 2004, seven federal agencies2 launched a multi-phase initiative to "explore the development of paper-based, alternative financial privacy notices...that are easier for consumers to understand and use" .
The Kleimann Communication Group  conducted the first-phase, which tested multiple designs across seven cities and collected consumers thoughts on current financial privacy notices.
In their final project report the KCG proposed a three-page design for further evaluation.
In December 2008, the second phase report was published by Levy and Hastak .
This report detailed a 1032-participant mail/interview study that tested four privacy notice formats for three fictional financial institutions.
Two of the four notices were developed by the KCG, with contextual information and an opt-out form.
The KCG table notice displayed financial institutions' practices in a grid format, whereas their prose notice used a bulleted list.
The two other notices were both heavier in text, with the "current notice" mimicking notices that financial institutions currently use, and the "sample clause" notice generated from GLBA provided phrases.
Levy and Hastak conclude that the KCG table notice performed the best.
They attribute this performance improvement to an increased level of comprehension, given the table notice's " of a fuller context...the part-to-whole display approach seems to help consumers focus on infor2 The seven federal agencies that enforce the GLBA are the Federal Deposit Insurance Corporation, the Federal Reserve Board, the Federal Trade Commission, the National Credit Union Administration, the Office of the Comptroller of the Currency, the Office of Thrift Supervision, and the Securities and Exchange Commission.
Natural language, full text policies are the de facto standard for presenting privacy policy information online.
For this experiment, we selected four policies from well-known companies.
Each policy was stripped of all formatting, retaining only internal hyperlinks to reference other areas of the policy, if available in the original.
All identifying branding was anonymized, including company and product names, affiliates, and specific corporate information such as addresses and contact information.
The standardized table format,  has ten rows, each representing a data category the company may collect, four columns detailing the ways that data may be used, and two columns representing ways that data may be shared outside the company.
This table is filled with four symbols, dark red to represent that your data may be used or collected in that way, light blue to represent that your data will not be used or collected in that way, and two intermediate options labeled "opt in" and "opt out."
This is a modified variant of the "nutrition label" format discussed above , based on follow-up design iterations.
The short standardized table  is a shortened version of our proposed tabular approach, which removes the data categories  that are never collected by a company.
These removed data categories are listed immediately following the table to maintain a holistic understanding of a company's privacy practices.
While the removal of data categories allows the format to fit into a smaller area, it may make comparisons less straightforward.
An example of the entire Privacy "Nutrition Label" or standardized table is shown on the left, next to the short standardized table on the right.
The comparison highlights the rows deleted to "shorten" this version.
These deleted rows are listed directly below the table.
While both formats contain the legend , it is displayed only once here due to space constraints.
The fifth and final policy format we tested is the layered privacy notice , as described by the law firm Hunton & Williams, mentioned earlier .
This format involves a summarized, one-screen privacy policy that can be formatted in a variety of ways, but are normally tabular in nature and retain all the links to the full natural language policy.
Layered policies are an excellent test candidate because some major corporations have already deployed them, making them a viable, real world summary form of privacy policies.
These policies were stripped of identifiable brand information, but the formatting and styles were retained.
We developed a custom survey management tool called Surveyor's Point to facilitate our data collection.
We developed our own survey management tool for two main reasons.
First, we wanted to provide a robust experience for comparing two  privacy policies.
Our implementation allows us to show respondents a single question on the screen along with links for switching back and forth between the two policies without needing to open up multiple browser tabs or windows.
This also allowed us to track the number of users who looked at each policy and the number of times they switched between them.
Second, we wanted to be able to instrument the policies we were testing to understand the way users interacted with them.
Not only did we collect the amount of time that users spent reading the policies, we also collected information about whether they clicked through to opt-out forms, to additional policy information links, or from a layered notice through to the full text policy.
In preparation for this study we first performed three smaller pilot tests of our survey framework.
We ran our three pilot studies with approximately thirty users, across 2-3 conditions.
3 The two systems are linked using a shared key that Surveyor's Point generates on the completion of our survey, which a participant then enters back into Mechanical Turk.
This allows us to link an entry in Mechanical Turk with an entry in Surveyor's Point and verify the worker completed the survey before payment.
We conducted an online user study in summer 2009 using Amazon's Mechanical Turk and Surveyor's Point.
Mechanical Turk allows workers across the world the ability to perform short tasks and get compensated through Amazon credits.
People can place jobs through Mechanical Turk, specifying the number of people they are looking for, qualifications those people must have , the amount they are willing to pay, and details about the task.
Mechanical Turk payments must be calibrated for the length of the task, for our approximately fifteen minute study, we paid $0.75 on successful completion.
Participants in Group B saw Microsoft's privacy policy as "Acme," and IBM's as "Bell."
As layered text policies are not widely used only Policy Group B had a layered text format option.
The policies range in length, but are representative of common practices.
For a summary of word counts across the full text, short text, and layered text policies see Table 2.
We then conducted our large-scale study.
Each of our 789 participants was shown two privacy policies and was asked to answer 26 questions.
The participants were paid $0.75 on successful completion of the study.
We designed the experiment to use a between-subjects design, with each participant assigned one of two policy sets, in one of five different formats.
The between-subjects design was chosen to remove learning effects and to allow the survey to take only approximately 15 minutes to complete.
All participants answered the same questions; only the policy formatting and content differed.
Our study was designed to include questions across seven blocks: 1.
Demographics We collected standard information about our participants including: gender, age, and current occupation.
Internet & Privacy We asked the participants four questions to better understand their internet usage and their prior knowledge of privacy.
These are detailed in our Demographics section.
Simple Tasks Participants were shown the "Acme" policy and asked six questions pertaining to it.
We refer to these information-finding tasks as simple questions as each question can be answered by looking at a specific row or column in the table.
The answer options for these questions  were "Yes," "No," or "The policy does not say."
Complex Tasks Participants were asked six questions .
We refer to these information-finding tasks as complex questions because each dealt with some interaction between some category of data and either data use or data sharing.
The answer options for these were "Yes," "No," "Yes, unless I tell them not to," "Only if I allow them to," or "The policy does not say."
Several specific goals led to our selection of policies.
It was important to us that we select real companies' policies that people are likely to give their actual data to.
It was also important that for the real-world policies we used actual existing policies.
Due to the infrequent use of layered policies by companies we selected only two companies that had actual layered policies in use on their website.
We used two sets of policies rather than just one so that we could test a wider variety of real policies .
We discuss the differences we observed in how participants interacted with these two sets of policies in our analysis.
We compared these four different well-known companies' policies in two pairings across five different policy formats .
We refer to anonymized versions of the Target and Disney privacy policies as Group A and to anonymized versions of the Microsoft and IBM privacy policies as Group B.
Participants were assigned to either Group A or Group B, and one of the five formats.
Single Policy Likeability After completing the simple and complex tasks, we presented a series of 7-point Likert questions for qualitative feedback on the format.
Comparison Tasks Participants were shown a notice stating that they would now be comparing two policies, the Acme policy, which they had already seen, with the policy for the Bell Group.
They were asked five comparison questions that required looking at both policies.
Policy Comparison Likeability Participants were asked three more Likert questions to collect qualitative feedback on the task of comparing two policies.
Additionally, we timed how long it took participants to complete each task.
We also performed an ANOVA analysis for the nine 7point Likert questions, throughout the study.
We excluded participant data from analysis if they did not complete the entire question set.
In addition, data from participants who completed the study in less than two standard deviations from the mean of the log-normalized4 times were excluded.
5 The data from the remaining 764 participants will be discussed for the rest of the paper.
Table 3 shows the gender and age breakdown of the participants by group, as well as the number of privacy policies participants reported reading in the previous six months.
56.4% of our participants reported reading at least 1 policy in the previous six months.
4 Log-normalization is used on analysis of timing information for the remainder of the paper to force a normalized distribution, allowing us to perform ANOVA analysis.
Timing information in charts will be displayed in seconds to assist understanding.
Cutoff point  A: 268 seconds, B: 262 seconds.
Our analysis, detailed below, is split into three portions.
We scored Simple, Complex, and Comparison tasks for accuracy.
We marked all questions as correct or incorrect .
We performed factorial logistic regressions across the policy formats.
We performed an ANOVA analysis on the log normalized timing information for the above tasks.
While this sample population from Mechanical Turk is certainly not a completely representative sample of American Internet users, this population is a useful one to study.
Our participants appear to read privacy policies more than the general population; however, it is possible that participants realizing that we were going to ask them to compare privacy policies may have sought to seem more knowledgeable about privacy policies.
Nutritional and drug labeling literature reports that standardization efforts assist most those who seek out the information.
If participants on Mechanical Turk do read more privacy policies than the general population then we may in fact be refining our label to help the group that will be most likely to leverage the information.
The complete accuracy results are presented in Table 4.
For analysis on a per question basis, we performed factorial logistic regressions with the standardized table as the base for comparison across formats.
We have shown significant differences  in formats with boldface accuracy scores.
We did not compare performance quantitatively between companies due to differences in practices and answers.
However, we discuss a number of observations about the differences between policies across conditions and their impact on performance.
We highlight several of the questions, based on what they were testing for, below.
Question 3 asked: "Does the policy allow Acme to collect information about your current location?"
This information is not collected in any of the conditions.
None of the conditions produced reasonable results.
Ranging from A: 19-48%, B: 453% accuracy, this question was difficult across all formats.
The lowest accuracy came from Policy B's full policy text condition.
A search for "location" results in the phrase: "and a general geographic location derived from your IP address," which makes it easy to see why 90% of the participants in that condition believed that Acme did collect their location information.
However, deriving a "general geographic location" is not considered storing "your current location," as made clear from our definition,6 a company would need to track a user based on GPS or cell information, not this abstraction of location from an IP address.
Each participant completed fifteen information finding tasks.
We scored each participant on a scale from 0 to 15, based on the number of these questions they answered correctly, and averaged those scores across conditions.
Note, correct answers varied by conditions since the policy content varied across conditions.
We present those aggregate results in Figure 4.
This summary shows a very clear divide, with the three standardized formats scoring between 62-69%, in light blue; while the two real-world text policies are 43-46%, in red.
In both policy groups, the standardized table significantly outperformed both of the real-world text policies .
Percentage of participants who answered each question correctly, by policy format and company group.
Percentages in bold indicate statistical differences  for formats compared against the standardized table, for that policy.
For this analysis two separate logistic regressions were performed, a 1x4 for Group A, and 1x5 for Group B.
Differences between companies are not compared.
Questions are listed exactly as asked, with the corresponding correct answers for each company.
Question 4 asked: "Based on the policy will Acme register their secure certificate with VeriSign or some other company?"
Generally, information about secure certificates is not included in privacy policies, and the registrar of the certificate is certainly out of scope, so the correct answer in all conditions is "The policy does not say."
As shown in Table 4, 79-88% of participants in the three standardized conditions were able to answer this correctly.
However accuracy dropped to 31-52% between the full policy text and layered text conditions.
Neither of the two policies  mentioned Verisign or any other certificate registrar nor did either policy have the word "certificate" in it.
While this was clear in the three standardized formats, participants with the full policy text format had a more difficult task as scanning for the absence of information over several pages of text is difficult.
The standardized short table format performed poorly  when medical information was absent, however the standardized short text format performed best  when medical information was absent, even though both had identical notices describing this absence.
This is probably due to the standardized short text format reserving the largest font size for things a company does not do, including in this case, collecting medical information.
While this is in the same font-size for the short table, the table itself overpowers the notice.
Moving onto complex tasks, Question 7 asked: "Does the policy allow Acme to share some of your information on public bulletin boards?"
For Policy Group A, the answer was "Only if I allow them to," which translates to an opt in, while for Policy Group B, the answer was simply "No."
In the tabular format, this question required the participants to find the column for public sharing, and see if any type of data would be allowed.
Across the standardized formats accuracy ranged from 59% to 76%.
In both policy groups, incorrect answers across the standardized formats were evenly distributed, with no clear incorrect answer trends.
Participants given the full policy text format have strikingly low results for this question, regardless of the policy they were assigned.
For Policy A, the largest contingent, 32% of participants,  reported that they believed that the policy did not specify whether information would be shared on public bulletin boards.
Question 5 asked: "Based on the policy may Acme store cookies on your computer?"
The answer for both policies was "Yes."
While this question was straightforward for most conditions, the standardized short text format did not perform as well in group A, with only 73% of participants answering correctly .
However, at only 175 words participants may not use the search or find functionality of their browser, thus missing the word cookie.
This is speculative; however, and a study with the paragraphs rearranged may lead us to better understand if any blind spots exist in this format.
Question 8 asked: "Does the policy allow Acme to share your home phone number with other companies?"
For Policy Group A, the answer was "Yes, unless I tell them not to," while for Policy B, the answer was "Yes."
To answer this question correctly in all conditions, the participants needed to realize that a home phone number was considered "contact information" in the standardized conditions or that it fell under a broad umbrella of "personal information" in both full text policies.
Looking at the standardized short text format for Policy Group B, we see that only 20% answered correctly, while 47% answered "Yes, unless I tell them not to," implying they believed an option existed where it did not.
We believe that this again comes from misreading the paragraph of text in the std.
There was an option in that paragraph, but only regarding telemarketing and not sharing.
The remaining five standardized format conditions had accuracy ranging from 63-69% with incorrect answers split evenly across answer choices.
Question 6 asked: "Does the policy allow Acme to collect information about your medical conditions, drug prescriptions, or family health history?"
For Policy A, the answer was Yes, and for Policy B, No.
The full text policy, again, performed badly, especially for Policy B.
Here, 29% of our participants correctly answered that Acme did not collect their medical information; however, 41% answered the policy does not say, which we marked as incorrect because the absence of this information means that they cannot collect health-related information.
One of the benefits of a standardized form is an empty row in a table, or a required text notice that lists information that is not collected.
For Policy A, only 49% of the participants correctly answered that they do collect medical information.
The policy itself references "counseling from pharmacists," an "online prescription refill service," and "prescription medications."
Again, the full policy text fared significantly worse: 65% of the participants answered either "Yes" or "Yes, unless I tell them not to," believing that the policy stated the inverse of what it actually did.
Across the standardized formats accuracy ranged from 51-79%.
Incorrect answers across the standardized formats varied, with the largest group  believing the sharing of cookie information was optin, when it was nonexistent.
The final five task-based questions  called for participants to answer questions that were based on two different policies.
The first policy was the Acme policy, which they had looked at for the prior 12 questions.
The second policy was representing a company we called "The Bell Group."
The first and final questions  were the same, asking participants to select which of the two companies they would prefer to make an online purchase from, while questions 14-16 asked for specific information comparisons.
For each of these questions, the participants started with only the question on the screen, and were presented with a "policy switcher" that allowed them to view either the Acme or Bell policy.
The Acme policy, which the participants had already answered twelve questions about was infrequently viewed.
As shown in Table 5, less than one-third of our participants viewed Acme, while nearly all participants reviewed the Bell policy for each of the questions in this section.
For the full policy text, 55% of the participants believed that both companies gave options regarding cookies.
This means that they incorrectly answered that the Acme policy had options regarding cookies.
Searching for "cookie" in that text brings up a section entitled "Use of Cookies," under which the fourth paragraph reads: "You have the ability to accept or decline cookies.
Most Web browsers automatically accept cookies, but you can usually modify your browser setting to decline cookies if you prefer..." Although this sounds like an option regarding the use of cookies, it is not one that the Acme company provides, rather a function of most web browsers.
The next line even states "If you choose to decline cookies, you may not be able to sign in...," making it obvious that Acme sites will use cookie information.
Policy Group A had a smaller range across conditions, 33% for the full policy text, with the standardized formats ranging from 46-59%.
Question 15 asked: "Does either company collect sensitive information ?"
For Policy Group A, the Acme policy does collect health information, whereas Bell does not.
For Policy Group B, neither company collects sensitive information.
Policy Group A performed, on average, worse than Group B, which we expected since the concept that neither company collects sensitive information is easier to understand and also more expected.
Specifically, the full policy text comparison had only 21% accuracy for Group A, but also again performed worst across all conditions.
Note that the standardized short text policy did well when the two companies' practices were aligned but accuracy dropped nearly 30% when the policies had different practices, a larger drop than that found in the other standardized policies.
This question relates back to question #6 which asked participants if the Acme policy collected medical or health information.
Participants who correctly answered that question should be more likely to answer this question correctly.
For Policy Group A, 84% of the people who answered question #15 correctly had already correctly determined that the Acme company collected medical information when answering question #6.
However, 44% of those who answered question #6 correctly did not come to the correct conclusion for question #15.
For Policy Group B, 72% of the people who answered question #15 correctly had already correctly determined that the Acme company did not collect medical information.
However, 18% of those who answered question # 6 correctly did not come to the correct conclusion for question #15.
The first comparison task, Question 14 asked: "Does either company give you options with regards to cookies?"
For Policy Group A, the Acme policy does provide an opt out regarding cookies while Bell does not.
For Policy Group B, the Acme policy does not provide any options regarding cookies while Bell does.
Focusing on Policy Group B, we note that the standardized short text received only 20% accuracy , with 49% of respondents incorrectly answering that neither company gave options with regards to cookies.
For this format, a participant must have understood that the first paragraph applied to cookies, and then noticed the ability to opt out of either use or sharing practices in the last two lines.
The gap between the full policy text and the standardized formats widens from about half a point when looking at a single policy to as much as one and a quarter points after making comparisons.
While the layered text notice performed quite similarly to the full policy text in accuracy measures, we see a very different result in participants' feelings about using layered notices.
The likert scores for layered policies were not significantly different than the standardized table format .
Percentage of participants who would choose to make a purchase from The Bell Group after answering comparison questions.
None of the groups answered this question significantly differently before answering the comparison questions.
Only the full policy text for Group A performed significantly differently from the standardized table .
Table 6 shows the results for the latter question, which are nearly identical to those before answering the comparison questions.
While Policy Group B results were consistent across the conditions, Policy Group A results were not.
In Policy Group A, we see that the standardized short text format favors the Bell policy by the largest margin, while the full policy text format is the only condition where the participants favored the Acme policy.
Our large-scale online study showed that policy formats do have significant impact on users' ability to both quickly and accurately find information and on users' attitudes regarding the experience of using privacy polcies.
The three standardized formats that were designed by researchers with usability and standardization in mind performed significantly better than the full and layered text policies that currently exist online today.
These two policy formats, across the variety of measures we tested, performed consistently worse.
The large amount of text in full text policies and the necessity to drill down through a layered policy to the entire policy to understand specific practices greatly lengthens the amount of time and effort required to understand a policy.
Additionally, more complex questions about data practices or data sharing frequently require reading multiple sections of these text policies and understanding the way different clauses interact, which is not an easy task for the average consumer.
Our earlier work  showed that the standardized table performed much better than text policies; however, it was unclear given our study design whether the improvement came from the tabular format or the standardization.
We have shown here that it is not solely the table-based format, but holistic standardization that leads to success.
Our standardized short text policy left no room for erroneous, wavering, or unclear text, serving as a concise textual alternative to tabular formats.
While the standardized short text policy we developed was successful for most tasks it may not scale as gracefully as the standardized tables.
The standardized short text policy did perform significantly more poorly than the standardized grid in one of the two policy groups.
It is this performance drop, that while slight, forces us to question the scalability of the format.
This is also evident in the information-collection tasks where users may not have been as capable of finding certain types of information in the short text, especially if it was in the middle of a block of text.
Because of the way we generate the text, complex policies are longer than simple policies; however, complexity is often privacy protecting and should not be cognitively penalized.
The short text policy could grow to up to ten paragraphs for complex policies, which is a concern for information finding.
Using our custom survey tool, we recorded the time it took participants to complete each question in our survey.
We examined completion times for the simple, complex, and comparison tasks, as presented in Table 7.
Statistical significance was tested using ANOVA on the log-normalized time information across policy formats.
For each of these three groups of questions, as well as the overall study completion time there were statistically significant differences across policy formats .
The standardized formats, on average were between 26-32% faster than the full text policy, and 22% faster than the layered text policy.
For the most qualitative of our measures, we asked the participants how they felt about looking at privacy policies.
We asked six 7-point Likert scale questions after they completed the single policy tasks and three more about comparing policies.
The results are summarized in Table 8.
While there were significant differences for nearly all the Likert questions, we will not go into the details of each question, but average across the two groups of questions.
For the single-policy tasks, participants across the board reported that they were confident in "my understanding of what I read of Acme's privacy policy."
The question with the most significant strength in the single policy tasks was the final question: "If all policies looked just like this I would be more likely to read them," with the three standardized policies scoring higher than the full policy text.
The three comparison Likert questions show a much larger shift towards the standardized formats and away from the full policy text.
Mean scores on 7-point Likert scale for single-policy questions , and comparison questions .
While participants feel neutral with a single policy, the range widens when comparing policies.
Rows marked with an asterisk represent statistically significant enjoyability differences between conditions .
Some questions could not be answered correctly from reviewing the layered policy without clicking through to the full policy.
However, in this study only 25 of the 79 layered-format condition participants ever clicked through the layered policy to access the full policy.
Those who accessed the full policy at least once took an average of 6.6 minutes longer to answer the study questions than those in the layered-format condition who never accessed the full policy.
Surprisingly, there were not significant differences in accuracy between layered-format participants who never viewed the full policy, and those who did access the full policy; both groups answered just under half the questions correctly.
The standardized formats performed the best overall, across the variety of the metrics we looked at.
The accuracy, comparison, and speed results drastically eclipse the results of the text formats in use today.
The standardized table and standardized short table overall performed very similarly.
While there are five cases where the full table outperforms the short table, and only one in the other direction, these differences are frequently small, and they perform similarly on the remaining 80% of tasks.
One concern in the design stage was that removing rows from the table would make comparisons a more cognitively difficult task.
This may be evidenced from the significant performance differences in questions 14 and 15; however, the differences in number of rows in the policies we selected were not extreme, never differing by more than one row.
It is not clear how great the differences in the types of data collected between real-world policies actually are.
There are still future refinements that can be made to these policy displays.
Users had difficulty with complex information-finding tasks even with standardized formats.
While the current accuracy with our best formats is better than simple guessing, there is still room for further study and improvement.
The short standardized text policy did perform well with information that was not collected, used, or shared, even in comparison to the short standardized table with which it shares an identical text notice for this information.
We believe that this can be attributed to the larger type size than the short text policy itself, while underneath the colorful and larger short standardized table, the notice is not as easily visible.
One area where the full text policies did perform as well as the other formats was on user enjoyment after the single policy tasks in one of the two policy groups.
This may be partially attributed to users' pre-existing familiarity with similar formats.
However, this dropped when users reached the comparison tasks, which we expected to be a difficulty with long text policies.
From our earlier work, we observed that when asked to compare the enjoyment of reading policies between the standardized table format and the full policy text, we noted steep improvements in enjoyment of the table format .
With this study's between-subjects design, we were not able to see such effects.
Enjoyability results for the layered policies were significantly better than for the full text policies, even though there were not significant differences in accuracy scores between layered and full policies.
Levy and Hastak reported that "consumers have little prior knowledge and experience with information sharing characteristics of financial institutions, they will find it more difficult to understand privacy notice information unless they are provided with more context than is presented in current notices," and continuing to provide better education and context will help consumers make better decisions .
While our attached list of definitions is a start, framing the policy with contextual information, and presenting comparisons in more useful ways would be productive direction to take future research in usable privacy policies.
A. McDonald and L. Cranor.
The cost of reading privacy policies.
In Proceedings of the Technology Policy Research Conference, September 26-28 2008.
A comparative study of online privacy policies and formats.
In Proceedings of 2009 Workshop on Privacy Enhancing Technologies.
R. Reeder, L. Cranor, P. Kelley, and A. McDonald.
A user study of the expandable grid applied to p3p privacy policy visualization.
In Workshop on Privacy in the Electronic Society, 2008.
The Center for Information Policy Leadership.
The Center for Information Policy Leadership.
Ten steps to develop a multilayered privacy notice, 2005.
Disclosure of institution privacy policy, 2008.
The Privacy Label was developed by the CyLab Usable Privacy and Security Laboratory with support from the U.S. Army Research Office contract DAAD19-02-1-0389  to Carnegie Mellon Universitys CyLab, NSF Cyber Trust UserControllable Security and Privacy for Pervasive Computing grant CNS-0627513, NSF IGERT grant on DGE-0903659 by Microsoft through the Carnegie Mellon Center for Computational Thinking, FCT through the CMU/Portugal Information and Communication Technologies Institute, and the IBM OCR project on Privacy and Security Policy Management.
The label design team was led by Patrick Gage Kelley and included Joanna Bresee, Aleecia McDonald, Robert Reeder, Sungjoon Steve Won, and Lorrie Cranor.
Additional thanks go to Cristian Bravo-Lillo, Lucian Cesca, Robert McGuire, Daniel Rhim, Norman Sadeh, and Janice Tsai.
S. Balasubramanian and C. Cole.
Consumers' search and use of nutrition information: The challenge and promise of the nutrition labeling and education act.
A. Drichoutis, P. Lazaridis, and R. Nayga.
Consumers' use of nutritional labels.
In Academy Marketing Science Review, 2006.
C. Jensen and C. Potts.
Privacy policies as decision-making tools: An evaluation of online privacy notices.
In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 471-478, Vienna, Austria, 2004.
A "Nutrition Label" for Privacy.
In Proceedings of the 2009 Symposium On Usable Privacy and Security , 2009.
Kleimann Communication Group Inc. Evolution of a prototype financial privacy notice., February 2006.
Consumer comprehension of financial privacy notices: A report on the results of the quantitative testing, 2008.
Despite the benefits they derive from social networking sites , members of those services are not always satisfied with their online behaviors.
The investigation of desires for behavior change in SNSs both provide insight into users' perceptions of how SNSs impact their lives  and can inform tools for helping users achieve desired behavior changes.
We use a 604-participant online survey to explore SNS users' behavior-change goals for Facebook, Instagram, and Twitter.
While some participants want to reduce site use, others want to improve their use or increase a range of behaviors.
These desired changes differ by SNS, and, for Twitter, by participants' levels of site use.
Participants also expect a range of benefits from these goals, including more free time, contact with others, intrinsic benefits, better security/privacy, and improved self presentation.
Based on these results we provide insights both into how participants perceive different SNSs, as well as potential designs for behavior-change mechanisms to target SNS behaviors.
We use a 604-participant online survey of SNS-behaviorchange goals to examine how SNS users want to change their behaviors on Facebook, Instagram, and Twitter.
Just as goals for improving health- or finance-related behaviors relate to how people see health or finance impacting their lives, looking at participants' behavior-change goals provides insights into how they view SNSs as potentially enhancing their lives or as being potentially detrimental.
We also compare participant goals across Facebook, Instagram, and Twitter, and find differences in how participants view the three sites.
Different types of goals require different approaches.
Thus, we look at participants' goals from the perspective of designing for behavior change, and draw on persuasive design techniques, like the Fogg Behavior Model , to explore how different levers could be applied to help participants achieve different types of goals.
We provide two primary contributions.
We expand on prior work on perceived benefits and tradeoffs of SNSs  by using behavior-change goals to explore how people view SNSs as impacting their lives, and by describing the range of goals participants have.
We also compare perceived benefits and downsides across Facebook, Instagram, and Twitter.
Additionally, we draw on the Fogg Behavior Model  to provide insight into persuasive-design levers that could be incorporated into mechanisms to help users achieve SNS-behavior-change goals.
We specifically address the following research questions: * Q1: What types of behavior-change goals do participants have for their SNS use?
How do these vary by SNS?
What steps do they think are necessary?
Many participants view SNSs as beneficial and want to increase use or posting to obtain benefits like increased contact with others, improved self-presentation, or more attention.
However, some participants see SNSs as detracting from their lives and want to use them less or better, often to free up time or reduce potential for security/privacy or self-presentation issues.
Social networking sites  provide many benefits including entertainment, information, and tools for staying in touch with others.
However, as in other areas of life like health or finance, people are not always satisfied with how they use SNSs.
Just as people sometimes want to eat less junk food or save more money, SNS users also sometimes want to change their behaviors on the sites.
Some want to try to increase perceived benefits, while others want to avoid perceived downsides of SNS use.
Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author.
Copyright is held by the author/owner.
While these may be appropriate or desired strategies in some cases, at other times users may wish to adjust, or eliminate the need for, these behaviors.
This dynamic may be reflected in behavior-change goals.
Different people may seek to avoid different types of regrets or may rely on various coping behaviors to different degrees and with different perceived levels of necessity.
We examine SNS users' behavior-change goals to better understand the full range of behaviors users may wish to change.
SNSs are an emerging part of peoples' lives and provide a range of benefits.
However, user actions sometimes result in regret or other sub-optimal outcomes like unnecessary selfcensorship or over-sharing.
Persuasive design and goals theory provide insight into mechanisms to address behaviors users wish to change.
SNSs like Facebook, Instagram, and Twitter provide a range of uses and benefits.
People use Facebook to connect and chat with others, post photos and updates, for games and apps, to look up others, to pass time, for entertainment, and for professional connections .
They also use the site to ask for and provide social support .
Different motives for using Facebook predict use of different features and different actions , and various uses also result in different benefits.
Receiving directed messages on Facebook, for example, was associated with increased bridging social capital, while passively reading the site was only associated with an increase in social capital for users with lower communication skills .
Similarly, on Twitter, a sense of "connection" increased with use of the site, but increased more if users tweeted .
We provide an expanded view on SNS uses and benefits by examining SNS-behavior-change goals.
Eliciting and exploring a range of goals for SNS-behavior change allows us to explore the full range of perceived current or potential impacts of SNS-use, including perceived benefits as well as perceived downsides.
We also contribute by comparing these goals across Facebook, Instagram, and Twitter, demonstrating differences in perceptions of the three SNSs.
Persuasive design, and specifically the Fogg Behavior Model  , provides a theoretical basis for examining behavior-change mechanisms.
We draw on this model to explore potential avenues for facilitating SNS-behavior change.
In the FBM behavior requires three factors: a person is motivated, has the ability to perform the behavior, and is triggered to carry out the behavior.
Motivation can be increased or decreased using pleasure/pain, hope/fear, and social acceptance/rejection.
Ability is based on the absence or presence of time, money, physical or mental effort, compliance with social norms, and how routine the behavior is.
Behavior-change designs can target these factors, can create or reduce behavior triggers, or can make triggers more or less salient .
The type of desired behavior can prompt additional insight.
In the Behavior Grid, Fogg categorizes behaviors based on whether the behaviors require increasing, decreasing, stopping, or doing a new or familiar behavior a single time, permanently, or for a specific duration.
These categories inform interventions for different types of behaviors.
For example, Fogg describes how some behaviors respond to predictable "cycle" triggers, while others require unpredictable "cue" triggers .
We use the FBM to examine participant goals.
We look at self-reported ability and motivation, as well as facilitators and barriers to achieving goals that roughly correspond to Fogg's motivation- and ability-based axes.
We also draw on Fogg's Behavior Grid to examine behaviors involved in steps participants believed they could take to achieve their goals.
Our survey design was also informed by goal-setting theory.
Goal-setting theory looks at the impact of goals, goal setting, and feedback on behavior change.
Goal setting supports behavior change, although its efficacy is mediated by factors such as the actual and perceived difficulty of the goal and belief in the goal's importance.
Work has also found that having more short term, actionable proximal goals combined with more distant, distal goals increases likelihood of success .
SNS users also sometimes behave in ways they might want to change, motivating behavior-change goals.
These goals may be prompted by the unique contexts and affordances- or lack thereof-presented by SNSs.
For example, "context collapse," the need to communicate with different groups, and allowances for broadcast communications, can make it difficult for users to properly determine audiences, and can prompt coping behaviors .
Regret can arise when SNS users' posts are viewed by unintended audiences, from unforseen consequences, or when users violate social norms, post critical messages, post in highly emotional states, or post content that reveals too much .
Users also sometimes employ coping strategies to try to avoid regret, which they may consider successful or suboptimal.
For example, users sometimes self-censor content that they might benefit from sharing with at least some people .
Similarly, some people may wish to share health struggles or successes via SNSs but choose not to do so because of a fear of boring others or appearing boastful .
We performed an online, Amazon Mechanical Turk  survey with 604 SNS users.
Participants were United States MTurk workers who self-reported having either a Facebook, Instagram, or Twitter account and having logged into it in the last month.
Each participant answered questions about one SNS, either Facebook, Instagram, or Twitter .
If a participant reported using more than one of the SNSs, one was randomly selected.
The survey format was loosely drawn from goal-setting theory .
Participants described "the one thing you would most like to change about your behavior on" the SNS, which was referred to as their goal.
They provided open-ended responses to explain the goal, why they wanted to change their behavior, benefits of achieving it, as well as anything that currently made it easier or more difficult.
They also described three concrete steps they could take toward achieving their goal.
These concrete steps were intended to reflect proximal goals from goal-setting theory , in contrast to the more distal goal described in the initial free response.
Participants also answered Likert scale questions about the difficulty and importance of achieving their goal.
Participants were then asked 23, 25, or 32 Likert scale questions, depending on the SNS.
These questions were primarily potential goals that emerged from prior work on SNS regrets and behaviors , tailored to each SNS.
Participants rated how much they agreed or disagreed with each .
They also answered questions about their SNS use, tailored to the SNS, and about their demographics.
The protocol was approved by the Carnegie Mellon University Institutional Review Board.
604 MTurk workers from the United States completed the survey.
40% of participants self-reported as male, ages ranged from 18 to 73 , 73% of participants reported opening and viewing their accounts at least once a day, and 14% were students .
Gender did not significantly differ across SNSs .
Login frequency was significantly different: 83% of Facebook, 64% of Instagram, and 52% of Twitter participants reported opening and viewing their accounts at least once a day .
Average participant age was also significantly different across SNSs .
Instagram participants tended to be younger  than Twitter  or Facebook  participants.
This may reflect the relatively large and growing use of Instagram among younger users .
We coded the free responses about the goals using five sets of codes .
We iteratively developed the codes, beginning by free-coding randomly selected sets of data.
For steps, facilitators, and barriers, the codes that emerged after several rounds of iterative coding resembled categories from the FBM.
Thus, we then drew heavily from various aspects of the FBM for additional rounds of code development .
After codebook development, one researcher coded all the responses.
A second coder coded 100 responses from each dataset to verify the codebooks.
Participants wanted to reduce, increase, and improve SNS use to free up more time and improve contact, self presentation, and their own security/privacy.
Goals varied by SNS; while participants tended to want to post more on Instagram and Twitter, they tended to want to reduce Facebook use.
For Twitter, some goals also varied by participant-use levels.
Participants also described a range of perceived steps toward their goals as well as factors that served as barriers and facilitators to achieving their goals.
Participant goals show how participants see the SNSs potentially adding to or detracting from their lives.
They described the one thing they most wanted to change about their SNS behavior  and rated agreement with supplied goals.
Many participants  wanted to use the site less to be more productive.
However, participants also wanted to improve how they posted  or used the site  or to increase how much they used the site  or posted  for reasons ranging from increasing contact with others to improving how they presented themselves.
To examine unprompted goals we asked participants to: "Think about how you typically use .
Please describe the one thing you would most like to change about your behavior on " and to explain "why you would like to change your behavior in this way."
We coded each set of responses into one of six goal types and one of nine reasons for wanting to achieve the goal .
Participants wanted to: post more, use the SNS more, improve their SNS use, improve their posting, and use the SNS less.
We compared frequencies of goals across SNSs using ANOVA tests, with Bonferroni correction for multiple testing.
Of the participants, 540 described goals .
We worded our question to encourage participants to volunteer a behavior-change goal based on earlier versions of the survey, in which we found that participants tended to opt out of describing a goal but then agreed with a range of provided goals.
However, 27 participants indicated they didn't want to change any behaviors.
We exclude these participants from our analysis.
An additional 37 participants described desired changes to the SNS interface, for example an increased Twitter character limit, instead of behavior changes.
Although desired interface changes may reflect underlying behavior-change goals, we focused on goals directly related to behaviors and also exclude these participants.
Three participants described only desired outcomes  but not a method to achieve them.
We code these three goals as having reasons, but not types, and include them in our analysis.
Instagram and Twitter respondents, alternatively, frequently wanted to post more for intrinsic reasons .
These Instagram and Twitter participants tended to believe they should use the sites better, or would generally benefit from more posting or use, for example: "I feel like I am not using Twitter as fully as I can."
Some participants felt a sense of responsibility to increase use or posting: "Others follow me and I don't write anything" or "I might be missing important information by skipping over certain content."
Many participants' goals related to posting on the SNS more , for example tweeting, commenting, or posting status updates or pictures , or increased use of other features, like reading feeds or messaging others  .
Frequencies of goals related to posting more were significantly different across the three SNSs .
Percent frequencies of goals related to using the sites more were not significantly different.
There were several trends in the reasons provided for wanting to post more.
Facebook participants tended to want to post more for contact-related reasons .
They described wanting to post more to stay in touch with people they were close with, family, or friends.
One participant, for example, wanted to "post more pictures" because "My mom is also on Facebook, and I know she would love to see more pictures of my kids."
This may reflect trends in related work, in which Facebook was used for social support .
The prevalence of wanting to reduce use varied across SNSs .
Participants who wanted to use Facebook less tended to want to do so for time-related reasons , describing, for example: "I lose too many hours per day while using it."
They also sometimes wanted to reduce use for intrinsic reasons , often because Facebook evoked negative emotions.
One participant explained that "it usually just ends up pissing me off."
Using the SNS better included changing use, for example filtering reading better or trading off use of one feature for another.
Posting better included changing how or what they posted, including posting less, different, or better content, or changing their attitude about posting.
There were no significant differences across the three SNSs for frequencies of wanting to post better  or use the sites better .
Across all three SNSs, participants  wanted to post better to improve selfpresentation.
They wanted to write more interesting or positive posts or self-censor more, for example posting less frequently while drunk or posting less profanity.
Notably, some Facebook participants  also wanted to improve use or posting for security or privacy reasons, often "Not posting personal info."
Participants also described "any benefits you think you would get by achieving your goal."
Most of the benefit codes corresponded to codes used for the reasons participants gave for their goals .
Reasons, however, were often behavioral , while benefits were often expected outcomes .
Participants  also sometimes perceived record creation as a potential benefit.
For many participants potential benefits corresponded to reasons for goals .
For example, a participant who wanted to improve their use of the site because "I have hurt people's feelings" felt the main benefit would be "No one angry at me.
Knowing I have not hurt anyone."
For other participants, however, potential benefits differed from their reasons .
Many participants who wanted to achieve goals for intrinsic reasons, for example, felt that the benefits would be attention- , contact- , or time-  related.
Some participants with time-related reasons felt that the benefits would be contactrelated .
These participants tended to feel they could free up time to spend with friends and family, often offline, for example: "Having more free time with family and friends and start to get outside more."
Participants rated their agreement with 23-32 goals drawn from prior work and tailored to the SNS.
They answered on a five-point scale from "Strongly disagree"  to "Strongly agree" , with an additional "Not applicable" option.
We used the Likert questions to create five  or six  scales based on theory and prior work, with scores averaged for each scale .
We removed several questions, prior to creating the scales, with agreement rates of less than 15% for the SNS.
People vary in level and type of SNS use.
We use logistic regressions with Bonferroni correction for multiple testing to examine associations between levels of SNS use and agreement with the different goals for each SNS.
Free responses about participants' most important goals provided categories of behavior-change goals and a view of participant perceptions of the three SNSs.
However, participants may want to achieve multiple goals to various degrees, and their desires to achieve these different goals may vary based on how they use the SNSs.
Because social aspects of the SNSs were often important in free responses, we also include use of SNS messaging , as well as age, gender, number of friends/followers/following and use of privacy features.
Questions differed by SNS, so we only perform analyses by, rather than across, SNS.
Facebook participants expressed more agreement with goals related to privacy and audience , presentation of self , and attention , relative to other types of goals.
For Facebook participants, there were no significant associations between use levels and goal types.
However, rarely looking at privacy settings was positively associated with agreeing to goals related to wanting to spend more time on Facebook  and goals related to wanting more attention .
Also, participants who used Facebook messaging weekly or more were significantly more likely to want to reduce their time on the site .
Twitter participants also expressed relatively higher agreement with presentation of self-related goals  but also tended to agree with goals related to attention  and increased time or activity .
This was consistent with free responses in which they often gave attention or self presentation-related reasons for goals  and wanted to increase site use and posting .
Twitter participants' average agreement for less time/activity-related goals was relatively lower than the other scales .
In the free responses Twitter participants also infrequently wanted to reduce use of the site .
Participants who tweeted regularly were less likely to agree with goals related to increasing their Twitter use  than participants who just reported site consumption and, conversely, were significantly more likely to agree to goals related to wanting to reduce site use .
Instagram participants also tended to have lower agreement for the scales related to spending less time on the site  relative to the other scales.
This was also consistent with the free responses; only 15/79 Instagram participants described wanting to use the site less.
There were no significant associations between site use and goal type.
However, presentation of self goals were more common for participants who regularly used Instagram's messaging feature  .
This may reflect a desire for presentation management among these users who send messages directly.
Identifying the perceived difficulty and perceived importance of behavior-change goals can guide designers to interventions that facilitate behavior change by enhancing ability  or enhancing motivation, respectively.
33% of participants  considered their goals neither difficult nor important.
Many of these goals related to posting more .
Participants considered 13%  difficult but not important, many of which related to reducing use .
Participants considered 35% of the goals  not difficult but important, many of which included using the site less  or posting more .
Finally, 19%  of the goals were related as difficult and important, many of which included using the site less  or posting better .
Figure 4 summarizes perceived difficulty and importance by the reasons given for the goals.
Goal attainment requires actionable near-term steps, similar to goal theory's "proximal goals."
It also requires drawing on facilitating factors that may already exist as well as overcoming existing barriers.
We used Fogg's Behavior Grid to examine opportunities to facilitate the steps participants believed they should take to achieve their goals.
We also use ability and motivational factors drawn from the FBM as a starting point to examine factors participants perceived as facilitators or barriers to their goals.
According to the FBM and goal-setting theory, goal attainment and strategies for attainment depend on perceived difficulty and importance.
We asked participants how much they agreed or disagreed with "Achieving my goal is important to me" and "is difficult" on five-point Likert scales from "Strongly disagree" to "Strongly agree."
After participants described their goals, we asked for "three specific steps" they could take "in the next month, toward achieving your goal."
The question was free response.
Some participants listed no steps and others listed less than three, resulting in 1,620 steps for 540 goals.
We coded the steps based on Fogg's Behavior Grid for classifying behavior for change .
Each was given a time interval, either a onetime action, a permanent or ongoing action, a permanent cued action, irregularly cued by events or time intervals , or a duration event that lasted a specific amount of time.
We also coded the steps for a type, either a decrease or increase in an activity or an amount , stopping an activity , or taking a new or familiar action.
Figure 5 describes these steps.
Many steps required making new or familiar actions  permanent.
Participants described actions that would last from that point forward and were not direct increases, decreases, or cessations of existing actions.
Some participants described self-monitoring strategies they thought they could take.
They described adjusting their awareness level or attitude, for example, "Exercise discipline," or "not take myself too seriously."
Other prospective steps related to making it easier or harder to perform behaviors participants sought to change, for example, "Read a book" as an alternative to using Facebook.
Participants who wanted to improve or increase use suggested steps to proactively create better content or social connections.
For example, a participant who wanted to post more varied pictures on Instagram suggested they could "Take some pictures of scenery."
For goals related to posting more or using the SNS better or more often, steps to permanently increase an activity or quantity were common .
These steps tended to involve increasing use of the SNS in various ways or creating more content to post on the SNS, for example: "I could take more photos" or "Follow more people."
For goals like posting better or using the SNS less or better, participants often described permanently decreasing similar activities .
These steps tended to involve reducing general activities on the site or specific types of posts.
Participants also sometimes described permanently stopping activities , for example, stopping certain behaviors to improve posting like: "not Drink" or "Avoid very personal posts."
Participants also suggested permanently increasing or decreasing cued behaviors, similar to Fogg's "cued" triggers .
These tended to be for goals related to using the site more or posting more .
Cued steps typically involved posting or using the site on a timed  basis, for example, "Post daily on Twitter."
Many participants wanted to decrease cued actions for goals related to using the SNS less .
These steps often involved limiting use to a specific number of times per day or a specific time period, limit, or schedule.
Alternatively, some steps were one-time actions.
To reduce use, this included one-time actions to limit access or use , such as deleting the SNS application.
Participants also described barriers, anything they thought "currently makes it difficult to achieve your goal" and facilitators, anything they thought "currently makes it easier to achieve your goal."
We coded each barrier into one of eleven categories  loosely based on ability and motivation factors from the FBM .
Across goal types, except using the SNS less, participants described barriers related to personality or attitude , for example, "My stubborness" or "lack of self-discipline" as well as traits like "I'm too blunt" or "I am a quiet, shy reserved person by nature."
Participants also noted lack of time or general busyness as challenges, especially for goals related to increasing posting or use .
Participants also described current habits as a challenge  for reducing SNS use , but also for posting more  or better .
They described the SNS as an "addiction" and how the behavior change was not part of their routine or what they currently do, for example: "I'm too comfortable in my routine."
For some participants , especially those who wanted to post better  or reduce use , challenges arose from pleasure in, or benefits from, behaviors they wanted to change.
For example, one participant wanted to reduce use but "I enjoy messaging my friends on Facebook, and wouldn't want to give that up."
Another participant didn't want to post when drunk but noted "I really like beer."
Participants who wanted to use the SNSs less also described access to the site or using it frequently as challenges .
Participants also described factors they felt would facilitate goal achievement.
We coded each facilitator into one of twelve categories  that were again loosely based on persuasive technology work .
Many of these categories overlapped the barriers.
The same factor can facilitate some goals while serving as a barrier to others .
Many participants described how motivation or awareness of the goal would make it easier to achieve .
This was common for participants who wanted to reduce use .
They described, for example, "will power."
Technology-related factors were also common, such as SNS features or general ease of use , or easy site access or high levels of activity , primarily for goals like posting or using the site more.
A few participants , mostly those who wanted to use the site better or less, mentioned lack of access or activity as facilitators.
Some participants also described social factors that could make their goals easier .
These included support or help from others, for example, "Everyone I know uses Twitter, so they can encourage and remind me" or more generally, "Having a partner to vent to."
Participants also sometimes described more general social pressures, like comparing oneself to others, as a facilitating force.
Facilitators and Barriers Tech- generally related to technology, technical features, technological difficulties, opportunities, or ease Tech: have access/activity- having access may make it harder to stop using or easier to use more/differently Tech: lack access/activity- lacking access may make it harder to use more/differently or easier to stop Time/money- availability of time/money can make it harder or easier to complete goals Attitude/personality- a certain attitude or state of being  makes it difficult or easier to change this behavior, also includes altered states.
Pleasure/temptation/motivation- enjoying/benefiting from the current behavior makes it hard to change or believing they will enjoy/benefit from the change in behavior is motivational Life events- the participant's current life state 
Barriers only: Lack motivation- doesn't want to do it, thinks it's difficult, doesn't make it a priority.
Habit/distraction- distraction prevents participants from making a change, remembering to make a change, or a change is hard to make because it is a habit or addiction.
Facilitators only: Take an action- making their goal easier will come through taking a concrete action, actively changing something.
Easy/fits in with life- thinking this change will easily fit into the participants life makes it easier.
We used MTurk to perform our study.
While the choice of any recruitment platform introduces some biases, MTurk gave us access to a sample in a time- and cost-effective manner, has relatively known demographics, and is comparable to other online sources .
We also used quality-control measures, including upfront open-response questions and filtering for participants who answered nonsensically, as well as requiring participants to meet a quality-control threshold.
We also chose a survey-based approach.
We intended to supplement prior work that highlighted aspects of the SNSbehavior-change space 
To minimize biasing participants or inducing survey fatigue we limited the length of our survey and were not able to provide the rich, qualitative data that could be drawn from other formats, like interviews or fieldwork.
This work is, instead, intended to provide initial insights over a wide range of dimensions, including types of participants, different SNSs, and types of goals.
We also asked participants for self-report data, allowing for a range of biases.
Responses are likely biased toward more socially acceptable and recent goals, and may over-represent memorable events.
We also purposefully biased participants towards responding with a goal to our free response question, based on pilot testing.
This push toward identifying a goal may partially explain why several participants later said they did not think the goals they named were important.
Participants described a range of behavior-change goals.
These goals varied by SNS and, for Twitter participants, by site-use level.
Facebook participants tended to want to reduce use, while Twitter and Instagram participants wanted to post more or improve their use.
Participants envisioned steps they thought they could take to achieve their goals, many of which entailed beginning permanent new or familiar behaviors or repeating cued behaviors.
Participants also described factors that could facilitate or serve as barriers to their goal achievement.
These factors could inform the design of mechanisms for helping users achieve various desired behavior changes.
Participants' behavior-change goals provide a view into how SNS users perceive potential benefits they could achieve from SNSs, as well as perceived potential downsides of SNS use.
Some goals demonstrate how participants feel SNSs could positively impact their lives and changes that might help them receive these benefits more reliably.
Participants described wanting to increase site use or use of specific features like posting or messaging to achieve a range of benefits including increased contact, more entertainment, or general better use of the sites.
Other goals focused on reducing potentially detrimental effects of SNSs.
Some participants generally wanted to reduce time on the sites because they felt it would help with busyness, lack of productivity, or a desire for more free time.
Alternatively, some participants wanted to target specific, potentially harmful behaviors.
For example some participants wanted to avoid drunken posting, posting too much personal information, or posting context- or audience-inappropriate content, behaviors that have been found to cause regret or other negative outcomes .
Overall goals varied by SNS, demonstrating that participants may associate different potential benefits and negative consequences with different SNSs.
For example, Facebook participants tended to want to use the site less , while Instagram and Twitter participants tended to want to use the sites more .
Facebook participants also tended to want to achieve their goals for contact-related reasons.
More work is needed to explore the basis for these differences in goals across sites, but they may partially reflect differences in the affordances offered by the structures of the three sites.
People tend to use Facebook for a range of content and interactions 
This may lead some people to feel like they waste time on the site because of large quantities of available content, and some people to feel that they should use it more for social connections because of the range of strong and weak tie relationships available.
Alternatively, Twitter and Instagram provide a more limited set of content and features.
The sites are also more frequently used for non-reciprocated follower/followee relationships than Facebook .
This may lead participants to see these sites as information sources and to aspire to increase consumption.
Twitter participants' goals also varied with different levels of site use.
Compared to participants who only read content, participants who regularly tweeted tended to disagree with goals related to spending more time on the site and, conversely, agree with goals related to spending less time on the site.
This may occur because users who already post regularly see the downsides of doing so , while users who do not post regularly only see the potential benefits.
More research is needed to probe the factors driving these differences.
Participants ranged in motivation for, and ability to achieve, their goals.
This demonstrates potential avenues for intervention that also are apparent in the short-term steps and perceived facilitators and barriers described by participants.
According to persuasive design, and specifically the Fogg Behavior Model, motivation is necessary for a behavior.
Many participants described facilitators or barriers related to a presence or lack of motivation.
Participants reported feeling motivation  related to behaviors they wanted to change or achieve.
Tools or mechanisms could increase or decrease these factors, for example increasing motivation by making future benefits clearer, or providing reminders of the importance of the change.
Ability to perform a task is also necessary under the FBM.
Several facilitators, barriers, and near-term steps, related to ability.
Participants pointed out how access to technology, time/money, habit/how well the behavior fit into the their lives, and how easy the technology was to use could facilitate or serve as a barrier to behavior change.
This was also reflected in steps in which participants described seeking education or more information.
Interventions could also target these areas to try to impact ability.
For example, tools could help users make time for an activity through reminders or scheduling.
Alternatively, a tool could help a user reduce time for an activity by helping plan alternatives.
In the FBM, a behavior also requires a trigger.
Implicit triggers were reflected in many of the steps participants described.
Some of these steps were cued by other behaviors or external factors; participants described taking an action every time something occurred or at regular intervals.
Interventions could create more explicit triggers.
For example, an alarm or reminder could help a user remember to perform an activity like posting content.
Different combinations of these levers could be included in intervention mechanisms to help users achieve different desired behavior changes.
Beyond using participant behavior-change goals to help understand how people view SNSs, examining these goals, facilitators, and barriers allows us to explore potential mechanisms for helping users achieve desired behavior changes.
Based on the range of participant goals, there are several categories of opportunities for behavior-change mechanisms.
First, many goals, like reducing use, could be addressed by productivity or time-management-focused behavior-change interventions.
Alternatively, other interventions could focus on helping users increase SNS use, for example, prompting use of specific features or trying to help users feel that they are making better use of the site.
Beyond insights for persuasive design levers, the range of participant goals also has implications for more general SNSbehavior-change design.
Prior work focused primarily on generalized interventions; however, overly general interventions may not apply to all users.
SNS-behavior-change goals, perceived facilitators and barriers, and perceived benefits vary widely.
Interventions, therefore, should be personalized, targeted, or adaptable to a range of goals or needs.
In some past work, for example Wang et.
However, goals  vary, and a lever that might facilitate one goal or need  might impede users with different goals or needs .
Future interventions should seek to match mechanisms to users' needs and goals.
Social factors may also play an important role in SNSbehavior-change design.
Many participants described social support or pressure as facilitators or barriers to their goals.
In SNS contexts, social connections can be powerful facilitators, especially if supported by, or included in, behavior-change mechanisms.
Motivation could be boosted by enlisting others' help or by allowing the user to compare themselves to others in a network.
Ability could be increased by helping the user reach out to people in their network who could help them perform relevant tasks.
People in a network could also provide reminders or other triggers for behaviors.
However, designers should also be careful about drawing on social factors.
For example, comparisons to other people can backfire if participants begin to perceive their current behaviors as typical of, or superior to, others .
Alternatively, use of social information may create tensions.
It may, for example, remind users that they like the social benefits associated with using a site, even if they would, overall, prefer to decrease use so they can be more productive.
Future work should examine the potential for using social factors in SNSbehavior-change interventions.
This material is based upon work supported by the National Science Foundation under Grants No.
0946825 and CNS1012763 , as well as by Google under a Focused Research Award on Privacy Nudges, the IWT SBO Project on Security and Privacy for Online Social Networks , Microsoft Research, Facebook, and the ARCS Foundation.
Designers of SNS-behavior-change interventions should also consider the broader SNS context.
Most SNSs have business models that rely on holding user attention.
While this may be consistent with some behavior-change goals , it is at least on the surface counter to others .
In the longer term, though, it may still be to the advantage of SNS businesses to balance support of use and non-use goals.
When users feel that they are acting counter to their interests or identity, or are concerned about sub-optimal behaviors, they can fall back on coping mechanisms that can include site nonuse behaviors, ranging from content deletion, de-friending and self-censorship to account deletion .
For longer-term user engagement, it may benefit SNSs to encourage interactions users consider beneficial.
Participants recognize a range of potential benefits from SNSs, including increased social contact, better self presentation, and more attention.
In some cases they want to change behaviors to take advantage of these benefits.
However, they also recognize potential downsides of the sites and sometimes want to use SNSs less or in improved ways to free up time or avoid negative self-presentation or security/privacy risks.
These participant goals vary by SNS.
For Facebook, participants tend to want to reduce use, while for Instagram and Twitter participants tend to want to increase use.
This range of behavior-change goals and perceived benefits shows potential for different types of SNS-behavior-change interventions and, along with provided steps and facilitators/barriers, provides insight into design for SNS-behavior-change mechanisms.
I tweet honestly, I tweet passionately: Twitter users, context collapse, and the imagined audience.
Newman, M. W., Lauterbach, D., Munson, S. A., Resnick, P., and Morris, M. E. It's not that I don't have problems, I'm just not putting them on Facebook: Challenges and opportunities in using online social networks for health.
Papacharissi, Z., and Mendelson, A.
Toward a new  sociability: Uses, gratifications and social capital on Facebook.
Pew Internet & American Life Project.
Demographics of key social networking platforms, 2013.
Potential-goal questions asked to participants as Likert scale questions .
The sets of Likert scale questions were collapsed into the provided scales for analysis.
Questions in gray were excluded because of low levels of agreement .
Abstract Text-based passwords remain the dominant authentication method in computer systems, despite significant advancement in attackers' capabilities to perform password cracking.
In response to this threat, password composition policies have grown increasingly complex.
However, there is insufficient research defining metrics to characterize password strength and evaluating password-composition policies using these metrics.
In this paper, we describe an analysis of 12,000 passwords collected under seven composition policies via an online study.
We develop an efficient distributed method for calculating how effectively several heuristic password-guessing algorithms guess passwords.
Leveraging this method, we investigate  the resistance of passwords created under different conditions to password guessing;  the performance of guessing algorithms under different training sets;  the relationship between passwords explicitly created under a given composition policy and other passwords that happen to meet the same requirements; and  the relationship between guessability, as measured with password-cracking algorithms, and entropy estimates.
We believe our findings advance understanding of both password-composition policies and metrics for quantifying password security.
Text-based passwords are the most commonly used authentication method in computer systems.
As shown by previous research , passwords are often easy for attackers to compromise.
A common threat model is an attacker who steals a list of hashed passwords, enabling him to attempt to crack them offline at his leisure.
Once these passwords have been cracked, they can be used to gain access not only to the original site, but also to other accounts where users have reused their passwords.
This is an important consideration because studies indicate that password reuse  is a common and growing practice as users acquire more online accounts .
To mitigate the danger of such attacks, system administrators specify password-composition policies.
These policies force newly created passwords to adhere to various requirements intended to make them harder to guess.
Typical requirements are that passwords include a number or a symbol, that they exceed a certain minimum length, and that they are not words found in a dictionary.
Although it is generally believed that password-composition policies make passwords harder to guess, and hence more secure, research has struggled to quantify the level of resistance to guessing provided by different passwordcomposition policies or the individual requirements they comprise.
The two most commonly used methods for quantifying the effect of password-composition policies are estimating the entropy of passwords induced by password-composition policies using NIST guidelines , and empirically analyzing passwords created under different password-composition policies with password-guessing tools .
The former, however, is not based on empirical data, and the latter is difficult to apply because of the dearth of password sets created under different passwordcomposition policies.
In this paper, we take a substantial step forward in understanding the effects of password-composition policies on the guessability of passwords.
Second, we develop approaches for calculating how long it would take for various password-guessing tools to guess each of the passwords we collected.
This allows us to evaluate the impact on security of each password-composition policy.
Our paper makes the following specific contributions: 1.
We implement a distributed technique  to determine if and when a given passwordguessing algorithm, trained with a given data set, would guess a specific password.
This allows us to evaluate the effectiveness of password-guessing attacks much more quickly than we could using existing cracking techniques.
We compare, more accurately than was previously possible, the guessability of passwords created under different password-composition policies.
Because of the efficiency of our calculations , we can investigate the effectiveness of multiple password-guessing approaches with multiple tunings.
Our findings show that a password-composition policy requiring long passwords with no other restrcitions provides  excellent resistance to guessing.
We study the impact of tuning on the effectiveness of password-guessing algorithms.
We also investigate the significance of test-set selection when evaluating the strength of different password-composition policies.
We investigate the effectiveness of entropy as a measure of password guessability.
For each composition policy, we compare our guessability calculations to two independent entropy estimates: one based on the NIST guidelines mentioned above, and a second that we calculate empirically from the plaintext passwords in our dataset.
We find that both measures of entropy have only very limited relationships to password strength as measured by guessability.
Mechanical Turk and controlled password collection.
As with any user study, it is important to reflect on the origin of our dataset to understand the generalizability of our findings.
We collected a dataset of 12,000 plaintext passwords using Amazon's Mechanical Turk crowdsourcing service .
Many researchers have examined the use of MTurk workers  as participants in human-subjects research .
About half of all Turkers are American, with Indian participation increasing rapidly in the last 2-3 years to become about one third of Turkers .
American Turkers are about two-thirds women, while Indian Turkers are similarly weighted toward men .
Overall, the Turker population is younger and more educated than the general population, with 40% holding at least a bachelor's degree; both of these trends are more pronounced among Indian Turkers .
This study, and others, found that well-designed MTurk tasks provide high-quality user-study data .
This analysis of MTurk has important implications in the context of studying passwords.
We expect our findings will be more generalizable than those from lab studies with a more constrained participant base.
Because we collected demographic information from our participants, our sample  can be more accurately characterized than samples based on stolen password lists from various websites collected under uncertain circumstances.
A related consideration is that while our participants created real passwords that were needed several days later to complete the study and obtain a small bonus payment, these passwords did not protect high-value accounts.
Password research has consistently been limited by the difficulty of studying passwords used for high-value accounts.
Lab studies have asked participants to create passwords that protect simulated accounts, $5, a chance to win an iPod in a raffle, or access to university course materials including homework and grades .
Other studies have relied on the leaked password lists like the RockYou set .
While this set contains millions of passwords, it also contains non-password artifacts that are difficult to filter out definitively, its provenance and completeness are unclear, and it is hard to say how much value users place on protecting an account from a social gaming service.
Other commonly used leaked password lists come from sites including MySpace, silentwhisper.net, and a variety of Finnish websites, with user valuations that are similarly difficult to assess .
Overall, although our dataset is not ideal, we contend that our findings do provide significant insight into the effects of password-composition policies on password guessability.
Because so little is known about this important topic, even imperfect information constitutes progress.
In Section 2 we survey related work.
We describe our data collection and analysis methodology in Sections 3 and 4.
We convey our main results in Section 5, and discuss the generalizability of our findings and some ethical considerations in Section 6.
We conclude in Section 7 with a discussion of the applicability of our work to future research and the implications of our findings on defining practical password-composition policies.
Research on passwords has been active for many years.
In this section, we review the work most closely related to our research, first summarizing the different types of data collection and analysis that have been used.
We then discuss work focused on evaluating the impact of password policies, followed by metrics proposed to evaluate password strength.
Collection and analysis of password data.
Many prior studies of passwords have used small sample sizes , obtained through user surveys or lab studies.
We also use an online survey, but we consider larger and more varied sets of passwords.
In addition, we recruit participants using Mechanical Turk, which produces more diverse samples than typical lab studies .
Other studies analyze large samples of passwords ostensibly created by users for actual accounts of varying importance .
Unlike these studies, we study the impact of different password policies on password strength and use passwords collected under controlled password-policy conditions.
Several studies have considered the impact of different password policies on password strength.
In lab studies, Proctor et al.
We consider similar data, but for a much larger set of users, allowing us to evaluate the effectiveness of various requirements more comprehensively.
Other findings suggest that too-strict policies  induce coping strategies that can hurt both security and productivity .
Further, Flor encio and Herley found that the strictest policies are often used not by organizations with high-value assets to protect, but organizations that do not have to compete on customer service .
An increasingly popular password-strengthening measure that we also investigate is subjecting new passwords to a blacklist check.
Spafford demonstrated that a blacklist check can be performed in constant time regardless of size , making large blacklist checking feasible.
This offers many theoretical advantages over other password-composition schemes.
Effective evaluation of password strength requires defining a proper metric.
One possible metric is information entropy, defined by Shannon as the expected value  of the information contained in a string .
Massey connects entropy with password strength by demonstrating that entropy provides a lower bound on the expected number of guesses to find a text .
A 2006 National Institute of Standards and Technology  publication uses entropy to represent the strength of a password .
Verheul derives a theoretical distribution of variable-length passwords with optimal entropy and guess resistance .
Flor encio and Herley estimated theoretical entropy for the field data they analyzed .
An alternative to entropy as a metric of password strength is the notion of "guessability," which characterizes the time needed by an efficient password-cracking algorithm to discover a password.
In one use of this metric, Weir et al.
Similarly to our work, Dell'Amico et al.
Marechal  and Weir  both examine this model and find it more effective for password cracking than the popular password-cracking program John the Ripper .
The authors found their technique to be more effective than John the Ripper.
In a separate study, Zhang et al.
In this work, we apply the Weir et al.
From August 2010 to January 2011, we advertised a two-part study on Mechanical Turk, paying between 25 and 55 cents for the first part and between 50 and 70 cents for the second part.
The consent form indicated the study pertained to visiting secure websites.
Each participant was given a scenario for making a new password, then asked to create a password that met a set of password-composition requirements; the scenarios and requirements are detailed in Section 3.3.
Participants who entered a password that did not conform to requirements were shown an error message indicating which requirements were not met, then asked to try again until a satisfactory password was created.
After creating a password, participants took a brief survey about demographics and password creation.
Participants were then asked to recall the password just created; after five failed attempts, the password was displayed.
For the second part of the study, participants were emailed two days later and asked to return to the website and recall their passwords.
We also measured the incidence of passwords being written down or otherwise stored .
Unless otherwise noted, only data from the first part of the study is reported in this paper.
Data from the second part of the study, which is primarily used to assess memorability and usability factors, is omitted due to space constraints.
Prior research has considered memorability and usability factors for a subset of the policies we examine ; we briefly revisit these findings when we discuss our results in Section 5.
We use six publicly available word lists as training data in our analysis and to assemble the blacklists used in some of our experimental conditions.
The RockYou password set  includes more than 30 million passwords, and the MySpace password set  contains about 45,000 passwords.
The inflection list1 contains words in varied grammatical forms such as plurals and past tense.
The simple dictionary contains about 200,000 words and is a standard English dictionary available on most Unix systems.
We also used two cracking dictionaries from the Openwall Project2 containing standard and mangled versions of dictionary words and common passwords.
The free Openwall list contains about 4 million words, while the paid Openwall list contains more than 40 million.
While these data sources are not ideal, they are publicly available; we expect attackers would use these word lists or others like them for training data.
In Section 5.2, we consider the effect of a variety of training sets drawn from these word lists as well as our collected password data.
Our participants were divided into eight conditions comprising seven sets of password-composition requirements and two password-creation scenarios.
We used two scenarios in order to measure the extent to which giving participants different instructions affects password strength.
The survey scenario was designed to simulate a scenario in which users create low-value passwords, while the email scenario was designed to elicit higher-value passwords.
All but one condition used the email scenario.
In the survey scenario, participants were told, "To link your survey responses, we will use a password that you create below; therefore it is important that you remember your password."
In the email scenario, participants were told, "Imagine that your main email service provider has been attacked, and your account became compromised.
You need to create a new password for your email account, since your old password may be known by the attackers.
Because of the attack, your email service provider is also changing its password rules.
Please follow the instructions below to create a new password for your email account.
We will ask you to use this password in a few days to log in again, so it is important that you remember your new password.
Please take the steps you would normally take to remember your email password and protect this password as you normally would protect the password for your email account.
Please behave as you would if this were your real password!"
The eight conditions are detailed below.
This is the only condition using the survey scenario.
Only the scenario differentiates this from basic8survey.
It may not contain a dictionary word."
We performed a dictionary check by removing nonalphabetic characters and checking the remainder against a dictionary, ignoring case.
This method is used in practice, including at our institution.
We used the free Openwall list as the dictionary.
It may not contain a dictionary word."
We performed the same dictionary check as in dictionary8.
This condition reproduced NIST's comprehensive password-composition requirements .
It may not contain a dictionary word."
We checked the password against the simple Unix dictionary, ignoring case.
Unlike the dictionary8 and comprehensive8 conditions, the password was not stripped of non-alphabetic characters before the check.
For this condition, we trained Weir et al.
Both the training and testing were conducted case-insensitively, increasing the strength of the blacklist.
Among participants who completed part one of our study, 55% returned within 3 days and completed part two.
We detected no statistically significant differences in the guessability of passwords between participants who took just the first part of the study and those who participated in both parts.
As a result, to maximize the participant data in our analyses and use the same number of participants for each condition, our dataset includes passwords from the first 1,000 participants in each condition to successfully complete the first part of the study.
To conduct a wider variety of experiments, we used data from an additional 2,000 participants each in basic8 and comprehensive8.
Among these 12,000 participants, 53% percent reported being male and 45% female, with a mean reported age of 29 years.
This makes our sample more male and slightly younger than Mechanical Turk participants in general .
About one third of participants reported studying or working in computer science or a related field.
The proportion related to computer science did not vary significantly across conditions, except between blacklistEasy and blacklistHard .
Participants in the basic16 condition were slightly but significantly older  than those in blacklistHard, basic8, and comprehensive8 .
We observed no significant difference in gender between any pair of conditions .
This section explains how we analyzed our collected password data.
First, and most importantly, Section 4.1 discusses our approach to measuring how resistant passwords are to cracking, i.e., guessing by an adversary.
We present a novel, efficient method that allows a broader exploration of guessability than would otherwise be possible.
For comparison purposes, we also compute two independent entropy approximations for each condition in our dataset, using methods described in Section 4.2.
However, as the computational power of potential adversaries increases, it becomes important to understand how many passwords an adversary could crack with many more guesses.
To this end, we introduce the guess number calculator, a novel method for measuring guess resistance more efficiently.
We take advantage of the fact that, for most deterministic password-guessing algorithms, it is possible to create a calculator function that maps a password to the number of guesses required to guess that password.
We call this output value the guess number of the password.
A new guess number calculator must be implemented for each cracking algorithm under consideration.
For algorithms like  that use a training set of known passwords to establish guessing priority, a new tuning of the calculator is generated for each new training set to be tested.
Because we collect plaintext passwords, we can use a guessing algorithm's calculator function to look up the associated guess number for each password, without actually running the algorithm.
We use this approach to measure the guessability of a set of passwords in several ways.
We compute the percentage of passwords that would be cracked by a given algorithm, which is important because the most efficient cracking tools use heuristics and do not explore all possible passwords.
We can also compute the percentage that would be cracked within a given number of guesses, or the number of guesses required to crack a certain percentage of passwords.
We also use calculators to compare the performance of different cracking algorithms, and different training-set tunings within each algorithm.
By combining guess-number results across a variety of algorithms and training sets, we can develop a general picture of the overall strength of a set of passwords.
We implemented two guess-number calculators: one for a brute-force algorithm loosely based on the Markov model, and one for the heuristic algorithm proposed by Weir et al., which is currently the state-of-the-art approach to password cracking .
We selected these two algorithms as the most promising brute-force and heuristic options, respectively, after comparing the passwords we collected to lists of 1, 5, and 10 billion guesses produced by running a variety of cracking tools and tunings.
From this point forward, we will refer to them as the brute-force Markov  and Weir algorithms.
Both algorithms for which we implemented calculators require a training set: a corpus of known passwords used to generate a list of guesses and determine in what order they should be tried.
We explore a varied space of training sets constructed from different combinations of the publicly available word lists described in Section 3.2 and subsets of the passwords we collected.
This allows us to assess whether complementing publicly available data with passwords collected from the system under attack improves the performance of the cracking algorithms.
We further consider training-set variations specifically tuned to our two most complex policy conditions, comprehensive8 and basic16.
Each of our experiments calculates guess numbers only for those passwords on which we did not train, using a cross-validation approach.
For a given experiment, we split our passwords into n partitions, or folds.
We generate a training set from public data plus  folds of our data, and test it on the remaining fold.
We use each of the n folds as test data exactly once, requiring n iterations of testing and training.
We recombine results from the n folds, yielding guess-number results for all of our passwords.
Because training often involves significant computational resources, as described in Section 4.1.3, we limit to two or three the number of iterations in our validation.
Based on the similarity of results we observed between iterations, this seems sufficient.
We describe our training and test sets in detail in Appendix A.
We do not claim these training sets or algorithms represent the optimal technique for guessing the passwords we collected; rather, we focus on comparing guess resistance across password-composition policies.
Investigating the performance of guessing algorithms with different tunings also provides insight into the kind of data set an attacker might need in order to efficiently guess passwords created under a specific password-composition policy.
The BFM calculator determines guess numbers for a brute-force cracking algorithm loosely based on Markov chains .
Our algorithm differs from previous work by starting with the minimum length of the password policy, and increasing the length of guesses until all passwords are guessed.
Unlike other implementations, this covers the entire password space, but does not try guesses in strict probability order.
The BFM algorithm uses the training set to calculate the frequency of first characters and of digrams within the password body, and uses these frequency to deterministically construct guessing orders of unknown passwords.
If A is the most likely starting character learned from the training data, the character most likely to follow A is B, and the character most likely to follow B is C, then the first guess will be ABC.
If the next-most-likely character to follow B is A, the second guess will be ABA, and so forth.
Our guess-number calculator for this algorithm processes the training data to generate a lookup table that maps each string to the number of guesses needed to reach it, as follows.
For an alphabet of N characters, and passwords of length L, any time the first character tried is incorrect, we know that the algorithm will try N L 1 incorrect guesses before switching to a different first character.
So, if the first character of the password to be guessed is the k -th character to be tried, there will be at least N L 1 incorrect guesses.
We can then iterate the computation: when the first character is correct, but the second character is incorrect, the algorithm will try N L 2 incorrect guesses, and so forth.
By looking up the order in which characters are tried, we can then simply add up the total number of incorrect guesses to discover how many iterations will be needed before hitting a successful guess for a given password, without having to actually try the guesses.
We also apply the principle of calculating guess numbers to Weir et al.
The Weir algorithm is explained in detail in , and uses the following definitions: structures are patterns of character types such as letters, digits, and symbols; a terminal is one instantiation of a structure; and a probability group is a set of terminals with the same probability of occurring.
As with the BFM calculator, we process training data to create a lookup table, then calculate the guess number for each password.
The mechanism for processing training data is outlined in Algorithm 1.
To calculate the guess number for a password, we determine that password's probability group.
Using the lookup table created from the training set, we determine how many guesses would be required to reach that probability group.
We then add the number of guesses required to reach the exact password within that probability group.
This works because once the Weir algorithm reaches a given probability group, all terminals in that group are tried in a deterministic order.
Because creating this lookup table is time-intensive, we set a cutoff point--50 trillion guesses, which allows most Weir-calculator experiments to run in 24 hours or less in our setup--past which we do not calculate the guess number for additional passwords.
By checking the available terminals, we can still determine whether passwords that are not guessed by this point will ever be guessed, but not exactly when they will be guessed.
Algorithm 1 Creation of a lookup table which, given a probability group, returns the number of guesses required for the Weir algorithm to begin guessing terminals of that group.
For example, for UUss9UUU, the l.c.s.
T = New Lookup Table for all structures s do for all probability group pg 2 s do for all l.c.s.
2 pg do ci =Number of terminals of l.c.s.
Calculating guess numbers for Weir's algorithm becomes data intensive for the sets of structures used in this work.
To accelerate the process, we implemented a distributed version of Algorithm 1 as follows.
Each iteration of the loop for the probability groups in s emits an intermediate tuple.
The intermediate tuples produced by all the tasks are grouped by probability ranges, sorted, and stored.
A final sequential pass over the sorted table adds the sum of prior values.
We implemented our distributed approach using Hadoop , an open-source version of the MapReduce framework .
Sorted T Identity Expand In the implemented approach, while all m part 1 structure tasks receive equally sized subsets of the input, the tasks perform different amounts of Expand Sorted T Identity work depending on the complexity of the structure part 2 structures in each respective input subset.
As a result, task execution times vary widely.
Shuffle Reduce Map Input Output Nevertheless, this approach enabled us to sort phase phase compute guess numbers for sets of 4000 passwords in hours, rather than days, in a 64- Figure 1: Distributed computation of the Weir-algorithm guess numbers node Hadoop cluster.
The resulting lookup using Hadoop.
The framework splits the set of input structures and exetables store on the order of hundreds of bil- cutes a map task for each subset.
The map tasks emit tuples of the form lions of elements with their associated prob- hprobability, pg, ci i.
The framework sorts the the intermediate tuples using abilities and occupy up to 1.3 TB of storage a total ordering partitioner.
A reduce task, which in this case is the identity reducer, writes to storage a subset of the sorted tuples.
In order to investigate how well entropy estimates correlate with guess resistance in practice, we compare our guess number results for each condition in our dataset to two independently calculated entropy approximations.
First, we apply the commonly used NIST guidelines, which suggest that each password-composition rule contributes a specific amount of entropy and that the entropy of the policy is the sum of the entropy contributed by each rule.
Our second approximation is calculated empirically from the plaintext passwords in our dataset, using the technique described by Shay et al.
In this method, we calculate for each password in the condition the entropy contributed by the number, content, and type of each character, using Shannon's formula .
We then sum the individual entropy contributions to estimate the total entropy of the passwords in that condition.
We calculated guess numbers under 32 different combinations of algorithm and training data.
Although we do not have space to include all the raw results, we distill from them four major findings with application both to selecting password policies and to conducting password research: * Among conditions we tested, basic16 provides the greatest security against a powerful attacker, outperforming the more complicated comprehensive8 condition.
We also detail a number of other findings about the relative difficulty of cracking for the different password-composition policies we tested.
While adding more and better training data provides little to no benefit against passwords from weaker conditions, it provides a significant boost against stronger ones.
We discuss these findings in detail in the rest of this section.
We introduce individual experiments in detail before discussing their results.
In this section, we compare the guessability of passwords created under the seven password-composition policies we tested.
We focus on two experiments that we consider most comprehensive.
In each experiment we evaluate the guessability of all seven password-composition policies, but against differently trained guessing algorithms.
Experiment P4 is designed to simulate an attacker with access to a broad variety of publicly available data for training.
It consists of a Weir-algorithm calculator trained on all the public word lists we use and tested on 1000 passwords from each condition.
Experiment E simulates a powerful attacker with extraordinary insight into the password sets under consideration.
It consists of a Weir-algorithm calculator trained with all the public data used in P4 plus 500 passwords from each of our eight conditions.
We test on 500 other passwords from those conditions, with two-fold cross-validation for a total of 1000 test passwords.
The results from experiments E and P4 are shown in Figures 2 and 3, respectively.
As suggested by these figures, which password-composition policy is best at resisting guessing attacks depends on how many attempts an attacker is expected to make.
At one million and one billion guesses in both experiments, significantly fewer blacklistHard and comprehensive8 passwords were guessed than in any other condition.3 At one billion guesses in experiment E, 9.5%, 1.4%, and 2.9% of passwords were cracked in basic16, comprehensive8, and blacklistHard respectively; 40.3% of basic8 passwords were cracked.
As the number of guesses increases, basic16 begins to outperform the other conditions.
At one trillion guesses, significantly fewer basic16 passwords were cracked than comprehensive8 passwords, which were in turn cracked significantly less than any other condition.
After exhausting the Weir-algorithm guessing space in both experiments, basic16 remains significantly hardest to crack.
The next best at resisting cracking were comprehensive8 and blacklistHard, performing significantly better than any of the other conditions.
Condition comprehensive8 was significantly better than blacklistHard in experiment P4 but not in experiment E. In experiment E, 14.6, 26.4, and 31.0% of passwords were cracked in basic16, comprehensive8, and blacklistHard respectively; in contrast, 63.0% basic8 passwords were cracked.
Although guessing with the Weir algorithm proved more effective, we also compared the conditions using BFM.
The findings  are generally consistent with those discussed above: basic16 performs better than the other conditions.
Prior research examining the memorabil70% ity and usability of a subset of the compob8s 60% sition policies we examine here found that b8 blE while in general less secure policies are more 50% blM usable, basic16 is more usable than compre40% d8 hensive8 by many measures .
This sug30% gests basic16 is an overall better choice than comprehensive8.
Although we do not believe this affects our overall findings, Figure 3: The number of passwords cracked vs. the number of guesses, per further investigation would be beneficial.
As b8 d8 a result, it is important to consider how c8 80% the choice of training data affects the sucb16 cess of password guessing, and consequently 60% the guess resistance of a set of passwords.
40% To address this, we examine the effect of varying the amount and source of train20% ing data on both total cracking success and on cracking efficiency.
Interestingly, we 1E6 1E12 1E18 1E24 1E30 1E36 find that the choice of training data affects different password-policy conditions differNumber of guesses  ently; abundant, closely matched training data is critical when cracking passwords Figure 4: The number of passwords cracked vs. the number of guesses, usfrom harder-to-guess conditions, but less so ing the BFM calculator trained on a combination of our data and public data when cracking passwords from easier ones.
We place a red vertical line at 50 trillion guesses to facilitate comparison For purposes of examining the impact with the Weir experiments.
We stopped the Weir calculator at this point , but because the BFM algorithm is so much less of training data, the password-policy condiefficient, we ran it for many more guesses in order to collect useful data.
For the rest of this section, we will refer to the harder-to-guess conditions of comprehensive8, basic16, and blacklistHard as group 1, and the rest as group 2.
We first measure, via three experiments, the effect of increasing the amount and variety of training data.
Experiment P3 was trained on public data including the MySpace and RockYou password lists as well as the inflection list and simple dictionary, and tested on 1000 passwords from each of our eight conditions.
Experiment P4, as detailed in Section 5.1, was trained on everything from P3 plus the paid Openwall list.
Experiment E, also described in 5.1, was trained on all the public data from P4 as well as 500 passwords from each of our conditions, using two-fold cross-validation.
Figure 5 shows how these three training sets affect four example conditions, two from each group.
The cracking totals in each experiment reflect the overall increase in knowledge as training data is added.
For group 1, adding Openwall increases total cracking on average 45%, while adding both Openwall and our data provides an average 96% improvement .
In group 2, by contrast, the increases are more modest and only occasionally significant.
At one trillion and one billion guesses, the results are less straightforward, but increasing training data remains generally more effective against group 1 than group 2.
Adding Openwall alone is not particularly helpful for group 1, providing few significant improvements at either guessing point, but it actually decreases cracking at one billion guesses significantly for several group 2 conditions.
At these guessing points, adding our data is considerably more effective for group 1 than adding Openwall alone, increasing cracking for each of the three conditions by at least 50% .
By contrast, adding our data provides little to no improvement against group 2 conditions at either guessing point.
Taken together, these results demonstrate that increasing the amount and variety of information available in the training data provides significant improvement in cracking the harder-to-guess conditions, while providing little benefit and sometimes decreasing efficiency for the easier-to-guess conditions.
Having determined that training with specalized data is extremely valuable for cracking group 1 passwords, we wanted to examine what quantity of closely related training data is needed to effectively crack these "hard" conditions.
For these tests, we focus on comprehensive8 as an example of a harder-to-guess condition, using the easier-to-guess basic8 condition as a control; for each of these conditions, we collected 3000 passwords.
We conducted five Weir-algorithm experiments, C8a through C8e, in which we trained on all the word lists described in Section 3.2, as well as between 500 and 2500 comprehensive8 passwords, in 500-password increments.
For each experiment, we tested on the remaining comprehensive8 passwords.
We also carried out a similar set of five experiments, B8a through B8e, in which we trained and tested with basic8 rather than comprehensive8 passwords.
Our results, illustrated in Figure 6, show that incrementally adding more of our collected data to the training set improves total cracking slightly for comprehensive8 passwords, but not for basic8.
On average, for each 500 comprehensive8 passwords added to the training set, 2% fewer passwords remain uncracked.
This effect is not linear, however; the benefit of additional training data levels off sharply between 2000 and 2500 training passwords.
The differences between experiments begin to show significance around one trillion guesses, and increase as we approach the total number cracked.
For basic8, by contrast, adding more collected passwords to the training set has no significant effect on total cracking, with between 61 and 62% of passwords cracked in each experiment.
No significant effect is observed at earlier guessing points including one million, one billion, or one trillion guesses, either.
One way to interpret this result is to consider the diversity of structures found in our basic8 and comprehensive8 password sets.
The comprehensive8 passwords are considerably more diverse, with 1598 structures among 3000 passwords, as compared to only 733 structures for basic8.
For comprehensive8, the single most common structure maps to 67 passwords, the most common 180 structures account for half of all passwords, and 1337 passwords have structures that are unique within the password set.
By contrast, the most common structure in basic8 maps to 293 passwords, the top 13 structures account for half the passwords, and only 565 passwords have unique structures.
As a result, small amounts of training data go considerably farther in cracking basic8 passwords than in comprehensive8.
The publicly available word lists we used for training are all considerably larger than the number of passwords we collected.
As a result, we needed to weight our data  if we wanted it to have significant impact on the probabilities used by our guess-number calculators.
Different weightings have no effect on the total number of passwords cracked, as all the same passwords are eventually guessed; however, they can affect the order and, therefore, the efficiency of guessing.
We tested three weightings, using 500 passwords from each of our eight conditions weighted to one-tenth, equal, and ten times the cumulative size of the included public lists.
Overall, we found that weighting had only a minor effect.
There were few significant differences at one million, one billion, or one trillion guesses, with equal weighting occasionally outperforming the other two in some conditions.
From these results, we concluded that the choice of weighting was not particularly important, but we use an equal weighting in all other experiments that train with passwords from our dataset because it provides an occasional benefit.
We also investigated the effect of training data on the performance of the BFM calculator, using four training sets: one with public data only, one that combined public data with collected passwords across our conditions, and one each specialized for basic8 and comprehensive8.
Because the BFM algorithm eventually guesses every password, we were concerned only with efficiency, not total cracking.
We found that adding our data had essentially no effect at either smaller or larger numbers of guesses.
Specialized training for basic8 was similarly unhelpful.
Specialized training for comprehensive8 does increase efficiency somewhat, reaching 50% cracked with about 30% fewer guesses.
Researchers typically don't have access to passwords created under the password-composition policy they want to study.
To compensate, they start with a larger set of passwords , and pare it down by discarding passwords that don't meet the desired composition policy .
A critical question, then, is whether subsets like these are representative of passwords actually created under a specific policy.
We find that such subsets are not representative, and may in fact contain passwords that are more difficult to guess than passwords created under the policy in question.
We performed this comparison with two different training sets: public data, with an emphasis on RockYou passwords that meet comprehensive8 requirements ; and the same data enhanced with our other 2000 collected comprehensive8 passwords .
Both experiments show significant differences between the guessability of comprehensive8 and comprehensiveSubset test sets, as shown in Figure 8.
In the two experiments, 40.9% of comprehensive8 passwords were cracked on average, compared to only 25.8% comprehensiveSubset passwords.
The two test sets diverge as early as one billion guesses .
Ignoring comprehensiveSubset passwords that were created under the basic16 condition allows us to analyze 171 passwords, all created under less strict conditions.
Only 25.2% of these passwords are cracked on average, suggesting that subsets drawn exclusively from less strict conditions are more difficult to guess than passwords created under stricter requirements.
To understand this result more deeply, we examined the distribution of structures in the two test sets.
There are 618 structures in the 1000-password comprehensive8 set, compared to 913 for comprehensiveSubset .
Fifty-two percent of comprehensive8 passwords have unique structures, compared to 85% for comprehensiveSubset.
This distribution of structures explains why comprehensive8 is significantly easier to guess.
We do not know why the two samples are different, although we suspect it may be related to the comprehensiveSubset subset isolating those users who make the most complex passwords.
Regardless of the reason for this difference, however, researchers seeking to compare password policies should be aware that such subsets may not be representative.
Historically, Shannon entropy  has provided a convenient single statistic to summarize password strength.
It remains unclear, however, how well entropy reflects the guess resistance of a password set.
While information entropy does provide a theoretical lower bound on the guessability of a set of passwords , in practice a system administrator may be more concerned about how many passwords can be cracked in a given number of guesses than about the average guessability across the population.
Although there is no mathematical relationship between entropy and this definition of guess resistance, we examine the possibility that the two are correlated in practice.
To do this, we consider two independent measures of entropy, as defined in Section 4.2: an empirically calculated estimate and a theoretical NIST estimate.
For both measures, we find that entropy estimates roughly indicate which composition policies provide more guess resistance than others, but provide no useful information about the magnitude of these differences.
We ranked our password conditions based on the proportion of passwords cracked in our most complete experiment  at one trillion guesses, and compared this to the rank of conditions based on empirically estimated entropy.
We found these rankings, shown in Figure 9, to be significantly correlated .
However, looking at the proportion of passwords cracked at a million or a billion guesses, the correlation in rankings is no longer significant .
These results indicate that entropy might be useful when considering an adversary who can make a large number of guesses, but is not useful when considering a smaller number of guesses.
Further, empirically estimated entropy was unable to predict correctly the ranking of dictionary8, even when considering a large number of guesses.
This condition displayed greater resistance to guessability than basic8, yet its empirically estimated entropy was lower.
This might indicate a flaw in how entropy was estimated, a flaw in the guessing algorithm, or an innate shortcoming of the use of entropy to predict guessability.
Since entropy can only lower-bound the guessability of passwords, it is possible for the frequency distribution of dictionary8 to have low entropy but high guess resistance.
If this is the case, Verheul theorized that such a distribution would be optimal for password policy .
Computing the NIST entropy of our password conditions produces three equivalence classes, as shown in Figure 9.
These arise because NIST entropy is not granular enough to capture all differences between our conditions.
First, NIST entropy does not take into account the size of a dictionary or its implementation.
All five of our dictionary and blacklist conditions meet the NIST requirement of a dictionary with at least 50,000 words .
Implementation details, such as case-insensitive blacklist checking or the removal of non-alphabetic characters before a dictionary check, are not considered in the entropy score.
Our results show that these details lead to password policies with very different levels of password strength and should be considered in a future heuristic.
Second, the NIST entropy scores for basic16 and comprehensive8 are the same, even though basic16 appears to be much more resistant to powerful guessing attacks.
This may suggest that future heuristics should assign greater value to length than does the NIST heuristic.
Perhaps surprisingly, the equivalence classes given by NIST entropy are ordered correctly based on our results for guessability after 50 trillion guesses.
Though its lack of granularity fails to capture differences between similar password conditions, NIST entropy seems to succeed at its stated purpose of providing a "rough rule of thumb" .
We stress that although both measures of entropy provide a rough ordering among policies, they do not always correctly classify guessability , and they do not effectively measure how much additional guess resistance one policy provides as compared to another.
These results suggest that a "rough rule of thumb" may be the limit of entropy's usefulness as a metric.
We next discuss a number of important issues regarding ethics, ecological validity, and the limitations of our methodology.
Most of our results rely on passwords we have collected during a user study .
However, we also use the RockYou and MySpace password lists.
Although these passwords have collectively been used by a number of scientific works that study passwords , this nevertheless creates an ethical conundrum: Should our research use passwords acquired illicitly?
We use these passwords only to train and test guessing algorithms, and not in relationship with any usernames or other login information.
Furthermore, as attackers are likely to use these password sets as training sets or cracking dictionaries, our use of them to evaluate password strength implies our results are more likely to be of practical relevance to security administrators.
As with any user study, the ecological validity of our approach is important to the generalizability of our results.
First, it is important to understand the results in the context of our participant sample.
As we describe in Sections 1 and 3.4, our sample of Mechanical Turk participants is somewhat younger and more educated than the general population, but more diverse than typical small-sample password studies.
A second factor inviting consideration is that the passwords we collected did not protect high-value accounts.
As we describe in Section 1, this is a longstanding limitation of password research.
To gain insight into the extent to which our participants behaved as they would in non-study conditions, we tested two password-creation scenarios : one was taking a survey, designed to observe user behavior with passwords for short-term, low-value accounts; and one was a simulated change to a longer-term, higher-value email account.
Our users provided stronger passwords  in the email scenario, a result consistent with users picking better passwords to protect a  high-value e-mail account than a low-value survey account.
All our conditions except basic8 used the email scenario.
In our study, as in the real world, some users wrote down or otherwise stored their passwords.
We asked participants who returned for the second half of the study whether or not they stored the password they had created , and we also instrumented the password-entry form to detect copy-paste and browser auto-fill behavior.
We detected about 6% of participants using these methods of storage, while overall about one third admitted storing their passwords.
Participants in comprehensive8 stored their passwords significantly more often than those in the other conditions .
We designed our study to minimize the impact of sampling and account-value limitations.
All our findings result from comparisons between conditions.
Thus, we believe it is likely that our findings hold in general, at least for some classes of passwords and some classes of users.
We tested all sets of passwords with a number of password-guessing tools; the one we focus on  always performed best.
There may exist algorithms or training sets that would be more effective at guessing passwords than anything we tested.
While this might affect some of our conclusions, we believe that most of them are robust, partly because many of our results are supported by multiple experiments and metrics.
In this work, we focused on automated offline password-guessing attacks.
There are many other real-life threats to password security, such as phishing and shoulder surfing.
Our analysis of password strength does not account for these.
The password-composition policies we tested may induce different behaviors, e.g., writing down or forgetting passwords or using password managers, that affect password security.
Although such effects have previously been studied for a subset of the policies in this study , space constraints dictate that a comprehensive investigation is beyond the scope of this paper.
Although the number and complexity of password-composition requirements imposed by systems administrators at a wide range of organizations have been steadily increasing, the actual value added by these requirements is poorly understood.
In this work, we take a substantial step forward in understanding not only these requirements themselves, but also the process of evaluating them.
We introduced a new, efficient technique for evaluating password strength that can be implemented for a variety of password-guessing algorithms and tuned using a variety of training sets to gain insight into the comparative guess resistance of different sets of passwords.
Using this technique, we were able to perform a more comprehensive password analysis than had previously been possible.
We found several notable results about the comparative strength of different composition policies.
Although NIST considers basic16 and comprehensive8 equivalent, we found that basic16 is superior for large numbers of guesses.
Combined with a prior result that basic16 is also easier for users, this suggests that basic16 is the better policy choice .
Our findings highlight several interesting points in the password-policy space and suggest some directions for further research to more fully detail a complete set of tradeoffs among composition-policy requirements.
Our results also reveal important information about conducting guess-resistance analysis.
Effective attacks on passwords created under complex or rare-in-practice composition policies require access to abundant, closely matched training data.
In addition, this type of password set cannot be characterized correctly simply by selecting a subset of conforming passwords from a larger corpus; such a subset is unlikely to be representative of passwords created under the policy in question.
Finally, we report that Shannon entropy, though a convenient single-statistic metric of password strength, provides only a rough correlation with guess resistance and is unable to correctly predict quantitative differences in guessability among password sets.
We thank Mitch Franzos of the Carnegie Mellon University Parallel Data Laboratory.
Here we detail the complete training and test data used in each of our Weir-algorithm experiments.
The first column gives the experiment number.
The next three columns list the three types of training data used to create a Weircalculator experiment.
The structures column describes the wordlist used to generate the set of character-type structures that define the Weir algorithm's search space.
The digits and symbols column lists the wordlist that determine the probabilities with which combinations of digits and symbols can be filled into those structures.
The strings column shows which wordlists provide the probabilities with which alphabetic strings are filled into structures.
In most cases, we train strings on as much data as possible, while restricting structure and digit/symbol training to those wordlists that contain a quality sample of multi-character-class passwords.
In the final column, we describe the set of passwords that we attempted to guess in a given experiment.
We also list the complete training and test data used in each of our BFM experiments.
The experiment number and test set columns are the same as in the Weir subtable.
Training for the BFM calculator, however, is considerably simpler, using only one combined wordlist per experiment; these lists are detailed in the training set column.
Abbreviations for all the training and test sets we use are defined in the key below the tables.
