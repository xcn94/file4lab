The multi-touch-based pinch to zoom, drag and flick to pan metaphor has gained wide popularity on mobile displays, where it is the paradigm of choice for navigating 2D documents.
But is finger-based navigation really the gold standard?
In this paper, we present a comprehensive user study with 40 participants, in which we systematically compare the Pinch-Drag-Flick approach with a technique that relies on spatial manipulation, such as lifting a display up/down to zoom.
While we solely considered known techniques, we put considerable effort in implementing both input strategies on popular consumer hardware .
Our results show that spatial manipulation can significantly outperform traditional Pinch-Drag-Flick.
Given the carefully optimized prototypes, we are confident to have found strong arguments that future generations of mobile devices could rely much more on spatial interaction principles.
The exploration of large 2D information spaces, such as maps, pictures and web documents, is a very common task carried out on mobile displays by millions of users every day.
Due to the rather small screen size of the devices, this often involves heavy usage of zoom and pan, usually performed using multi-finger gestures.
In this context, the Pinch-Drag-Flick paradigm has proven to be one of the most  successful gesture sets: pinch to zoom, drag and flick to pan.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
Besides that, ambiguities of gestures may trigger unintended actions, such as the accidental selection of items, or may require users to explicitly switch between edit and navigation mode, which in turn may induce disorientation .
These shortcomings motivated the development of navigation techniques that, for example, employ different finger gestures  or extend the interaction to the side  or the back of devices .
Although such approaches soften some of the issues of Pinch-Drag-Flick, the underlying input strategy still remains surface-constrained and thus continues to rely mostly on fine finger motor skills.
In this paper, we study a radically different approach that is based on an alternative input channel: the spatial position and orientation of mobile displays .
Wellknown examples of such spatially aware displays are the Chameleon  and the Peephole Displays .
In contrast to the metaphor of grabbing a document, they build upon the concept of Magic Lenses  and thus use the metaphor of moving a viewport  over a virtual information world.
For this purpose, distinct motion patterns are mapped to specific navigation tasks, e.g., horizontal movements may change the viewport center , whereas lifting a display up/down may control the zoom factor.
As this requires users to move a display through the physical space surrounding them, the motor space is increased considerably  and a different set of motor skills is addressed .
We see this difference in motor control as a significant opportunity that may help overcome the problems of con-
This implies many advantages including the addition of a more natural way of interaction by addressing principles of spatial manipulation, the support of longer travel distances per gesture, a reduction of item occlusions on the screen, and less mode switches, e.g., by assigning spatial input to navigation and touch input to selection.
For these reasons, we consider spatially aware displays an important research topic that has the potential of changing the way we interact with mobile displays - with 2D document navigation being only one possible use case.
Surprisingly little practical work has been done on systematically studying how both navigation approaches perform against each other on mobile displays.
Previous attempts either addressed different setups, e.g., involving a wall , occupied both hands for spatial input , or did not succeed in finding hard evidence in favor of the spatial approach  - a gap that we fill with our work.
In this paper, we contribute a comprehensive user study with 40 participants that we conducted using state-of-the-art mobile displays .
We found overwhelming proof that spatial input-based navigation does - if designed and implemented properly - outperform Pinch-Drag-Flick for 2D document navigation.
We believe that this is due to our design decisions, e.g., regarding the importance of an easy to use clutch and the role of a high quality prototype.
The remainder of this paper is organized as follows.
First, we review related work and discuss key design decisions.
We then outline the scope of the study and present the method, results, and discussion.
This is followed by design recommendations for future generations of mobile displays as well as conclusions and an outlook on future work.
Two recent projects compared touch- and spatialbased 2D document navigation on handheld devices that share some similarities with our work, yet are based on completely different setups: Kaufmann & Ahlstrom  projected the workspace onto a wall with a Pico projector and Radle et al.
Both projects found advantages of the spatial techniques - particularly in terms of recall performances, which was not our focus.
They are one of the first to show that spatial input can be as good as touch.
In some respects, our spatial technique is a simplified variant of their "1 button simultaneous" condition, as our clutch works differently and our technique does not require a second hand for pointing in mid-air .
These are two likely reasons for why fatigue  was only a negligible problem in our study, as opposed to Jones et al.
To our knowledge, there is only one previous work that bears considerable resemblance to our work: the Lens Chameleon  by Pahud et al., who were driven by similar motivations.
They also conducted a series of experiments to compare spatial-based navigation with standard Pinch-Drag-Flick.
In contrast to our work, their implementation of spatial-based navigation was significantly slower than Pinch-Drag-Flick.
This may be attributed to our design decisions: no use of clutching and a lack of state-of-the-art technology .
Pinch-Drag-Flick is a well understood and established technique.
We will therefore restrict our review of related work to spatial input-based interaction and its evaluation.
One of the first spatially aware mobile displays is the Chameleon presented by Fitzmaurice .
Inspired by the notion of see-through interfaces , it serves as a "peephole in hand" providing access to a virtual world that can be explored by moving the device around.
This concept was later adapted to arm-mounted displays, e.g., the Boom Chameleon , or to a tabletop environment, e.g., PaperLens .
These systems add further aspects to the overall interaction equation, e.g., the opportunities of multi-display environments or additional input modalities, such as digital pens.
While we studied the specifics of mobile displays, we believe that our findings are transferable to such setups.
Evaluating specific spatially aware display systems has been the goal of some research projects.
Oh and Hua tested various aspect ratios and sizes of spatially aware peepholes .
One key question is how to properly map the physical space to the Space-Scale-Diagram .
In pre-tests, we tried various mappings and finally decided on a dynamic mapping that uses the current orientation of the display as the new reference plane for future interpretations of motions.
This means that zooming is mapped to movements along the normal of the display , whereas motions within the display's XY-plane define panning.
Our experiences show that the dynamic mapping supports body-centric usage even better than a spherical mapping , which was also recently confirmed in .
In addition, it has the benefit of working independently of the user's position, thus simplifying the interaction design and spatial tracking.
We therefore think that spatial input should be inactive by default, only to be enabled on purpose for a brief moment of interaction - by activating a clutch.
With a clutch, the nature of spatial navigation can be changed from absolute to relative mode.
We believe that this is a very important and necessary step to support mobile usage.
In relative mode, the "volumetric" 2D document  travels along with the device like a bubble surrounding it.
This enables users to put away the phone, e.g., into the pocket, and to resume navigation later on with the last visited position.
While we fiercely advocate the use of tactile clutches , we decided on using a touch-based clutch for the user study.
This choice was primarily motivated by practical reasons: Existing volume buttons are known to be unsuitable for this purpose  and it proved to be challenging to build an adequate alternative in the given time.
In our prototype, users can activate the clutch by touching the screen with one or more fingers, e.g., close to the screen bezel in order to prevent occlusion of items in focus.
Likewise, removing the finger deactivates the clutch.
We believe to have found a close enough approximation of tactile clutches, as this enables users to quickly access the clutch without spending much mental effort on locating it.
Hence, we expect that our findings will also apply to the latter case.
Special care must be taken for handling document boundaries, e.g., to prevent users from traveling into the void .
For Drag-Flick-based panning, this may be accomplished by stopping pan motions at the document boundaries .
For spatial input, we adjusted the boundaries to guarantee that users can align even the document corners to the zoom center  as illustrated in Figure 2c.
For the reasons discussed above, the main focus of this work is on designing, conducting and evaluating a user study that systematically investigates and compares the two navigation techniques  on state-of-the-art mobile displays.
In particular, we pursued two major goals: G1 Efficiency: We aimed at comparing how fast users perform common navigation tasks with the techniques.
G2 User satisfaction: We aimed at investigating how users relate to different usability aspects of the techniques.
We considered the following major factors in our study: Navigation Technique - Our primary attention was, of course, on the two navigation techniques .
Target Visibility - Zoom and pan are used in diverse application contexts with a variety of intentions that have an influence on the visibility of target items.
In our study, we considered on-screen targets, i.e., items that are  visible on the display and off-screen targets, i.e., items that are not initially visible on the display, e.g., a distant node in a node-link diagram.
Display Size - The screen size is another key factor that can influence the cognitive performance of users  and thus the time required for completing navigation tasks .
We focused on the two predominant classes of mobile display: phones and tablets.
These do not only differ in screen size, but also in weight, device size, pixel resolution and density that may also affect the navigation performance.
Gender - Previous studies  show that women and men differ in their cognitive strategies when performing navigation tasks.
In order to compensate for such effects, it is vital to properly incorporate the gender into the study design.
At first sight, the choice of the proper zoom direction may appear trivial: "If you observe how users react when they can't see something, they always bring the device closer to their eyes" as one of our reviewers wrote.
Having implemented both variants, we conducted an informal pre-test with 5 users.
All of them preferred the opposite zoom direction, i.e., zoom out when the display gets closer to the user.
A look into the literature  confirmed this finding.
Apparently, the inverted zoom direction would not match the peephole-in-hand metaphor that most users are familiar with , e.g., from using a magnifying glass or looking through a camera.
Hence, we decided to conduct the study with the "zoom-out-when-getting-closer" option .
In an early version of the prototype users could dynamically reposition the zoom center via the touch point on the screen .
Sufficient ecological validity was very important to us.
While we believe to have taken into account the most essential factors, there are further variables that may additionally affect the navigation performance.
For practical reasons, we limited our investigations to a user standing in the middle of a free space in an office-like lab  performing simple navigation-only tasks on a mobile display with a touch-based clutch.
We decided to focus on technologically affine users of both genders with advanced multi-touch experiences.
This decision was motivated twofold.
First, we wanted our baseline  to score very well.
Second, we expect that sooner or later the majority of people will acquire similar skills as mobile displays become more widespread.
This ensured the touch technique to be a strong baseline condition, as these devices provide a high standard of Pinch-Drag-Flick navigation out of the box.
Spatial Tracking: We opted for an optical tracking based on 12 infrared  cameras .
This provided precise spatial device positions and orientations at 100Hz with an error of less than 1mm within the tracking volume.
Its projected area on the floor was about 3x3m2.
A designated server  streamed the spatial raw data over a local Wi-Fi network in a standardized form .
This included time stamps, device IDs, and 4x4 transformation matrices describing the spatial position and orientation in six degrees of freedom .
Marker Design: We glued 6 IR-reflective stickers to the iPad's display bezel .
Only 3 of them needed to be visible at a given time, enabling users to hold it freely in their hands without accidentally interrupting the tracking.
Due to the smaller device size, this was not practicable on the iPhone.
Here, we built a small, lightweight plastic frame  with 4 IR-reflective balls that we plugged into the iPhone's headphone output .
This guaranteed a robust tracking and ensured enough flexibility for operating the phone with one or both hands.
App Development: We implemented the prototype in Objective C using iOS 6.0 .
We integrated the touch and spatial technique within a single universal app that runs on both devices.
A major problem was the limited RAM of the devices - a problem others  faced, too.
We solved this by combining several strategies: A zoom pyramid consisting of three layers containing different resolution of the scene, with the most detailed one being built up of tiles that are loaded on demand.
Forty unpaid students of different departments at the local university participated in the study.
Genders were evenly distributed .
All participants  were daily users of smart phones or tablets and thus considered themselves as experienced with such devices.
This implies that they were confident in performing Pinch-Drag-Flickbased navigation, which we verified in a pre-test.
We designed a controlled lab experiment with four independent variables.
Our main focus was on the navigation technique , which was the primary independent variable.
Display size , target visibility , and gender  were the secondary independent variables.
We conducted the user study as a mixed-model design.
For display size and gender, we used an in-between subjects design, i.e., participants were either assigned to work with a phone or a tablet .
For navigation technique and target visibility, we chose a within-subjects design , i.e., each participant performed both techniques exactly once  using the same task sequence.
We used the same task sequence for all users.
In order to minimize side effects caused by prior knowledge of data, we used an abstract 2D scene for both conditions .
The scene provided visual context to avoid disorientation  .
For this purpose, the scene background featured a thin grid and several distinctively colored and textured shapes .
We used a maximum zoom factor of 5 , which translates to approx.
In the scene, participants completed a pre-defined sequence of 128 navigation tasks using one of the two devices held in portrait mode.
They had to match a red rectangular search target with a black reference frame in the middle of the screen .
Only one target was visible at a time.
If a search target came close to the reference frame, it automatically snapped in and the task was done .
Then, the rectangle turned green and a progress bar was shown .
After 2 seconds, the next target appeared in the scene.
The use of red search targets above a green-bluish background was motivated by the feature integration theory , as it reduces the cognitive load .
The main part of the study consisted of three phases that were executed in two runs, once for the touch condition and once for the spatial condition .
Trial Phase: Depending on the group, the participant either started with the touch or spatial technique that we explained and demonstrated using the iPhone or iPad prototype.
This also included a brief explanation of the underlying interaction metaphor, e.g., the possibility of clutching .
We then invited the participant to perform a few exercise trials using an example dataset, until he or she felt confident with the technique.
In most cases, this took no longer than 5min, even for the spatial condition.
Interaction Phase: In both conditions, participants were asked to walk to the center of the interaction space, marked with a cross on the floor.
We enforced a standing usage.
Participants were free to move within an area of 2x2m2.
Assessment Phase: After completing the tasks, we asked the participant to sit down and to fill out a questionnaire.
We then conducted a brief interview, where we encouraged the participant to provide additional feedback in form of free comments.
Before commencing with the second condition, participants were allowed to remain seated and to rest as long as they wanted .
To test the user performances depending on the navigation intent , we designed a single sequence of 128 navigation tasks.
We wanted this sequence to contain a well-balanced combination of pure and mixed pan/zoom tasks with both on- and off-screen targets.
To achieve this, we defined a set of basic composition rules .
To address target visibility, we placed the navigation targets either within  or outside the display .
At the beginning of a task, onscreen targets appeared fully visible on the tablet, yet only partially on the phone .
We used these rules in a script that produced a sequence of 120 navigation tasks  in a random order.
The script also created small randomized local positional offsets.
Hence, we obtained a total number of 128 navigation tasks.
We analysed the data using three-way repeated measurement ANOVAs.
For all ANOVAs, the in-between factors were display size  and gender .
The repeated-measures factor depended on the analyzed data type.
We either used the navigation technique , the target visibility , or their combination .
All p-values were GreenhouseGeisser corrected.
The alpha level for tests of statistical significance was set to  = .05.
When effects were significant, we reported Bonferroni adjusted p-values for post hoc comparisons .
For descriptive data, we provided mean values  and standard deviations .
Participants completed the study within 50 to 70 minutes.
Before conducting the experiment, we had collected basic demographic information about potential participants via an online form.
This included the personal experience with touch screens to sort out applicants with insufficient multitouch skills.
We grouped suitable candidates so that exactly half of the women and men worked with an iPhone or iPad.
For spatial input, the collected raw performance data consists of the spatial position and orientation  of the devices over time with a sampling rate of 30 Hz as well as the start/end time of each clutch-cycle.
For touch input, we logged relevant events provided by the iOS-framework, e.g., the gesture type, on-screen positions, and start/end times.
We also recorded the start and end time of each task.
We used the times that participants spent on completing the tasks as a measure of performance.
We considered two variants: The total completion time is the overall time that participants needed to finish all 128 navigation tasks with either the touch or the spatial condition.
In contrast, the task completion time is the average time that participants spent on finding on-screen and off-screen targets, respectively.
All times are in seconds.
We investigated how many discrete actions were initiated for on-screen and off-screen target tasks.
We did this by counting the number of clutches  and individual touch gestures .
We used these numbers as a measure of handicap.
The rationale behind this is that starting a new action interrupts the navigation and thus negatively affects the overall performance.
For example, executing three drag gestures in a row requires the user to lift the finger from the screen two times more if compared to just performing a single continuous drag.
The same applies to the spatial condition, where releasing the clutch, e.g., to move the display to a more conformable position, briefly pauses the actual navigation.
Our analysis confirmed a main effect of target visibility .
Post-hoc comparisons show that participants reached on-screen targets significantly faster  with the spatial  than with the touch condition .
While the display size had only little influence on the completion time for on-screen targets , participants reached off-screen targets significantly faster with the tablet than with the phone .
The relative performance gain on the tablet was only marginally higher for touch  than for spatial input .
For on-screen targets, the number of touch gestures did not vary much between devices.
For this purpose, we extracted the 3D bounding box of the physical space that participants used while solving the tasks .
This was done for each of the 120 tasks .
We then computed an average bounding box for on-screen and off-screen targets, respectively.
We analysed both bounding boxes in terms of the maximum extent along each of the three principle axes .
We can show that the amount of used motor space significantly depends on the target visibility .
Table 2 summarizes the results that reflect the frequency of fatigue among participants, but not its intensity.
Both techniques caused fatigue in the shoulders and the upper arms.
The touch technique was more demanding for the fingers, the neck and the forearm, whereas spatial input affected the back and the upper arm more frequently.
To our surprise, 10 participants did not experience fatigue for spatial input, yet there were only 4 for the touch condition.
Usability Ratings: We compiled a questionnaire with 36 items using a 7-point Likert scale ranging from 1  to 7 .
These items addressed generic usability aspects  as well as specific issues regarding the tested techniques, in particular the perceived influence of zooming and horizontal/vertical panning on the overall performance.
To ensure a high degree of validity, we used 3 to 6 items per usability issue .
Both techniques were generally assessed very positively without significant differences, except for ease of use, efficiency to use, user experience, and zooming that were rated in favor of the spatial condition .
Free Comments: Participants gave us very positive feedback about the spatial technique.
Some  were even a little surprised that completing the 128 tasks with the touch technique "felt somehow more difficult than with the other one  ".
One user said that she "could almost `see' the map behind and beside the iPad, making it easier to decide where to move the device to next".
Fatigue: All participants completed the tasks without a break.
After each condition, participants were asked to choose from a list, which part of their body felt tired.
Not a single participant completed the 128 navigation tasks faster with the touch than with the spatial condition, even though all of them were multi-touch-experienced and used the spatial technique for the first time.
When performing the tasks, several  participants already expressed that they liked the spatial technique, even though we had not asked for that yet.
While we had predicted a decent performance for the spatial approach, it outperformed PinchDrag-Flick by 36.5%  on overall average - a finding that we had not expected in such clarity.
Independent of the technique, participants benefited most from a larger display for off-screen targets .
There are two reasons for that.
First, search tasks typically start by zooming out for overview.
This phase is usually shorter on devices with a larger screen, because search targets appear earlier on the display.
Second, users see more information on a larger display allowing them to come to navigation decisions earlier, e.g., see .
We observed that most participants used the phone with one hand, while the tablet was usually taken into both hands.
By design, all participants had to use the clutch in order to activate spatial input.
Hence, the minimum number of clutches per task was one.
Apart from that, clutching could also be used to move the device to a more convenient position, e.g., closer to the body.
In the data, a clutch number larger than one reflects this.
For on-screen targets, participants rarely used the clutch for this purpose .
For off-screen targets, however, where the size of the motor space was utilized much more, we counted one or two extra clutches for most participants .
Only the minority of participants  did not make use of the technique.
Instead, they stretched out their arms farther or did an extra footstep forward.
When asked why, common reasons included convenience issues, avoidance of slowing down, or being oblivious of the possibility.
All participants learnt how to use the spatial technique very quickly.
The majority  needed less than 5 minutes for that.
Supported by the steep learning curve of the young, healthy and technological-affine users, we expect our findings can be transferred to other groups of the population.
One example for this are elderly people or persons having difficulties in precisely controlling their fingers, e.g., due to age-related motor impairment, Gout, or Osteoarthritis.
We believe that these groups may particularly benefit from the different motor skills that are relevant for spatial input.
In our study, we repeatedly witnessed participants having difficulties with the pinch gesture.
As a consequence, many participants found it easier to lift a display up/down for zooming, which is reflected by the user ratings for zooming, as depicted in Figure 7.
Apart from that, touch input also requires a high visual attention, e.g., due to little tactile/haptic feedback.
In contrast, spatial navigation explicitly supports proprioception, i.e., the sense of relative positions of neighboring body parts.
We believe that such kinesthetic cues can reduce the demand of visual attention.
As hinted in , such cues can also enable users to associate important regions in the document with specific physical positions around their body, making it easier for them to quickly travel within the document.
Another important benefit of spatial navigation is simultaneous zooming & panning, which is naturally supported by moving a display diagonally through the space-scale diagram .
Participants made use of this very frequently in the study.
One key benefit of the spatial navigation technique is the size of its motor space.
If we consider the space between hip and chest as the preferable interaction zone, then this space is more than one order of magnitude larger than the average mobile touch screen.
This is an important advantage in terms of physical resolution and accuracy.
By performing only one continuous gesture, it allows users to cover very long distances within the document - by maintaining a high level of precision at the same time.
For touch gestures, travel/zoom distances are considerably smaller per gesture , thus forcing users to perform multiple gestures to achieve the same result.
We found clear evidence supporting these claims.
Users performed many touch gestures for both on-screen  and off-screen target tasks .
In contrast, clutching was used only marginally in the spatial condition .
Here, participants clearly benefited from the larger motor space that they used more extensively for off-screen targets if compared to on-screen targets .
As standard deviations of completion times indicate, most participants were similarly fast with the spatial technique, but showed diverse performance times for Pinch-Drag-Flick - even though they had prior experiences with the latter technique.
One possible reason for this might be that participants were motivated more to succeed in the spatial technique, because it was new to them.
Yet, we believe the high performance variations for touch can also be attributed to issues with the touch condition: First, displays were prone to get soiled by a thin film of sweat and grease after working on them for some time.
As this affected the touch recognition, we carefully cleaned the displays each time before a participant started to work with them.
Second, while holding the device, the  thumb of participants occasionally came in contact with the display, thus accidentally interfering with the detection of other touch gestures.
Third, we witnessed a few female participants  who had problems, caused by their fingernails.
Although the nails were not unusually long, these women struggled with a less reliable touch recognition.
Many participants reported that they had experienced similar problems before, e.g., when working with their personal phone.
Hence, we conclude that these issues do not weaken our findings, but rather reflect the condition of the world outside the lab.
In retrospect, the use of a dynamic spatial mapping based on local device orientations has proven to be a good choice.
However, our observations indicate that there is room for improvement.
In interviews, several participants  asked for a finer mapping, allowing them to move the device less by still covering the same virtual distance in the document.
We propose to provide a user setting for this, though it may be worth investigating suitable thresholds that might depend, for example, on the display size.
This is also supported by our observations and interviews with participants.
Hence, we propose to equip future generations of mobile devices with a clutch that provides some form of tactile feedback.
This may be a simple physical button, though it should be larger than the tiny volume controls usually found on mobile phones.
Ideally, the clutch would be readily usable independent of the current orientation of the display, for example, by squeezing the display bezel .
One major technical challenge is the support of reliable 6DoF spatial tracking for real mobile usage, where requirements different from those in the lab apply: First, the workspace is not stationary anymore.
Therefore, external sensors are unavailable and light conditions are likely to vary considerably.
Second, spatial tracking should be relative to users, so they can walk without affecting the interaction .
Third, the algorithm should be energy efficient to ensure long working times.
Prior to conducting the study, we experimented with alternative sensing approaches that use the built-in sensors of mobile displays, e.g., gyroscopes .
Accelerometers are energy efficient and offer low-latency feedback, though cannot detect positional changes of the user  and also are prone to induce error drifting.
However, this limits the interaction to 3DoF and does not work when the user's face is not within the camera's field of view.
To overcome these shortcomings, sensor fusion  combines gyroscopic data with face tracking.
Yet, this approach still suffers from potential inaccuracies and technical pitfalls, so we found a proper implementation to be too time-consuming.
It is also questionable, whether this or similar approaches are already advanced enough in terms of fidelity, spatial range and energy consumption.
Nonetheless, we believe that integrating such capabilities into future generations of mobile displays is the one major technical challenge that needs to be solved before spatial interaction becomes more widespread.
In this paper, we presented a comprehensive user study that systematically compares the efficiency and user satisfaction of two contrarian input strategies for 2D document navigation on mobile displays: the predominant touch-based Pinch-Drag-Flick approach with a spatial-input-based approach that utilizes positional changes of a mobile display in the physical space surrounding a user.
The results surpassed our expectations in various ways.
On average, participants were more than 35% faster with the spatial approach, even though all of them were conversant with Pinch-Drag-Flick and used the spatial technique for the first time.
This finding was further supported by the questionnaires, where participants rated the spatial approach at least as good as or even better than the touch-based counterpart.
To the best of our knowledge, we are the first who provide such clear evidence in favor of spatial input.
This was only possible by building high quality prototypes that make use of state-of-the-art mobile devices.
Considering the popularity of Pinch-Drag-Flick, our findings could be of interest for future interaction designs of mobile devices - as a complimentary method of interaction, yet not as a complete replacement.
Because there are also limitations: social protocols may limit its application, users may perform differently when sitting, and users may prefer to put a display on a desk for certain tasks.
However, given the additional advantages of a supplemental input channel, we hope that our findings will help mobile computing embrace spatial interaction principles much more than before.
For future work, we plan to address the technical challenges and design recommendations that we discussed in the previous section, in particular device-intrinsic spatial tracking via sensor fusion and tactile clutches.
With this technology, we will then continue our investigations by testing how inthe-wild usage  affects performances as well as the accuracy and recall .
Beyond that, we intend on studying compound tasks  that involve additional tasks, such as selection or annotations, and thus may particularly benefit from combining touch with spatial input.
In our prototype, we used touch input for the de/activation of spatial input, which was primarily due to the lack of alternatives.
While this worked generally well in our study , we do not consider touchbased clutches as our preferred solution.
There are several reasons for that.
First, touching the screen with a finger occludes parts of the viewport.
Second, mixing on-screen touch input with spatial input is contrary to the philosophy of hybrid input paradigms, where different input channels should work independently from each other.
Third, the clutch must be easily detectable preferably by non-visual cues so users can keep their visual attention on the document.
Fourth, users should be provided with eyes-free feedback regarding the current state of a clutch.
This is to provide precise control on when and how long the clutch is activated.
