Although existing mobile interface widgets are often designed to be easily operated by finger, the tension remains between the low precision of finger input and the high precision needed by graphical user interfaces for the following reasons.
First, it consumes precious screen real estate to make UI widgets large enough to be fingeroperable, especially as the resolution of mobile devices increases, e.g., the iPhone 4's resolution is 640 x 960 .
Secondly, in a web interface, many UI widgets, e.g., hyperlinks or an embedded Flash player, are still small and hard to operate by finger.
Lastly, high precision interactions are more challenging for mobile users because they are often on the go, e.g., walking or taking a bus, and cannot pay a close attention to the interface.
Although many solutions have been explored for addressing the fat finger and occlusion problems, most of them still require highly precise visual perception and motor control such as carefully adjusting a finger's position .
Finger-based touch input has become a major interaction modality for mobile user interfaces.
However, due to the low precision of finger input, small user interface components are often difficult to acquire and operate on a mobile device.
It is even harder when the user is on the go and unable to pay close attention to the interface.
In this paper, we present Gesture Avatar, a novel interaction technique that allows users to operate existing arbitrary user interfaces using gestures.
It leverages the visibility of graphical user interfaces and the casual interaction of gestures.
Gesture Avatar can be used to enhance a range of mobile interactions.
A user study we conducted showed that compared to Shift , Gesture Avatar performed at a much lower error rate on various target sizes and significantly faster on small targets .
It also showed that using Gesture Avatar while walking did not significantly impact its performance, which makes it suitable for mobile uses.
Touchscreen mobile devices have become prevalent in the recent years .
However, although finger-based touch input is intuitive, it suffers from low precision due to two fundamental problems: the area touched by the finger is much larger than a single pixel--the fat finger problem-- and the pointing finger often occludes the target before touching it--the occlusion problem .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Figure 1:  Small mobile interface widgets are difficult to operate through fingers.
In contrast, gesture-based techniques, e.g., panning a map by swiping a finger across the touch screen, allow for casual interaction.
However, gesture-based interactions tend to lack visibility compared to graphical user interfaces whose operation semantics are embodied by their interface components .
Prior work such as Escape  creatively combined the advantages of both worlds using rectilinear gestures to select graphical objects.
However, it is limited in that it requires a target interface to be designed in a specific way.
In addition, prior work mostly focused on the one-shot target acquisition, leaving other common interactions, such as drag-and-drop, unexplored.
To address these issues, we designed and implemented Gesture Avatar, a novel interaction technique that allows users to operate mobile user interfaces using gestures.
Figure 1 provides a quick demonstration of the technique.
Gesture Avatar leverages the visibility of graphical user interfaces and the casual interaction of gestures.
A user can dynamically associate a gesture shape with an arbitrary GUI widget and then interact with the widget through the drawn gesture, which is conceptually akin to an avatar.
Gesture Avatar can be used to enhance a range of mobile interactions.
A user study we conducted showed that in target acquisition tasks, Gesture Avatar achieved a significantly lower error rate than Shift  and performed faster on small targets .
It also showed that walking had no significant effect on using Gesture Avatar, which makes it suitable for mobile uses.
We demonstrate the feasibility and ease of use of Gesture Avatar by integrating it with existing systems such as a mobile web browser.
In the rest of the paper, we first provide a brief introduction of how a user interacts with the existing mobile interfaces through Gesture Avatar and then discuss related work.
Next, we provide a detailed description of our design and implementation of Gesture Avatar along with more example applications that our technique can enhance.
We then evaluate Gesture Avatar by comparing it to Shift.
Finally, we discuss the factors in designing Gesture Avatar and possible future work.
This creates a gesture avatar, the gesture stroke with a translucent background.
The bounding box of the link that the avatar is associated with is highlighted .
The user can then easily trigger the link "SPORTS" by tapping on the gesture avatar, which is a much larger target.
As we will discuss in the following sections, to form the association between a gesture and an object, we leverage both the shape of the gesture and its distance to the objects on the screen.
From this example, we can see that a gesture avatar provides a larger effective area for a user to comfortably interact with a small target.
The type of interaction that Gesture Avatar can enhance is not limited to tapping.
A gesture avatar essentially re-dispatches whatever touch events it receives to the associated object.
For example, a user can long press an avatar to long press the associated object to bring up a context menu.
The gesture that is used to create an avatar can also be arbitrary.
It can be a character or an arbitrary shape.
For example, the user can draw a box to create an avatar for controlling the progress bar of a media player as seen in Figure 1.
The user can then move the knob of the progress bar by dragging the avatar.
An avatar may be associated with an undesired object due to the inaccuracy of gesture recognition and the essential ambiguity of the objects on the screen.
For example, in Figure 2a, when the user draws an "S", both "SPORTS" and "SCIENCE" are good candidates.
When an avatar is associated with an undesired object, the user can either dismiss the avatar by tapping outside of the avatar and redraw a gesture, or re-assign an object to the avatar using directional  gestures.
When the user draws directional gestures outside the avatar , Gesture Avatar will find the next best match in the direction of the stroke and update the avatar's position on the screen accordingly .
The user can repeat this process until the avatar is associated with the target.
In this section, we describe how a user would interact with mobile interfaces using Gesture Avatar.
Assume that a user opens the New York Times website  in a mobile phone web browser.
The interaction objects such as hyperlinks are viewable but too small to be easily tapped on by a finger.
Here the user wants to navigate to the sports section.
However, they cause the user's fingers to be occluded by the device, causing large error rates.
LucidTouch  addresses the problem by introducing pseudo-transparency, which creates an illusion that users can see their occluded fingers, decreasing error rates.
It first assigns directions to all objects.
Users then can press their fingers near the target, followed by a directional stroke to specify the direction of the target.
Through gestural interaction, Escape greatly reduces the effort in human visual feedback loop and avoids the occlusion and fat finger problems.
The major limitation of Escape is that the appearance of the user interface has to be modified in order to make the assigned directions visible.
Escape is demonstrated with a map-like application where using directional balloons is possible.
However, it is often inappropriate or impossible to change the appearance of an application's user interface, which makes the technique less applicable.
Moreover, as mentioned in their own paper, the density of the objects  is limited due to the fact that close objects cannot have similar directions, and the technique is limited near screen edges, as gestures cannot go beyond screen edges.
A large corpus of prior work has focused on facilitating target acquisition.
Most of this work generally reduces the Fitts's Index of Difficulty  by increasing a target's size.
Semantic pointing  dynamically adjusts the controldisplay  ratio as the cursor approaches the target.
Bubble cursor , which is based on area cursor , dynamically changes its size according to the proximity of surrounding objects so that the pointer is snapped to a nearby object.
High-precision pointing is hard with an imprecise input such as a bare finger.
Many methods and techniques have been presented for high-precision touchscreen interaction.
Albinsson and Zhai  propose two techniques using widgets for precise finger positioning by zooming, reducing CD gain, and discrete pixel-by-pixel adjustment.
They show that though these techniques are faster than Offset Cursor for targets smaller than 0.8 mm, they are slower for targets larger than 3.2 mm.
Much work has also been done in avoiding finger occlusion.
Offset Cursor  eliminates finger occlusion by showing a cursor above users' fingers.
However, users can no longer aim for the target directly even when the target is big enough.
Instead, users have to first touch the surface to show the cursor and then move the cursor onto the target to select it.
This contradicts the intuitiveness of direct touch.
Shift  improves Offset Cursor by showing a copy of the occluded screen in a callout above the user's finger.
Unlike Offset Cursor, users still can aim for the actual target with Shift.
Their user study shows that Shift has a much lower error rate than unaided touchscreen and is faster than Offset Cursor for large targets.
In this section, we describe the details of how Gesture Avatar works.
Our design goal is to allow users to easily interact with mobile interfaces, especially when the target  is small and users are on the go.
As a result, we designed Gesture Avatar with reducing or eliminating high precision interaction in mind.
Figure 4 illustrates the interaction flow of Gesture Avatar.
The interaction with Gesture Avatar involves four states.
The interaction process starts from the Initial state.
Once the finger touches the screen, it enters the Gesturing state, in which the touch trace is rendered on top of the underlying user interface that is being operated.
When the user lifts her finger and the trace is a valid gesture , the process enters the Avatar state, in which the drawn gesture stroke forms the avatar with a translucent background.
The object that is associated with the avatar is also highlighted.
In the Avatar state, the user can choose to operate the associated object through the avatar, or adjust the association if the avatar is associated with an undesired object.
Character and shape recognition have several fundamental differences, and distinct approaches have been developed for handling each case .
As a result, we first classify whether the gesture is a character or a shape and then employs different recognizers for each case.
For character matching, Gesture Avatar employs a neural network handwriting recognizer that recognizes letters and numbers used in English.
The recognized character is then used to search against the content of each object on the screen.
For shape matching, Gesture Avatar uses a template-based shape recognizer that is conceptually akin to .
The templates, i.e., the contours of all the objects on the screen, are added on the fly.
The shape recognizer then finds the object that has the most similar contour to the gesture.
Note that rather than only keeping the best guess, this calculation gives a distribution of all possible characters and shapes, which is fed into P of Equation 2.
We discuss how these two quantities are calculated in the following sections.
Since users tend to aim for the target, the position of the gesture for interacting with the target is often a strong indicator of where the target is.
As a result, it is reasonable to assume that the closer an object is to the gesture, the more likely it is the target.
To capture this intuition, we use a 2D Gaussian distribution over the distance between the center of an object and the bounding box of the gesture.
To save computation when there are numerous objects on the screen, we use the Manhattan distance, instead of the Euclidean distance, of an object to the bounding box .
The output of the Gaussian provides an indication of how likely the object is the target, i.e., P.
In our design, we allow the user to go to the next best match in the given direction by drawing a directional stroke.
Directional strokes are fast to perform and the outcomes of them are more predictable.
Based on the directional stroke the user draws, we infer the next best match based on not only how well an object matches the avatar gesture semantically, but also the angular distance between the directional stroke and the direction from the currently matched object to the object being evaluated.
Again, we use a Gaussian function to give each object a score based on its angular distance .
To reduce computation cost, we restrict the search to the objects within 45 of the stroke direction.
In the previous sections, we demonstrated the use of Gesture Avatar through a media player and a mobile browser.
In this section, we discuss two more examples.
The first example  applies Gesture Avatar to moving the caret in a text box.
The on-screen keyboards on most touchscreen mobile phones do not have dedicated keys for moving the caret.
Instead, users need to directly touch between two characters.
Since the text is small, moving the caret is error-prone.
The iPhone uses a Shiftlike technique to address this problem.
With Gesture Avatar, the user can draw the character before  the desired position to assign an avatar to it and then tap on the right  half of the avatar to move the caret.
The second example  applies Gesture Avatar to Google Maps.
Previously, maps have been a heavily used example to demonstrate and evaluate different target acquisition techniques .
Locations on a map are typically represented by distinct letters.
This property makes Gesture Avatar promising for acquiring locations,
The 2D Gaussian distribution employs a changing variance that is dynamically adjusted based on the length of the longer edge of the gesture's bounding box .
This design of our algorithm is based on the observation that users draw gestures in different sizes, and the smaller the gesture is, the more specific area the user is targeting at.
Due to the inaccuracy of gesture recognition and the inherent ambiguity of objects available on the screen, it is unavoidable that an avatar is associated with an undesired object.
For example, multiple objects with similar content might be cluttered in one area.
Furthermore, objects on the edges of the screen are also prone to incorrect matching, since users cannot draw gestures over these objects and the Gaussian distribution cannot efficiently capture this situation.
One typical approach to correct a mismatch is to ask the user to dismiss the avatar and then recreate it.
Gesture Avatar was primarily written in Java using Android SDK 2.2.
For image contents, we first compute their edges using the Canny edge detector and then pass them to our shape recognizer.
The image processing and edge detection were written in C++ using OpenCV  and built with a variation of Android NDK.
The example applications that we have discussed in the paper were written in Java.
We built these examples by wrapping an existing user interface, e.g., a web browser or a text input field, with an additional interaction layer, i.e., Gesture Avatar.
In the mobile web browser example, the current version of mobile WebKit does not expose its UI structures, i.e., the DOM tree, in Java but only accessible through JavaScript.
As a workaround, we implemented the logic for searching the best matches in JavaScript and inject the JavaScript code upon loading a webpage.
As discussed earlier, Gesture Avatar addresses not only the acquisition of small targets, but also the follow-up interaction such as dragging an object after it is acquired.
There has been little prior work in addressing the latter.
So Gesture Avatar makes a clear contribution in that regard.
As a result, we here focus on its performance on target acquisition compared to Shift.
We hypothesize that Gesture Avatar outperforms Shift on small targets and is better for mobile uses.
Specifically:  Gesture Avatar will be slower than Shift on larger targets, but faster on small targets.
The experiment was conducted on a Motorola Droid running Android 2.2 .
The walking tasks were conducted on a treadmill desk from Steelcase, with a maximum speed of 2.0 mph .
Walking on a treadmill offers a reasonable simulation that captures common properties of various "on the go" scenarios, such as device shaking and significant body movement.
At the same time, an indoor treadmill setting allowed us to conduct the experiment in a more controlled manner.
A within-subjects factorial user study was used in this experiment.
Half of the participants learned and performed Gesture Avatar first and then Shift, while the other half did the opposite so that the order of two techniques was counterbalanced.
For each technique, participants were asked to complete the tasks in two conditions: sitting and walking as shown in Figure 9.
The objects in the tasks were represented as letter boxes .
Participants were asked to acquire targets of different sizes, positions, letters, and ambiguity using both Gesture Avatar and Shift.
In the starting screen of a task , the participants were shown 24 small letter boxes.
The target box, which was always near the center, was highlighted in red.
To eliminate the time for searching for the target visually, which was not the focus of this experiment, a magnified version of the target was shown 300 pixels below the target.
Performance times were measured between the tap on the magnified target and the selection of the target.
We chose 24 letter boxes for our study for several reasons.
First, we wanted to cover the entire spectrum of semantic ambiguity in targets, from very ambiguous  to non-ambiguous cases .
As a result, the number of letter boxes should not be larger than 26, the size of the English alphabet.
Second, we wanted the number of unique letters shown on the screen to be a factor of the total number of boxes so that the distribution of the letter usage is uniform, i.e., each letter can have an equal number of instances and affect the gesture recognizer equally.
Lastly, 24 objects offer an information density that reasonably simulates real life scenarios.
The ambiguity of the target, i.e., the number of objects that have the same letter near the target, may affect the performance of Gesture Avatar.
We simulated the ambiguity by controlling the distance between objects and the number of letters used.
Since the performance of Shift correlates to the size of the objects instead of the distance between them, we fixed the margin of objects to 5px and varied the size of the objects to affect both techniques.
Some objects at a small size such as 1mm would become uncomfortable or difficult for users to read.
However, many UI widgets in a web interface are often presented at such a scale.
In addition, mobile users are often on the go.
The short attention span of a mobile user as well as interacting in motion affects the performance of human motor control.
These factors can greatly decrease the effective size of a target, such that a large target in such circumstances might have a much smaller effective size.
When using Gesture Avatar to acquire the target, we highlighted the currently matched object with an orange border .
Participants could tap on the avatar to acquire the matched object.
For this comparison, we reimplemented Shift and used the escalation time 0, 50, and 200 milliseconds for 10, 15, and 20 pixel objects, respectively.
To make the finger position more stable, we averaged the finger positions with a smoothing factor of 0.25 as a low pass filter for the finger position signals.
While performing these techniques, participants were required to acquire targets with the index finger of their dominant hands while holding the device in their nondominant hands, as required in previous studies.
They were allowed to practice until they felt comfortable using it.
During the test period, participants could take a break between tasks.
For each technique, participants performed the first 12 test sessions while sitting on a stool  and the second 12 while walking on a treadmill .
The walking speed was between 1.8 and 2 mph depending on participants' preference.
The 12 sessions covered all combinations of box sizes and numbers of unique letters.
In each session, there were 10 tasks, with the same box size and same number of unique letters.
They were drawn randomly from a pre-generated task pool .
The independent variables were Technique , MobileState , TargetSize , and NumOfLetters .
We performed the same analysis on Shift and Gesture Avatar separately.
We found that for Gesture Avatar, there was no significant difference across different MobileStates and TargetSizes.
For Shift, TargetSize=10px was significantly slower than larger sizes at all MobileStates, and sitting was significant faster than walking when the TargetSize is 10px.
For the error rate, we also performed a within-subjects ANOVA for Technique x MobileState x TargetSize x NumOfLetters.
Tukey's post-hoc pair-wise comparison showed that Gesture Avatar had lower error rates than Shift on all TargetSizes, while there was no significant difference for Gesture Avatar across different TargetSizes.
We performed the same analysis on Shift and Gesture separately.
We found that for Gesture Avatar, there was no significant difference across all MobileStates and TargetSizes.
For Shift, TargetSize=10px when walking had a significantly higher error rate than any other TargetSizes and MobileStates.
In the post-study questionnaire participants were asked about their preferences for the two techniques.
10 of the 12 participants preferred Gesture Avatar, 1 Shift, and 1 both.
All commented that Shift was more stressful especially while walking.
10 participants agreed that Gesture Avatar is useful, 1 strongly agreed and 1 neutral.
8 participants agreed that Gesture Avatar is easy to use, 3 strongly agreed and 1 neutral.
Our results support hypothesis H1.
Figure 11 and Figure 12 show that Shift's performance time increases as the target size decreases.
However, Gesture Avatar's performance time almost remains unaffected by the change of the target size.
As a result, Gesture Avatar is slightly slower than Shift when the target size is 20px, but quickly catches up and becomes a lot faster when it is 10px.
This also implies that Gesture Avatar scales well to small targets.
Our results support hypothesis H2.
Figure 13 and Figure 14 show trends in error rates that are similar to those of performance time.
However, Gesture Avatar has much lower error rates than Shift.
Though Gesture Avatar has a correction stage, achieving such low error rates with little sacrifice in performance is still surprising.
Our results also support hypothesis H3.
This indicates that Gesture Avatar is promising when users are on the go.
One surprising finding is that the number of unique letters had no significant influence on Gesture Avatar, as we thought that more content ambiguity would decrease the effectiveness of the gesture recognizer, and therefore decrease the overall performance.
However, in our study, we observed that the participants tended to draw smaller gestures when there was more ambiguity and as a result the matched objects were often either the targets or very close to the targets.
We also observed that all participants quickly converged to "flicking"  towards the targets as their correction strokes, which was very efficient.
As a result, many existing techniques are inefficient in acquiring moving targets.
In Shift, the target can move out of the callout view during the interaction, and in Escape, the target can move to items with the same or similar directions.
In contrast, for Gesture Avatar, moving targets are not a challenge.
Since the target matching algorithm works on a static state of the user interface, all the claims in this paper should still hold for moving targets.
We do need to update the avatar view constantly in order to attach the avatar to the moving targets.
As we have discussed in the previous sections, Gesture Avatar offers unique advantages for manipulating small targets beyond target acquisition.
As a result, it can address many common interaction behaviors that prior techniques cannot support, e.g., adjusting a slider, drag-and-drop, double tapping, and long pressing.
However, for a target that only accepts one-shot interaction, e.g., the only valid interaction for a button is to be clicked, it might be unnecessary to show the avatar when the match is highly unambiguous.
For example, a user can simply draw a gesture near a target  and if the ambiguity is low, the target should be triggered immediately, instead of having to display an avatar for the user's confirmation.
In such case, Gesture Avatar supports a one-shot interaction as prior techniques did, although Gesture Avatar still has the advantage of allowing casual interactions.
Another design alternative is to show the avatar at the end of gesturing, but before the user lifts her finger, which gives the user an opportunity to confirm the match.
If the match is incorrect, the user can either continue gesturing, or dwell the finger, e.g., for a half second, on the touch screen to bring up the avatar.
She can then adjust the matching using directional gestures.
This design would require us to detect the potential ending of a gesture stroke being drawn.
We explored various representations for gesture avatars.
We focused on two representations in our iterative design process: stroke and lens representations .
We employ the stroke representation in the current design, i.e., an avatar is shown as the gesture stroke with a translucent background.
The lens representation is to show a magnified version of the matched target and has been explored in literature .
Both representations highlight the bounding box of the original target.
We chose the stroke over the lens representation for the following reasons.
First, a lens often affords different interaction behaviors that are inconsistent with the avatar metaphor that we intended to support.
For example, dragging lens would be naturally interpreted as shifting the view rather than moving the magnified object.
Secondly, a magnified view often has a more complicated appearance than a gesture stroke.
As a result, the lens representation is more likely to obscure the underlying user interface and adds visual complexity.
In contrast, the stroke representation is more lightweight.
Prior work on target acquisition often assumes fixed locations for targets, which is not always the case.
As we have seen in the previous media player example , the knob of the progress bar moves towards the right end while the media player is playing.
As another example, in a real-time location information application, objects such as cars can move quickly on a map.
Similar to many other gesture-based techniques, mode switching  is also an issue in integrating Gesture Avatar into existing user interfaces.
For example, when gesturing on a map, it is necessary to distinguish the initial gesturing for creating an avatar from actions such as panning.
In our implementation of Gesture Avatar, a user gestures by first pressing and holding a holding finger, and then drawing gestures with a different finger, the gesturing finger.
Gesture Avatar needs to be aware of the objects available on the user interface being operated, although it does not require these objects to be in a specific presentation.
The more Gesture Avatar knows about the underlying user interface, the better it can perform.
The ideal situation is that we can access the structure of the underlying user interface.
For example, the structure of a webpage can be accessed via its DOM tree .
However, this information can be inaccessible to Gesture Avatar .
One possible solution is to reverse engineer the underlying user interface from its pixels as previously explored in Prefab .
The limitation of this kind of approach is that it only works for known UI widgets.
To make Gesture Avatar a general technique and avoid the effort for implementing it for every user interface, we designed Gesture Avatar as an interaction layer that can be easily used to wrap around a specific interface via a simple API.
We implemented a toolkit based on the Android platform and all the examples demonstrated in the paper were implemented using the toolkit.
This paper presents Gesture Avatar, a gesture-based interaction technique for operating mobile interface that supports casual interaction of the existing mobile interfaces.
We discussed the interaction design of Gesture Avatar.
We showed how Gesture Avatar can improve user experience on mobile devices via four example applications.
A controlled user study and the range of interactions that are supported indicate that Gesture Avatar outperforms its peers in many ways.
