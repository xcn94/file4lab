In this work, we quantitatively analyze how human perception is affected by audio-head motion characteristics of talking avatars.
Specifically, we quantify the correlation between perceptual user ratings  and joint audio-head motion features as well as head motion patterns in the frequency-domain.
Our quantitative analysis results clearly show that the correlation coefficient between the pitch of speech signals  and head motions is approximately linearly proportional to the perceptual user rating, and a larger proportion of high frequency signals in talking avatar head movements tends to degrade the user perception in terms of naturalness.
Moreover, previous research studies  show that head motion has a strong correlation with acoustic speech features.
However, these studies were primarily focused on qualitative understanding of human perception on avatar head movements.
The quantitative association between human perception and the audio-head motion characteristics of talking avatars remains to be uncovered, to the best of our knowledge.
Specifically, it consists of the following steps:  Participants are asked to rate a number of constructed audio-head animation clips ;  joint features of speech signals and head motions are extracted from the audio-head animation clips; and  quantitative analysis is performed to study the association between the extracted features and the obtained subjective user ratings, including: a canonical correlation analysis based approach to model the correlation between joint audio-head motion features and subjective human ratings, and a frequency-domain analysis on the userrated head movement patterns.
Embodied interface agents  have been increasingly used in human-computer interfaces  .
In these HCI systems, perception of any human-like behaviors on avatars, such as facial expressions, hand gestures, lip movements, and head movements, can strongly influence user interaction experience.
Among many human-like avatar behaviors , talking avatar head movement is considered as one of the crucial communication cues that can facilitate the social interaction between a human being and a humanoid avatar.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The audio-head motion dataset used in this work was acquired from an actress using a VICON optical motion capture system.
The actress was asked to speak a corpus  while keeping her head movements as natural as possible.
The original sampling frequency of the motion capture data was 120 Hz.
The voice was recorded simultaneously using a close talking microphone at the sampling rate of 48 kHz.
Based on the above acquired audiomotion data, three Euler angles of head rotation in each frame were computed using a Singular Value Decomposition  based technique .
Note that head translation is not the focus of this work.
In this work, we used a Fourier analysis based cepstrum method to extract the F0 frequency from speech signals.
The used window size of cepstrum sampling was 30 ms, and the output frame rate was set as 100 samples per second.
RMS energy was extracted as the average squared intensity in a Hamming window .
Finally, all the extracted Euler angles, pitches, and RMS energies were downsampled to the same 24 frames per second for our quantitative analysis.
We generated 60 audio-head animation clips as the visual stimuli in our perceptual study.
Specifically, based on 15 speech clips randomly selected from the recorded dataset, we used the following four different audio-driven head animation generation techniques to produce 60 audio-head animation clips :  playing back the original captured head motions ,  the HMMs-based head motion synthesis algorithm  ,  the mood-swings head motion synthesis framework  , and  randomly generated head motions .
The main consideration of choosing four different head motion generation approaches is that if all the animations were generated by a single technique, their user evaluations might fall into a narrow range, which will directly affect the generality of our quantitative analysis.
To suppress the potential influences of other visual/animation factors on user perception, we further postprocessed the audiohead animation clips as follows.
Its main consideration is that we attempted to minimize or even remove the influence of lip-sync quality on the participants' perception on head motions.
Note that a similar face masking methodology was used in previous face recognition research .
The left-top and left-bottom panels of Figure 1 show an example frame of our generated audio-head animation clips.
The used rating scale was from 1 to 5 .
The participants viewed the clips in a 42 inches TV screen  and then wrote down their ratings .
For each participant, we first showed three example clips as test trials to allow him/her to get familiarized with the study procedure.
The clip order for each participant was randomly generated to ensure the counter-balance of the experiment.
Finally, we computed the average rating of each clip.
For each audio-head animation clip, we obtained its average user rating, its 3D head rotation  sequence, and its speech pitch and RMS energy sequences.
Our quantitative analysis is conducted in the following two ways.
First, we analyze the quantitative correlation between the user rating and joint features of head motion and speech signals .
Second, we perform a frequency-domain analysis to study the association between the user ratings and head motion patterns.
As reported in previous literature , head motion has a strong correlation with the pitch  and the RMS energy of speech signals.
Inspired by their work, we look into the quantitative association between human perception and joint audio-head motion feature.
Specifically, we use Canonical Correlation Analysis  to measure the correlation between head motion and the two types of fundamental speech features: pitch and RMS energy.
For each used audio-head animation clip, we compute its CCA coefficient between its head rotation Euler angle sequence and its pitch sequence.
As clearly shown in Figure 2, the highly rated animation clips  typically have high pitch and head motion CCA coefficients.
Meanwhile, the lowly rated clips  have low pitch and head motion CCA coefficients.
We also perform Pearson analysis to quantify the linear relationship between the computed pitch and head motion CCA coefficients and the subjective user ratings.
The computed Pearson coefficient , r = 0.731, which measurably proves the existence of a linear correlation between the pitch and head motion CCA coefficients and the perceptual user ratings.
We conducted a user study on the above 60 visual stimuli.
A total of 18 student volunteers who were not informed the purpose of the study were invited to rate the naturalness of these clips individually.
The demographics of the participants are: ages are from 23 to 28; 3  are female and 15  are male; and most of them are international students but all of them have been studying in the US for at least three years and can speak fluent English.
Our finding is contradicted with the previous results by Munhall et al.
Our finding suggests that such delayed head movement generation strategy will potentially degrade human perception due to the importance of precise timing between head rotation and pitch, and for talking avatar head motion generation, designers need to heavily exploit vocal prosody timing  in order to build a more perceptually believable head motion predictor.
Also, our finding further gives a new and precise linear correlation between user ratings and the pitch and head motion CCA coefficients.
As reported by Azuma and Bishop , human beings are more sensitive to the velocity and acceleration of head movements than static head poses.
Thus, in this work, we use a frequency-domain analysis method to study the impacts of head motion spectrum patterns on human perception.
Although frequency-domain analysis is not a new technique used in HCI community, previous frequency-domain analysis for head motion focused on analyzing the predicted results from linear head motion predictors , while our analysis intends to quantify the affects of frequency-domain head movements on human perception.
Our frequency-domain analysis of head motion  first separates 3D head motion Euler angles into three 1D signals inspired by the work of Azuma and Bishop .
This makes the analysis more accurate on each rotation axis.
Then, we apply the Fourier and Z-transforms to the head rotation vector of each animation clip to derive its corresponding frequency spectrum.
A classical Fast Fourier Transform  algorithm is used to compute the spectrum.
The FFT point parameter is set as the smallest power of two that is greater than or equal to the absolute value of frame length of each animation clip.
Figure 4 shows the frequency-domain analysis results of all the 60 clips, with an order from the highest user rating  to the lowest user rating .
For a better illustration, in Figure 4, we only plot certain magnified part of head rotation spectrum with the squared magnitude less than 5 degree.
Note that the degree unit  is the squared magnitude of three Euler angles.
As shown in Figure 4, most of the highly rated audio-head animation clips have a low frequency pattern .
Plotting of the computed RMS energy and head motion CCA coefficients and the corresponding average user ratings.
In a similar way, we also compute the CCA coefficients between head rotation Euler angles and the RMS energies of speech signals for all the 60 clips.
Figure 3 plots the computed RMS energy and head motion CCA coefficients versus the obtained average user ratings.
In the future, we plan to further extend the proposed methodology to systematically study head movement patterns in multi-party conversations.
R. Azuma and G. Bishop.
A frequency-domain analysis of head-motion prediction.
C. Busso, Z. Deng, U. Neumann, and S. Narayanan.
Natural head motion synthesis driven by acoustic prosody features.
E. Chuang and C. Bregler.
Mood swings: expressive speech animation.
Frequency and velocity of rotational head perturbations during locomotion.
A. Jones, D. E. Callan, T. Kuratate, and E. Vatikiotis-Basteson.
Visual prosody and speech intelligibility: Head movement improves auditory speech perception.
E. Wang, C. Lignos, A. Vatsal, and B. Scassellati.
Effects of head movement on perceptions of humanoid robot behavior.
M. Williams, S. Moss, and J. Bradshaw.
A unique look at face processing: the impact of masked faces on the processing of facial features.
N. Yee, J. N. Bailenson, and K. Rickertsen.
A meta-analysis of the impact of the inclusion and realism of human-like faces on user experiences in interfaces.
H. C. Yehia, T. Kuratate, and E. Vatikiotis-Basteson.
Linking facial animation, head motion and speech acoustics.
Can local avatars satisfy a global audience?
As pointed out by Grossman et al.
Therefore, during head rotations, if the maximum head velocity exceeds the range, the VOR mechanism will stabilize the motion spontaneously.
Thus, our finding in this work is consistent with their results .
Furthermore, our quantitative analysis finding also suggests that 012 Hz is the comfortable zone in which VOR enables human beings to maintain natural head movements.
This leads to one important implication for avatar-based humancomputer interfaces, that is, since humans tend to give a lower rating to head motions with a larger portion of high frequencies, synthetic talking avatar head motions should be smoothed or simply cropped as a post-processing step.
In this way, more natural avatar head movements can be obtained.
Our quantitative analysis results clearly show that the coupling between the pitch of speech signals and head motion has a strong correlation with human perception.
Generally, a higher coupling coefficient between pitch and head motion leads to a higher user rating on the audio-head animation.
Animation clips used in current study only enclose a single avatar.
However, we believe that our current findings can be soundly applied and generalized to more realistic scenarios due to two main reasons.
First, although we only acquired the head movements of one female subject, she was participating in a natural two-party conversation  during the data capture procedure.
