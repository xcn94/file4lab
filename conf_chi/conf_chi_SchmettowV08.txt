Most topics raised throughout the paper can be interpreted for usability testing studies as well.
In particular, the models in Predicting Process Outcome have equally been applied to data from user testing and expert inspections .
Here, the reason for focussing on inspections is that the approach we introduce is of specific value for comparing method performance.
This is a critical question with inspection methods but not so much with the practice of user testing.
Usability evaluation methods have a long history of research.
Latest contributions significantly raised the validity of method evaluation studies.
But there is still a measurement model lacking that incorporates the relevant factors for inspection performance and accounts for the probabilistic nature of the process.
This paper transfers a modern probabilistic approach from psychometric research, known as the Item Response Theory, to the domain of measuring usability evaluation processes.
The basic concepts, assumptions and several advanced procedures are introduced and related to the domain of usability inspection.
The practical use of the approach is exemplified in three scenarios from research and practice.
These are also made available as simulation programs.
A primary aim of measuring inspection performance is to plan and control inspection processes.
This is the case when a certain outcome of the process has to be guaranteed.
In situations where usability is mission-critical , stakeholders rely upon the inspection process to yield at least, for example, 95%, of the usability defects.
The prediction of process outcome may be conducted prior to the process or adaptively as online monitoring during the process.
The a priori prediction relies upon previously assessed performance measures in order to determine the number of estimators to achieve a certain goal.
In contrast, online monitoring aims at estimating the proportion of defects currently found, which usually requires to estimate performance measures from the previous course of the process.
The a priori measures are usually obtained under experimental conditions or with defined testing procedures.
The earliest model of planning the inspection process for a guaranteed outcome was proposed by Virzi .
This model assumes a single parameter  , which denotes the probability of one inspector identifying one defect.
Formally, the identification of a defect is regarded as a series of Bernoulli experiments  with p =  until the defect is detected the first time.
Under the assumptions of independent events and fixed  this leads to the cumulative geometric distribution  with the basic formula:
Usability inspections play a major role for early quality assurance in the development of interactive software.
The focus is usually on identifying usability defects .
Whereas usability testing is usually held to be the most effective method to identify defects, inspection methods are often preferred because of their assumed cost efficiency.
However, low costs is one side of the coin.
It is as well an obligation to prove that a particular inspection method yields sufficient detection performance to satisfy the goals of quality assurance processes.
The measurement of detection performance is a sine qua non to plan, predict and monitor inspection processes.
Last but not least, Reprint of a paper originally published at the CHI2008, Florence.
Cite as: M. Schmettow and W. Vietze.
Introducing item response theory for measuring usability inspection processes.
Lewis resolved this by estimating the actual  for a study from the first few sessions , still retaining the assumption of all inspectors and defects having the same probability.
There have already been a few suggestions how to incorporate inspector skills and defect detectability into the model.
Caulton extended the model for heterogeneity of groups regarding defect detectability in usability testing .
He assumed that there exist distinct subgroups of test persons that are sensitive to certain subsets of usability defects.
The assumption of distinct subgroups might eventually be required for usability testing, but we believe that for inspection it would for now suffice to assume a continuum of a general skill to detect defects .
This would result in the following model for predicting the detection probability of defects:
The basic performance criteria are derived from these measures as:
Both approaches regard an additional source of variance and are likely to predict the inspection process better than the simple Virzi model.
Nevertheless, at the moment there are no approaches known to us which take this one step further and account for both, the inspector skills and the defect detectability.
And, as will be argued in the next section, the estimators for these parameters lack generalizability and thus are not comparable between different settings.
While this framework definitely is an advance, it still has several inherent problems: The method performance is expressed as a single score obtained in a single experiment with a particular sample of inspectors and a particular set of defects.
With the previous assumption that inspectors vary in their skills to detect defects, the results from the study are not easily comparable to other groups of inspectors.
To achieve unbiased location parameters for the performance measures would at least require a carefully assembled sample that is representative for all inspectors out there.
Unfortunately, this is a catch-22 as the model does not provide any means to estimate individual inspectors' skills which are independent of the set of defects under consideration.
The same problem arises with defects: The experimenter had to test a representative sample of applications in order to gain an unbiased location parameter.
Furthermore, for the purpose of reliably predicting inspection processes no estimators for standard errors have been published so far.
These are required for statements of confidence.
As a main drawback of restricted generalizability, inspection methods cannot easily be compared across studies regarding performance.
Within-subjects studies would avoid individual differences, but are usually not adequate because of learning effects.
Meta studies might mitigate this problem.
But it is far more desirable to have a measurement model which deals with variability of both performance influencing factors right from the start.
In the early nineties the  Heuristic Evaluation  and several other known or not so known inspection methods appeared and promised what was termed "discount usability" - good enough usability efforts at low costs.
It is remarkable that since then, measuring the performance of inspections has been a main research aim in Usability Engineering with numerous empirical studies being conducted .
Unfortunately, many of these studies suffered from severe methodological flaws, which led to a harsh critique by Gray & Salzman in their well-recognized "Damaged Merchandize" paper .
In the meantime, several researchers es-
The "signal detection" measurement framework accounts for performance on a set of defects with aggregated scores only.
Consequently, comparison of inspection methods can only have the form of a single difference score between two methods.
But this is only true under the assumption that two inspection methods do exactly the same but with a single effectiveness delta.
This assumption appears unplausible regarding the very different ideas current inspection methods are based on.
For example, the Heuristic Evaluation is usually treated as an universal instrument to predict usability prob-
This makes it important to develop performance measures that are differential with respect to certain types of defects.
An ontology of defect types could, for example, follow the Usability Problem Taxonomy, suggested and validated by Keenan et al.
Indeed, the existence of differential detection profiles has been indirectly shown by an advantage of mixing methods in inspection groups .
It was argued there that assessing the differential anatomy of inspection methods would eventually allow for significantly improving inspection processes and would also inspire for improving inspection methods.
Sure enough, this aim presupposes a measurement approach that reveals phenomena on the level of individual defects and inspectors.
This allowed for comparing data from experiments with the set of defects held fixed, but different inspector populations.
In case of method comparisons the remaining performance difference can then unambiguously be assigned to the contribution of different methods.
On the other hand, the independence of inspector ability estimates from the set of defects will allow for valid performance tests for inspectors.
With such a test the abilities in a team of inspectors can be assessed in advance and used for precisely planning an inspection process towards a guaranteed detection rate.
Finally, the model should allow for easy incorporation of additional parameters .
One such parameter is the contribution of a particular method to the inspection performance, but others can be thought of, for example inspectors' domain expertise.
In the following, we introduce a model that meets all of these requirements and can thus significantly enhance measurement of inspection processes.
It is known as the Item Response Theory , which has become popular in the field of psychometrics and recently econometrics.
As a preliminary remark, in the following sections we will only refer to the measure of thoroughness.
Most arguments also hold for validity, usually under the opposite sign.
In order to account for both sources of variance in the inspection process, a model is needed, which assigns an individual detection probability to each combination of inspectors and defects.
This model has the complication that the detection probability is no longer a variable, but a function  which assigns a probability i j to each combination of inspector ability i    and the defect's inherent difficulty  j  E to be detected,
And, because i j is a probability,  and E must be measures on a metric scale level .
In addition, an estimation procedure for both parameters is required that yields standard errors.
Only this allows for probabilistic statements regarding the process - especially the confidence of process predictions .
As said before, such a model is appropriate for user testing as well: The only difference lays in the interpretation of , which appears as inspector skill for inspections or as a user's proneness to stumble across a defect in user testing .
If measuring validity from the false alarms is at stake,  can be interpreted as the inspectors proneness  to falsely report pretended defects, whereas E reads as the property of a pretended defect to appear as a true defect.
For enabling cross-study comparisons of methods, some degree of generalizability has to be achieved.
As argued in Comparing Method Performance, it is unpractical to achieve this with a reference to the real distribution of inspectors or defects.
The Item Response Theory  has its origins in the field of psychometric testing.
It appeared as an alternative to the so-called classical test theory .
In contrast to CTT the formal model of IRT treats individual test items and persons as primitives, instead of summative test scores.
This has several advantages which correspond to the requirements we have proposed for a measurement model of inspections.
With respect to our assumptions that inspector and defect variability account for the outcome of an inspection process, the simplest appropriate model from IRT is the Rasch model.
Accordingly, we will focus on this model.
But we will also refer to alternative models in case the assumptions of the Rasch model are prone to certain violations.
The basic random variable considered in IRT models is the response of an individual person to a particular item in a test.
In contrast, the signal detection measurement approach like the CTT views the complete test score, respectively inspection process outcome, as the basic random variable.
The Rasch model restricts to dichotomous response categories like "correct/incorrect".
The responses on a set of items by a set of persons is organized in the response matrix , which is the basic data structure for estimating parameters and checking the model fit.
In the domain of inspections these responses can be denoted as usability defects, which are detected  or missed  by individual inspectors.
In contrast to the "signal detection" measurement model , IRT assumes that the probability of a correct response is a function of a latent person ability  and the difficulty  of the item.
This performance gain of a method can be introduced as an additional parameter  in the summative part of the item characteristic function and would appear as horizontally shifting the complete set of ICCs:
A set of items can thus be depicted as a set of non-crossing sigmoid curves  giving the probability of solving each item depending on  , as is illustrated in figure 1.
There are some remarkable properties of the Rasch model, which are of large practical use for the problems of measuring inspections.
First, the logistic function is a candidate for  as stated above .
A special property of the function is that  and  relate additively to each other.
Therefore, it is possible to directly compare persons and items on one scale in that the ability of an individual inspector is expressed as a probability to solve a particular item .
The Rasch model is usually held to measure on at least interval scale level, which fulfills our claim of metric scale measurement .
Irtel even claims ratio level scaling .
For a mathematically sound discussion of the scale level the reader is referred to .
Also, with the Rasch model there is no need to adjust the parameters with respect to a certain empirical distribution.
Instead, they can be estimated from the set of correct/incorrect responses alone.
This is one consequence from the property of specific objectivity, which the Rasch model fulfills .
In general, specific objectivity denotes that ability estimators are invariant to the subset of items chosen, and vice versa.
Response matrices are a typical outcome of inspection experiments, and it is quite plausible to assume defect detectability and inspector ability as the impacting factors.
Thus, it appears obvious to transfer the concepts of IRT in order to advance the domain of inspection measurement.
But IRT, and especially the Rasch model, is an axiomatic theory and poses strict assumptions to be fulfilled by the data at hand.
In the following, we discuss these assumptions with respect to the domain of inspection processes.
For many inspection experiments, restricting to a dichotomous response variable, as required by the Rasch model, fully suffices.
In this case a rating scale variant of the Rasch model is available .
As depicted in figure 1 the Rasch model assumes that the ICCs are monotonically increasing and that the slopes do not intersect .
In the inspection realm this means that inspection outcome relies on one principal latent ability, which has a monotonic increasing impact on defect detection.
It has often been reported that there is also some impact of application domain knowledge on inspection success .
If domain knowledge affected certain defects differently, the assumption of equal sensitivity is violated, because the proportional impact of the principal detection ability differs.
There are two solutions in case this violation comes true in real inspection experiments: First, under controlled experimental conditions it is possible to choose an application domain all inspectors are completely unfamiliar with.
This would, however, add a pessimistic bias to the measures, because inspectors are usually expected to acquire and utilize domain knowledge.
Alternatively, one chooses a generalization of the Rasch model, which allows for differing sensitivity by adding a multiplicative parameter  to the ICC.
Another property of the Rasch ICCs is that the slopes tend to zero with decreasing ability.
This disallows for a chance of guessing the items right.
This may happen in fixed-response multiple-choice questions, but is unlikely a problem with defect identification, which is a free-response task.
In particular, this forbids any ordering effects.
In the domain of inspections this assumption may be challenged: First, if inspection sessions are too long, the probability of finding a defect later in the walkthrough might increasingly suffer from fatigue.
If an inspector finds one of these defects he or she might more easily find similar defects, which introduces a stochastic dependence between events.
These concerns are relevant, but they can be mitigated by adequately designing an inspection experiment.
Obviously, the duration should be kept short enough to not introduce fatigue effects.
In general, the experiment must not put any time restrictions, but it must allow each participant to encounter every defect.
If this is not feasible, the coverage of each individual inspection has to be carefully tracked in order to exclude defects from the estimation that have not been reached.
Basically this means to measure for effectiveness, whereas efficiency can easily be obtained by tracking the time each participant required.
For learning between defects two approaches can be taken: Either the object and scope of inspection should be selected carefully so that all defects are pairwise unsimilar.
Alternatively, detection events, where the defect is suspected to having a similar predecessor, are excluded ex post.
And again, there is a Rasch model derivate available, where learning effects are explicitly handled .
Fortunately, there is usually no influence of previous inspectors in the current practice of inspection experiments and processes, where sessions are held independent.
A central theorem in IRT is given by the proof of Andersen : Roughly, it states that if the above assumptions hold true then the sum of correct responses in a given set of items is a minimal sufficient statistic.
This means that the sum of correctly identified defects is indeed an appropriate measure of inspector ability.
This is what the signal detection approach implicitly assumes as well.
But in contrast, the IRT provides means to check for these assumptions and draws this as a conclusion in a mathematically exact manner.
As we have argued, there might appear phenomena in inspection processes causing a violation of the Rasch model's assumptions.
In any case, this can be solved by either con-
The basic data structure for the IRT is the response matrix.
For the inspection realm we assume that a set of inspectors conducts an inspection on a certain application with the same method .
For now, we regard the dichotomous case only, where a defect is either detected or missed.
As is typical for inspection processes, there has to follow a aggregation procedure, where similar reports of the inspectors are grouped under a single defect.
Finally, the proposed defects have to be verified via the procedure of falsification testing  resulting in a verified response matrix.
Note that falsification testing also yields a dichotomous matrix for false identifications, which can be analyzed quite analogously in order to measure the aspect of validity.
The preferable procedure is the CML estimation, which computes the item parameters first, then the person parameters.
This is tightly connected to the existence of a minimal sufficient statistic for  .
CML is known to be robust, does not presume a certain distribution type and is implemented in various tools for IRT analysis .
Contrary to JML estimates, CML estimates are asymptotically consistent under mild conditions.
Because of the exponential family, the estimated asymptotic covariance matrix can be obtained .
Wald test focus on meaningful subgroups of the sample of respondents.
On item-level it is applied by splitting subjects into subgroups, either median split or with deliberate split.
Furthermore, the data must conform to model assumptions about dimensionality.
In case of the Rasch model the measured constructs are unidimensional.
Which means that they have one prominent factor underlying them with all other factors being functionally insignificant.
A technique specifically developed to check for unidimensionality in IRT models is the Dimtest .
The analysis of real data from inspection experiments may reveal violations of certain assumptions of the Rasch model.
As depicted above, a variety of derivates of the model with relaxed assumptions exist.
E.g., in the Birnbaum model the ICC do not have to appear parallel by introducing a further parameter.
In order to compare the fit of different models the Akaike Information Criterion  is available from the ML estimation.
The AIC enables to choose the model that fits for the data at hand best with respect to parsimony.
Another convenient property of CML estimates is that various model tests exist for checking the assumptions.
An important consequence of the Rasch model is that consistent item parameter estimates can be obtained from a sample of any subgroup of the population.
Thus, on item level sample independence must hold.
If the sample is split in, e.g.
A first assessment of homogeneous item functioning is made by a graphical test, plotting the different parameter estimates as depicted in figure 2.
Predictions, based on the estimated model, should be examined in cross-validation samples.
Anderson's LR  Test is specifically used with CML, because item parameters can be consistently estimated in any sample of the population, where the model applies .
As mentioned, if items fit the model, the parameter estimates are equal apart from random fluctuations in any subset of the sample.
Figure 2 depicts how to conduct a graphical test based on this idea, where one item appears inconsistent.
This led to the concept of differential item functioning  accounting for the fact that items might function differently in different groups.
The original goal is to expose these differences and eliminate unfair items .
Instead, in the domain of usability inspection measurement the concept of DIF is promising to analyse relevant differential phenomena.
Obviously, the DIF concepts can be utilized to compare different inspection methods regarding the overall performance.
DIF analysis can also reveal differences in sensitivity to certain defect types as we suggested in Anatomy of Inspection Methods.
Further on, there are interesting questions regarding the impact of expertise and learning that could be explored via DIF.
In general, the introduction of a particular inspection method can be regarded as a learning process, which changes the detection difficulty of defects.
Several types of change can be proposed and tested via DIF: 1.
A particular method adds a constant amount of detectability to the defects.
This results in ICCs that are shifted horizontally without change in order or intervals.
A particular method equalizes the difficulty of defects, such that previously hard-to-detect defects are now easier being detected.
This results in a horizontal shift of ICCs with closing intervals but ordering preserved.
Two methods have an arbitrary differential impact on defects, with different orderings of ICCs.
This may reveal differences in the methods' focus in certain defect types.
Different approaches have been established to identify DIF between two subgroups: The non-parametric Mantel-Haens-
If an item is "fair", the odds of getting the item correct at a given level of the matching variable is the same in different groups across all M levels of the matching variable.
The second approach utilizes the parameters of the IRT : In order to identify the specific types of DIF, procedures for graphical analysis of ICCs have been suggested .
The difference between the two ICCs can be measured by the total area enclosed between the curves.
This area represents the discrepancies in probabilities of correct responses between the two groups.
For the convenience of calculation, the  scale can be divided into very small discrete intervals at increments of  .
In the Birnbaum Model the discrimination factor might differ in the groups so the ICCs can intersect each other.
In this case it is required to calculate absolute values:
The MH procedure involves a comparison of the log-odds ratio of endorsing keyed responses for different groups, computed after partitioning the sample into categories on the basis of counting correct scores.
The null DIF hypothesis for MH method comparing two groups can be expressed as:
A1i =  |PXi  - PXi | *  Further guidelines for evaluating particular hypotheses on DIF are given by .
A summary of three approaches to test for significant DIF is given by : Likelihood ratio tests, Wald tests and Lagrange Multiplier tests.
Each test can, in principle, be applied to a single item, a number of items or the test as a whole.
In a recent study, the variance of evaluation outcome was examined based on real data , with the result that with increasing sample size the variance diminishes.
This could as well have been shown with solely simulated data employing the Virzi formula.
With the Rasch formula this can be taken one step further in that the impact of the variance of defect detectability and inspector ability on process variability can be analyzed.
For  we choose the distribution N  and for  a normal distribution with  = -1.1.
This yields an average detection probability of i j  .3, which is typical for inspections .
In order to investigate the impact of variance in inspector ability, a simulation is run with different   = .
As shown in figure 3, the variance in group thoroughness depends largely on the team size , but with smaller groups  there also appears a considerable impact of the variance of inspector ability.
This has to be taken into account, if a guaranteed outcome of inspection processes is required.
Here, it serves as one example that the Rasch formula is a powerful means to model, simulate and understand inspection processes.
So far, we have transferred the basic and some advanced concepts of the IRT to the domain of inspection process measurement.
Researchers and practitioners may apply the model to their data and verify whether the model is formally appropriate and provides practical added value.
In order to ease the latter, we now present three basic application scenarios.
Each scenario is accompanied by a demonstrator program employing the freely available R Statistical Computing Environment  and the extended Rasch modelling package eRm .
For the sake of space, the procedures and results of the scenarios are only outlined.
Details can be obtained from the previous sections, general statistics textbooks or by running and inspecting the demonstrator programs, which are available for download  or on request.
A preliminary step for several further applications of Rasch measurement in inspection research and practice is a test for inspectors' skills.
In the following scenario a diagnostic instrument for assessing inspectors ability is set up, which, by the way, is close to the original purpose of the IRT approach.
First, it is required to establish a test: A sample of participants  may be asked to fully inspect a sample application with previously known usability defects .
From this matrix the defect difficulty parameters are estimated with the CML method.
The null hypothesis of the values of  being equal in subgroups can be retained, which means the Rasch model holds.
The most basic application of the IRT is using the Rasch formula  for modelling the core inspection process.
This demonstrates the utility of the function i j =  for investigating the behavior of complex inspection processes accounting for both impact factors - difficulty and ability.
Finally, a table with each possible raw score  and the associated person parameter is computed.
This is possible due to the raw score being a sufficient statistic .
As an application consider a team of five inspectors  to be tested.
These participate in the test by conducting an inspection of the test application.
Again, raw scores are computed and the person parameters are simply obtained from the test score table.
Alternatively, the complete test construction procedure may be run with the added participants, which further improves the estimates due to the larger sample and yields standard errors for person parameter estimates.
Figure 4 shows the test response matrix and the person parameters compared to the distribution of the calibration sample.
The scenario can be enhanced in several ways.
For observed response vectors the standard error of the person parameter can be obtained and can act as a criterion for reliability of an individual measure.
Finally, any  subset of the defects can be selected for testing; for example, if one wants to assess the impact of training with a retest so that two different sets of items are required.
We expect the latter model to better predict the outcome of the process.
For each tested to obtain the estimated person parameters  group an inspection experiment was simulated with 100 defects .
This was repeated 300 times to gather data.
Then a linear regression  was conducted in both conditions.
As can be seen in figure 5, the Virzi predictor predicted the average expected outcome quite well but does not account for the variance.
In turn, the Rasch predictor accounts for the variance caused by individual i and predicts the outcome much better.
A subsequent analysis of variance reveals that the residual sum of squares with the Virzi predictor  is about six times as large as in the Rasch condition .
This means a much better prediction of the process, if inspector abilities are prior assessed with a Rasch-based test.
Finally, this scenario shows that a reasonable sample size of 30 participants may suffice for the estimation of parameters in order to get results of practical value.
The outcome of an inspection process can be regarded as a three-fold random experiment: A sample of inspectors is chosen from the population, a sample of defects is chosen from the defect population and each pair undergoes a Bernoulli process.
All three sources count for undeterminism of process outcome.
In this final demonstration we show that the process is predicted more accurately, if the individual abilities in the sample of inspectors are assessed a priori.
This is achieved by testing the inspectors as depicted in the previous scenario.
Of course, the individual defect parameters cannot be estimated in advance.
We transferred the concepts of psychometric measurement with item response theory to the domain of measuring inspection processes.
We regard this approach as a promising advance in the field for practical appliance and research on usability evaluation methods.
It was demonstrated how the Rasch formula can be employed to study the inspection process under various conditions.
For that purpose, we chose a simulation approach which is quite flexible and intuitive.
In future, the simulation findings may lead to  mathematical proofs of certain characteristics of this particular stochastic process.
Relaxing the homogeneity asssumption in usability testing.
Gilbert Cockton, Darryn Lavery, and Alan Woolrych.
In Julie A. Jacko and Andrew Sears, editors, The human-computer interaction handbook: fundamentals, evolving technologies and emerging applications, pages 1118-1138.
History and Development of DIF, chapter 2, pages 25-29.
Neil J. Dorans and Paul W. Holland.
DIF Detection and Description: Mantel-Haenszel and Standardization, chapter 3, pages 35-66.
Beyond the five-user assumption: Benefits of increased sample sizes in usability testing.
Derivations of the Rasch Model.
Gerhard H. Fischer and Ivo W. Molenaar, editors.
Rasch models: Foundations, recent developments and applications.
Springer-Verlag, New York Berlin Heidelberg, 1995.
Wayne D. Gray and Marilyn C. Salzman.
A review of experiments that compare usability evaluation methods.
H. Rex Hartson, Terence S. Andre, and Robert C. Williges.
Criteria for evaluating usability evaluation methods.
Morten Hertzum, Niels Ebbe Jacobsen, and Rolf Molich.
Usability inspections by groups of specialists: perceived agreement in spite of disparate observations.
Herber Hoijtink and Anne Boomsma.
On Person Parameter Estimation in the Dichotomous Rasch Model, chapter 4, pages 53-68.
The major benefit is the opportunity for comparing performance measures across studies which results from the specific objectivity in IRT.
Another advantage is the availability of standard errors of the parameters.
This truly acknowledges the uncertainty underlying the inspection process and still allows for honest predictions.
According Jared Spool  in his keynote on the British HCI 2007 conference this is one of the major research challenges in Usability Engineering: In modern e-business applications usability becomes a mission-critical property and the question is no longer if testing 5 or 8 users suffice for an 80% detection rate, but if it suffices to identify 99% or even closer to 100% of the usability defects.
As a next step, it is inevitable to investigate to what extent experimental data fulfills the strict axiomatic assumptions of the Rasch model.
In case the Rasch model's assumptions are not fully satisfied we directed several alternatives from the family of IRT models.
Anyways, an advantage of IRT is that previously implicit assumptions can explicitly be tested.
Even if certain violations are observed, this still yields deeper insight into the anatomy of inspection processes - for example the lack of objectivity when rating usability problems.
This is taken one step further with the concept of differential item functioning.
In usability research DIF analysis has the potential to resolve some outstanding questions about usability evaluation processes: in particular, the impact of expertise and sources of knowledge , the diversity of results from different methods and teams  or the synergy in mixed-method processes .
Alan Woolrych, Gilbert Cockton, and Mark Hindmarch.
Falsification Testing for Usability Inspection Method Assessment.
Alan Woolrych, Gilbert Cockton, and Mark Hindmarch.
Knowledge Resources in Usability Inspection.
Conditional Independence and Differential Item Functioning in the Two-Parameter Logistic Model, chapter 6, pages 109-130.
Volume 157 of Boomsma et al.
Paul W. Holland and Howard Wainer, editors.
The uniqueness structure of simple latent trait models.
In G. H. Fischer and D. Laming, editors, Contributions to Mathematical Psychology, Psychometrics, and Methodology, pages 265-275.
Susan L. Keenan, H. Rex Hartson, Dennis G. Kafura, and Robert S. Schulman.
The Usability Problem Taxonomy: A framework for classification and analysis.
Asymptotic properties of the ML estimator of the ability parameter when item parameters are known.
Evaluation of procedures for adjusting problem-discovery rates estimated from small samples.
Patrick Mair and Reinhold Hatzinger.
CML based estimation of extended Rasch models with the eRm package in R. Psychology Science, 49:26-43, 2007.
Patrick Mair and Reinhold Hatzinger.
Extended Rasch Modeling: The eRm Package for the Application of IRT Models in R. Journal of Statistical Software, 20:1-20, May 2007.
Estimation of Item Parameters, chapter 3, pages 39-52.
Finding usability problems through heuristic evaluation.
Jakob Nielsen and Thomas K. Landauer.
A mathematical model of the finding of usability problems.
In CHI '93: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 206-213, 1993.
R: A Language and Environment for Statistical Computing.
R Foundation for Statistical Computing, Vienna, Austria, 2006.
The Growing family of Rasch Models, chapter 2, pages 25-42.
Volume 157 of Boomsma et al.
Towards a pattern based usability inspection method for industrial practitioners.
In Proceedings of the Workshop on Integrating Software Engineering and Usability Engineering , 2005. http://www.se-hci.org/bridging/ interact2005/03_Schmettow_Towards_UPI.pd%f.
Introducing IRT for measuring usability inspection processes - accompaning programs.
Martin Schmettow and Sabine Niebuhr.
A pattern-based usability inspection method: First empirical performance measures and future issues.
In Devina Ramduny-Ellis and Dorothy Rachovides, editors, Proceedings of the HCI 2007, volume 2 of People and Computers, pages 99-102.
Alicia P. Schmitt, Paul W. Holland, and Neil J. Dorans.
Evaluating Hypotheses about Differential Item Functioning, chapter 14, pages 281-315.
Heuristic walkthroughs: Finding the problems without the noise.
Jared Spool and Will Schroeder.
Testing web sites: Five users is nowhere near enough.
In Proceedings of ACM CHI Conference on Human Factors in Computing, Seattle, WA, USA, 2001.
Lawrence Erlbaum Associates, Hillsdale, New Jersey, 1990.
David Thissen, Lynne Steinberg, and Howard Wainer.
Detection of Differential Item Functioning Using the Parameters of Item Response Model, chapter 4, pages 67-113.
A dynamic generalization of the Rasch model.
Refining the test phase of usability evaluation: How many subjects is enough?
Cathleen Wharton, John Rieman, Clayton Lewis, and Peter Polson.
The cognitive walkthough method: A practitioner's guide.
In Jakob Nielsen and Robert L. Mack, editors, Usability Inspection Methods, pages 105-140.
A. Woolrych and G. Cockton.
Why and when five test users aren't enough.
