We ask how to best present social annotations on search results, and attempt to find an answer through mixed-method eye-tracking and interview experiments.
Current practice is anchored on the assumption that faces and names draw attention; the same presentation format is used independently of the social connection strength and the search query topic.
The key findings of our experiments indicate room for improvement.
First, only certain social contacts are useful sources of information, depending on the search topic.
Second, faces lose their well-documented power to draw attention when rendered small as part of a social search result annotation.
Third, and perhaps most surprisingly, social annotations go largely unnoticed by users in general due to selective, structured visual parsing behaviors specific to search result pages.
We conclude by recommending improvements to the design and content of social annotations to make them more noticeable and useful.
Annotations are added to normal web search results to provide social context.
The presence of an annotation indicates that the particular result web page was shared or created by one of the searcher's online contacts.
For example, suppose that a friend of Aunt Carol on Google+ or Facebook searched for "Le Bernardin", and Aunt Carol had previously posted a restaurant review on Le Bernardin's Yelp page.
If one of the search results happened to be that same yelp.com restaurant page, the searcher would see an annotation like in Figure 1 on that result, explaining that Aunt Carol had reviewed it on some date.
To enable social results in Google search, users have to be signed in and connect their accounts on various social sites to their Google profiles.
For social search on Bing, users have to be signed in with their Facebook accounts.
After enabling social search, users will occasionally see social annotations on some results when they search the web.
Social annotations so far have generally a consistent presentation: they combine the profile picture and the name of the sharing contact with information about when and where the sharing happened.
The annotation is rendered in a single line below the snippet.
While there is some apparent convergence in current practice, we know of no research on what content and presentation make social annotations most useful in search for users.
In this paper, we set out to answer this particular question, equipped with intuitions derived from past research on search behavior and perceptual psychology.
Intuitively, one could argue that social annotations should be noticeable and attentiongrabbing: it is well-documented that faces and face-like symbols capture attention , even when users are not looking for them and do not even expect them .
Moreover, social annotations should also be useful: users collaborate on search tasks with each other by looking over each others' shoulders and by suggesting keywords , so marking results that others have found useful should help.
However, it is not immediately clear how valid these intuitions are in the domain of web search.
When searching for a restaurant in New York, does knowing that aunt Carol tweeted about Le Bernardin make it a more useful result?
In real life, all of us rely on information from our social circles to make educated decisions, to discover new things, to stay abreast of news and gossip.
Similarly, the quantity, diversity, and personal relevance of social information online makes it a prime source of signals that could improve the experience of our ubiquitous search tasks .
Search engines have started incorporating social information, most prevalently by making relevant social activity explicitly visible to searchers through social annotations.
For instance, Figure 1 shows social annotations as they appeared on Google and Bing search results in early September 2011.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In particular, users pay almost exclusive attention to just the first few results on a page .
It seems that they have learned to trust search engines, because this behavior persists even when the top few results are manipulated to be less relevant .
So how does adding an extra layer of social cues in the form of social annotations affect the search process?
Are their friends' names and faces powerful enough to draw users attention?
Or do learned result-scanning behaviors still dominate?
We designed and conducted two consecutive experiments, which we describe in detail in two major sections below.
The first experiment investigated how annotations interacted with the search process in general, while the second focused on how various visual presentations affected the visibility of annotations.
In our first experiment, we found that our intuitions were not quite accurate: annotations are neither universally noticeable nor consistently useful to users.
This experiment unveiled a colorful and nuanced picture of how users interact with socially-annotated search results while performing search tasks.
In our second experiment we uncovered evidence that strong, general reading habits particular to search result pages seem to govern annotation visibility.
In the next section, we give an overview of related research in this area.
In social question-answering systems, topical expertise is important to the overall system design.
People's knowledge on various topics is indexed, and used to route appropriate questions to them .
Evans, Kairam and Pirolli  studied social question answering for an exploratory learning task, and found expertise to be an important factor in peoples' decisions to ask each other for information.
In real-world question-answering situations, Borgatti and Cross  studied organizational learning and identified the perceived expertise of a person as one of the factors that influenced whether to ask them a question.
Further bearing out the importance of expertise, Nelson et.
There are also suggestions that certain topics such as restaurants, travel, local services, and shopping are more likely to benefit from the input of others, even strangers.
For instance, Amazon, Yelp and TripAdvisor have sophisticated review systems for shopping, restaurants and travel respectively.
Moreover, Aardvark.com reports that the majority of questions fall into exactly these topic types .
The literature that is perhaps most immediately connected to our research questions addresses the use of the "social information trail" in web search.
Yet, to the best of our knowledge, the work in this area has focused on behind-the-scenes applications, not on presentation or content.
Many have proposed ways to use likes, shares, reviews, +1's and social bookmarks to personalize search result rankings , but there is comparatively little on making this information explicitly visible to users during web search.
More generally, studies of the information seeking process can help us understand what types of social information could be useful to searchers, and when.
Several proposed models capture how people use social information and collaborate with each other during search.
Evans and Chi  found that people exchanged information with others before, during and after search.
Pirolli  modeled the cost-benefit effects of social group diversity and structure in cooperative information foraging .
Golovchinsky and colleagues  categorized social information seeking behaviors in according to the degree of shared intent, search depth, and the concurrency and location of searches.
Beyond web search and social question-answering systems, simply displaying or visualizing past history can be useful to collaborators.
Past research on collaborative work suggests that it is useful to show users a history of activity around shared documents and shared spaces.
One of the first ideas in this area was the concept of read wear and edit wear  -- visually displaying a documents reading and editing history as part of the document itself.
A document's read and edit wear is a kind of social signal.
It communicates the sections of the document that other people have found worth reading or worth editing.
Erickson and Kellogg  introduced the term "social translucence" to describe the idea of making collective activity visible to users of collaborative systems.
They argued that social translucence makes collaboration and communication easier by allowing users to learn by imitation, and take advantage of collective knowledge.
They also argued that, when users' actions are made visible to others, social pressures encourage good behavior.
Social annotations can be seen as a kind of social translucency.
As mentioned before, searchers look to other people for help before, during and after web search .
They have to resort to over-the-shoulder searching, e-mails, and other ad-hoc tools because of the lack of built-in support for collaboration .
Researchers have responded with a variety systems that aid collaboration.
In our experiments, we use eye tracking to study user behavior while varying search-result-page design.
In the past, Cutrell and Guan analyzed the effects of placing a "target" high-quality link at different search result ranks  on various task completion metrics.
The effects of "short", "medium", and "long" snippet lengths on search task behavior was also studied by Guan and Cutrell .
The research cited above suggests that social annotations are broadly useful, perhaps especially on certain topics and from certain people.
Our objective was to find out how social annotations influence web search behavior.
Specifically, we wanted to understand the importance of contact closeness, contact expertise, and search topic in determining whether or not an annotation is useful.
We performed a retrospective-interview-based study with N=11 participants.
The participants did not know the intent of the study, and were told that we were evaluating a search engine for different types of search tasks.
Eye-tracking data was recorded so that the researcher could observe the participant more closely.
The participants' gaze movements were displayed to the researcher on a separate screen, and recorded for replay during the interview.
This way, the researcher could monitor whether participants were looking at or ignoring social annotations, and concentrate on those events during the retrospective interview.
A major challenge in this study was to make the study experience as organic as possible.
We wanted users to truly reveal their natural search tendencies without suspecting that we were studying social annotations.
This meant that we would have to design tasks for each participant individually, and could not simply insert fake annotations into a standard set of tasks.
Even if the names and faces were personalized individually to be real contacts, there would be no guarantee that the tasks we created would match those contacts' expertise areas.
To ensure that we were eliciting representative reactions to social annotations, we designed the search tasks individually for each participant by looking at the links and blog posts shared by the contacts in social networks they had linked to their accounts.
Every social annotation seen by each participant was real, just like they would have seen outside the lab, with no modifications.
This meant that we had to infer, for each participant in advance of the study, which queries would bring up social annotations, and design individual search tasks around these URLs for each participant.
In this way, we created 8-10 social search tasks for each participant across different topics .
It was somewhat difficult to find enough participants with these constraints.
Our first step was to find participants who were willing to allow enough access to their personal data to personalize the study.
We recruited around 60 respondents who gave us this permission.
Of these, we found 11 respondents with enough data in their social annotations to see annotations in 10 different searches.
In the first half of the study, participants performed a series of 18-20 search tasks, randomly ordered.
Half the search tasks were designed so that search queries would bring up one or two social annotations in the top four or five results.
In the second half of the study, immediately after the tasks, participants were interviewed in retrospective think-aloud  format.
They were asked to take the researcher through what they were doing on some of the tasks while watching a replay of a screen capture  of the tasks with the researcher.
The researcher would ask probing questions for clarification, and then move on to talk about another task.
If the participant never mentioned the social annotations, even after an interview on all the tasks in which they were present, the interview procedure was slightly different.
The researcher would return to a screen capture of one of the tasks in which there was a social annotation.
The researcher would point out the annotation explicitly and ask a series of questions, like, "What is that?
Tell me what you think about this."
The goals were to find out what the participant thought the annotation was, whether they noticed it at all, and to understand whether or not they perceived it as useful, and why.
After the annotation had been discussed, the researcher would revisit the remaining social annotations and repeat the procedure for each of them.
Half the tasks were designed to bring up social annotations, and the other half were "non-social".
The tasks were framed as questions, and not as pre-assigned keyword queries.
We did not provide pre-assigned queries because we did not want to raise suspicions that there was something special about the search pages or the particular keyword combination.
Some sample search tasks are shown in Table 1.
In the case of social tasks, prompts were worded so that many reasonable keyword combinations would bring up one or two social annotations in the top four or five search results.
Topic How-to Recipe Product Local Entertainment News Fact-finding Navigation Search Task How do you make a box kite?
How do you make milk bar pie?
Find a good laptop case for your macbook pro.
Find a good sweet shop in Napa, CA.
Is the album "The Suburbs" by Arcade Fire any good?
What is going on with stem cells latelyI?
What is the website of Time Warner's youtube channel?
The following summary of our participants reactions reveals concerns about privacy and objectivity, as well the value of closer ties and known expertise.
Participant 5  noticed annotations on two successive queries and commented on them unprompted while searching.
She highlighted the first annotation she saw with her cursor and exclaimed that the annotation was "creepy" even though the annotation was from "a friend."
This time, the annotation was from her personal trainer on a fitness-related topic.
This annotation was better because she trusted her trainer's expertise on the subject, and would actually turn to him for advice in real life.
The other participants did not comment on social annotations while searching, but had informative reactions during the RTA.
P11 explained why he clicked on a Yelp result on which he noticed a friends name and face: "Yeah, I directly know her, it's not just somebody, like a friend of a friend... " Finally, P4 and P6 had both seen social annotations outside the lab, but did not click on the annotations even though they saw them.
They gave different reasons why.
P6 passed over a friend-annotated blog post, and instead chose Wikipedia because: "The immediate thing I thought was that he  edits Wikipedia pages, he's been doing it for a long time" P4, however, gave a different reason for not clicking on an annotated search result: "I don't necessarily want to see what they've looked at, I want to see sources that I think are credible..."
Participants each performed up to 4 tasks in each of these topic categories, of which two had annotations.
We thought annotations may not be useful for navigation and fact-finding, so participants performed 2 tasks in each of those categories, one with annotations and one without.
The inclusion of a category was subject to the availability of social annotations in that category.
For example, if we could not find any product search annotations for a participant, we did not ask them to perform any product search tasks at all.
This resulted in minor variation in the number of search tasks.
Some participants had more annotations in more categories than others.
To understand the space of participants' responses, we first created an affinity diagram by grouping responses that seemed to be similar in meaning.
Once categories of responses emerged, we went through the responses yet again, coding the responses into different categories, and counting the numbers in each category.
We did not analyze the eye tracking data we gathered due to the heavily-personalized and unrestricted nature of the search tasks: each participant saw a different set of pages, and participants were allowed to scroll and to revisit search pages with the back button.
This made standardized region-of-interest annotations very difficult to make.
We used the eyetracking videos primarily as a memory aid for the participant during the RTA's, but also as a supplement to the interview results, to arrive at the conclusions below.
Figure 2 shows that most social annotations were not noticed by participants.
Of the 45 annotations in our experiment that appeared above the page fold, 40  were not noticed.
By "not noticed", we mean that participants explicitly said, when the annotation was pointed out, that they did not notice the annotation while they were searching.
Often, the participants were surprised they had missed it because it was from a close friend, co-worker, old friend, or boss.
Ten out of the eleven participants mentioned that social annotations would be useful on topics like restaurants, shopping, searching for businesses, shopping for expensive items, and planning events for other people.
The remaining participant did not specify a topic.
When asked to generalize, participants used the words "social", "subjective" or "reviews" to describe the category of topics.
When asked why the information would be useful, participants said that it would be good extra information in decision-making if they knew the person had good taste, was knowledgeable, or was trusted by them to have good information on the topic.
Another category of useful topics, brought up by 7 out of the 11 participants, is best described as personal, or hobbyrelated.
All participants said that annotations would be useful when they came from people whom searchers believed had good taste, knowledge or experience with the topic, or had similar likes and dislikes.
This was not restricted to contacts they knew personally, as celebrities, bloggers, or respected authorities on a topic were also indicated to be useful.
Nevertheless, a challenge emerged, which is our need to understand the lack of attention to social annotations, and finding ways to improve their presentation.
Having confirmed that many types of social information could indeed be useful to searchers, we had to ask why the annotations conveying this information were largely ignored.
In the next section, we describe the follow-up experiment we conducted to get to the bottom of this mystery.
Nine out the eleven participants said that annotations from strong-tie contacts  would be more useful than more distant contacts.
Four participants made the distinction between interestingness and relevance.
To paraphrase their responses, annotation from a very close friend might be interesting because of the information it gave about the friend's interests or activities, but it may not provide any relevant useful information about the quality of the result, or make it any easier to complete the task.
Seven out of the eight participants who saw annotations from strangers indicated that they would ignore those annotations.
This included people they did not recognize, and people they did not have a significant relationship with.
One participant said that he would be confused, and would want to know what his relationships to that person was.
When asked whether seeing strangers was a negative, or simply irrelevant, 7 participants responded that it was irrelevant.
Past work shows that people discriminate between the different parts of a search result and do not linearly scan pages from top to bottom.
Titles, URLs, and snippets receive different amounts of attention , and, in the sample of over 600 gaze paths analyzed in , 50% contained regressions to higher-ranked results and skips to lower-ranked results.
As stated above, the goal of our second study was to find out why so many of the social annotations in the first study went unnoticed.
We designed an experiment to investigate what would happen to users' page-reading patterns when social annotations were added in various different design variations.
We hoped to find behaviors anchored in the familiar presentation of search results that would explain why the social annotations in the first experiment were ignored.
Our particular research questions were: 1.
Will increasing the sizes of the profile pictures make social annotations more noticeable?
Are there learned reading behaviors that prevent participants from paying attention to social annotations?
Three out of the eleven participants explicitly mentioned that they would only click on the social annotation or talk to their friend later on about seeing the social annotations if they had time, or were simply exploring a topic space.
The remaining participants did not specify when they would click or followup with a friend on a social annotation.
This study revealed a counter-intuitive result.
Despite having the names and faces of familiar people, and despite being intended to be noticeable to searchers, subjects for the most part did not pay attention to the social annotations.
Our questions about contact closeness, expertise, and topic were answered by the reactions captured during the retrospective interviews.
These interviews revealed the importance of contact expertise and closeness, and the importance of the search topics in determining whether social signals are useful, thus echoing pas findings on the role of expertise in social search .The interviews also provided some high-level understanding of the ways that people use, and want to use, social information during web search.
We recruited 15 non-computer-programmers from within our organization, but had to discard data from 3 of them due to interference with their eye glasses.
As our second study focused on presentation issues only, we decided on less-personalized annotations than in the first study.
Accordingly, we did not have to analyze participants private data, and thus ended up with a simpler recruiting process.
In order to control the stimuli presented to participants, we did not personalize the search tasks.
We used the same set of tasks across all participants.
The only personalization was the names and faces of people in the annotations.
These were participants' real co-workers, but annotations appeared on results of our choosing, and not results that had really been shared by those people .
The social pages were generated with an image editor.
We generated pages with different snippet lengths, annotation positions, and picture sizes.
Then, we pasted in the names and faces of office-mates and team-mates to personalize the mockups for each participant.
The non-social pages were generated by taking screenshots of search results.
Due to their prominent size, we suspected that the big 50x50px pictures might prime the participants to the social nature of the experiment.
We therefore divided the participants into two conditions to avoid an undetected priming bias: the first group  saw the big-picture variants first, before any other type of annotation, and the second group  saw the big-picture variants last, only after they had seen all the 21x21px variants.
Their names and faces were then pasted into the static mockups of web pages with social annotations.
In the first part of the study, participants performed 36 consecutive search tasks.
For each task, they were first shown a screen with a task prompt, and asked to imagine having that task in mind.
Once they had read the task prompt, they pressed the space bar.
This took them to the search page mock-up.
They were instructed to view the page as they would normally, and to a click on a result in the end.
In the second part, the participants were retrospectively interviewed about some of the search tasks.
The researcher played back a screen capture of their eye movements, and asked questions.
Unlike the first study, the interviews were short.
We directly asked whether they had noticed the annotation, who the person was, and which annotation presentation they preferred: above-snippet, below-snippet, or big picture.
Our goal was to see whether changing snippet length, annotation placement, and picture size changed the amount of attention  given to the annotation.
The annotation's presentation within the result was varied to be either above the snippet or below the snippet .
The annotated result was either the first result on the page or the second result.
Additionally, to test our hypothesis about the faces in the annotations being too small to be noticed, we added another annotation presentation condition by using a 50x50 picture placed in-line with the snippet, as shown in Figure 3c.
Together these annotation variations, snippet length variations, and result position variations created a 3x3x2 = 18 different conditions, as follows:  x  line snippets x .
These variations were interleaved with an equal number of baseline non-annotated result pages, bringing the total to 36 tasks.
For all participants, we measured how the number of fixations on annotations varied with snippet length, annotation placement, and annotated result.
In addition to the results reported in the following sections, we performed a linear regression, controlling for between-participant variation, and picture order.
Further, for succinct visual evidence, we have supplemented some of the quantitative results below with gaze maps averaged across all the participants.
Our results are the same when analyzed using fixation count or fixation duration.
We chose fixation count as the presented metric because it is more intuitive to think about whether users actually moved their eyes to the annotations.
Participants viewed and clicked on 36 mock-ups of search result pages.
Half of these had social annotations, and half did not, and the social and non-social pages were interleaved.
The motivation for using both annotated and non-annotated mock-ups was twofold.
First, we wanted to avoid raising suspicions about the nature of the study, and second, the nonannotated pages provided identical baselines on which we could compare all participants.
The graph in figure 4 shows the average number of fixations on various elements of the search result item, compared across different snippet lengths.
We can see that annotations below a 1-line snippet get almost twice as many fixations compared to annotations below a 4-line snippet.
Average fixation count on annotations vs. annotation placement, for each of the different presentation variations .
Annotations were placed either above the snippet or below the snippet.
The snippet-length effect shown for the annotation-belowsnippet condition.
The different lengths were 1-line  2-line  and 4-line .
These averaged gaze maps show that the longer the snippet, the fewer the fixations on the annotation.
An example of this effect for the below-snippet presentation is shown in the averaged gaze heat-maps in figure 5.
It is clearly visible that, on average, the longer the snippet above the annotation, the fewer fixations it got .
The effects of snippet length on the other result elements are in line with past findings .
Fixations to the snippet increase with snippet length, and fixations to URL and title are relatively constant.
Annotations above the snippet get uniformly more fixations than annotations below the snippet.
The graph in figure 7 shows that the effect is true for all snippet lengths and result positions.
The above-snippet condition received a positive coefficient in our linear regression  = 2.03, p < 0.04, meaning that annotations above the snippet got more fixations.
The heat map in Figure 8 shows an example of the placement effect in the 4-line-snippet condition.
It is obvious from the figure that the annotation got more fixations  when it was placed above the snippet.
As one might expect intuitively, the 50x50 pictures have a dramatically larger average number of fixations than the smaller 21x21 pictures.
Figure 6 shows the effect on number of fixations to pictures.
The critical threshold here is a value of 1.
A value above 1 means that the element, on average, receives attention.
A value below 1 means the opposite.
Figure 6 shows that the big pictures receive around 1.3 fixations on average, but the small pictures only receive 0.1.
Not surprisingly, the conclusion therefore is that big pictures of faces get noticed, whereas small ones generally do not.
Figure 9 shows the effect of result position on attention to annotations, averaged across all annotation types, snippet lengths, and picture sizes.
Annotations on the first result receive about 1.3 fixations on average, but annotations on the second only receive 0.8 fixations on average.
Users are so focused on performing their task, and social annotations are not part of their existing page-reading habits, so they simply skip over them and act as if they are not there.
The phenomenon of lack of attention causing functional blindness to clearly visible stimuli has been documented with many different types of activities.
Pilots have failed to see another plane blocking a landing runway  and spectators of dodgeball have failed to notice a person in a gorilla suit walking across the playing field .
Mack, Rock, and colleagues , studied the phenomenon extensively, and gave it the name "inattentional blindness".
The bigger profile pictures, however, drew attention as expected from studies of attention capture by faces .
So, if the pictures in the first study had been bigger, the annotations might have been noticed more.
At a small size however, they were not capable of disrupting users' page scanning patterns.
In search result pages, titles stand out with their blue color and large font, urls stand out in green, and matching keywords are marked out in bold text.
Human beings have a cognitive bias that leads us to learn and remember information that is visually prominent .
Highlighted or underlined text is remembered and learned better than normal text, even if the highlights are not useful .
Highlighted text is also given increased visual attention as measured by an eyetracker .
The observed selective attention to certain elements might stem from this effect, combined with learning over time.
However, the results also suggest that we can direct more attention towards a social annotations by manipulating page structure to our advantage.
Attention can be gained by placing annotations above the snippet, shortening the snippet, and increasing annotation picture size.
While we can manipulate the visual design to make annotations more prominent, we must also learn when they are useful to the user, and call attention to them only when they will prove productive.
The experiment produced only one data point per participant for each configuration of annotated result, placement, and snippet length, giving N = 12x18 fixation-count measurements.
Therefore, we fit a simple additive model to find the effects of each variable on fixation.
The additive model is a crude approximation, and the data are non-normal, so our p-values should only be interpreted as a rough guide to statistical significance.
In the first study, participants were often surprised when social annotations were pointed out to them.
From their comments, they seemed to believe they did not notice the annotations because they were engrossed in their search tasks.
In our second study, we found that  users always paid attention to the URLs and titles, and increased their attention to the social annotations when  the summary snippets were shorter,  pictures were bigger, and  when the annotations were placed above the snippet summary.
Together with past research on search-page reading habits, the second study's results suggest that users perform a structural parse: they break the page down into meaningful structures like titles, urls, snippets, etc.
Based on past research on social information seeking, we have certain intuitions about how users should behave around social annotations: they should find them broadly useful, and they should notice them.
Our results indicate that, in reality, users behave in a more nuanced way.
Our first study yielded two unexpected results.
First, in some contexts, social annotations shown on search result pages can be useless to searchers.
They disregard information from people who are strangers, or unfamiliar friends with uncertain expertise.
Searchers are looking for opinions and reviews from knowledgeable friends, or signs of interest from close friends on hobbies or other topics they have in common.
The more counterintuitive result from our first study was that subjects did not notice social annotations.
Users deconstruct the search results: they pay attention to titles and URLs and then turn toward snippets and annotations for further evidence of a good result to click on.
Moreover, the reading of snippets and annotations appears to follow a traditional top-to-bottom reading order, and friend pictures that are too small simply blend into snippets and become part of them.
These focused attention behaviors seem to derive from the task-oriented mindset of users during search, and might be explained by the effect of inattentional blindness .
All of this makes existing social annotations slip by, unnoticed.
Our findings have implications for both the content and presentation of social annotations.
For content, three things are clear: not all friends are equal, not all topics benefit from the inclusion of social annotation, and users prefer different types of information from different people.
For presentation, it seems that learned result-reading habits may cause blindness to social annotations.
The obvious implication is that we need to adapt the content and presentation of social annotations to the specialized environment of web search.
The first adaptation could target broad search-topic categories: social annotations are useful on easily-identified topics such as restaurants, shopping, local services, and travel.
For these categories, social annotations could be made more visually prominent, and expanded with details such as comments or ratings.
Our observation that the friend's topical expertise affects the user's perception of the social annotation and search result relevance allows for additional, fine-grained adjustments.
With lists of topics on which friends are knowledgeable, we could give their annotations more prominence on those topics.
The areas of expertise or interest of a specific user could either be provided explicitly by the user  or inferred implicitly from content created or frequently consumed by the user .
The inference can be done using standard text classification or clustering techniques .
To achieve the desired effect, we can manipulate the presentation of social annotations in a variety of ways to give them more prominence.
For instance, we can increase the picture size, change the placement of the annotation within the search result, or alter its wording and information content.
In the future, we would like to conduct a third experiment to test this newly-gained understanding of social annotations.
Using the insights from the second experiment, we could design a study in which social annotations are prominent.
Then, we could test the qualitative claims of our first experiment by showing annotations from different types of contacts, on different verticals and topics.
