Walkthrough techniques have been shown to be art effective supplement to empirical testing methods for evaluating the usability of software systems .
Unfortunately, structured walkthrough procedures tend to be time-consuming used on information minimizing substantial and unpopular tasks.
Each iteration of this process is termed a chromatographic run.
In addition to the software that performs packages segment.
By using video recording equipment and an informal, jogthrough problems techniques.
Once this has been achieved, the chromatographer specifies the data handling parameters which control processing of the raw data and the basic results calculations.
Finally, parameters which control any application-specific calculations and report generation are specified.
As shown in the figure, each of these steps consists of specifying the instructions, invoking graphical the from actions, knowledge and inspecting the utilize textual domain and to results.
Permission to copy without fee all or part of this material IS granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying ISby permission of the Association for Computing Machinery, To copy otherwise, or to republish, requires a fee and/or specific permission.
This is facilitated when the the various that provide individual applications are easily workstation services chromatography accessible, interact seamlessly, and are consistent in their We have established a user interface look and feel.
The team is composed of user interface designers, software engineers and in-house "users" who We in are knowledgeable to provide graphical in the tasks of our target markets.
The team develops project-wide guidelines, templates, and examples, and it evaluates interface proposals for new applications and It also serves as a modifications to existing ones.
Due to time and resource constraints placed on the project, empirical usability testing on interface prototypes was not considered to be a feasible evaluation technique for the situation.
Co-operative Evaluation had been practiced inhouse to gain some insight on what was required to use this procedure successfully .
Based on our experiences, we felt that in order to do meaningful evaluation using this procedure, a prototype modeling the system behavior in a rather convincing manner was needed.
We also believed that we needed ready access to subjects who were familiar with the application and who adequately represented our customer profile.
We had attempted to use in-house customer representatives, but found that the bias they had developed while working intimately with our products influenced the way in which they performed user tasks.
Therefore, only a subset of the task, the specification and verification of the applicationspecific parameters, was selected for the first evaluation.
We hoped  to cover a substantial portion of this task subset in a single ninety minute session.
This was done in a session preceding the actual evaluation session.
During the walkthrough, the Presenter traversed the preferred path by executing all the actions required to alternative navigate through it.
The Presenter identified paths, and the consequences of pursuing everyone them.
Any The uncertainties Moderator regarding in the proposed ran the meeting, the process.
Methods development is a far more interactive task, requiring constant attention of the user.
The burden on the user interface this task since the "modify-perform-inspect" be optimized to the users' work is greater for loop needs to flow.
We used a modified version of the evaluation sheet presented in the Lewis/Poison paper .
In order to help ease the burden of recording all the evaluators' comments, the Recorder used an electronic template of the evaluation sheet that reduced the amount of repetitive typing required.
The Evaluators answered each of the questions on the evaluation sheet for each step taken towards the goal state.
Actions and choices were ranked according to the percentage of potential As likely users that were expected to have problems.
After the first immediate goal was presented, the group proceeded to identify the first atomic action that was likely to be taken by most users.
The action was executed The evaluation and the system response recorded.
Suggestions for alternate design approaches were tabled by the Moderator so that the evaluation could continue within the time limits that had been defined.
At the end of the ninety minute period, only ten atomic actions were covered.
This is roughly equivalent to opening a single file and displaying a windowful of application parameters to edit .
We were able to make use of thk test logging software during the evaluation in a similar manner -- to log significant events in real time.
The role of the Recorder, therefore, becomes one of camera operator and The test logging software allows the event logger.
In some cases it was agreed that certain concerns about the interface could not be resolved in the evaluation session and would require further empirical study.
In the ninety minute jogthrough, roughly thirty user actions were covered.
Atomic actions that would have been covered individually in the walkthrough procedure were combined when the evaluators felt that individual examination of the actions would not provide any useful information.
Certain areas of the proposed design that had not been fully developed were discussed and various alternative design suggestions were made.
The focus of this procedure, as with the Cognitive Walkthrough, is to evaluate how the user might choose from the available options the one considered most likely to result in a state closer to the goal state.
When design suggestions were made, they were immediately evaluated with this in mind.
This may not be the case with the jogthrough if the evaluation team chooses to skip over a step that is considered insignificant.
A greater burden is placed on the Moderator and Presenter during the jogthrough since the criteria used to determine when it is appropriate to discuss design alternatives are left up to them.
Recorder was forced to omit certain comments simply to keep up with the flow of the discussion.
This resulted in frustration during the session as the participants constantly had to be told to slow down, and frustration later on when the interface designers had to decipher the cryptic abbreviations that the Recorder resorted to under pressure.
In fact, it is probably not feasible to evaluate all core tasks in our system in this manner.
Therefore, task selection becomes a very important issue, and sometimes decisions have to be made to neglect areas that may present usability problems in the product.
Under strict schedule constraints, a proposed user interface was evahtated using several techniques.
A Cognitive Walkthrough approach was used as an alternative to empirical usability testing due to both schedule and resource constraints.
The Cognitive Walkthrough proved laborious and did not make the best use of the evrduators' limited time.
Jogthrough of experts on the technique took advantage of the our evaluation suggestions of the us to resolve team members.
We feel that we retained enough stmcture to provide a thorough review of the interface, but enough flexibility to change the focus of our attention during the session when we felt it appropriate.
By using a video camera in conjunction fully with test logging software, we were able to the pace of 7. document the session without hampering 6.
