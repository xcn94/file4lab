We explore how to track people and furniture based on a high-resolution pressure-sensitive floor.
Gravity pushes people and objects against the floor, causing them to leave imprints of pressure distributions across the surface.
While the sensor is limited to sensing direct contact with the surface, we can sometimes conclude what takes place above the surface, such as users' poses or collisions with virtual objects.
We demonstrate how to extend the range of this approach by sensing through passive furniture that propagates pressure to the floor.
To explore our approach, we have created an 8 m2 back-projected floor prototype, termed GravitySpace, a set of passive touch-sensitive furniture, as well as algorithms for identifying users, furniture, and poses.
Pressure-based sensing on the floor offers four potential benefits over camerabased solutions:  it provides consistent coverage of rooms wall-to-wall,  is less susceptible to occlusion between users,  allows for the use of simpler recognition algorithms, and  intrudes less on users' privacy.
In order to provide this support, smart rooms track users and try to automatically recognize their activities.
In systems like EasyLiving, this was done by pointing tracking equipment, such as cameras, at the interior of the room .
The direct observation of scenes using computer vision is of limited reliability, because of illumination and perspective effects, as well as occlusion between people.
The latter also affects more recent approaches based on depth cameras .
We propose an alternative approach to tracking people and objects in smart rooms.
Building on recent work on touchsensitive floors  and pose reconstruction, such as , we explore how much a room can infer about its inhabitants solely based on the pressure imprints people and objects leave on the floor.
Such smart rooms support users by offering not only a series of convenient functions, like home automation, but also by acting pro-actively on the user's behalf.
Similar systems have been proposed to monitor the wellbeing of  inhabitants .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Figure 1 shows our floor installation GravitySpace with three users and three pieces of furniture.
To illustrate what the system senses and reconstructs about the physical world, the prototype displays its understanding of the physical world using a mirror-metaphor, so that every object stands on its own virtual reflection.
Based on this mirror world, we see that GravitySpace recognizes the position and orientation of multiple users, the identity of users as demonstrated by showing their personalized avatars, selected poses, such as stand-
GravitySpace updates in real-time and runs a physics engine to model the room above the surface.
To convey the 3D nature of the sensing, this photo was shot with a tracked camera--this camera tracking is not part of GravitySpace.
GravitySpace consists of a single sensor, namely the floor itself which is pressure-sensitive, while the seating furniture passively propagates pressure to the floor.
All the tracking and identification shown in Figure 1 is solely based on pressure imprints objects leave on this floor as shown in Figure 3.
In addition, GravitySpace recognizes users based on their shoeprints, similar to Multitoe , but optimized for the 20 times larger floor size and a larger number of simultaneous users .
Figure 4 shows our current GravitySpace prototype hardware.
It senses pressure based on FTIR , using a camera located below the floor.
It provides an 8 m2 interaction surface in a single seamless piece and delivers 12 megapixels overall pressure sensing resolution at a pixel size of 1x1 mm.
Our prototype also offers 12 megapixel back projection.
While not necessary for tracking, it allows us to visualize the workings of the system, as we did in Figure 1.
Our approach is based on the general principle of gravity, which pushes people and objects against the floor, causing the floor to sense pressure imprints as illustrated in Figure 3.
While the pressure sensor is limited to sensing contact with the ground, GravitySpace not only tracks what happens in the floor plane , but is able to draw a certain amount of conclusions about what happens in the space above the floor, such as a user's pose, or the collision between a user and a virtual ball.
In addition, GravitySpace senses what takes place on top of special furniture that propagates pressure to the floor.
We expect sensing hardware of comparable size and resolution to soon be inexpensive and mass available, for example in the form of a large, thin, high-resolution pressure sensing foil .
We envision this material to be integrated into carpet and as such installed in new homes wall-to-wall.
Since the technology is not quite ready to deliver the tens of megapixel resolution we require for an entire room, our FTIR-based prototype allows us to explore our vision of tracking based on pressure imprints today.
The main contribution of this paper is a new approach to tracking people and objects in a smart room, namely based on a high-resolution pressure-sensitive floor.
While the sensor is limited to sensing contact with the surface, we demonstrate how to conclude a range of objects and events that take place above the surface, such as a user's pose and collisions with virtual objects.
We demonstrate how to extend the range of this approach by sensing through passive furniture that propagates pressure to the floor.
We have also implemented algorithms for tracking and identifying users, furniture, and poses.
Our main goal with this paper is to demonstrate the technology.
We validate it using a technical evaluation of user identification and pose recognition accuracy.
The philosophy behind smart rooms, their applications and usability, however, are outside the scope of this paper.
Compared to traditional camera-based solutions, the proposed approach offers four benefits:  It provides consistent coverage of rooms, wall-to-wall.
Camera-based systems have a pyramid-shaped viewing space.
Motion capture installations resolve this by leaving space along the edges, but that is impractical in regular rooms, leading to uneven or spotty coverage.
Floor-based tracking in contrast, can be flat, integrated into the room itself, and provides consistent coverage across the room.
The perspective from below is particularly hard to block--simply because people tend to stand next to each other, resulting in discernible areas of contact.
From a more general perspective, the benefit of pressure sensing is that mass is hard to "hide".
Mass has to manifest itself somewhere, either through direct contact or indirectly through another object it is resting on.
Camera-based systems, in contrast, may suffer from users occluding each other if the cameras mounted in one spot .
Systems distributing multiple cameras around a room  still suffer from dead spots .
Our approach reduces the recognition problem from comparing 3D objects to comparing flat objects, because all objects are flat when pressed against a rigid surface.
This limits objects to three degrees of freedom  and allows us to match objects using simple, robust, and well-understood algorithms from digital image processing .
While floor-based sensing captures a lot of information relevant to assisted living applications , it never captures photos or video of its inhabitants, mitigating privacy concerns .
On the other hand, our floor-based approach is obviously limited in that it can recognize objects only when they are in direct contact with the floor.
While we reduce the impact of these limitations using 3D models based on inverse kinematics, events taking place in mid-air can obviously not be sensed, such as the angle of an arm being raised or a user's gaze direction.
The approach is also inherently subject to lag in that the floor learns about certain events only with a delay.
We cannot know the exact position of a user sitting down until the user makes contact with the seat.
As we place the avatar in between, it is subject to inaccuracy.
The work presented in this paper builds on smart rooms, interactive floors, and user identification in ubiquitous computing.
The concept of integrating computing into the environment goes back as far as Weiser .
The concept has been researched in the form of smart components in a room, e.g., in multi-display environments such as the Stanford iRoom  or roomware .
Alternatively, researchers have instrumented the room itself, e.g., using cameras and microphones , making user tracking a key component of the system.
The Georgia Tech Aware Home  tracks users based on multi-user head tracking and combined audio and video sensing.
Most recently, Wilson and Benko demonstrated how to instrument rooms using multiple depth cameras .
A series of floor prototypes have used a range of pressure sensing technologies offering a variety of resolutions.
The projection-less magic carpet senses pressure using piezoelectric wires and a pair of Doppler radars .
Z-tiles improved on this by introducing a modular system of interlocking tiles .
Pressure sensing has been implemented using forcesensing resistors .
FootSee  matches foot data from a 1.6 m pressure pad to pre-recorded pose animations of a single user with in a fixed orientation.
In the desktop world, the UnMousePad improves on resistive pressure sensing by reducing the number of required wire connections .
Based on this, Srinivasan et al.
Since none of the existing technologies scale to the megapixel range of resolution, GravitySpace is built on an extension of the Multitoe floor.
Multitoe uses high-resolution FTIR sensing and allows users to interact using direct manipulation .
Other instrumented floors are tracked using ceilingmounted cameras  or front diffuse illumination .
Purposes of interactive floors include immersion , gaming , and multi-user collaborative applications .
A series of research and products use pressure-sensitive devices and furniture to monitor health, for instance, to prevent Decubiti , orthopedic use inside of shoes , and to sense pose while sitting .
The pressure-transmitting furniture presented in this paper builds on the concept of sensing through an object.
The concept has been explored in the context of tangible objects.
Mechanisms include the propagation of light through holes  and optical fiber  and the propagation of magnetic forces, sensed using pressure sensors .
The presented work essentially reduces a 3D problem to a 2D problem, allowing us to apply well-explored traditional algorithms from digital image processing .
The majority of large-scale touch technologies, such as diffused illumination , front-DI , and FTIR  are ignorant of who touches.
DiamondTouch improved on this by mapping users to seat positions .
Bootstrapper identifies tabletop users by observing the top of their shoes .
Recognizing fingerprints has been envisioned to identify users of touch systems ; Holz and Baudisch implemented this by turning a fingerprint scanner into a touch device .
User identification has been used for a variety of applications including the management of access privileges  and to help children with Asperger syndrome learn social protocol .
Olwal and Wilson used RFID tags to identify objects on the table .
The screen-less Smart Floor identifies users by observing the forces and timing of the individual phases of walking .
While floors so far did not have enough resolution to distinguish soles, footprints have been analyzed as evidence in crime scene investigation .
Sole imprints and sole wear has been used to match people either by hand and using semi-automatic techniques based on local feature extractors, such as MSER .
Multitoe distinguishes users based on their shoeprints using template-matching .
In the following walkthrough, we revisit the elements from Figure 1 including user identification and tracking, pose detection, and pressure-transmitting furniture.
Given that this paper is about a new approach to tracking, this walkthrough is intended to illustrate GravitySpace's tracking capabilities; it is not trying to suggest a specific real-world application scenario.
As before, we render GravitySpace's understanding of users and furniture as a virtual 3D world under the floor.
A detailed description of the shown concepts and their implementation can be found in Section Algorithms.
Figure 5 shows Daniel on the left as he is relaxing on the sofa and GravitySpace's interpretation of the scene in the mirror world.
The same scene is shown from a pressure-sensing perspective in Figure 6.
GravitySpace parses this pressure image to identify the sofa based on embedded pressure markers and to locate someone sitting on top of the sofa based on the pressure imprint of Daniel's buttocks.
The sofa is filled with elements that transmit pressure to the floor.
GravitySpace combines buttocks and the two feet next to the sofa into a pose.
It also identifies Daniel based on his shoeprints.
Using this information, Daniel's avatar is selected from a user library and positioned onto the imprints of feet and buttocks.
When Daniel's friend, Rene, comes in he is likewise identified.
GravitySpace positions his personalized avatar by fitting a skeleton to Rene's pressure imprints using inverse kinematics, based on three control points: the two imprints of Rene's feet in their respective orientation, as well as Rene's center of gravity.
The latter it placed above the center of pressure between Rene's shoes.
As Rene walks across the room, GravitySpace continuously tracks his position.
By observing the pressure distribution of the foot on the ground, GravitySpace predicts where the foot currently located in midair is expected to come down.
This allows it to animate the avatar without having to wait for the foot to touch down.
Rene and Daniel decide to play a video game.
As shown in Figure 7, they interact with the game without a dedicated controller by simply shifting their weight to control virtual racecars.
Rene and Daniel accelerate and brake by leaning forward or backward; they steer by leaning left and right.
GravitySpace observes this through the sofa and the cube seat.
Also shown in Figure 7, Andreas, a common friend has sat down on the floor to watch the other two playing.
GravitySpace determines his pose based on the texture and spatial arrangement of contact points on the floor.
Figure 9 shows how we constructed a cube seat.
We use regular drinking straws as transmitters, making the furniture light and sturdy.
1,200 straws  fill each cube seat; 10,000 fill the sofa, which is based on the same principle.
The backrest and armrest of the sofa are pressure-sensitive as well--they are filled with longer "sangria" drinking straws.
We obtain the desired curved shape by cutting the straws to length a layer at a time using a laser cutter.
The three friends then decide to play a round of virtual soccer .
The game requires them to kick a virtual ball.
GravitySpace cannot directly observe what happens above the surface.
Instead it observes weight shifts within the other foot, and concludes where the kicking foot must be located.
Using inverse kinematics, it places the avatars and GravitySpace's physics engine computes how the avatar kicks the ball.
While it is quite possible to create furniture with active pressure sensing , we have created passive furniture that transmits high-resolution pressure, rather than sensing it.
This offloads sensing to a single centralized active sensing component, in our case the floor.
Passive furniture also reduces complexity and cost, while the absence of batteries and wires makes them easy to maintain .
Everyday furniture already transmits pressure.
Furniture imprints, however, are limited to representing overall weight and balance.
While locating the center of gravity has been demonstrated by many earlier systems , this limits our ability to detect activities taking place on top of the furniture .
In order to recognize identity and poses of the object on top in more detail, we have created the furniture pieces featured in Figure 1 and the walkthrough.
They transmit pressure in comparably high resolution.
We accomplish this by using an array of "transmitters".
Straws are held together by a frame made from 15 mm fiberboard.
We stabilize the straws in the box using a grid made from plywood connected to the frame, which essentially subdivides the box into 3x3 independent cells.
The grid minimizes skewing, thus prevents the box from tipping over.
We cover the bottom of the box with Tyvek, a material that crinkles but does not stretch, which prevents the bottom from sagging, yet transmits pressure.
In addition to the leather we added a thin layer of foam as cushioning to the top of the cube seats for added comfort.
Weight shifts on top of the box can cause the box to "ride up" on the straws, which can cause an edge of the box to lose traction with the ground.
To assure reliable detection, we create markers from weight rods that slide freely in plastic tubes and held in by the Tyvek.
We use an asymmetric arrangement of rods to give a unique ID to each piece.
Figure 10 summarizes the pipeline we implemented to process touches that occur on our floor, including recognizing, classifying and tracking events, users and objects based on the pressure patterns they leave.
We optimized our pipeline to process the entire 12 megapixel image in real-time .
GravitySpace recognizes objects with texture  by extracting the imprint features they leave in the raw image.
For objects with little discernible texture or changing texture , we add features using pressure-based markers.
Our GravitySpace implementation supports three key elements:  Pose reconstruction using pressure cluster classification,  joint estimation based on pressure distributions and inverse kinematic, and  user identification based on shoeprints.
Areas covered by furniture pieces are ignored for this classification in order to minimize noise.
In order to classify each pressure cluster, GravitySpace extracts 16 fast-to-compute image features from the respective area in the image, including image moments, structure descriptors using differences of Gaussians, as well as the extents, area, and aspect ratio of the bounding box around the cluster.
We trained a feedforward neural network, which assigns probabilities for each type of contact to each cluster.
All processing starts by thresholding the pressure image to remove noise.
Our algorithm then segments this image and extracts continuous areas of pressure using a connected component analysis.
In the next step, GravitySpace merges areas within close range, prioritizing areas that expand towards each other.
We call the result pressure clusters.
A pressure cluster may be for example a shoeprint or the buttocks of a sitting user.
GravitySpace then tracks these clusters over time.
Pressure imprints of larger objects, such as furniture, provide little distinguishable texture on their own.
In addition, the overall texture of seating furniture changes substantially when users sit down.
GravitySpace therefore uses dot-based pressure-markers to locate and identify furniture.
Figure 11 shows the imprint of a cube that we equipped with a marker.
This particular marker consists of five points.
Marker points are arranged in a unique spatial pattern that is rotationinvariant.
We designed and implemented marker patterns for a sofa, several sitting cubes, and shelves.
Whenever the users' feet are in contact with the ground--for example when standing or sitting, but not when lying-- GravitySpace will recognize users by matching their shoeprints against a database of shoes associated with user identities.
As users register with both of their shoes, our approach also distinguishes left and right feet.
Due to the large floor area, previous approaches to user identification on multi-touch floors are not sufficient on GravitySpace, such as template matching in Multitoe , which observed an area of only a 20th of the size.
To recognize markers, GravitySpace implements brute-force matching on the locations that have been classified as marker points, trying to fit each registered piece of furniture into the observed point set and minimizing the error distance.
To increase the stability of recognition, our implementation keeps objects whose marker patterns have been detected in a history list and increases the confidence level of recently recognized objects.
We also use hysteresis to decide when marker detection is stable based on the completeness of markers and their history confidence.
For each pressure cluster in the camera image, GravitySpace analyzes the probability of being one of the contact types shown in Figure 12.
GravitySpace distinguishes hands, knees, buttocks, and shoes, thereby further distinguishing between heel, tip, and edge of a shoe.
To match shoeprints with the same resolution as previous systems , GravitySpace uses an implementation of SIFT  that runs on the GPU.
Using SIFT as the feature detector and descriptor algorithm allows us to match shoes with rotation invariance.
A feature thereby matches if the angular distance between the two descriptor vectors is within close range.
As the number of detected features varies substantially between different sole patterns, we normalize the distance .
When a user walks on the floor, only a small part of their shoe appears in the image at first and then becomes larger as their shoe sole rolls over the floor from heel to toe.
Since the camera consecutively captures images of each such partial shoe imprint, GravitySpace merges all partial observations in successive frames into an aggregated imprint, which allows us to capture an almost complete shoe sole.
This concept is also commonly used to obtain a more encompassing fingerprint by rolling a finger sideways while taking fingerprints.
To predict the location of users' next steps when walking, GravitySpace leverages the orientation of the shoes on the floor.
GravitySpace determines shoe orientations directly after matching shoeprints by registering front and back of each database shoeprint with the observed shoe on the floor.
Our system transforms both shoeprints into spectrum images and applies log polar transforms to then compute the translation vector and rotation angle between the two shoeprints using phase correlation.
All shoes in the database thereby have annotated locations of heel and toes, which happens automatically upon registration by analyzing the direction of walking.
To classify body poses from the observed pressure imprints, GravitySpace performs pose matching based on the location and classified type of observed pressure clusters.
For example, GravitySpace observes the spatial configuration of pressure clusters shown in Figure 14, i.e., the imprints of buttocks, two feet, and two hands as a user is sitting on the floor.
To match a pose, GravitySpace uses a set of detectors, one for each pose that is registered with the system.
Each detector is a set of rules based on contact types and their spatial arrangement.
GravitySpace currently distinguishes five poses: standing, kneeling, sitting on the floor, sitting on cube seat or sofa, and lying on a sofa.
GravitySpace feeds all pressure clusters to all detectors.
Each detector creates a set of hypotheses.
Each hypothesis, in turn, contains a set of imprints that match the pose described by the detector.
For example, hypotheses returned by the sitting detector contain buttocks and two feet.
Optionally, there may also be two hands if users support themselves while leaning backwards as shown in Figure 14.
Each detector returns all possible combinations  of imprints that match the pose implemented by this detector.
Each hypothesis thus explains a subset of all imprints.
We compute the probability of a hypothesis by multiplying the classification probabilities of all contained imprints with a pose-specific prior.
From these individual hypotheses , we compute a set of complete hypotheses; each complete hypothesis explains all detected imprints by combining individual hypotheses.
We calculate the probability of a complete hypothesis as joint probability of individual hypotheses, assuming that individual poses are independent from each other.
We track complete hypotheses over multiple frames using a Hidden Markov Model with complete hypotheses as values of the latent state variable.
GravitySpace also tracks body parts that are not in contact with the floor, such as the locations of feet above the ground while walking or kicking.
Our system also tracks general body tilt, for example when a user leans left or right while sitting.
This allows for predicting steps before making contact with the floor to reduce tracking latency or to interact with virtual objects on the floor.
Obviously, our approach cannot sense events taking place in mid-air, such as raising an arm or changing the gaze direction.
We estimate the location of in-air joints by analyzing the changing centers of gravity within each pressure cluster.
We then try to best fit a skeleton to the computed locations of all joints using a CCD implementation of inverse kinematics.
GravitySpace finally visualizes the reconstructed body poses with 3D avatars.
GravitySpace enables users to interact with virtual objects, such as by kicking a virtual ball as shown in Figure 8.
To simulate the physical behavior of virtual objects, GravitySpace first computes the position of feet above the ground.
Since a foot is not in contact with the ground when kicking, GravitySpace reconstructs its location by analyzing the changing pressure distribution of the other foot, which is on the ground as shown in Figure 15.
Our algorithm first calculates the vector from the center of pressure of the cluster aggregated over time to the center of pressure of the current cluster.
This vector corresponds to the direction that a person is leaning towards, and is used to directly set the position of the foot in mid-air.
To track a user's body tilt, for example when leaning left or right when playing the video game described in the walkthrough, GravitySpace observes multiple pressure clusters as shown in Figure 16.
The system first computes the joint center of pressure over all pressure clusters of a user by summing up zero and first order moments of the individual pressure images.
We then exploit that the center of pressure directly corresponds to a body's center of gravity projected on the floor.
Once the center of gravity is determined, GravitySpace sets the corresponding endpoints of the skeleton's kinematic chains; all other joints then follow automatically based on the inverse kinematic.
Test Data: Following the same procedure, we collected data from another four participants, for testing.
Evaluation Procedure: We manually annotated all training samples to provide ground truth.
We then fed the test data into the trained neural network, taking the contact type with the highest probability as outcome.
Note that our algorithm does not discard the probability distributions provided by the neural network, but feeds them into the following pose recognition as additional input.
Results: Our approach achieved a classification accuracy of 86.94% for the seven contact types shown in the confusion matrix of Figure 17.
If the entire shoe, ball, rim, and heel are grouped and treated as a single contact of type "shoe", as done by the pose recognition, classification accuracy reaches 92.62%.
We conducted a technical evaluation of three system components, namely pressure cluster classification, user identification, and pose recognition.
In summary, the algorithms of our prototype system allow for  distinguishing different body parts on the floor with an accuracy of 92.62% based on image analysis of pressure clusters,  recognizing four body poses with an accuracy of 86.12% based on type and spatial relationships between pressure clusters, and  identifying 20 users against a 120-user database with an accuracy of 99.82% based on shoeprint matching.
To evaluate pressure cluster classification, we trained a feedforward neural network with data from 12 participants, and tested its classification performance with data from another four participants.
Training Data: We asked 12 participants to walk, stand, kneel, and sit on the floor, in order to collect data of the seven different contact types required for pose recognition, namely hand, shoe , knee, and buttocks.
In total, we collected 18,600 training samples.
For each participant and pose, we recorded a separate pressure video sequence.
Evaluation Procedure: To provide ground truth, we manually annotated all frames with the currently shown pose.
We then ran our algorithm on all frames of the recorded videos, and compared the detected poses to ground truth annotations.
We determined the user identification accuracy of our implementation with 20 users.
Registration: To populate the user database, each participant walked in circles for about 35 steps on the floor.
GravitySpace now selected one left and one right shoeprint for each participant, choosing the shoeprint with the minimum distance in the feature space compared to all other shoeprints of the same participant and foot.
Test Data: After a short break, participants walked a test sequence of about 60 steps.
Shoeprints were in contact with the floor for an average of 0.92 s .
Participants then did another round.
This time, however, they were instructed to walk as fast as possible, resulting in a sequence of about 70 steps with a lower average duration of 0.38 s .
We evaluated the test set against two databases, one containing 20 study participants, and one enlarged with data from 100 additional people .
Figure 19 shows the identification accuracy using these two databases for both walking slow and fast.
As expected, larger shoeprints aggregated from more frames resulted in better recognition.
For the 20-user database, the classification accuracy reached 99.94% for shoeprints with an area between 180 and 190 cm .
When walking fast, recognition rates slightly dropped to a maximum classification accuracy of 99.19%.
This is expected as shoeprints were more blurry.
We then reran the classification against the 120-user database.
Our approach correctly identified 99.82% shoeprints when walking slowly, and 97.56% when walking fast.
In comparison, Orr et al.
Speed: Feature extraction took 47.7 ms  per shoeprint, which is independent of the number of registered users.
Each additionally registered user increases the runtime by 2 ms. To maintain a frame rate of 25 fps, GravitySpace runs user identification asynchronously.
Before identification is completed, users are tracked based on heuristics .
Once identified, user tracking relies on this information.
To reduce delays due to identification, GravitySpace caches recently seen users: new contacts are first compared to this short list of before falling back to the entire participant database.
Evaluation Procedure: We evaluated the identification performance by running our algorithms on the recorded test data.
Obviously, the slower participants walked, the longer their feet were in contact with the floor, and the more frames were available.
The foot part in contact with the floor varied while walking, rolling from heel to toe.
As described above, our algorithm reconstructed shoeprints by merging successive pressure imprints.
We ran our identification algorithms on all aggregated imprints with an area greater than 30 cm2, which is the minimum area for discernible shoe contacts as determined during the previous pressure cluster evaluation.
We have demonstrated how to track people and furniture based on a high-resolution 8 m2 pressure-sensitive floor.
While our sensor is limited to sensing contact with the surface, we have demonstrated how to conclude a range of objects and events that take place above the surface, such as user pose and collisions with virtual objects.
We demonstrated how to extend the range of this approach by sensing through passive furniture that propagates pressure to the floor.
As future work, we plan to combine GravitySpace with other touch-sensitive surfaces into an all touch-sensitive room, and explore the space of explicit interaction across large floors.
