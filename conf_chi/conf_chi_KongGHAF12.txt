Tutorials and sample workflows for complicated, featurerich software packages are widely available online.
As a result users must differentiate between workflows to choose the most suitable one for their task.
We present Delta, an interactive workflow visualization and comparison tool that helps users identify the tradeoffs between workflows.
We conducted an initial study to identify the set of attributes users attend to when comparing workflows, finding that they consider result quality, their knowledge of commands, and the efficiency of the workflow.
We then designed Delta to surface these attributes at three granularities: a highlevel, clustered view; an intermediate-level list view that contains workflow summaries; and a low-level detail view that allows users to compare two individual workflows.
Finally, we conducted an evaluation of Delta on a small corpus of 30 workflows and found that the intermediate list view provided the best information density.
We conclude with thoughts on how such a workflow comparison system could be scaled up to larger corpora in the future.
Furthermore, researchers have begun to build systems that automatically record user action streams, which can be used by others to learn a workflow .
As the corpora of these online learning resources continue to grow, users are likely to find multiple workflows for the same task.
Workflows can vary in the tools they use, the quality of the results, or may have differing prerequisites .
The challenge for a user is no longer searching for and finding any workflow - the challenge is identifying the most suitable workflow from potentially hundreds of possible candidates.
In addition, individual websites tend to use a wide range of layouts and formats for presenting workflows, potentially making it difficult for users to identify differences in the workflows themselves.
To date, there is little research investigating how users explore and identify possible workflows to accomplish a task when multiple candidates exist.
Furthermore, no tools have been explicitly designed to support the process of exploring and comparing workflows.
In this paper we focus on image-editing workflows for Photoshop, because it is a well-known design application with a large number of tutorials available online.
We first perform a study to understand users' workflow searching behaviors, and identify the criteria they use when choosing between multiple image-editing workflows.
Our main finding is that users primarily consider the quality of the result, their knowledge of commands , and the efficiency of the workflow.
The results of this study guide us in the design of Delta, a new interactive tool to aid the process of workflow comparison and exploration.
Given a corpus of up to 50 workflows that describe the same task, our tool allows users to understand the scope of available workflows; explore each of the workflows in detail; and understand the similarities and differences in the structure  of individual workflows.
Our tool allows users to visualize and compare workflows at three levels of granularity: a high-level clustered view; an intermediate-level list view that contains workflow summaries; and a low-level detail view that allows users to compare two individual workflows.
To ease comparisons, the tool displays all workflows with a uniform presentation format within each of the three granularity levels.
To understand the potential benefits of the tool, and to evaluate the different comparison granularities which it provides, we conducted a user study.
Design software, such as image editors, animation tools, and 3D modeling applications, provide users with powerful ways to create and modify virtual content.
Despite advances in HCI research, the interfaces for such applications are complex and can be difficult to learn .
While software documentation can provide assistance with locating and using individual components, users often refer to online tutorials to learn the required workflows for completing higher-level tasks.
For example, a Photoshop user wishing to correct red-eye in a photograph could search online to find step-by step instructions.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Observations and comments also indicated that the intermediate list view provided the ideal information density for most of the workflow comparison tasks.
We conclude by describing limitations, design refinements, and implications for workflow browsing.
In this section, we formally define terms that we use throughout the paper.
A command is a user-invoked operation in an application.
Examples of Photoshop commands include the activation of palette tools, such as the brush tool, or menu options that open dialog boxes, such as the image level tool.
The complexity of a command can vary from inverting the image to manually specifying a color curve.
A workflow is a sequence of commands and can take multiple representations.
One common representation is a tutorial, which describes a workflow in a sequence of steps, frequently with image or video illustrations and explanations for why certain commands are used.
Workflows can vary greatly in their length.
Our tool has been designed to work with relatively short workflows - anywhere from 5 to 30 steps long - as this is the range of steps we observed while sampling a collection of online tutorials for common image-editing tasks.
Delta is fundamentally an information search tool.
Some researchers have developed theoretical models for how people find information.
Information foraging  has been applied to model and predict user behavior in web search .
A key concept of information foraging theory is information scent, or the contextual cues that allow users to assess the benefits and costs of distal  content .
In this work, we identify the contextual cues that users find most effective when they search for workflows.
Workflows are a specific type of structured data.
Researchers have developed systems for searching or accessing large collections of unstructured data, such as documents , images , or web pages for design .
Others have looked at systems for exploring more structured data.
One example is VisComplete, a dataflow suggestion system built within a visual programming system for visualization .
As users create a dataflow out of a sequence of function modules, the system makes suggestions of other dataflows that are similar to the current dataflow.
Our tool is designed to accommodate workflows, and also uses similarity metrics to suggest similar workflows in the context of exploring the entire corpus.
Closest to our work is AdaptableGIMP , which integrates image-editing workflow search into GIMP.
Workflows are stored on a wiki and users can access these wiki pages from within GIMP.
They can also directly invoke GIMP commands from the wiki pages.
In contrast, our tool is designed to help people compare workflows, in addition to aiding search tasks.
To better understand which workflow attributes users consider most important when comparing and assessing workflows, we ran a small study.
We used the results of this study in the design of Delta to emphasize the most important attributes.
Users looked at a set of workflows that described the same task and chose the one that they would most likely use.
We used three different workflow presentation styles.
While our goal was not to compare the presentation styles themselves, we reasoned that each presentation style may prompt users to consider different attributes when comparing workflows.
Figure 1 shows the three presentation styles we used: text, text+image, and graph.
The text representation is a paragraph of text with inline tool icons.
It deemphasizes the length of the workflow and the quality of the result while surfacing palette tools using icons.
By removing clearly delineated steps and images, it also makes it more difficult for users to skim the workflow, forcing them to read closely.
The text+image representation is a step-by-step description annotated with images illustrating each step.
It also contains before and after images, which allows users to assess the result quality, and it emphasizes the length of the workflow by numbering steps.
Its layout closely followed the tutorials generated in Grabler et al.
Finally, the graph representation is a visualization that displays commands as nodes in a graph.
It emphasizes the commands and parameters used in the workflow.
Each node displays canvas state, similar to Chen et al.
Finally, many researchers have created systems for recording workflows.
These systems are complementary to our tool, as their output can be fed into our system.
Much of the work in this area focused on recording interaction histories in domains such as painting applications , visual analysis , and GUI applications .
More recent work has focused on enabling users to effectively explore longer workflows by chunking operations.
Chronicle  and MeshFlow  allow users to replay and explore workflows for an image manipulation application and a mesh creation system, respectively.
Workflow representations in the first study.
The icon here is added for illustration.
Command names are shown as regular rectangles, parameters as rounded rectangles.
The study consisted of three trials, one for each representation.
In each trial, we showed participants three different workflows for the same Photoshop task.
We gave participants a target image and asked them to think aloud while selecting one workflow they would choose to apply to the target image.
In each trial, participants saw a different task: red eye correction, applying a sketch effect, and image sharpening.
We chose tasks that were short but complex enough to permit multiple different approaches.
We obtained tutorials from the web and transcribed workflows manually.
Each participant saw every representation, and no two participants saw the same representation/task pair.
We recorded audio during each of the trials.
After the participants had completed all three tasks, they completed a short questionnaire where they scored a list of workflow attributes  on how important the attributes were when comparing or choosing between workflows.
The list of attributes, shown in Figure 2, was based on our experience.
Although our focus was on the workflow attributes users chose, we also had users rank the effectiveness of the representations based on how clearly they displayed the attributes.
However, we identified overlap in the workflow attributes participants used when selecting a workflow.
Five of six participants found the text+image representation most useful at surfacing almost all of the attributes; one participant preferred the text and graph representations, saying that the text+image representation "seemed like too much to have to look through."
Figure 2 shows user-rated importance of workflow attributes when comparing workflows.
None of the differences in the means are significant after a Bonferroni corrected paired t-test However, users' comments during the think-aloud portion of the study corroborated the general trend of these rankings.
During the think-aloud trials, all of the participants mentioned quality of the result, knowledge of the commands, and workflow efficiency .
The second attribute category subsumes a number of workflow attributes: when speaking about commands, users referenced their familiarity with the commands and their knowledge of the commands' effects, which is an indicator of a workflow's flexibility.
Although users rated "the workflow can be used in other situations" as an important attribute, no user mentioned this in the think-aloud.
Users may have interpreted this attribute to mean that the workflow is flexible, or that it can apply widely to a variety of images .
Perhaps unsurprisingly, users considered holistic attributes of workflows  more important than mechanistic attributes .
No users mentioned either of these mechanistic attributes during the think-aloud.
Although we only had six participants, we encountered a diverse cross-section of Photoshop users, whose aims varied when choosing a workflow.
For example, P2 was very interested in learning new Photoshop features and would occasionally browse for tutorials to improve his knowledge.
The attributes related to the quality of the result and knowledge of the tools are not independent: users were better able to anticipate the result of a workflow if they were familiar with the tools and their effects, and how they could be parameterized to change the result.
This was particularly apparent in the text representation, since it did not contain any images.
For example, in the image sharpening task shown in a text representation, P2 chose workflow 1, explaining that: "... with workflow 1 ... when you're playing around with your `find edges' and `levels' and everything, it's only going to pick up what you're looking for, which would probably just be the squirrel in the image."
In this case, P2 used his prior knowledge of the effects of the commands and how they would affect the target image.
Likewise, P5 noted that the before/after images for the workflows in the sharpening task were too small to see differences, and so chose a workflow using his "experience", saying: "... allows you to have more control over the important edges.
Gives you better contrast control."
Finally, all users mentioned workflow length in the thinkaloud, which is related to the efficiency of the workflow.
However, only one participant, P3, placed a larger emphasis on the length of the workflow rather than the result quality.
For example, in the sketch effect task displayed as text, P3 selected workflow 1 as it was "pretty straightforward, pretty efficient in terms of time," even though she noted that workflow 2 might produce a better result but was too "labor-intensive."
Other users thought efficiency was important, but took efficiency to mean that the workflow produces a good result and does not contain any extraneous commands, even if the commands in the workflow require many clicks to execute .
We were also interested in users' current practice to find workflows.
We asked participants how often they used tutorials and where they typically found them.
All of our participants used search engines to find tutorials.
One also used books, and another also used magazines.
However, they both reported that their predominant method to seek out tutorials was web search.
One of our participants preferred video tutorials, while another disliked them; the others did not have a preference for the modality of the tutorial.
Finally, users reported that they selected tutorials based on sites originating from a trusted domain , and search engine ranking.
These results corroborate prior work on information seeking behavior via search engines .
The results of this study provide some guidance on the design of a tool to support workflow comparison, and in particular how workflows should be represented.
A consistent and compact representation of a workflow that displays the most important information  will make it easy for users to compare collections of workflows, or two specific workflows.
The main implications from the initial study are as follows: * Images of the results and intermediate steps are important.
Users wanted to not only see the before/after images for each workflow, but also images illustrating inbetween steps to help them visualize operations.
Users used their knowledge of commands and their effects to assess how effective or flexible a workflow was .
Some users wanted to know how quick or efficient a workflow was.
A workflow comparison tool should display the number of commands or steps as a proxy for this information.
Potential metrics include the number of shared commands; a workflow familiarity rating based on a users' personal usage profile; or the edit distance  between two sequences of commands.
For our system, we used an edit-distance metric, described later.
Ideally, a workflow comparison tool would be able to algorithmically suggest similar or different workflows to the user.
Some systems designed for the exploration of large corpora of unstructured data, such as documents or images, do this by using clustering .
One challenge for creating a clustering interface for workflows is quantifying how similar two workflows are; once this metric is defined, we can use it to create clusters of similar workflows.
However, workflows could be grouped using different notions of similarity: for example, workflows could be grouped by result quality , or by how many commands have been used by the user.
Similarity metrics can also be used to rank workflows.
In this section, we explore the space of similarity metrics and how they relate to the attributes of workflows users deemed important in the first study.
In our first study, we identified three workflow attribute types users consider when comparing workflows: quality of the result, knowledge of the tools, and workflow efficiency.
For each of these attribute types, we present potential similarity metrics that could be used to cluster, or rank, a workflow collection.
A similarity metric could also be a combination of metrics from these types.
Although it was a lesser concern for participants, the efficiency of a workflow was identified as an important differentiating attribute.
Explicitly determining the efficiency of a workflow would be challenging.
However, grouping or ranking workflows by their length, either in the time taken or the number of commands used, may serve as a suitable approximation.
For our system, we use the number of commands to rank workflows.
In this section, we describe the design of our new workflow exploration tool, Delta.
Based on study participants' reports and our own experience, we designed Delta's interface to explore corpora of that are representative of collections of image manipulation tutorials online: Delta targets collections of several dozen workflows, each containing up to 30 commands.
Three complementary views provide information about the workflow corpus at different levels of detail : a cluster view presents a high-level overview of the entire corpus; a list view provides workflow summaries; a detail view lists individual commands and enables pairwise comparisons.
We describe each in turn.
Workflows may be grouped by the results they produce, which would help users choose the workflow with the desired quality.
Similarity metrics of this type are more difficult to operationalize than metrics of other types, as result quality is a subjective judgment.
However, result similarity metrics may be amenable to crowdsourcing techniques .
A user might also wish to find a workflow that matches their task.
For example, a user may wish to remove red eye from a photo of a child, and so would want to find a red eye removal workflow that also uses a photo of a child.
A result similarity metric based on image attributes, such as the objects in the photo 
The cluster view provides a high-level summary of the corpus by grouping workflows by similarity.
Clusters enable users to rapidly distinguish between similar and different workflows.
We selected a command similarity metric for clustering - it can be computed without requiring additional metadata such as ratings, and it is closely related to the knowledge of commands workflow attribute.
We computed similarity by modeling each workflow as a sequence of commands, then computing the edit distance  between sequences of commands.
The edit distance is the sum of the costs of the insertions, deletions, and substitutions that are required to transform one command sequence into another.
We used the distance between commands in the Photoshop menu hierarchy to set the substitution cost: for example, substituting Gaussian Blur for Motion Blur incurs a low substitution cost, because both are siblings in the Blur menu.
We used hierarchical agglomerative clustering  to create clusters, although other methods  are possible.
Finally, we laid out the clusters using the quantum treemap algorithm , as it was designed to compactly lay out items with a minimum display dimension, such as our workflow images.
Delta labels each cluster with the five most common tools in the cluster's workflows.
Labels may be abbreviated because of cluster sizes; the full label is displayed at the bottom of the view when users mouse over a cluster.
Users can click on an image in the cluster view to cause the list view to scroll to the corresponding workflow.
Finally, users can adjust the number of clusters using plus and minus icons to explore coarser or finer corpus differences.
The list view  uses workflow summaries  to surface the most salient attributes we found in Study 1.
Reading an abstract summary can enable quicker comparison than consulting the source documents.
Summaries show result quality by displaying the before and after thumbnail images and a title.
Users can click on thumbnails to view a full-resolution version.
Summaries enable users to judge command familiarity through a list of unique commands in the workflow.
The summary also displays the total number of commands, which is a coarse indicator of efficiency.
Each workflow view also contains a link  to the source webpage.
Users can sort the list view by either the number of commands or by cluster .
Figure 5 illustrates the creation of the union graph, which is produced using the edit distance computed for the cluster view.
Commands in both workflows that have the same name and parameters are considered to be shared between workflows and are visually centered in the graph; in Figure 5, "Find edges" is the only shared command.
Commands without parameters are displayed as rectangles and commands with parameters are displayed as rounded rectangles.
Finally, node tooltips show the full name of the command and any parameters for that command.
The graph allows the user to judge similarity by identifying shared commands.
You were following this sketch effect workflow and got stuck midway because one step was not clearly explained.
Find the workflow most similar to this workflow.
Find the sketch effect workflow that would be quickest to complete.
Find the sketch effect workflow that would expose you to the most new or unfamiliar tools.
We designed a study to evaluate how effectively the different views of our system were able to aid users in exploring a small corpus of workflows.
We also compared Delta to web search in order to contrast usage behaviors.
However, the primary purpose of the study was to observe user behavior with Delta and gain insights into potential design enhancements.
We asked users to complete a set of workflow comparison and search tasks on a corpus of 30 similar workflows while thinking-aloud.
Specifically, we asked users to tell us which features of the workflows or interface they were attending to while completing the tasks.
In each interface condition, participants saw a different set of workflows: one for image sharpening, and one for applying a sketch effect.
Each corpus contained 30 workflows, which we manually transcribed from tutorials taken from the top Google search results.
The source workflows were of similar length and scope as the workflows we presented to users in Study 1.
We found that 30 workflows gave us a good variety of approaches, although the small corpus may have limited the utility of our system.
The ordered tasks we gave participants were as follows : 1.
Find a workflow you would use to apply a sketch effect.
Find three workflows that each use a different approach to apply a sketch effect.
We used an online classified posting to recruit participants, asking for people who had at least beginner-level Photoshop experience.
Twelve participants  completed the study.
They ranged in age from 26-67 years old and reported between two months and 20 years of Photoshop experience.
We first discuss our observations of user behavior when using Delta, then briefly report user ratings of their preferred interface for each task.
We do not report time-tocompletion for either interface as the corpus was small enough that users tended to learn the workflows quickly, which meant that they were sometimes able to complete the later tasks very quickly by referencing a workflow they had already seen.
When using Delta, we noted that users depended heavily on the list view for all of the tasks.
All users quickly completed the search tasks by taking advantage of the list view's sort by number of commands, from fewest to most.
For the Quickest task, users scrolled to the top of the list and chose one of the top workflows based on the result quality and their own familiarity with the commands.
In the Most new commands task, users scrolled to the bottom of the list to find the workflows with the most commands, then read the list of commands in the workflow summaries and chose the one that contained the most unfamiliar commands.
In the comparison tasks, all users made use of the command list in the workflow summaries as their primary way of finding candidate workflows.
In fact, P9 exclusively used the list view for both tasks.
He rapidly judged similarity from the command list; for example, in the Most similar task, he said, "both  have similar tools" and did not look at the details of the workflows.
On the other hand, only three users interacted with clusters, either in the cluster view or by reordering the list view by cluster.
P3 clicked once in the cluster view during the Three approaches task and sorted by cluster in the Most new commands task.
P7 reordered the list view by cluster during the Three approaches task, but relied on the content of the source images to choose three different workflows.
P4 used the cluster view in the Most similar task to choose a workflow in the same cluster as the supplied workflow.
P4 then selected the workflow in the same cluster that had the most command overlap.
Four of the twelve participants used the detail view in the Find a workflow task, and ten participants used the detail view to read individual steps in the workflow when completing the comparison tasks.
They would first use the list view to find interesting workflows, then select the workflow and read the details.
Seven of the twelve participants used the union graph in at least one of the comparison tasks.
P11 commented that, "A lot of the commands are joined" while completing the Most similar task.
Others, such as P2, noted shared commands, but also commands that are shared but out of order: "The only  they share is layer via copy ... and some of their directions are pretty much reversed.
One has a layer  after a layer via copy, the other has a layer  after another command."
This observation indicates that users understood the concept of the union graph and found it useful for completing the comparison tasks.
It also suggests that explicitly indicating shared, but out of order, commands is potentially useful.
Although users understood the main concept of the union graph, there were some features that caused confusion.
Some users did not recall that hovering on a command in the graph activated a tooltip with the full name of the command and its parameters; when referencing a command, participants sometimes used the shortened display name of the command rather than the actual command.
A number of users were also confused that commands with the same name were not joined in the union graph unless they also shared the same parameters.
None of the participants used the parameters of commands when choosing workflows; this implies this information does not need to be present in the views, as users can retrieve the information from the source document.
All of the participants viewed the full resolution versions of the before or after images to compare the results, and many users viewed the full resolution images that illustrated each step.
However, although our design allowed users to pop open multiple full-resolution images to aid comparison, only P11 moved image windows around to compare a fullresolution image to another image.
We asked users to tell us what they liked and disliked about Delta.
We categorized their open-ended responses: Figure 6 shows what users liked about the application and Figure 7 shows what users disliked about the application.
Overall, users liked how the application enabled them to compare workflows, and two users mentioned the union graph in particular, both referring to the graph as a "flow chart."
We also wanted to learn about how users would cluster workflows, to better understand what areas of the similarity metric space would be useful.
We gave the same twelve participants from our user study 10 printed workflows for red-eye correction and asked them to create groups of workflows that use similar approaches to complete the task.
The printed workflows were in the text+image format from Study 1.
We intentionally left the prompt vague to allow participants to use their own notion of similarity.
We asked participants to think-aloud while grouping workflows, and specifically asked them to tell us why they placed workflows in the same group.
The clustering study took approximately 10 minutes to complete.
The cluster view may have been unnecessarily high level.
We hypothesize that as the number of workflows increases, the cluster view may provide more benefit.
The larger the corpus, the more beneficial a high-level overview of the collection is likely to be, as prior clustering systems have shown .
In comparison tasks, users used the list view to identify interesting workflows, then read the details of the workflow to make a final judgment.
Although one user only used the workflow summaries to complete the comparison tasks, the others read the details of the workflow.
This indicates that providing access to the source material is imperative.
We noticed two primary methods of clustering: using commands, or command groups, and using the complexity of the workflows .
Some users combined these two methods when creating their clusters.
Seven users used command groups to group workflows.
For example, P1 separated the workflows into one group which involved "filling in the eye with different colors", one that "changed colors using channels", and one that involved "painting on new layers."
A few made a distinction between workflows that used layers versus those that used painting tools, such as P9.
Five users clustered workflows based on complexity.
A few clustered based on how easy the workflow was to follow; for example, P5 created one cluster with workflows that had "too much information," and one cluster with workflows that were "nice and simple."
P12 split workflows into familiar and unfamiliar groups, and further clustered the familiar group using commands.
These results indicate that individual users may group commands differently, and use those groups to judge similarity.
In addition, users thought it was important to differentiate between easy and difficult workflows.
The implication for design is that an effective clustering metric would combine both command similarity and a measure of workflow complexity.
One of the biggest problems users encountered was information overload; in fact, one user did not complete the Three approaches task, saying he was "overwhelmed."
A possible solution lies in our observation that users did not rapidly switch between views.
In addition, although the list view and cluster view were linked , users did not make use of this link.
This means that the system could display panels separately instead of all at once on the screen.
Our study has several limitations worth noting.
Participants were introduced to Delta through a five minute walkthrough of the interface by the experimenter.
Given this short amount of time, users may have preferred familiar interface components, such as the list view, at the expense of unfamiliar components, such as the cluster view.
Extended use with the system may result in different behavior.
In Study 1, users reported that they used tutorials serially: they would find a workflow, try it out in Photoshop, and only look for another if the first wasn't satisfactory.
Users might exhibit different behavior if our system were integrated into Photoshop; we leave this to future work.
Finally, we did not test how accurate users' choices were.
For example, we do not know if users chose the workflow that would be quickest for them to complete, as we did not have them complete the workflow after they had chosen it.
However, our tool was designed to elicit user behavior and was not designed as a final solution to the problem of workflow exploration.
We have studied users' criteria when choosing workflows and designed a new interactive application to support that process based on the results.
Our study indicates that the application provides subjective benefits in comparison to the traditional means of using a web browser to locate and compare workflows.
The results of our studies provide many directions for future work.
In particular, we found that users consider command familiarity when comparing workflows.
Integrating this tool into a system would allow us to use an individual's command use to provide better ordering in the list view, or a better similarity metric for the cluster view.
In addition, we did not use any "broken" workflows in our study, or workflows that do not result in their claimed effect.
Testing whether our system allows users to avoid such workflows is another direction for future work.
We would also like to explore how well our system scales and what designs will work best as the number of workflows increases, or when the workflows become longer, such as those in Chronicle  or MeshFlow .
Finally, we are actively working on automatically extracting workflows from websites that contain tutorials.
Our results indicate that just the accurate extraction of commands and illustrative images from a tutorial would greatly enhance search efficacy.
In addition, automatic workflow extraction will help us explore how our tool scales by opening our tool to a large number of workflows.
Although using workflow recording systems would be an easy method to provide data to our system, such systems are currently not widely used.
