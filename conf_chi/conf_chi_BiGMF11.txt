Despite the prominence of multi-touch technologies, there has been little work investigating its integration into the desktop environment.
Bringing multi-touch into desktop computing would give users an additional input channel to leverage, enriching the current interaction paradigm dominated by a mouse and keyboard.
We provide two main contributions in this domain.
First, we describe the results from a study we performed, which systematically evaluates the various potential regions within the traditional desktop configuration that could become multi-touch enabled.
The study sheds light on good or bad regions for multi-touch, and also the type of input most appropriate for each of these regions.
Second, guided by the results from our study, we explore the design space of multi-touch-integrated desktop experiences.
A set of new interaction techniques are coherently integrated into a desktop prototype, called Magic Desk, demonstrating potential uses for multi-touch enabled desktop configurations.
Text entry is cumbersome , and the "fat finger" problem limits the precision of touch input .
With the existence of such challenges, it is hard to imagine that our mouse and keyboard devices, which provide precision input, could be completely replaced by multi-touch surfaces.
Instead, we foresee that future computing environments will be a blend of keyboards, mice and touch devices.
With the release of Microsoft Windows 7, which supports multi-touch , and the commercial availability of multitouch monitors  and laptop displays , the industry has already moved in this direction.
But, this begs the question: is a vertical display monitor the right way to integrate multi-touch into the desktop experience?
Other planar regions for touch input include the areas on the desk surrounding the mouse and keyboard.
To integrate touch into the desktop experience successfully, it is crucial to understand the properties of different touch regions and their relationships with the devices we already use.
In this paper, we provide two main contributions, to advance our understanding of the integration of multi-touch and desktop configurations.
First, we systematically investigate users' single and multi-touch input abilities on the potential touch regions in a desktop computing environment, including the vertical display monitor.
The vertical display performed poorly in both one- and two-hand touch tasks, showing that the main option commercially available today might in fact be the worst one.
Second, guided by the study results, we explore the design space of multi-touch integrated desktop experiences, with the design and implementation of a set of interaction techniques, such as an enhanced task bar and multi-functional touch pad.
All of the techniques were coherently integrated into a desktop prototype called Magic Desk .
In recent years, multi-touch displays  have received a great deal of attention, both in the research community, and in consumer devices.
The research literature has shown numerous benefits of multi-touch input, such as increasing the bandwidth of communication between human and computers  and its compatibility to control multiple degrees-of-freedom .
Because of its unique affordances, research in multi-touch applications generally involves a standalone touch sensitive device, sometimes with peripheral displays , with a custom designed UI optimized for touch .
Less explored is how multi-touch could be integrated into our current desktop experience.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Constrained by the lengths of human arms and rotation angles of joints, a user's "reach" heavily impacts how a table can be used: it dictates the space available for interaction.
Anthropometrical research has determined where the user is able to reach when sitting in front of a horizontal table ..
Hedge  also proposed a model predicting maximum comfortable reach  Scott et al.
In a desktop computing environment, the existence of mice and keyboards will probably affect the accessibilities of various touch regions.
Specific studies investigating the interplay between keyboard/mouse and various touch regions are required.
The Mouse 2.0  and the Apple Magic Mouse  allow users to perform multi-finger gestures on the surface of the mouse.
While these explorations are all promising, the surfaces they provide for touch are non-planer and have limited space.
In contrast, we will be exploring combinations of planar multi-touch surfaces with traditional input devices, to support traditional, larger-scale, multi-touch interactions.
The Pebbles project  explored an enhanced PC computing environment with a touch-screen mobile device, but the device had limited input space and only provided single point input.
The Bonfire system  enhanced mobile laptop interaction by projecting interactive displays on both sides of a laptop keyboard, however the implementations were focused more on augmenting the surrounding environment, rather than supporting desktop activities.
They focused on interactions among co-located groups around a large table.
We focus on enhancing a single person's desktop work.
In summary, there has been a large amount of research on touch enhanced tables and environments, but we are unaware of any which systematically investigates the integration of multi-touch technologies into a desktop environment.
Numerous researchers have explored how tabletop devices could facilitate daily desktop work.
Hancock and Booth  investigated menu selection on horizontal and vertical display surfaces, leading to a menu placement strategy for a tabletop display.
We complement these highlevel studies by systematically assessing the human's interaction abilities in the different touchable regions.
Some important examples include Krueger et al.
More recent work has looked at bringing physical simulation into touch-screen interaction .
Common among these foundational research projects is that they utilize the interactive tabletop as a standalone input platform, and do not consider the integration of multi-touch with a desktop configuration.
In a traditional desktop environment, where a user sits in front of a desk and input is performed with a mouse and keyboard, the planar regions available for touch input include the entire space surrounding the keyboard and mouse.
However, in terms of commercial availability, multi-touch is typically constrained to the vertical display monitor.
With little current understanding of the benefits or drawbacks of using the planar regions surrounding the keyboard, and how these regions compare to a multi-touch vertical monitor, we are motivated to investigate the effect that touch region has on interaction capabilities.
In addition, we investigate the transition cost when a user changes input channels from a keyboard or mouse to any of these regions, and effects on fatigue.
The main independent variable for the study is the touch region: top , bottom , left , and right  regions of the desk surface, and the vertical screen  .
To determine reasonable sizes for these touch regions, we surveyed 20 daily computer users on their normal seating and keyboard positions.
Results showed that 95% of users put the keyboard centered in front of their body.
We designed three tasks, as abstractions of the type of gestures that might be performed with multi-touch.
This task represents a simple single finger gesture.
Initially, a start circle, gesture direction line, and objective line  appeared on the touch screen .
The center of the starting circle was in the center of one of the 9 cells, and the direction of the gesture line was either up, down, left or right.
The distance between the center of the starting circle and the objective line was 125 pixels.
A participant had to touch the starting circle with one finger and move to cross the objective line.
The widget turned white when the circle was touched , and gold when the finger crossed the objective line, indicating completion of the task .
If the user failed to cross the target line, the gesture would need to be repeated.
According to Hedge et al's  model, the Zone of Comfortable Reach area  on a table is a spherical shell centered on each shoulder, the radius of which is the asromion to grip distance.
Guided by the size of a regular keyboard  and a normal human arm's length , we used a 44 x 33 cm rectangle for the top, left and right regions, which would cover more than 90% of the ZCR area in these regions.
Constrained by the sitting distance, we set the size of the bottom region to 44cm x 17cm.
The size of the vertical screen was also 44 x 33 cm, which is a reasonable size for a monitor.
The distance between the screen and the center of the body was fixed at 68cm, which was the average eye-screen distance from the survey.
Given the average human arm length of 75 cm, most users can comfortably touch the screen from such a position.
On the right side we offset the screen from the keyboard by 8cm to leave room for the mouse, since we felt it would be impractical to have a multi-touch device right beside the keyboard, where it would get occluded by the mouse.
This task was designed to represent a one-handed, multi-finger task.
Initially, one small green square  and one large yellow square , appeared on the screen .
The participants were asked to dock the green square by moving, rotating, and scaling it to cover the yellow square .
The borders of both squares turned gold when the green square was successfully docked .
The participant could manipulate the green squares with commonly used manipulation gestures: translate by dragging it with one or more fingers, scale by moving two fingers apart/together, and rotate by rotating the fingers.
Participants were only allowed to use one hand, either left or right, during this task.
The task position was controlled by placing the yellow square in the center of one of the 9 cells.
The initial distance between the centers of the green and yellow squares was 500 pixels.
The relative offset angle of these two squares was randomized.
This task was designed to investigate performance of two-handed tasks.
We used a 21" multi-touch enabled screen with a resolution of 1600 x 1200 to simulate each of the touch regions.
For the l, r, t, and b regions, the screen was placed horizontally on the table, and the keyboard and mouse were raised to the same plane as the screen .
Execution time is the elapsed time between the first finger contact and the moment the participant's fingers finally leaves the experiment area.
The switch back time is the time elapsed between removing the hand from the touch surface and performing the end trial action.
A "clutch" occurs when a participant lifts all fingers off of the surface, and then proceeds to touch the surface again.
Since a trial does not end until it has been successfully completed, this measure might provide indication of the difficulty level of the task.
To measure muscle fatigue, participants were asked to rate fatigue level after each block .
A 5-minute break was enforced between regions to increase the likelihood that each region began with a 0 fatigue rating.
Since our goal was to measure how fatigue level changed as time progressed in a touch region, no break was taken within each region.
To test the transition costs between devices, we considered two common desktop configurations of our hands: Keyboard+Keyboard: In this mode, the trial begins and ends with both hands on the keyboard, when the user simultaneously presses the F and J keys with their left and right hands respectively.
Keyboard+Mouse: In this mode, the participant begins and ends the trial with one hand on the keyboard and one hand on the mouse.
The participant would simultaneously press the F key with their left hand and the mouse left button with their right hand to start and end a trial.
For both the gesture and one-hand docking conditions, users could use either hand to complete the task.
An experimenter recorded which hand was used for each task.
We used a within-subject, full-factorial repeated measure design for all the experiments.
Each participant first performed all trials for the gesture task, followed by the onehand docking, and finally the two-hand docking task.
For each task, the independent variables were touch region , grid cell within a region , and start-end position .
The orders of the touch regions were counterbalanced using a Latin Square.
Half of the participants performed the tasks with keyboard+keyboard mode first, followed by keyboard+mouse mode.
For each start-end position within a region, the participant performed tasks in three blocks with each block having 9 trials.
Within each block, each grid cell value appeared exactly once, in random order.
The design resulted in a total of 270 trials per task, for each participant.
Prior to formally starting each task, participants performed three warm-up trials to become familiar with the task.
After completing each of the three tasks, each participant rated the five regions according to their overall feelings.
No significant main effect was observed for start-end position on completion time.
ANOVA showed a significant Region x Grid Cell interaction on completion time , but not for Region xStart-end Position or Start-end Position x Grid Cell.
Figure 8a visualizes the mean completion time of each grid cell.
In the t, l and r regions, the completion times were shortest in the cells closest to the keyboard/mouse , and increased as the distance from the keyboard/mouse grew.
In the b region, completion times were more uniform.
No significant main effect of region on number of clutches was observed.
Similarly, ANOVA did not show significant main effects of start-end position, or grid cell on number of clutches.
We specifically investigated the average fatigue level across all the three blocks.
All participants reported that gesture tasks were easy to perform and they felt little fatigue.
All the participants performed gesture tasks with their left hands on the l region, and right hands on the r region.
Participants performed gesture tasks with their right hands for 67%, 71% and 82% of trials in b, t and s regions respectively.
They commented that they preferred to perform tasks with their dominant hands.
The mean completion time in the s condition was the longest among the five tested regions .
Pairwise mean comparison showed significant differences for bXl, bXr, bXs, bXt, lXs, and rXs, indicating that users clutched least often in the bottom region.
ANOVA did not show significant main effects for either start-end position or grid cell on number of clutches.
A Significant Region x Grid interaction  was observed, but not for Region xStart-end Position or Start-end Position x Grid Cell.
No significant main effect of region, startend position or grid cell on average fatigue level was observed.
All participants performed one-hand docking tasks with left hand on the l region, and right hand on the r region.
They performed one-handed docking tasks with right hand for 75%, 77% and 82% of trials in b, t and s regions respectively.
ANOVA showed that there was a significant main effect of region on rates in the two-handed tasks , with the b and t regions being the easiest and the right region being the worst.
No significant main effect was observed for regions on one-handed tasks.
These results are consistent with completion time results: it is easier to perform two-handed tasks in the b and t regions.
As expected, users performed one-handed tasks the fastest in zones close to the keyboard or mouse  due to the short travel distance.
The entire Bottom region performed particularly well in both the gesture and onehand docking task: the mean completion time of most grid cells in bottom regions is faster than the average completion time of every other region.
Some users commented that it was easier and more comfortable to touch in the bottom region by just withdrawing a hand back than reaching it out to make contact in the other regions.
Additionally, since the reachable area in the bottom region is smaller than those in other regions, the average hand traveling distance is shorter, which also contributes to the faster completion times.
One problem with the bottom region is occlusion of the display caused by the user's hands.
Two participants reported this occurring.
These problems could be alleviated by designing occlusion-aware interfaces .
Given the relative prevalence of touch sensitive monitors, we believe it is a very important result that users performed one-handed tasks poorly on the screen, where mean completion time was the longest in one-hand docking and second longest in the gesture task.
We argue that lifting arms up from the desk surface to the monitor leads to a greater switching cost, and operating in the air could lead to poor performance.
Six out of ten participants reported that performing tasks on the vertical surface was more difficult than on the horizontal surface because they could not rest their arms while performing the tasks.
In summary, the study reveals the users' capabilities of performing one-handed tasks: * * * Users perform one-handed tasks efficiently in zones close to keyboard or mouse.
Users perform one-handed tasks generally well across the entire bottom region.
The vertical screen is a poor region for performing one-handed tasks.
Different from both gesture and one-hand docking tasks, touch region had a significant main effect on average fatigue level , with the t being the least fatiguing and right being the most fatiguing region.
Pairwise mean comparison showed significant differences for bXr, and tXr.
6 out of the 10 participants reported that they disliked performing the two-handed docking task in the right region because they had to rotate their waists significantly to complete the task.
No significant main effect was observed for either start-end position or grid cell for fatigue levels.
A Significant Region x Grid interaction  was observed, but not for Region xStart-end Position or Startend Position x Grid Cell.
Participants rated each region according to their overall satisfaction after completing all the tasks.
The three tasks were classified into two categories: one-handed tasks  and two-handed tasks .
For the question, "Are the tested tasks easy to perform in each region ?
Overall, the results show that users performed two-handed docking tasks quickly, and felt less fatigue in the bottom and top regions than in the other three regions.
We argue that this may be due to ergonomic issues.
In the right and left regions, users had to rotate their torso for two-handed tasks to get both hands over to one side of the keyboard.
This body rotation might lead to muscle fatigue and poor performance.
Similar to one-handed tasks, users com-
Based on these results, we draw the following conclusions about the users' abilities to perform two-handed tasks: * * The best zones for performing two-handed tasks are the bottom and top regions.
Users perform two-handed tasks poorly in the left, right and screen regions, and these regions also caused increased levels of fatigue.
As users process increasing amount of digital information, they desire more flexibility of managing windows, such as moving/resizing multiple windows simultaneously, and arranging multiple windows to form a semantic layout .
To enhance the flexibility and increase the input bandwidth of managing windows, we designed an enhanced task bar , allowing users to simultaneously manage multiple windows directly with two hands.
Thumbnails of open windows are displayed in the enhanced task bar and the location and sizes of these thumbnails conveys the spatial location and sizes of open windows on the monitor.
Since the Enhanced Task bar has a wider aspect ratio than the vertical computer screen, overlapping windows are spread out more horizontally and thus are more accessible for manipulation.
Moreover, the following operations are enabled: Resize.
Moving fingers apart  on the thumbnails enlarge  the corresponding windows.
Double tapping on the thumbnail maximize/restore the corresponding window.
Flicking the thumbnail down minimizes the corresponding window and sends the thumbnail to a bottom strip.
Flicking the thumbnail up from the bottom region restores the window.
As the enhanced task bar technique involves a rich set of two-handed operations, we suggest placing this component at either bottom or top region.
In the current Magic Desk system, the enhanced task bar is coupled to the bottom edge of the keyboard.
In both one- and two-handed tasks, some of the results are within our expectation.
For example, zones close to the keyboard and mouse are good for one-handed tasks, and the top and bottom suit two-handed tasks well.
In doing this study, we validated such expectations, and in addition, provided a quantitative analysis and in-depth understanding of each zone.
Specifically, we have captured the precise magnitude of effects by each region and its 9 grid cells.
In addition, the study also reveals some interesting findings.
First, the bottom region suits both one- and two-handed interaction very well.
Second, the vertical screen is less efficient for touch interaction.
This is a particularly important finding, given that touch screen computers are becoming more prevalent .
Guided by the study results, we designed and implemented a set of interaction techniques integrating multi-touch input with a mouse and keyboard to facilitate desktop work.
Our purpose is to demonstrate example interactions and usages, and in particular, demonstrate how different regions within the desktop environment can be used for touch, and how such interactions can be guided by our study results.
The interaction techniques are coherently integrated into a desktop prototype, called Magic Desk .
We demonstrate our new techniques in an environment which has all five planar touch surfaces available.
The current system was implemented on a Microsoft Surface with a Dell Multi-touch display.
A QWERTY keyboard and wireless mouse are used, and have tags so that their position and orientation can be recognized by the surface.
Two-handed interaction has been shown to be beneficial in certain interaction tasks, such as controlling multipledegrees-of-freedom .
By designing a multi-functional touch pad on the left side of the keyboard, we enable such interaction paradigm in a desktop work environment: the right hand interacts with the mouse while the left hand uses the touch pad.
We implemented the following functions: Controlling multiple degrees of freedom.
The mouse is used to select a target, while the left hand fingers control additional degrees of freedom  .
Adjusting Control-Distance Gain of a mouse.
Through the mouse speed region on the touch pad , users can move their fingers apart to increase the CD gain and together to reduce it.
This can be done in parallel to a mouse operation task with the right hand.
A secondary cursor controlled by the left hand on the touch pad is introduced to work in parallel to the primary cursor operations.
Using a relative mapping, the user can move the secondary cursor, that is constrained within a tool palette, to select different tools, while controlling the main cursor on the canvas to draw graphics .
Users can directly tap the content on the clipboard to paste it at the location of the cursor.
Bringing the common commands on the digital mouse pad allows users to quickly access them, albeit it might require users switch eye-gazing position from the screen to the table.
However, as users are familiar with the locations of menus on the digital mouse pad, this switch cost might be reduced.
An alternative is to display virtual representations of users' hands on the screen, so that they do not have to look at the table during the interaction.
The experiment results indicated that the region close to the right side of the keyboard is one of the high-performing zones for one-handed tasks.
UI elements  on this region can be easily accessed.
The touch regions on a desk can be combined with the touch screen to provide a continuous work space.
The continuous workspace supports the following operations: Adapted window content.
Users can freely drag windows between displays using fingers or a cursor to take advantage of the extra display surfaces.
Since interaction focus is usually located on the monitor, applications on the tabletop mostly play a supportive role in displaying peripheral information .
UI elements within a window are rearranged to be close to the keyboard , because these areas are best for performing one-handed direct touch.
The UI elements are also enlarged to suit touch interaction.
Using the entire desk for interaction may be well-suited for specific tasks such as previewing images, navigating maps, and annotating documents.
When the keyboard is moved out of the way, the window on the horizontal table will automatically expand to fill the entire desk.
The horizontal table now becomes a full multitouch display, on which users can freely pan and zoom displayed pictures with fingers .
Placing the keyboard back to the center of the desk returns the table to the standard mode.
The minimal use of touch on the main monitor was driven by its poor results from the study.
Instead, touch for the vertical screen was only used to send content to the horizontal surface.
In addition, the adapted UI layouts were guided by our finding that touch regions should be made as close to the keyboard as possible.
The multi-Functional touch pad can also serve as a repository for storing commonly used UI elements.
For example, Figure 11d shows a touch pad with touch buttons and sliders for a text editing program.
To add a new element to the palette, a user duplicates it from the monitor by flicking it down with a single finger.
The flicked element then animates to the touch pad.
Dragging an undesired element out of the palette removes it.
All of the interactions on the multi-functional touch pad are one-handed.
More specifically, most interactions are performed with the left hand while the right hand is operating the mouse.
According to the experiment results, the optimal region for left-hand operations is the rightmost area of the left region.
Therefore, the touch pad is coupled with the left edge of the keyboard.
We asked each of six users to freely and extensively try the interaction techniques on the table for 40 to 50 minutes In general, they commented that interaction techniques were easy to learn and use.
Our implementation of Magic Desk was carried out on a Microsoft Surface, which allowed us to prototype interactions within each region surrounding the keyboard.
However, one could imagine numerous other configurations supporting one or multiple regions of multi-touch interaction .
Figure 14a shows the scenario where the entire tabletop is both display and touch capable.
Figure 14b, c and d illustrate how a subset of the touch and display regions could be reproduced by using auxiliary devices.
For example, a multi-touch tablet, such as an iPad, could be placed next to the keyboard .
This would support interactions such as our Multi-Functional Touch Pad.
Additionally, an ultra-thin, multi-touch display pad, possibly implemented by layering a transparent UnMousePad  on top of an e-ink display, could be positioned below the keyboard , thus enabling enhanced task bar techniques.
An additional touch-tablet device could be positioned underneath the mouse to support digital mouse pad operations .
Since the mouse would sit on top of the display, the display could be positioned next to the keyboard, possibly mitigating some of the negative effects associated with the right region in our study, which was displaced from the keyboard to leave room for the mouse.
Potential configurations for multi-touch desktop computing.
Some desktop users tend to clutter desk space with various physical objects such as paper documents.
To cope with cluttered desk space, we suggest using automatic occlusion reduction , adaptive layout , freeform display representation , or customized tabletop widget  techniques.
These physical artifacts  could also be virtually augmented with multi-touch surfaces.
These cluttered desks can be considered as subsets of the complete multi-touch enabled desktop : only parts of the entire desk would be available for touch interaction.
Our observation study indicates that most users have some spare real-estate in at least one of the regions we studied.
Touch interaction techniques could be implemented in these areas.
In addition, technology development could in turn affect users' behaviors.
It is possible that users may adapt their workplaces to create room for supplementary multi-touch surfaces, thus benefiting from the proposed interaction techniques.
A related issue is the potential problem of false touch activations, potentially from the hands resting on a touch enabled surface.
Many multi-touch systems, such as the Microsoft Surface, already have finger detection libraries, and can ignore non-finger input.
This worked sufficiently in our implementation, but warrants further investigation.
In this paper, we explored both theoretical and practical issues related to integrating planar multi-touch surfaces into a desktop computing environment.
We systematically studied user's touch input abilities and transition costs between keyboards/mice and the five planar touch regions via controlled experiments.
Guided by the study results, we explored the design space of a multi-touch integrated desktop environment, by designing and implementing a set of interaction techniques utilizing planar touch regions with a mouse and keyboard to facilitate desktop work.
All the interaction techniques were coherently integrated into a desktop prototype called Magic Desk.
The system demonstrates various possibilities of integrating multi-touch with a mouse and keyboard in desktop work.
