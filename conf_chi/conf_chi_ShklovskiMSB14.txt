Mass data collection is becoming "designed in" to everyday life  as we increasingly rely on technologies capable of monitoring, storing and distributing information about us.
Occasionally data collection by governments, organizations, corporations or even small application developers comes to light causing a public outcry .
Yet much of the tracking and data collection does not necessarily happen with insidious goals, but is a result of businesses attempting to deliver personalized services and advertising more effectively .
Despite potentially positive outcomes of this kind of tracking, the public response remains negative often because people simply find it creepy.
At the same time, people continue to use the technologies and applications implicated in data collection even as they express outrage over such activities.
This is typically seen as an expression of the "privacy paradox" where intentions and behaviors around information disclosure often radically differ .
Over the last decade, especially since the launch of the first app store by Apple in 2008, computing has moved out of the home and into our pockets, purses, hands, and even bedsides, bringing with it a new wave of concerns, increasingly framed in terms of "creepiness."
We will define creepy more carefully later on in the paper but we note here that we use this term to denote an emotional response to a sense of wrongness that is difficult to clearly articulate.
We argue that the notion of creepiness is a concept worth unpacking, and by way of example analyze a corpus of data recently collected from smartphone app users in a number of western countries using perspectives of bodily integrity, personal space and territoriality .
From this analysis a different notion of "users" emerges.
Mobile devices are playing an increasingly intimate role in everyday life.
However, users can be surprised when informed of the data collection and distribution activities of apps they install.
We report on two studies of smartphone users in western European countries, in which users were confronted with app behaviors and their reactions assessed.
Users felt their personal space had been violated in "creepy" ways.
Using Altman's notions of personal space and territoriality, and Nissenbaum's theory of contextual integrity, we account for these emotional reactions and suggest that they point to important underlying issues, even when users continue using apps they find creepy.
I feel like this is a part of daily life now" - Survey, USA What does it mean to "feel" like you are constantly tracked by unseen and largely unknown entities?
In the quote above who was doing the tracking was not named and for most of our respondents through the interviews and the survey they remained unknown, largely abstract entities that somehow collect personal, sometimes even intimate information.
With the rise of personal computing in the 1980s and especially of the world-wide web in the 1990s and 2000s, and the movement of computers out of offices and into homes,
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
For the purposes of this paper, when we say "people" we are referring to fairly affluent consumers in developed Western countries.
Questions of personal space and privacy are clearly culturally dependent, and from, e.g., Chinese or Global South perspectives smartphones may have very different meanings.
On the other hand, in order to empower themselves through this boundary-enlarging technology, they felt they had to accept the possibility of unknown, foreign access into this zone of expanded intimacy.
Reframing the "privacy paradox" in these terms opens different opportunities and responsibilities for technology development and policy design.
Territorial behavior is another type of a social regulation mechanism.
Altman argues that just like animals are territorial about their feeding or mating areas, people also regard certain places and objects as their personal or primary territories.
A primary territory usually refers to an area, a place or an object that fulfills certain needs or motives and where ownership is clearly conveyed through some form of personalization.
Entry into another person's primary territory is typically done with permission only and trespassing is considered an invasion that can elicit intense emotional responses and physical action .
Several researchers have explored user decisions to allow physical access to their smartphones and personal data, showing that people regard their phone as highly private and are reluctant to share it with others, expressing discomfort even when sharing it with close friends .
Research has also shown that smartphone users are surprised and feel violated when they find out that applications are accessing data on their smartphones ostensibly without their knowledge .
In other words they feel that this data access breaches their privacy norms and should be done with their informed consent, the same reaction people show when their primary territory is breached.
Increased computerization affects individuals and society as a whole as it brings increased imbalance in power between individuals and large institutions.
In the case of smartphones, the power balance between the smartphone user and the application developers is quite uneven.
The user has few means of safeguarding his territory and often she is unaware of the encroachment.
Her options include shifting the desired level of privacy towards the actual level; a common approach used when repeated attempts to regulate privacy have failed.
Ubiquitous, nearly infinitely personalizable and most often individually owned at least in the Western world, mobile phones have variously been discussed as deeply personal extensions of the self  or even organic parts of the self .
As mobile technologies developed and become domesticated, smartphones have moved from an expensive curiosity to a commodity deeply integrated into their users' everyday lives.
The functions these devices perform are not just about practical problem solving, escapist entertainment or convenient extensions of sociability, but also about projecting and constructing the self .
Like our homes, smartphones are implicated in the way of being and deeply embedded in the life experience.
Yet in HCI research aesthetic or emotional experiences are less of a concern than the serious business of sociability, friendship, collaboration and getting ahead in life.
Finding a particular experience "creepy" is a kind of emotional experience, but it can also serve as an indication of a disjuncture between the way developers and users experience and interpret functions or applications.
Much of the discussion of discomfort or feelings of creepiness in the course of technology use in HCI has been conflated with user privacy concerns of data access and security.
We use two theoretical frameworks familiar to HCI researchers for thinking about privacy, but extend the conversation to consider how creepiness might be conceptualized in this context.
Irwin Altman's theory of privacy regulation has been discussed in prior work .
Altman conceptualizes privacy as a process through which people attempt to achieve a desired privacy level in any situation in life by selectively adjusting access to themselves through controlling information they disclose or receive .
In other words, people keep secrets and manage themselves and their secrets through ongoing acts of revealing and concealment .
Altman conceptualized privacy as one of the four key concepts central to the study of environment and social behavior: privacy, personal space, territoriality and crowding.
Altman explains personal space and territorial behavior as mechanisms people use in the service of privacy goals to regulate interpersonal boundaries and environmental factors.
Personal space is typically conceptualized as a "boundary around a person, intrusion into which is often uncomfortable and generally not permitted" .
Helen Nissenbaum's theory of contextual integrity complements Altman's notions of personal space by calling attention to context-dependent social norms and regulated information flows .
Even from within the most private, personal space, people never interact with information in the abstract, but always as it is embedded in a particular social context.
Nissenbaum defines contexts as "structured social settings characterized by canonical activities, roles, relationships, power structures, norms , and internal values " .
Smartphones may be extensions of personal space, in Altman's terms, but through them people participate in a variety of quite different contexts.
Contextual integrity is maintained if information flows according to contextual norms.
However, digital technologies make it easy to disregard such norms either through ignorance or intentionally .
That a digital system is respecting contextual integrity may be difficult and expensive to attain, often beyond the capabilities of any particular individual.
However, individuals can sometimes  detect when a breach of norms has occurred when they see that information they consider off limits with respect to the context in which it was given is being used in ways antithetical to its socially sanctioned expectations.
In a survey of 5000 mobile users reported by Think Insights, 82% of the participants claimed to notice ads on their smartphones and 49% had taken action based on those ads2.
Advertising in applications has a substantial effect on users and their purchase patterns.
The global mobile advertising market was valued around 5.3 billion dollars in the year 2012 illustrating just how valuable it is for advertisers to have access to user patterns and personal data .
Ad libraries and analytic companies buy the data that smartphone apps collect, analyzing and aggregating user data to build a profile for targeted advertising.
Imagine a smartphone owner who has on her phone a typical flashlight application like the High-Powered Flashlight from iHandy inc., and a simple one-player game application like Fruit Ninja from Halfbrick Studios.
Both apps require access to the phone number and device ID, the remote number if a call is active and which other applications are used on the device.
Additionally the flashlight app has access to precise location, hardware controls and system tools that enable it to change system configuration and display settings.
The Fruit Ninja app also requires access to the list of accounts known by the device.
The information these apps collect is sold to several third parties, one of which is Flurry, a big ad and analytics company that according to its website prides itself on taking in two terabytes of data from 2.8 billion app sessions per day .
Flurry buys data from both these apps and can build a profile of a user that includes information about what type of phone she has, what applications she uses and how frequently, what accounts she has on her phone, which numbers she calls  and precise location of the phone which, if logged with timestamps, gives an accurate estimate of where she lives, works and travels.
By properly aggregating this information the company gets a detailed profile of her as a consumer, information that is worth money.
Smartphone users' response to tracking and data collection as well as the willingness to share data with others varies but users are invariably surprised at the extent of data leakage on their phones .
Although many are willing to share data with developers in the right context, particularly when the type of information collected and its purpose is made clear, it is difficult to justify why gaming applications that are about fruit slicing or sling shooting birds would need the user's location, phone number or information about other running apps.
The user in the example above has given her permission for the Flashlight and Fruit Ninja apps to collect and distribute her information at the point of installation when she clicked through the End User License Agreement .
Encountering a violation of contextual integrity of one's information can result in a range of emotional responses, though it may not necessarily lead to outward action to alleviate the problem.
One can be outraged, exasperated, horrified, even cynically bemused.
Along with or in addition to these, people often describe their discomfort by referencing the word "creepy".
Collins English dictionary defines creepy as "having or causing a sensation of repulsion, horror, or fear, as of creatures crawling on the skin" .
The reference to creepycrawlies on the skin grounds this experience in a certain kind of fundamental, embodied boundary violation, in Altman's terms.
In Nissenbaum's terms, creepy information flows often involve realizations that personal secrets have been, or could be, revealed to those who have not been explicitly granted access to them.
Discussions of information disclosure practices via mobile phones have been extensive though mostly focused on location-based services and interpersonal information disclosure .
Researchers have turned their attention to smartphone app data distribution, often termed "data leakage" to third parties without direct owner notification only recently .
Ostensibly, users give permission for smartphone applications to access their use data and to utilize it for a range of purposes when they press "agree" on the End User License Agreement  screen in the process of installing a smartphone application.
Despite this, revelations of data distribution by many common applications to third party advertisers have resulted in expressions of concern and discomfort by smartphone users .
Moreover, studies show that most users simply ignore permissions completely .
People ignore EULAs for various reasons, for example warning fatigue , lack of motivation , or an overriding desire to install the app where reading the EULA is seen as a cost without a clear benefit .
However, upon closer inspection, calling this leakage captures an important aspect of the consequences of installing the app: if the user conceptualized her personal space as having integrity, and a more-or-less controllable border, from her perspective the app makes this border "leaky", and intentionally so.
While ensuring readability and usability of EULA is important, here we consider why people continue to use apps even when they know these apps reveal personal data to advertisers.
We also discuss why it is important to consider these issues both at the individual level and in the broader social context.
The studies we present consider what it means to loose control over access to the self to unknown third parties with few concrete or obvious consequences.
Why does it matter that third-party advertisers can gain access to certain types of personal information via smartphones?
How do people react to finding this out and what might they be willing to do about it?
We chose to use the Android OS as our research platform because the system presents the user with the permission screen each time she downloads an application.
Our participants then would have encountered the terms of use more often than an iOS smartphone user.
The interviews were conducted in English, Danish or Icelandic depending on participant preference and lasted 40-70 minutes each.
We started with general questions regarding smartphones and applications and then asked participants to think aloud while demonstrating how they usually go about accessing the Google Play Store on their mobile phones and downloading an application of their choosing.
After our participants downloaded an app, we moved on to questions regarding attitudes and beliefs about data privacy.
We also asked users what they think their apps do with the data they access, to get insight into participants' mental models of data security on smartphones.
In the second half of the interview we introduced some of the most common permission statements found on popular apps in Google Play and discussed what they mean.
We also showed them a website that allows users to scan apps, and asked users to download an app in Google Play that scans permissions and analytic libraries connected to apps currently located on the user's mobile phone3.
Finally, we asked participants to download and try out a popular game on Google Play, Fruit Ninja, then explained its main functions and discussed the permissions our users had accepted before downloading the app to their mobile devices.
Three weeks after the initial interviews we sent our participants a follow-up questionnaire where we asked them questions regarding their use patterns and conducted a follow up interview via email or phone.
All interviews were transcribed and coded.
We used an iterative analysis process of open and thematic coding throughout the data collection process.
This allowed us to test emergent themes in follow up interviews.
We conducted two studies investigating how smartphone users feel about data access on their phones.
The first study was a qualitative investigation conducted in two countries in order to assess whether smartphone users may be willing to change their behavior once they have been informed about tracking and data leakage.
We exposed our participants to data leakage to see if they made changes in their user patterns when informed of potential privacy breaches.
In the second study we conducted a survey on an international opportunity sample where we explored how data privacy sentiments might affect smartphone users when choosing and downloading applications.
Furthermore we explored how important it was for our participants to maintain their privacy while using applications on smartphones.
In February 2013 we conducted 13 semi-structured interviews in two countries: 6 in Iceland 7 in Denmark.
We recruited users from our own personal networks and by advertising on Facebook.
The prerequisite was that our users owned and used Android smartphones and were between 25 and 60 years of age.
In March and April of 2013 we deployed a survey to gain a better understanding of privacy sentiments of smartphone users more broadly.
We obtained an opportunity sample by recruiting via our own networks on Facebook and by sending participation requests to university student mailing lists in Iceland and Denmark.
A total of 272 respondents answered our survey and 187 completed it .
Although this was an opportunity sample it was quite diverse.
Over 60% were highly educated  and approximately 13% had less than a bachelor degree.
However, the sample included participants from 12 countries.
Although slightly over half of our respondents were veteran smartphone users , having used a smartphone for over 2 years, 18% were new to the smartphone world, having used the device for less than a year.
We measured attitudes toward app stores, personalized advertising and general sentiment toward data collection by businesses as well as the general level of concern about threats to personal privacy.
We also developed a set of questions focused specifically on attitudes towards data collection and management by smartphone apps.
In order to measure attitudes toward data collection by mobile applications respondents were randomly presented with either Fruit Ninja game app or Flashlight app with the developer description of the app.
Below the description we listed permissions that the app requires the user to agree to before downloading and asked respondents to indicate which they felt permissions were appropriate or inappropriate given the functionality of the app.
On the next page we explained which ad-libraries and analytics companies received the data the apps collected from the mobile phones and added the explanation these companies generally give for how this data is utilized.
We followed up with questions regarding awareness about data collection and distribution in mobile applications and whether participants read the EULA prior to downloading apps.
The final question of the survey was free response, asking users to share how they felt about data distribution to third parties in general.
Due to space constraints all survey instruments are available upon request.
Nearly 50%  of participants that had completed the survey responded to this question and we coded these responses using an iterative coding process by moving from an initial open coding through axial and thematic coding to distill thematic categories.
Where necessary, quotes presented in this paper were translated from Icelandic or Danish.
Quotes and excerpts are marked Interview or Survey to indicate the source and Country to indicate location of the respondent.
Just like the participants in  and  our interviewees ignored the EULA when they were asked to install an application of their own choosing from the app store during the interview.
Yet they had typically expressed concern about their privacy in the prior portion of the interview, discussing privacy implications of smartphone application data leakage.
When asked why they had ignored the EULA two explanations dominated similar to those described in .
First, they had never had any real negative consequences from data collection.
Second, the desire to have the application trumped any concerns for data collection, which was explained away as "the way things are."
I know I am agreeing to this .
If I really want the app, I take my chances."
In the final portion of the interview, participants were asked to install Fruit Ninja and researchers walked them through the EULA explaining all permissions requested by the application.
Similar to , this exercise revealed a mismatch between the interviewees' mental models of what Fruit Ninja might reasonably request in order to function and the kind of data it actually requested.
That is, they expected the data collected via the app to be contextually appropriate to a single-player game of slicing fruit.
Not surprisingly, this mismatch caused participants significant discomfort similar to : "I feel deceived, I had no idea it was this systematic" - interview, Iceland During the follow-up interview three weeks later our participants said they wanted to be more cautious and were trying to pay attention to what they were downloading and installing on their mobile phones.
More than half  had deleted an application although in most cases it was Fruit Ninja: "I deleted Fruit Ninja because it was gathering a lot of information, although it might also be the case with other apps on the phone I don't want to delete them, I simply enjoy them too much."
Even though participants were not willing to trade personal data for useless applications they accepted the possibility that other more "meaningful" applications might be doing the same thing.
Although some approached their smartphones with more caution they admitted they had not changed their practices very much:
In broad strokes, many of our findings confirmed other studies of attitudes toward mobile phone data leakage and privacy.
Similar to the findings in  our interview participants became more concerned with data leakage as a kind of privacy violation throughout the interview.
Similar to findings in  they also indicated that these privacy concerns were often overridden by other factors when they made decisions about whether to install a particular application.
To some extent I just have to accept this."
Our participants were generally aware of data collection and distribution to third parties and believed that more data was collected than necessary.
Nevertheless, few respondents restricted mobile data while using their phones.
Indeed, 78% had data turned on continuously with 56% saying they use data a lot.
Similar to findings in  nearly 57% said they had deleted an app and roughly 62% had aborted the installation process of an application because of privacy concerns.
Overall survey respondents were concerned about information disclosure to businesses  and reported moderate levels of concern regarding threats to personal privacy .
Participants were also quite concerned about smartphone integrity and their privacy in regards to that, which manifested in low scores on our scale of how trusting smartphone users were toward the integrity of app stores and app developers .
Amazon book suggestions, but I do not approve the selling of my data to third parties, especially when I am not getting a share of the revenue!"
After all, people did want personalization of services, although they tended to identify these activities with expected, and thus contextually appropriate data usage.
Results from both studies clearly illustrated what is commonly called a privacy paradox where our participants expressed concerns about data leakage, but their actions suggested otherwise.
In both studies participants expressed a desire for greater transparency and control over information disclosure.
Where none of this is particularly surprising, we were struck by the fact that sentiments expressed by survey participants about data collection and smartphone usage seemed to fall into three broad categories .
We then explored more closely the sentiments expressed by both survey and interview study participants about data leakage and why they said they continued to use their phones the way they did.
Our findings suggest that the notion of "privacy paradox" that is often used to explain these findings may obscure complex dynamics around technology use and data disclosure.
The reality of the information economy is such that in the course of mundane interactions with everyday technologies users move smoothly between consumption of services and production of content that is in turn monetized by service providers.
This kind of information economy turns all data produced by consumers into commodities that are either fed back to the consumer through improved services and personalization or sold elsewhere.
Many of our participants understood this dynamic quite well: "I understand the appeal from the developer's perspective -being able to collect large amounts of data about users could be crucial to targeted ads and later versions of an app."
Many participants said they didn't care if advertisers had data about their application usage or how many times they played a game on their phone during work hours.
Yet a vague concern for how this data might eventually be used and interpreted remained: "I'm ok with them collecting some information about me, but it is a slippery slope and the developer should only engage in the activity if they are willing to accept the consequences and responsibility of handling it properly."
Identifying what kind of data is collected, where it is delivered and how it might be used is no small task and most of the time it is impossible.
Even the most technically savvy smartphone users often do not conceptualize quite the extent of data leakage.
Where the general sentiment here was that the practice of data collection and tracking should be somehow limited or stopped, only the very few actively limited their own app usage.
Although many called for greater transparency, few admitted to reading the EULA before installing applications of their choice.
In the vast majority of cases the same participants expressed another sentiment, that of compliance in the face of constant demands on data.
In most cases, news of data leakage elicited expressions of mild discomfort and references to creepiness: "I think it's creepy and makes me think about all the apps I have on my phone.
I use flashlight very often and I find it disturbing that it can collect personal information off my phone" - survey, Iceland In several cases, however participants expressed significant outrage.
When asked why, in all cases they explained they had not expected data collection to be quite so rampant, but perhaps the level of outrage was associated with the kinds of potential outcomes they envisioned: "This is completely ridiculous, I would not invite people into my closet, this is way out of line.
No I don t find it appropriate to give up personal information in exchange for this game and that they don t need more approval than they apparently do."
In the quote above, the interviewee equates the smartphone to a closed personal space  -- entry into which would require high levels of intimacy and trust.
People react to these breaches in different ways.
Some uninstalled the offending application immediately, while others called for some sort of regulation: "No such data collection and/or distribution should be permitted."
Several noted that this was the function of the app stores.
Most, however, appealed to some sort of vague general policies or even laws that "ought to be in place."
Many also called for greater transparency.
After all, much of what goes on with user-
Taking care of private information on smartphones takes effort; reading the EULA, scanning the phone and monitoring application updates which often involve increased numbers of permissions.
Some of our participants stated that in order to use their smartphone they had to cave in and accept data collection.
After all, what's the harm anyway?
It has become a part of their expectations and the more that this continues to happen the more they will expect it.
But I don t see how I can be personally affected by them knowing stuff about me."
To some extent people want the option to be included in the negotiations, not to be robbed of their data.
If you want the app you just have to accept this.
Otherwise you are not using the phone the way it was intended."
Nine of the 13 interviewees told us that although the information they learned from us did not result in them changing how they used their phones, they didn't quite feel as comfortable about it anymore.
The level of resignation was striking in most of these responses.
More often than not, the explanation for why our participants didn't spend the time to read the EULA, overlooked that some of their favorite applications leak data and ignored the assortment of tools available for limiting data leakage of this kind was one of helpless compliance: "It's unsettling and not ok, but I feel very powerless against it."
Although we might expect people to exhibit high levels of concern for their privacy, how can we expect them to act on these concerns if they are convinced that there is not much they can really do?
According to Altman  such lack of recourse can result in adjustments to the desired privacy level towards actual privacy level.
People do so when they feel that they have no boundary control options to fend off repeated encroachments.
After all maybe we are not looking at a privacy paradox.
What we are seeing is a certain shift in norms and tensions between what our participants want, increased control and transparency and what they realize they can get.
This is where our participants exhibit a pragmatic attitude and choose the benefits of owning and using a smartphone without limitations over the cost that comes with guarding their personal data from overprivileged applications.
Smartphone users do not want to uninstall all of their overprivileged applications.
They accept that this is payment for getting something for "free", and they feel at ease with paying for it as long as nothing negative happens, or as one respondent phrased it: "I believe the adage: "When you think you're getting a free lunch; you're actually being served to someone else."
It's a trade-off for free entertainment."
Learned helplessness typically happens when people come to believe that a situation is unchangeable or inescapable and will often construct reasons for why this is so even if solutions become available later on.
Consider the following statements: "I silently accept it.
When you make me think about it, I kind of don't like it, but have probably forgotten all about it next time I download an app."
Because it is so ubiquitous, I think that it's likely that this sort of thing will never go away."
The implication here is that perhaps there are other explanations for what has been termed the privacy paradox , beyond decisionmaking conundrums and situational constraints.
When informed of information sharing practices of certain apps on their smartphone, our research participants for the most part expressed dismay, even outrage -- but then proceeded with business as usual when it came to using their smartphones.
To the directed advertising industry, this can be seen as good news, suggesting that consumers may protest information sharing but deep down do not really care enough to actually alter their behavior.
We take from our findings a different message: that at this stage in the smartphone era, many people are essentially creeped out  by the information sharing behaviors of the apps with which they have outfitted this extension of their personal space and primary territory.
They may have formally agreed to it, and are able to rationalize their use of such apps when asked; and they may prefer to put the creepiness out of their minds in order to enjoy their moments of interacting with the apps; but this foundation of creepiness undermines and makes precarious the standing of the smartphone as part of daily life.
From a business perspective, our finding is that any business model that depends on taking personal data from smartphone users without first establishing a solid basis for the level of emotional trust this entails is ripe for disruption.
We do not see the solution to these privacy issues in improving EULAs and in making informed consent more robust.
Given that privacy concerns are generally rooted in complex social configurations, such approaches would seem to depend on people predicting that something might go badly and mitigating information flows on the front end.
The reality is, however, that people have great difficulty predicting such outcomes because in fact there are far too many variables to consider - and the vast majority of times the negative consequences are the result of "not thinking" ahead.
Altman notes that repeated invasion into a primary territory can have serious consequences to a person's self-identity and inability to regulate access can in the long run cause a lack of self-esteem .
Invading a person's privacy is paramount to taking control of a person's life away from that person which can seriously affect an individual's autonomy and dignity .
According to Altman "it is a loss of control to others that is serious, not so much the mere exposure of information" .
Repeated invasions into a persons' privacy and a conviction that there is no recourse can result in learned helplessness , when people stop responding to invasions even when presented with ways to defend themselves.
Learned helplessness was originally identified in an experiment where a dog was put in an inescapable situation, as it was experiencing electric shock.
Shortly thereafter, it was put into a different kennel where it could stop the shocks by performing a simple action.
The dog did not attempt to evade the shocks; it just remained seated and stoically endured electric shocks, while the dogs in the control group quickly learned how to avoid the shocks .
Instead, our analysis suggests that designers of apps and their data sharing policies need to confront the nature of creepiness head on.
For this, we need a practical theory of creepiness, its varieties, and its temporalities .
This paper is a first step in this direction but much more work needs to be done.
We also need apps that behave, and can be seen to behave, in ways that respect the user's personal space and the integrity of the context of use, much better than they currently do.
Apps like High-Powered Flashlight or Fruit Ninja present themselves as simple, even trivial consumer products or services; people expect them to be simply fun ways of outfitting their personal space .
Their creepiness lies in the realization that they are more than they seem - they are actually conduits for personal information to "leak"  out of my personal space, indeed, out of myself.
Upon discovering their underlying information gathering and dispersing nature, I suddenly find that I am actually sharing my personal space with unknown, and unruly entities that I did not really invite in, regardless of the EULA.
Telling people, in rational terms, that apps have this nature -- of extending me beyond my personal space, and of allowing outside influences in -- is likely not enough.
Perhaps we  need to find ways to make people feel apps' active, connective nature in some visceral way.
So far, the only visceral way people feel these sensations of discomfort and personal zone invasion/boundary collapse is what gets termed creepy.
Are there more positive, visceral, affective responses that we could design for?
Or can apps become effectively transparent in this regard -- rather than a EULA, their design itself communicating a sense of access/thrill/risk .
Altman posited negative psychological consequences if the person whose privacy is repeatedly violated is unable to regain control and to successfully manage access to the self .
From our data, we are not able to demonstrate that there are long-term negative consequences for consumers .
But we suspect there are, perhaps through mechanisms that create and sustain bioneurological stress, the harmful health consequences of which are increasingly being documented.
We also acknowledge that creepy experiences may not always be negative and unwanted, as recent HCI research into "uncomfortable user experience" has begun to explore .
From a values in design perspective, what is at stake here is the kind of future we want to live in, beyond just how we wish people would use apps and what kind of responsibility we wish they could take for their own data.
The issue of designing the kinds of sharing and disclosure applications is in part hitting the right balance between what can be seen and what can be kept secret, between control and automaticity.
In part it is also about pushing changes in the norms around information sharing -- we are fumbling with the now norms rather than looking to the where these norms might move as a result of technologies we are developing.
Cultural norms change over time, subject to a great many short- and long-term forces, technical, economic, social, and legal.
Acceptable entertainment app behavior in 2013 will likely be different than in 2023.
This will not eliminate creepy experiences, but will change the conditions under which they are encountered.
Nor does the inevitability of cultural change diminish the need for responsible, valuesensitive design.
Learned helplessness: Theory and evidence.
Understanding users' requirements for data protection in smartphones.
Nafus, D. & Tracey, K.  Mobile phone consumption & concepts of personhood.
Perpetual contact: Mobile communication, private talk, public performance.
Chicago ; London: The University of Chicago Press.
Nissenbaum, H. F.  Privacy in context: Technology, policy, and the integrity of social life.
Stanford Law Books, Stanford, CA.
Norberg, P., Horne, D. & Horne, D. The privacy paradox: Personal information disclosure intentions versus behaviors.
Oksman, V. & Rautiainen, P.  "Perhaps it is a Body Part": How the Mobile Phone Became an Organic Part of the Everyday Lives of Finnish Children and Teenagers.
Machines that become us: The social context of personal communication technology.
Transaction Publishers, New Brunswick, N.J 34.
Palen, L. & Dourish, P. Unpacking "privacy" for a networked world.
Learned helplessness: A theory for the age of personal control.
Rose, N.  Powers of Freedom: Reframing Politica l Thought, NY: Cambridge University Press 37.
Unlocking the privacy paradox: do cognitive heuristics hold the key?
In CHI '13 Extended Abstracts, Paris, France: ACM 39.
Vijayan, J. Flashlight app vendor settles with FTC over privacy violations.
The personalization privacy paradox: An exploratory study of decision-making process for location-aware marketing.
Taming information-stealing smartphone applications .
Unpicking the privacy paradox: can structuration theory help to explain location-based privacy decisions?
