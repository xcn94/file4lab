ABSTRACT A recent development in advanced interface technologies is the virtual acoustic display, a system that presents threedimensional auditory information over headphones .
The utility of such a display depends on the accuracy with which listeners can localize the virtual, or simulated, sound sources.
Synthesis of virtual sources involves the digital filtering of stimuli using filters based on acoustic HeadRelated Transfer Functions  measured in human ear-canals.
In practise, measurement of the HRTFs of each potential user of a 3-D display may not be feasible.
Thus, a critical research question is whether listeners from the general population can obtain adequate localization cues from stimuli based on non-individualized filters.
In the present study, 16 inexperienced listeners judged the apparent spatial location  of wideband noisebursts that were presented either over loudspeakers in the fkee-field  or over headphones.
The headphone stimuli were synthesized using HRTFs from a representative subject in a previous study .
Localization of both t%efield and virtual sources was quite accurate for 12 of the subjects, 2 showed poor elevation accuracy in both free-field and headphone conditions, and 2 showed degraded elevation accuracy only with virtual sources.
High rates of confusion errors  were also observed for some of the subjects and tended to increase for the virtual sources.
In general, the data suggest that most listeners can obtain useful directional information from an auditory display without requiring the use of individually-tailored HRTFs, particularly for the dimension of azimuth.
However, the high rates of confusion errors remain problematic.
Several stimulus characteristics which may help to minimize these errors are discussed.
Recently, a considerable amount of attention has been focused on the development of a truly three-dimensional, interactive display called the virtual interface .
As with most research in information displays, virtual displays have generally emphasized visual information.
Many investigators, however, have pointed out the importance of the auditory system as an alternative or supplementary information channel .
A threedimensional auditory display cart potentially enhance information transfer by combining directional and iconic information in a quite naturalistic representation of dynamic objects in the interface.
Primary advantages of the binaural auditory system include the ability to monitor and identify sources of information from all possible locations, improved intelligibility of sources in noise and enhanced segregation of multiple sound sources , and in conjunction with other modalities, the reinforcement of information combined with a greater sense of presence or realism .
These features could be critical for applications that are inherently spatial, such as an air traffic control display, applications involving encoded non-speech messages, such as the acoustic "visualization" of multidimensional data  or alternative interfaces for the visually-impaired , and telepresence applications, such as advanced teleconferencing environments and shared electronic workspaces .
Thus, the combination of veridical spatial cues with good principles of iconic design could provide an extremely powerful and information-rich display which is also quite easy to use, A three-dimensional auditory display can be realized with an array of real sound sources or loudspeakers .
An alternative approach is to generate externalized, threedimensional sound cues over headphones in realtime using digital means .
Headphone presentation is desirable because it enables complete control over the acoustic waveforms delivered to the two ears, a feature which is critical to the control of apparent spatial position.
The realtime capability, coupled with a head-tracking device, allows the user to experience virtual sounds interactively, virtual sources can thus be either moving or static and they can respond appropriately to the listener's head movements.
Permission to copy without fee all or part of this material is grantad provided that the copies are not made or distributed for direct commercial adventage, the ACM copyright notice end the title of the publication and ite date appear, and notice ie given that copying ia by permission of the Association for Computing Machinary.
To copy otherwiaa, or to rapublish, requires a fea andlor specific permission.
The utility of a 3-D auditory display depends critically on the user's ability to localize the various sources of information in auditory space.
Thus, the design of a 3-D auditory display must carefully consider the acoustic cues needed by human listeners for accurate localization.
Since the time of Lord Rayleigh , much of the research on human sound localization has emphasized the role of two primary cues, intermural differences in time of arrival at lowfrequencies and interaural differences in intensity at high frequencies.
However, more recent research points to serious limitations with this view, identifying the important contribution to localization of the direction-dependent filtering which occurs when incoming sound waves interact with the outer ears or pinnae.
Experiments show that spectral shaping by the pinnae is highly directiondependent , that the absence of pinna cues degrades localization accuracy , and that pinna cues are primarily responsible for externalization or the "outside-the-head" sensation .
Such data suggest that perceptually-veridical localization over headphones might be possible if the spectral shaping by the pinnae as well as the intermural differences of time and intensity can be adequately synthesized.
One technique for capturing both pinnae and intermural difference cues involves binaural recording with microphones placed in the cars of a manikin  or the earcanals of a human .
When stimuli recorded this way are presented over headphones, there is an immediate and veridical perception of 3-D auditory space .
Our procedure is closely related to binaural recording.
Rather than record stimuli directly, we measure the acoustical transfer functions, from free-field-to-eardrum, at many source positions, and use these transfer functions as the basis of filters with which we synthesize stimuli.
The transfer functions are generally called Head-Related Transfer Functions  and are measured in the ear-canals of individual subjects .
The working assumption of the synthesis technique is that if, using headphones, one can produce ear-canal waveforms identical to those produced by a free-field source, the freefield experience will be duplicated.
The only conclusive test of this assumption must come from psychophysical studies in which the perception of real and synthesized stimuli are directly compared.
HRTFs from each potential listener.
It may also be the case that the user of such a display may not have the opportunity for extensive training.
Thus, a critical issue for the design of virtual acoustic displays is the degree to which the general population of listeners can obtain adequate localization cues from stimuli based on non-individualized HRTFS.
There is some evidence that the ability to localize, and especially to determine the elevation of a sound source, appears to depend on the use of acoustical cues provided by one's own ears .
Localization with the non-individualized transforms was only somewhat degraded compared to performance with both free-field sources and individualized HRTFs, as long as the transforms were derived from what one might call a "good Iocalizer .
Further, when performance degradations did occur, they were primarily seen in the subjects' elevation judgments for all stimulus conditions.
The individual differences in performance appeared to be correlated with the presence or absence of elevation-dependent, acoustical features in the 5 to 10 kHz region of the subjects' HRTFs .
The preliminary results of Wenzel, et.
The present study represents a more comprehensive comparison of free-field and virtual free-field localization by inexperienced listeners using nonindividualized transfer functions.
The virtual free-field stimuli were generated digitally using HRTFs measured in the ear canals of a representative subject, SDO, from the study by Wightman & Kistler .
A relatively large number of inexperienced subjects were tested in an attempt to assess whether the general population of listeners could readily use a virtual acoustic display,
Sixteen young adults served as paid volunteers.
All had screening normal hearing, verified by standard audiometric at 15 dB HL, and reported no history of hearing problems.
None of the subjects had any previous experience in psychoacoustical experiments, and all were naive regarding the purpose of the experiment.
HRTFs would be the most likely to replicate the free-field experience for a given listener.
In general, Wightman & Kistler reported that localization accuracy for the free-field and headphone stimuli was comparable for the experienced listeners of their study.
However, some minor degradation of localization accuracy was observed with headphonepresented sounds.
The rate of front-back confusions increased from about 6 to 1170 and source elevation appeared to be less well-defined for virtual sources.
The basic stimulus consisted of a train of eight, 250-ms bursts of Gaussian noise with 20-ms, cosine-squared ramps at onset and offset and 300-ms of silence between the bursts.
The overall level of the train was approximately 70 dB SPL in both the tke-field and headphone conditions.
The scrambling algorithm divided the spectrum into adjacent critical bands and assigned a random intensity level  to the noise in each band.
Stimuli for a given subject and a given set of trials were pre-computed and converted to analog form via 16-bit D/A converters at a rate of 50 kHz.
The stimuli were transduced either by loudspeakers or by headphones.
Details of the pmentation system can be found in .
In the free-field case, six speakers were mounted on a semicircular arc, 2.76 m in diameter, at intervals of 18 degrees elevation .
The arc was located in an anechoic chamber and could be rotated around the vertical axis, allowing stimulus presentation at any azimuth, with the subject seated on an adjustable stool such that his/her head was at the center of the am.
For the headphone conditions, each stimulus was digitallyprocessed so that it would simulate a particular free-field location.
The processing was based on the directionspecific, outer ear characteristics  measured for subject SDO in the study by Wightman & Kistler.
Briefly, HRTFs are estimated by deconvolving the loudspeakers, test stimulus, microphone, and headphone responses from the recordings made with probe microphones.
Further details of the measurement technique, synthesis procedure, and psychophysical task can be found in .
Subjects heard stimuli from 24 different source positions in the free-field  and the same 24 positions in the virtual free-field  conditions.
The source locations were chosen from the 144 positions tested by Wightman & Kistler  with the aim of sampling the possible range of azimuths and elevations equally.
Each of the 24 positions was heard once in a single block of trials with a different randomized order for each 10-minute block.
A total of 18 blocks of trials were run over a period of two days, with subjects giving 9 judgments of apparent spatial location for each of the 24 locations.
Blocks of trials for the fiedield and headphone conditions wem alternated in order to minimize order effects.
Prior to the experimental runs, a 15-minute training session was conducted which included a verbal explanation of the response co-ordinates and a practice block in the free-field condition.
Relatively few positions were tested and minimal training given because it was important for the purposes of this study that subjects' performance reflect that of listeners inexperienced with locrdizing free-field or virtual free-field stimuli.
At the beginning of each session, subjects were blindfolded, led into the artechoic chamber, and seated at the center of the loudspeaker arc, Subjects never saw the inside of the anechoic chamber at any time during the study, whether in the free-field or headphone conditions.
The subject was instructed to orient straight ahead and not move the head while a trial was in progress.
Head position and stability was verified visually by the experimenter.
For each trial, the subjects heard the train of eight identical 250-ms bursts of spectrally-shaped noise, and called out estimates of azimuth and elevation during a 5-s response interval.
The experimenter recorded their responses and no feedback was given.
Following a free-field block, the subjects remained in the anechoic chamber but donned the headphones in preparation for the virtual free-field conditions.
The trial sequence was essentially the same as for the free-field condition.
The goal of this experiment was to compare the inexperienced subjects judgments of the apparent locations of sound sources in free-field with their judgments of locations in a virtual free-field.
The virtual sources were synthesized from a "generic" set of HRTFs in the sense that they derived from a single individual, SDO, whose localization accuracy was representative of seven of the eight subjects in the study by Wightman and Kist.ler .
The data from this subject showed good localization accuracy in both azimuth and elevation and an average  rate of front-back confusions in both free-field and headphone conditions.
Further, we hoped to compare the data from this study with those from the previous experiment in which free-field performance was contrasted with headphone performance with each subject using his/her own HRTFs.
Thus, the same "absolute judgement" paradigm that was used to estimate perceived spatial location in the Wightman & Kistler  experiment was also used here.
In both freefield and headphone conditions, the subject indicates the apparent spatial position of a sound source by calling out numerical estimates of azimuth and elevation  using a modified spherical co-ordinate system.
Analysis of the results of a localization study such as the one described here is complicated by two factors.
One is the fact that the stimuli and responses are represented by points in three-dimensional space; in particular, as points on the surface of a unit-sphere since distance remained constant in this experiment.
For these spherically-organized data, the usual statistics of means and variances are potentially misleading.
For example, art azimuth error of 15 degrees on the horizontal plane is much larger in terms of absolute distance than a 15-degree error at an elevation of 54 degrees.
A second complication in the analysis of localization data, observed in nearly all such studies, is the presence of frontback "confusions".
These are responses which indicate that a source in the front hemisphere, usually near the median plane, was perceived to be in the rear hemisphere.
Occasionally, the reverse situation is also heard.
Further, in this experiment we also observed confusions in elevation, with up locations heard as down, and vice versa.
These types of azimuth and elevation errors are illustrated in Figure 1.
It is difficult to know how to treat these types of Since the confusion rate is often low, errors fairly.
Otherwise, estimates of error would be greatly inflated.
On the other hand, if we assume that subjects' responses correctly reflect their perception, resolving such confusions could be misleading.
Thus, the rate of confusions is usually reported as a separate statistic.
Since we were primarily interested in comparing free-field and headphone conditions, we elected to resolve both front-back and up-down confusions and report the rates for each type of confusion.
If the angle between the target and judged location was made smaller by reflecting the judgement about the vertical plane passing through the subject's ears , the judgement was coded in reflected form and the confusion count was increased.
Judgement centroids were computed separately for each of the 24 positions in both the free-field and headphone conditions, for each of the 16 subjects.
Figures 2, 3, and 4 show the relation between target and response azimuth  and the panels on the right show the relation between target and response elevation  in both the free-field and virtual source conditions.
Because of space limitations, only three types of plots are shown which represent the three response patterns observed in the experiment.
Figure 2 plots centroids for 12 subjects who behaved similarly, while Figures 3 and 4 plot data from individual subjects who reflect the two remaining behavior patterns.
Table 1 shows the total percentage of azimuth confusions  and elevation confusions  for the free-field and headphone conditions, for each subject.
Chi-square values, which reflect whether a significant difference in confusion rates occurred between the the free-field and headphone conditions, are also given.
Table 2 gives a breakdown of the confusion percentages by hemisphere.
The data reveal several important features of localization behavior by inexperienced subjects.
One is the presence of individual differences evidenced in both the judgments of source identification and the confusion rates.
For example, three types of response patterns are shown in the plots of resolved location judgments.
An example of the first pattern can be seen in Figure 2.
Illustration of the types of confusion errors observed for location judgments in the study.
Reversals of perceived azimuth, with respect to the target azimuth, are shown on the left .
Analogously, elevation reversals are illustrated on the right .
ScatterPlots of centroids based on 12 individual subjects' centroids originally estimated from 9 judgments per target.
Actual source locations versus judged source locations in both the free-field  and headphone  conditions are plotted.
The panel on the left plots azimuth judgments and the panel on the right shows elevation judgments.
All 24 source positions are plotted in each panel.
Thus, data from 6 different source elevations are combined in the azimuth txmel and data from 18 different source azimuths are combined in the elevation panel.
Sources at "left" azimufis and "down" ~levations we shown as negative numbers.
Of the remaining subjects, all show comparable performance in azimuth judgments for both the free-field and headphone conditions .
However, SID and SIE  show poor localization in elevation for both free-field and virtual freefield conditions, while S11 and SIM  only show a degradation in elevation accuracy with virtual sources.
Thus, as in the experiment by Wightman & Kistler , differences among subjects emerge most clearly in the judgments of source elevation.
Perhaps the most striking feature of the data for these inexperienced listeners is the relatively high rate of azimuth confusions for both free-field and headphone conditions.
Reversals also tend to increase for virtual sources when compared to free-field conditions; azimuth confusion rates increased significantly for 11 of the subjects' judgments .
For the 8 experienced subjects of the Wightman & Kistler  study, front-back confusion rates for the freefield ranged from 3 to 12%  and approximately doubled under headphone conditions using the subjects' own HRTFs.
Here, half of the subjects also showed relatively low free-field confusion rates  which increased, on the average, by a factor of 3.8 for virtual sources synthesized from SDOS HRTFs.
The other 8 subjects had much higher free-field confusion rates for azimuth judgments  with an average increase of 1.26 for the headphone conditions.
A few recent studies have also examined azimuth reversal rates.
For example, the extensive free-field localization studies conducted by Oldfield & Parker  report average rates of 3.4Y0, and 26~0 for normal free-field and pinnae-occluded conditions, respectively.
Similarly, Makous & Middlebrooks  report an average rate of 6%  for free-field localization.
One subject showed low reversal rates  for both synthesis conditions while the other showed 7.470 and 14.8% for stimuli synthesized from his own and the other's HRTFs, respectively.
These authors also examined performance for sources synthesized from simplified HRTFs .
The rate of azimuth confusions increased substantially  as the modeled HRTFs were simplified, with the majority reversing to the rear for the smallest-order filters.
Curiously, one of the subjects showed consistently fewer reversals when listening with the other subject's HRTFs.
To our knowledge, no previous studies have analyzed confusions in elevation.
The rates observed here suggest that this type of reversal is less common; 14 of the 16 subjects showed free-field rates which were less than 10~0.
The data indicate that the attempt to simulate free-field localization for inexperienced subjects with nonindividualizcd transfer functions was largely successful for positions in azimuth when the criterion is the comparison to a subject's own free-field performance.
That is, the target-response plots imply strong correlations between the judged and actual source locations with a close correspondence between free-field and virtual sources.
Similar to Wightman & Kistler's  observations using individualized HRTFs, when performance degradations do occur with virtual stimuli they are in the dimension of elevation.
In fact, although judgement of elevation was poor for 4 of our subjects, only 2 showed a discrepancy between free-field and headphone conditions.
Azimuth judgments, on the other hand, appear to be uniformly robust when compared with free-field performance in both experiments.
Percentages of front-back  and up-down  confusions for free-field and headphone listening.
Chi-square statistics are also indicated for those subjects which showed a significant difference in azimuth and/or elevation confusions for the free-field versus headphone conditions.
Free-field data are in boldface type and data for the synthesized stimuli are in plain text.
Breakdown of confusion percentages by hemisphere for the free-field and headphone conditions.
Percentages for the free-field  and headphone  stimuli are shown.
Percentages were based on the total number of possible azimuth or elevation judgments.
Thus, a value of 50$% implies all judgments were reversed in that category.
As noted above, the most remarkable difference between the studies is in the high rate of front-back confusions.
For free-field versus simulated free-field stimuli, the experienced listeners exhibit azimuth confusion rates of about 6 vs. 11%  while our inexperienced listeners show average rates of about 19 vs. 31 Yo.
In both studies these confusions are largely due to reversals to the rear.
Further, although the average rates are lower, confusions in elevation are also a potential source of increased error in the simulation of localized sources.
In general, the data suggest that most listeners can obtain useful directional information from an auditory display without requiring the use of individually-tailored HRTFs, particularly for the dimension of azimuth.
However, the high rates of confusion errors remain problematic.
Comparison of these results to the study in which free-field listening is contrasted with localization of sources synthesized from a subject's own HRTFs , suggests that the use of non-individualized transforms primarily results in an increase in the rate of azimuth confusions, particularly front-to-rear confusions.
Note, though, that the existence of free-field confusions indicates that these reversals are not strictly the result of the simulation.
It is possible, as Asano, et.
The difference in free-field confusion rates between the inexperienced listeners of this experiment and the more experienced subjects of Wightman & Kistler tend to support this view.
Perhaps the higher rates seen for virtual sources are at least partially the result of unfamiliar stimulus features contributed by the synthesis process which can eventually be ovemome by adaptation.
Although the reason for such confusions is not completely understood, they are probably due in large part to the static nature of the stimulus and the ambiguity resulting from the so-called cone of confusion .
Assuming a stationary, spherical model of the head, a given intermural time difference correlates ambiguously with the direction of a sound source, with a conical shell describing the locus of all possible sources.
However, cone-of-confusion effects alone cannot explain a front-to-back response bias, and it may be that visual dominance plays a substantial role in auditory localization.
That is, given an ambiguous acoustic stimulus in the absence of an obvious visual correlate, it may be that the perceptual system resolves the ambiguity with a heuristic that assumes the source is behind where it can't be seen.
Several stimulus characteristics may help to minimize these errors.
For example, the addition of visual cues, dynamic cues correlated with head-motion, and well-controlled environmental cues derived from models of room acoustics may improve the ability to resolve these ambiguities.
A related problem in synthesizing veridical acoustic images is the fact that such stimuli sometimes fail to externalize, particularly when the signals are unfamiliar  and simulated from anechoic measurements of HRTFs.
However, the specific parameters used in such a model must be investigated carefully if localization accuracy is to remain intact.
For example, Blauert  reports that the spatial image of a sound source grows larger and increasingly diffine with increasing distance in a reverberant environment, a phenomenon which may tend to interfere with the ability to judge the direction of the source.
The simulation techniques investigated here provide both a means of implementing a virtual acoustic display and the ability to study features of human sound localization that were previously inaccessible due to a lack of control over the stimuli.
The availability of realtime control systems  further expand the scope of the research, allowing the study of dynamic, intersensory aspects of localization which may do much toward alleviating the problems encountered in producing the reliable and veridical perception which is critical for applied contexts.
