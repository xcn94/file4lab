What does a user need to know to productively work with an intelligent agent?
Intelligent agents and recommender systems are gaining widespread use, potentially creating a need for end users to understand how these systems operate in order to fix their agent's personalized behavior.
This paper explores the effects of mental model soundness on such personalization by providing structural knowledge of a music recommender system in an empirical study.
Our findings show that participants were able to quickly build sound mental models of the recommender system's reasoning, and that participants who most improved their mental models during the study were significantly more likely to make the recommender operate to their satisfaction.
These results suggest that by helping end users understand a system's reasoning, intelligent agents may elicit more and better feedback, thus more closely aligning their output with each user's intentions.
These intelligent agents are computer programs whose behavior only becomes fully specified after they learn from an end user's training data.
Because of this period of in-the-field learning, when an intelligent agent's reasoning causes it to perform incorrectly or unexpectedly, only the end user is in a position to better personalize--or more accurately, to debug--the agent's flawed reasoning.
Debugging, in this context, refers to mindfully and purposely adjusting the agent's reasoning  so that it more closely matches the user's expectations.
Recent research has made inroads into supporting this type of functionality .
Debugging, however, can be difficult for even trained software developers--helping end users do so, when they lack knowledge of either software engineering or machine learning, is no trivial task.
In this paper, we consider how much ordinary end users may need to know about these agents in order to debug them.
This paper, in contrast, considers whether users actually need a sound mental model, and how that mental model impacts their attempts to debug an intelligent agent.
Toward this end, we investigated four research questions: : Feasibility: Can end users quickly build and recall a sound mental model of an intelligent agent's operation?
To answer these research questions, we conducted an empirical study that investigates the effects of explaining the reasoning of a music recommender system to end users.
H.5.m : Miscellaneous; Intelligent agents have moved beyond mundane tasks like filtering junk email.
Search engines now exploit pattern recognition to detect image content ; Facebook and image editors take this a step further, making educated guesses as to who is in a particular photo.
Netflix and Amazon use collaborative filtering to recommend items of interest to their customers, while Pandora and Last.fm use similar techniques to create radio stations crafted to an individual's idiosyncratic tastes.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Half of the participants received detailed explanations of the recommender's reasoning, while the other half did not.
Our paper's contribution is a better understanding of how users' mental models of their intelligent agents' behavior impacts their ability to debug their personalized agents.
Mental models are internal representations that people build based on their experiences in the real world.
These models allow people to understand, explain and predict phenomena, and then act accordingly .
The contents of mental models can be concepts, relationships between concepts or events , and associated procedures.
For example, one mental model of how a computer works could be that it simply displays everything typed on the keyboard and "remembers" these things somewhere inside the computer's casing.
Mental models can vary in their richness--an IT professional, for instance, has  a much richer mental model of how a computer works.
Making an agents' reasoning more transparent is one way to influence mental models.
Examples of explanations by the agent for specific decisions include why... and why not... descriptions of the agent's reasoning , visual depictions of the assistant's known correct predictions versus its known failures , and electronic "door tags" displaying predictions of worker interruptibility with the reasons underlying each prediction  .
Recent work by Lim and Dey has resulted in a toolkit for applications to generate explanations for popular machine learning systems .
Previous work has found that users may change their mental models of an intelligent agent when the agent makes its reasoning transparent ; however, some explanations by agents may lead to only shallow mental models .
Agent reasoning can also be made transparent via explicit instruction regarding new features of an intelligent agent, and this can help with the construction of mental models of how it operates .
None of these studies, however, investigated how mental model construction may impact the ways in which end users debug intelligent agents.
Making an intelligent agent's reasoning transparent can improve perceptions of satisfaction and reliability toward music recommendations , as well as other types of recommender systems .
However, experienced users' satisfaction may actually decrease as a result of more transparency .
As with research on the construction of mental models, these studies have not investigated the link between end users' mental models and their satisfaction with the intelligent agent's behavior.
There are two main kinds of mental models: Functional  models imply that the end user knows how to use the computer but not how it works in detail, whereas structural  models provide a detailed understanding of how and why it works.
Mental models must be sound  enough to support effective interactions; many instances of unsound mental models guiding erroneous behavior have been observed .
Mental model completeness can matter too, especially when things go wrong, and structural models are more complete than functional models.
While a structural model can help someone deal with unexpected behavior and fix the problem, a purely functional model does not provide the abstract concepts that may be required .
Knowing how to use a computer, for example, does not mean you can fix one that fails to power on.
To build new mental models, it has been argued that users should be exposed to transparent systems and appropriate instructions .
Scaffolded instruction is one method that has been shown to contribute positively to learning to use a new system .
One challenge, however, is that mental models, once built, can be surprisingly hard to shift, even when people are aware of contradictory evidence .
There has been recent interest in supporting the debugging of intelligent agents' reasoning , but the mental models users build while attempting this task have received little attention.
An exception is a study that considered the correctness of users' mental models when interacting with a sensor-based intelligent agent that predicted an office worker's availability 
To explore the effects of mental model soundness on enduser debugging of intelligent agents, we needed a domain that participants would be motivated to both use and debug.
Music recommendations, in the form of an adaptable Internet radio station, meet these requirements, so we created an Internet radio platform  that users could personalize to play music fitting their particular tastes.
To match real-world situations in which intelligent agents are used, we extended the length of our empirical study beyond a brief laboratory experiment by combining a controlled tutorial session with an uncontrolled period of field use.
The study lasted five days, consisting of a tutorial session and pre-study questionnaires on Day 1, then three days during which participants could use the AuPair prototype as they wished, and an exit session on Day 5.
AuPair allows the user to create custom "stations" and personalize them to play a desired type of music.
Users start a new station by seeding it with a single artist name .
Users can debug the agent by giving feedback about individual songs, or by adding general guidelines to the station.
To add general guidelines about the station, the user can tell it to "prefer" or "avoid" descriptive words or phrases .
Users can also limit the station's search space .
AuPair was implemented as an interactive web application, using jQuery and AJAX techniques for real-time feedback in response to user interactions and control over audio playback.
We supported recent releases of all major web browsers.
A remote web server provided recommendations based on the user's feedback and unobtrusively logged each user interaction via an AJAX call.
AuPair's recommendations were based on The Echo Nest , allowing access to a database of cultural characteristics 
We built our music library by combining the research team's personal music collections, resulting in a database of more than 36,000 songs from over 5,300 different artists.
The Echo Nest developer API includes a dynamic playlist feature, which we used as the core of our recommendation engine.
Dynamic playlists are put together using machine learning approaches and are "steerable" by end users.
Artist similarity in AuPair was based on cultural characteristics, such as the terms used to describe the artist's music.
The algorithm uses a clustering approach based on a distance metric to group similar artists, and then retrieves appropriate songs.
The user can adjust the distance metric  by changing weights on specific terms, causing the search to prefer artists matching these terms.
The opposite is also possible--the algorithm can be told to completely avoid undesirable terms.
Users can impose a set of limits to exclude particular songs or artists from the search space.
Each song or artist can be queried to reveal the computer's understanding of its acoustic and cultural characteristics, such as its tempo or "danceability".
Our study was completed by 62 participants, , ranging in age from 18 to 35.
Only one of the 62 reported prior familiarity with computer science.
These participants were recruited from Oregon State University and the local community via e-mail to university students and staff, and fliers posted in public spaces around the city .
Participants were paid $40 for their time.
We randomly assigned participants to one of two groups--a With-scaffolding treatment group, in which participants received special training about AuPair's recommendation engine, and a Without-scaffolding control group.
Upon arrival, participants answered a widely used, validated selfefficacy questionnaire  to measure their confidence in problem solving with a hypothetical  software application.
Both groups then received training about AuPair, which differed only in the depth of explanations of how AuPair worked.
The Without-scaffolding group was given a 15minute tutorial about the functionality of AuPair, such as how to create a station, how to stop and restart playback, and other basic usage information.
The same researcher provided the tutorial to every participant, reading from a script for consistency.
To account for differences in participant learning styles, the researcher presented the tutorial interactively, via a digital slideshow interleaved with demonstrations and hands-on participation.
The With-scaffolding group received a 30-minute tutorial about AuPair  that was designed to induce not only a functional mental model , but also a structural mental model of the recommendation engine.
This "behind the scenes" training included illustrated examples of how AuPair determines artist similarity, the types of acoustic features the recommender "knows" about, and how it extracts this information from audio files.
Researchers systematically selected content for the scaffolding training by examining each possible user interaction with AuPair and then describing how the recommender responds.
For instance, every participant was told that the computer will attempt to "play music by similar artists", but the Withscaffolding participants were then taught how tf-idf  was used to find "similar" artists.
In another instance, every participant was shown a control for using descriptive words or phrases to steer the agent, but only With-scaffolding participants were told where these descriptions came from .
After this introduction, each participant answered a set of six multiple-choice comprehension questions in order to establish the soundness of their mental models.
Each question presented a scenario , and then asked which action, from a choice of four, would best align the station's recommendations with the stated goal.
Thus, as a measure of confidence, each question also asked how many of the choices could be eliminated before deciding on a final answer.
A seventh question asked participants to rate their overall confidence in understanding the recommender on a 7-point scale.
The entire introductory session  lasted 30 minutes for Without-scaffolding participants, and 45 minutes for With-scaffolding participants.
Both groups received the same amount of hands-on interaction with the recommender.
Over the next five days, participants were free to access the web-based system as they pleased.
We asked them to use AuPair for at least two hours during this period, and to create at least three different stations.
Whenever a participant listened to music via AuPair, it logged usage statistics such as the amount of time they spent debugging the system, which debugging controls they used, and how frequently these controls were employed.
After five days, participants returned to answer a second set of questions.
These included the same self-efficacy and comprehension questionnaires as on Day 1 , plus the NASA-TLX survey to measure perceived task load .
We also asked three Likert-scale questions about user's satisfaction with AuPair's recommendations, using a 21-point scale for consistency with the NASA-TLX survey, and the standard Microsoft Desirability Toolkit  to measure user attitudes toward AuPair.
We used participants' answers to the comprehension questions described earlier to measure mental model soundness.
Each question measured the depth of understanding for a specific type of end user debugging interaction, and their combination serves as a reasonable proxy for participants' understanding of the entire system.
These values were summed for each question i to create a participant's comprehension score, ranging from -24  to +24 .
Mental models evolve as people integrate new observations into their reasoning , and previous studies have suggested that participants may adjust their mental models while working with an intelligent agent that is transparent about its decision-making process .
Hence, we also calculated mental model transformation by taking the difference of participants' two comprehension scores .
This measures how much each participant's knowledge shifted during the study, with a positive value indicating increasing soundness, and a negative value suggesting the replacement of sound models with unsound models.
Table 1 lists all of our metrics and their definitions.
Understanding how intelligent agents work is not trivial-- even designers and builders of intelligent systems may have considerable difficulty .
Our first research question  considers the feasibility of inducing a sound mental model of an algorithm's reasoning process in end users--if participants fail to learn how the recommender works given a human tutor in a focused environment, it seems unreasonable to expect them to learn it on their own.
We tested for a difference in mental model soundness  between the With-scaffolding group and the Without-scaffolding group.
Neither group's mean comprehension score changed significantly during the 5-day study .
Participants also showed differences in their perceived mental model soundness, at least at first.
On Day 1, the Without-scaffolding group was significantly less certain that they accurately understood how the system selected songs and responded to feedback  than the With-scaffolding group  .
By Day 5, however, the Without-scaffolding group's responses had risen to a mean of 5.25, with no evidence of statistical difference against the With-scaffolding group .
These results provide insights into four aspects of the practicality of end users comprehending and debugging the reasoning of an intelligent agent.
First, even a short 15-minute scaffolding tutorial effectively taught participants how the recommender "reasoned".
With-scaffolding participants were significantly more likely to correctly and confidently answer the comprehension questions.
This in turn suggests that the With-scaffolding participants should be better equipped to debug the recommender's reasoning than the Without-scaffolding participants, a point we investigate in RQ2.
Responses to comprehension questions .
Response to Likert question "Are you confident all of your statements are accurate?"
Post-task mental model soundness minus pre-task mental model soundness.
Number of actions a participant used to debug the playlist , from the automated log files.
Length of time a participant spent on the task, i.e.
Response to Likert question "Do you feel the effort you put into adjusting the computer was worth the result?"
Response to Likert question "How satisfied are you with the computer's playlists?"
This is in contrast to recent work in interactive machine learning, which has found that for some systems , repeated use taught people the most salient aspects of how the system worked .
Third, the soundness of participants' mental models largely persisted for the duration of the study.
This appeared to be the case for both the Without-scaffolding and Withscaffolding groups, with neither groups' comprehension scores significantly changing between Day 1 and Day 5.
This bodes well for end users retaining and recalling sound models initially learned about an intelligent agent.
Fourth, however, is the issue of initially building unsound models: once incorrect models were built, they were hard to shift.
Even though the Without-scaffolding group formed less sound mental models, their confidence in their mental models increased, suggesting that they had convinced themselves they were, in fact, correct.
Making in situ explanations available on an ongoing basis, such as in , may be a way to address this issue.
Together, these findings provide evidence that furnishing end users with a brief explanation on the structure of an intelligent agents' reasoning, such as the attributes used, how such attributes are collected, and the decision-making procedure employed, can significantly improve their mental model's soundness.
A recommender's effectiveness is in the eye of the beholder.
Personalized recommendations cannot have a "gold standard" to measure accuracy--only the end users themselves can judge how well an agent's recommendations match their personal tastes.
Hence, for our second research question , we turned to a pair of more appropriate measures to explore the effects of mental model soundness on "accuracy"--cost/benefit and participant satisfaction.
In theory, a sound mental model enables a person to reason effectively about their best course of action in a given situation .
Thus, we expected participants with sounder mental models  to debug more effectively than those with less sound models.
For example, knowing that the recommender could be steered more effectively by using unique, highly specific words  rather than broad, common descriptors  should have helped such participants debug the agent's reasoning more effectively than participants who did not understand this.
Surprisingly, when using participants' perceptions of cost/benefit as a surrogate for effectiveness, the soundness of participants' mental models showed little impact on this measure of debugging effectiveness.
However, mental model transformation was tied with cost/benefit:
Participants' opinions of effectiveness were confirmed by their debugging interactions to adjust or assess AuPair's recommendations .
The count of these debugging interactions was significantly correlated with the improvement in mental model soundness for Withscaffolding participants, while no such correlation existed among Without-scaffolding participants .
Sounder changes to the mental model, then, may have had a positive effect on debugging, whereas changes in an initially unsound model did not serve the Without-scaffolding participants as well.
Further, participants who most improved the soundness of their mental models spent significantly less time on their interactions than others .
In light of the increases in perceived cost/benefit and debugging interactions, this suggests positive mental model transformations were linked to more efficient debugging.
An alternative explanation of the above results is that debugging interactions were responsible for participants' mental model transformations, rather than the other way around.
Recall, however, that the Without-scaffolding group showed no correlation between debugging interactions and mental models .
Thus, the evidence suggests that it was the in situ enhancement of relatively sound models that was linked to improved attitudes toward debugging.
Our second measure of debugging effectiveness and the accuracy of the result was participants' satisfaction with AuPair's resulting recommendations.
To measure this, we asked participants  "How satisfied are you with the computer's playlists?"
As with the cost/benefit results, neither treatment nor mental model soundness was predictive of participant satisfaction .
However, here again, transformation of mental models appeared to matter-- mental model transformation was marginally predictive of how satisfied participants felt with AuPair's playlists .
For example, the participant whose mental model's soundness decreased the most expressed dissatisfaction and a feeling of being unable to control the computer: "The idea is great to be able to `set my preferences', but if the computer continues to play what I would call BAD musical choices--I'd prefer the predictability of using Pandora."
Conversely, one of the participants whose mental model most increased in soundness expressed a feeling of being more in control: "I like the idea of having more control to shape the station.
Controls made sense and were easy to use.
The user has a lot of options to tune the station."
Perceived cost/benefit from debugging the recommender was also significantly correlated with participant satisfaction --further evidence that satisfaction was indicative of an increased ability to debug the agent's reasoning.
To ensure that participant satisfaction was not simply a result of time and effort invested, we tested for a relationship between reported satisfaction and the number of debugging interactions each participant performed, but found no evidence of a correlation .
It should be noted that one additional factor may have affected participant satisfaction.
Our music database held songs by just over 5,300 artists--pandora.com, by comparison, has over 80,000 different artists .
As one participant commented: "The songs played weren't what I was looking for, the selection was poor.
The system itself was excellent, but I need more music."
Despite this potential factor, the confluence of several metrics  suggests that transformations in mental model soundness translated to an improved ability to debug the recommender's reasoning, resulting in more satisfaction with AuPair's recommendations.
Because our evidence suggests mental model transformations  helped participants debug more efficiently and effectively, continuing to provide explanations of an intelligent agent's reasoning while end users interact with the agent may help to increase their ultimate satisfaction with the agent's decisions.
Such online explanations, however, were not investigated by the current study; we focused our exploration on the impact of explanations prior to  user interaction with an intelligent agent.
Instead, the most effective participants may have learned to debug by using the system.
However, this alternative explanation is weakened by the fact that the prototype was not transparent about how it made its decisions; the only time when participants were presented with explanations of AuPair's reasoning occurred during the With-scaffolding tutorial.
Presenting a complex system to unsuspecting users could overwhelm them.
We are particularly concerned with peoples' willingness to debug intelligent agents--some people  may perceive a risk that their debugging is more likely to harm the agent's reasoning than to improve it.
Similarly, computer anxiety  is known to negatively impact how  people use technology, and is negatively correlated with computer self-efficacy .
As Table 3 shows, almost three-quarters of the Withscaffolding participants experienced an increase in their computer self-efficacy between Day 1 and Day 5.
Withoutscaffolding participants, conversely, were as likely to see their computer self-efficacy decrease as to increase.
A X2 comparison showed that With-scaffolding participants were significantly more likely than a uniform distribution  to increase their computer self-efficacy .
This suggests that exposure to the internal workings of intelligent agents may have helped to allay, rather than to increase, participants' perceived risk of making their personalized agents worse.
As further evidence that it was understanding how the system worked  that influenced participants' computer self-efficacy, participants' perceived mental model soundness was significantly correlated with their computer self-efficacy at the end of the study .
Additionally, there was no evidence of a correlation between the number of debugging interactions participants made and their self-efficacy at the end of the Did Improve Withoutscaffolding Withscaffolding 16 22 Self-Efficacy Did Not Improve 16 8 Average Change 3.29% 5.90%
Thus, participants who at least thought they understood the nuances of AuPair's reasoning scored higher on the computer self-efficacy questionnaire than those who expressed little confidence in their knowledge of the recommender's logic.
We hope further research will shed additional light on this preliminary link between learning how an intelligent computer program reasons, and increasing levels of computer self-efficacy .
Challenging tasks, when successfully accomplished, have been found to have a significantly larger impact on self-efficacy than overcoming small obstacles .
Personalizing intelligent agents seems exactly the sort of difficult computer task that, successfully carried out, may make people say, "If I could do that, surely I can do this...", thereby reducing the obstacles of risk and anxiety toward future computer interactions.
For our final research question, we looked at the potential effects of mental model soundness on perceptions of experience, such as cognitive demands and emotional responses.
Prior work has found that explaining concrete decisions of an intelligent agent's reasoning to end users in situ created an increase in participants' frustration with, and mental demand of, debugging the agent  .
We suspected that end users might experience similar effects when presented with prior structural knowledge.
However, the With-scaffolding participants showed no significant difference to Withoutscaffolding participants' TLX scores.
While acquiring a sound mental model undoubtedly requires mental effort on the part of end users, we encouragingly found no evidence that this was any greater than the mental effort required to interact with an intelligent agent without a clear understanding of its underpinnings.
This suggests that end users' experience with intelligent agents does not necessarily suffer when they are exposed to more knowledge of how the agent works.
We used the Microsoft Desirability Toolkit  to investigate participants' user experience with the AuPair music recommender.
Participants were given a list of 118 adjectives and asked to underline each one they felt was applicable to their interactions with AuPair.
While not statistically significant between groups, these numbers suggest that the With-scaffolding participants  may have viewed the overall experience of interacting with AuPair in a more positive light than Without-scaffolding participants.
Participants' descriptions revealed a subtler picture of the difficulties they faced.
Word clouds--in which a word's frequency is indicated by its size--of the negative descriptions show that the With-scaffolding group's complaints may have stemmed more from difficulties using the system than difficulties understanding it; these participants were apt to complain the system was "simplistic", "annoying", and "frustrating" , while the Without-scaffolding group appeared to have trouble even understanding the impact of their debugging interactions, citing the system as "confusing", "complex", "overwhelming", and "ineffective" .
Participants' choices of positive descriptions provide further evidence the With-scaffolding participants' mental models contributed positively to interacting with the agent .
The phrase "easy to use" dominated their responses, alongside "innovative" and "accessible".
In contrast, the Without-scaffolding participants focused on the visual appearance of the agent, with words like "clean" and "appealing".
Participants with a deeper understanding of the system may have placed more emphasis on the interaction experience than aesthetics.
This paper provides the first empirical exploration of how mental models impact end users' attempts to debug an intelligent agent.
By scaffolding structural models for half of our study's participants, we learned that: * Despite the complexity inherent to intelligent agents, With-scaffolding participants quickly built sound mental models of how one such agent  operates "behind the scenes"--something the Withoutscaffolding participants failed to accomplish over five days.
Participants with the largest transformations were able to efficiently adjust their recommenders' reasoning, aligning it with their own reasoning better  than other participants.
These same participants were also likely to perceive a greater benefit from their debugging efforts.
Numerous benefits are associated with sound mental models, and in the case of this intelligent agent, it appears possible to gain these without impairing the user experience.
This is encouraging for the feasibility of enduser debugging of recommendation systems , especially when the user associates a benefit with debugging the agent's reasoning.
Our results suggest that such an approach could better support end-user personalization of intelligent agents--telling an end user more about how it does work may help him or her tell the agent more about how it should work.
We thank the study participants for their help and WengKeen Wong for comments on this paper.
Amershi, S., Fogarty, J., Kapoor, A., and Tan, D. Examining multiple potential models in end-user interactive concept learning.
Bandura, A. Self-efficacy: Toward a unifying theory of behavioral change.
Benedek, J. and Miner, T. Measuring desirability: New methods for evaluating desirability in a usability lab setting.
Bozionelos, N. The relationship of instrumental and expressive traits with computer anxiety.
Compeau, D. and Higgins, C. Application of social cognitive theory to training for computer skills.
Fiebrink, R., Cook, P., and Trueman, D. Human model evaluation in interactive supervised learning.
Hart, S. and Staveland, L. Development of a NASATLX : Results of empirical and theoretical research, Hancock, P. and Meshkati, N.
Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness.
Kapoor, A., Lee, B., Tan, D., and Horvitz, E. Interactive optimization for steering machine classification.
Kolb, D. A. Experiential Learning.
Kulesza, T., Wong, W.-K., Stumpf, S., Perona, S., White, R., Burnett, M., Oberst, I., and Ko, A. J. Fixing the program my computer learned: barriers for end users, barriers for the machine.
