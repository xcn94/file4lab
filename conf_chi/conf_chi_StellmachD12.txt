While eye tracking has a high potential for fast selection tasks, it is often regarded as error-prone and unnatural, especially for gaze-only interaction.
To improve on that, we propose gaze-supported interaction as a more natural and effective way combining a user's gaze with touch input from a handheld device.
In particular, we contribute a set of novel and practical gaze-supported selection techniques for distant displays.
Designed according to the principle gaze suggests, touch confirms they include an enhanced gaze-directed cursor, local zoom lenses and more elaborated techniques utilizing manual fine positioning of the cursor via touch.
In a comprehensive user study with 24 participants, we investigated the potential of these techniques for different target sizes and distances.
All novel techniques outperformed a simple gaze-directed cursor and showed individual advantages.
In particular those techniques using touch for fine cursor adjustments  and for cycling through a list of possible close-to-gaze targets  demonstrated a high overall performance and usability.
Regardless of the specific interaction setup used, the selection of targets is one of the fundamental tasks that need to be supported in any application.
Gaze is a promising input modality to bridge the gap between a user and a distant display as illustrated in Figure 1.
In this respect, it can even be a more efficient means for pointing tasks than traditional input devices .
Even though target acquisition seems to be a simple process which basically involves positioning a cursor and confirming a selection, it imposes several challenges when using eye gaze.
Among them are inherent inaccuracies caused by the physiological nature of our eyes and by measurement errors of the tracking systems which lead to jitter and offsets .
Thus, for more precise selections it is essential to address these two problems.
Jittering can, for example, be compensated by stabilizing the gaze cursor .
Offsets are difficult to handle as the degree of the offset is usually not known.
Common solutions for gaze interaction include: * Large-sized or magnified graphical user interfaces  , * A combination of gaze and manual  input to perform exact positioning manually , * Invisibly expand targets in motor space , * Intelligent algorithms to estimate the object of interest .
In this paper, we focus on the first two approaches as they provide high flexibility for diverse settings.
They neither require any changes of conventional GUIs, nor substantial a priori knowledge about the distribution of items.
Beside accuracy issues, the Midas touch problem , the unintentional execution of actions, is often described as one of the major challenges for gaze-based interaction.
The combination of gaze with other input modalities can solve these problems .
The diversity of display setups is increasing and with that is the need for more efficient means to interact with them.
While traditional mouse input works excellent for pointing tasks in desktop environments, it does not apply well for situations in which the user is standing in front of a powerwall or sitting on a couch to interact with a large-sized television set.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
This especially supports people who are not able to use their hands to interact with a digital system, for example, because of disabilities or because their hands are busy.
To alleviate the stated problems, this work aims at supporting precise target acquisition in a natural way while still maintaining sufficient task performance.
In this context, eye movements may serve well as a supporting interaction channel in combination with other input modalities, such as speech, hand and body gestures, and mobile devices.
We call this type of interaction gaze-supported interaction.
For this work, we propose to conveniently use eye gaze in combination with touch input from a handheld device following the principle gaze suggests and touch confirms.
Note that we do not aim at replacing or beating the mouse, but instead motivate gazesupported interaction for diverse display setups.
This may include public screens or large projection walls, for which we see a particularly high potential for this type of distant interaction.
Another specific goal of this work is to support accurate selections even of small and densely positioned targets.
In this paper, we contribute a set of practical and novel gazesupported selection techniques using a combination of gaze and touch input from a handheld touchscreen.
The techniques utilize principles such as target expansions and separating coarse and fine positioning of a selection cursor by means of gaze vs. manual touch input.
We carefully investigated the design space and developed five solutions which were tested and compared in a comprehensive user study.
In particular those techniques using touch for fine cursor adjustments  and for cycling through a list of possible close-to-gaze targets  resulted in very promising results with respect to their overall performance and perceived usability.
The remaining paper is structured as follows: First, we discuss how gaze has been used for target acquisition tasks in previous work.
Based on that we elaborated five gazesupported selection techniques that are described in the Design section.
These techniques have been tested by 24 participants which we report in the User Study section.
The paper concludes with a discussion of the results from the user study and an outlook to future work.
Fono and Vertegaal  use eye input with either dwell time or a key for zoom activation.
The latter was preferred by users over automatic activations.
For this, they introduce the concept of look-presslook-release.
On pressing a keyboard button, the viewed region is enlarged.
Different keys on the keyboard are assigned to various actions, such as single click, mouse over, and double click.
However, the magnified view is based on a screen capture.
Thus, no dynamics  are possible during this mode.
The point-of-interest moves towards the center of the screen while zooming and thus provides a better feedback for more precise selections.
They distinguish between discrete and continuous zooming tools for step-wise zooming.
While their zooming tools improve hit rates, it takes longer to perform a selection compared to the non-zooming interface.
The idea is to warp the cursor to the vicinity of the user's point-of-regard prior to moving the mouse.
Then the cursor can be manually positioned using the mouse for more precise selections.
Drewes and Schmidt  point out that the problem of this technique is overshooting: the cursor is only set to the gaze position after a mouse timeout and after the mouse is then moved again.
Thus, the mouse is already in motion when the pointer is positioned which is difficult to coordinate.
Instead, Drewes and Schmidt  use a touch-sensitive mouse button.
Thus, when touching the mouse key , the mouse pointer is set to the gaze position.
In general, gaze input has a high potential for fast pointing tasks and may even outperform traditional selection devices such as a mouse .
In the following, we will investigate gaze-based selections in combination or in context with  target expansions,  a manual input device,  distant displays, and  a supportive modality.
Empirical evidence shows that eye pointing speed and accuracy can be improved by target expansions .
In this respect, Ashmore et al.
Several studies indicate that gaze can be faster than mouse input .
In this respect, gaze-based input is acknowledged a particularly high potential for a more convenient interaction with high-density information on large  displays .
However, a main issue remains for accuracy versus speed.
If messages lie on top of each other, the user can look at them and they will get separated from each other.
A 3D push-and-pull gesture is used to control the zoom for different applications, such as a geographical information system.
Ware and Mikaelian  compare three gaze-supported selection techniques: a button press, gaze dwell, and an onscreen button to confirm a selection.
Although dwell time and button activations resulted in similar completion times, dwell-based activations were more error-prone.
Salvucci and Anderson  also use a button press for confirming a gazebased selection for which they report errors due to a leavebefore-click issue.
This means that the gaze was already fixating a new target when pressing the button.
After all, Ware and Mikaelian  conclude that eye tracking can be used as a fast selection device, if the target size is sufficiently large.
First, when clicking the mouse, the closest item to the current gaze position is selected.
This is especially useful for selecting small-sized targets, however, it has no advantage for closely positioned objects.
Second, the cursor position can be manually adjusted with the mouse.
Third, the first two approaches were combined and showed to be the fastest technique, even beating mouse input.
So far, few researchers have investigated a combination of gaze with a mobile touch-enabled device for interacting with distant displays .
As indicated by Stellmach et al.
In this respect, Cho et al.
While participants found tilt most interesting to use, buttons offered the most control.
The basic interaction vocabulary - besides gaze for cursor positioning - is briefly outlined here: On mobile touchenabled devices held in one hand, the simplest way of interaction is to tap a particular button which is typically used for confirming a selection, changing modes or activating other functions.
Another one is to use a sliding gesture for controlling a numeric value such as a zoom factor.
Next, the thumb can be positioned and dragged around continuously within a particular area, which can often be used for panning and positioning tasks.
Finally, built-in sensors such as accelerometers can be employed to recognize tilting or rotating the device.
Since this is continuous input, too, it can be used for zooming, panning or adjusting other values.
Using these basic interaction techniques and combining them with gaze allowed us to contribute novel gaze-supported selection techniques.
For this, we developed an interface prototype that is shown in Figure 2.
For our current prototype, we use virtual buttons on the mobile device to confirm selections as they offer more control compared to tilt input .
Further details  are discussed in context with the individual selection techniques in the following.
For each technique, we provide a brief description first.
Then, we go into detail about the specific mapping of interaction methods to the envisioned functionality as we have used it for our implementation.
Finally, we briefly discuss particular advantages and disadvantages of each technique.
Please note that we explicitly aim for diverse techniques that can be later combined in a complex interaction set benefitting from their particular advantages.
For the design of gaze-supported target selection techniques, the first design decision is the choice of an additional input modality.
In this work, we decided to use a small touchenabled device, because smartphones are very commonplace and easy to use.
It can be held in the user's hand and combined with his/her direction of gaze.
For this, we assume that the eyes are tracked to deliver gaze positioning information with respect to a distant display.
In addition, the handheld display allows for confirming a selection and for additional functionality addressing the problems of small targets and targets being too close to each other to be easily selected with gaze input only.
Moreover, for advancing gaze-supported selection in combination with a mobile touch display, we elaborated the following design goals: * * * * * * Possibility to interact with standard GUIs Possibility to select small and closely positioned targets Prevent performing involuntary actions  Subtle gaze interaction - should not overwhelm the user Support of eyes-free interaction with the mobile device One-handed interaction: hold mobile device in one hand and interact with the thumb only 
A gaze-directed cursor is the most basic technique, depicting an icon at the user's point-of-regard .
Internally it is represented by a single gaze position .
This is a common approach for substituting mouse with gaze input .
Different zones on the mobile display can be used for supporting toggle and multiple item selection .
The user simply touches the mobile screen  to highlight currently viewed objects.
When releasing the touchscreen, the currently highlighted item is selected.
If the user does not want to select an item, he/she can simply look at a void spot or look away from the distant display and lift the finger from the touchscreen.
An advantage of this pointing technique is that it is easy to adapt to common mouse-based interfaces.
However, the pointing is imprecise as it does not take inherent eye tracking inaccuracies into account.
As mentioned before, jittery gaze movements can be compensated by stabilizing the gaze cursor .
However, the offset problem remains.
On the other hand, if the finger is dragged across the touchscreen, the cursor will move according to the relative movement from the initial touch position .
This aims at supporting the user in keeping the view on the distant screen instead of switching his/her attention to the mobile device for manual cursor positioning.
The user can activate the selection mask by touching the mobile screen.
As illustrated in Figure 2, the selection mask can be deactivated without performing a selection by touching the no selection area at the top of the mobile screen.
Analogous, a selection can be confirmed by touching the selection area at the bottom of the mobile screen.
Furthermore, large targets can be directly selected without the need to activate the selection mask first.
This is achieved by looking at the target and touching the selection area immediately.
The cursor remains at the center of the selection mask and does not move.
Items intersecting the selection mask can be discretely cycled through by using, for example, a continuous sliding gesture on the touchscreen.
Another interaction option includes tilting the device to the left or right to browse through the collection of intersected items.
Thus, MAGIC tab is similar to using the tab button on a keyboard.
The closest item to the user's point-ofregard when activating the selection mask is automatically highlighted .
For going through the other objects intersecting the selection mask, we suggest using a horizontal slide gesture .
If the user performs such a gesture and does not release the touch, the list is further passed through.
Confirmation of an item's selection is again done via touch on a virtual button as described for MAGIC touch .
In addition, we propose a vertical slide gesture to alter the size of the selection mask from very small  to a maximum value .
This helps in confining the number of selectable items.
Concerning the order of highlighted items, the sliding direction could indicate a clockwise or counterclockwise selection.
Alternatively, items could be cycled through according to their distances to the mask's center.
With the MAGIC techniques gaze is no longer required after activating the selection mask and control is entirely handed over to manual fine selection.
As presented by Zhai et al.
We adapt this concept to touch input and further extend it.
For that, we first define a selection mask in whose proximity more precise selections can be performed using the mobile touch device .
As with the previously described technique gaze suggests the target, but here touch allows for fine adjustments before confirming the selection.
Once the selection mask is activated , the cursor does not follow the gaze anymore.
For performing the manual fine selection, we contribute two variations: MAGIC touch and MAGIC tab, which are described in the following.
The cursor can be manually moved according to the touch position on the mobile screen.
For this purpose, a representation of the selection mask is shown on the mobile screen .
We propose a differentiation between absolute and relative positioning.
This means, if the mobile screen is only briefly touched in the circular touch area , the cursor will jump to the respective absolute po-
MAGIC touch allows for fine tuning the cursor position with the finger.
MAGIC tab is decoupled from the size and distance of targets, because candidate objects are discretely highlighted one after each other.
This may have a disadvantage for the selection of an item from a larger group of close targets as more objects need to be cycled to reach the desired target.
The possible manual change of the selection mask's size alleviates this problem.
In addition, the order in which the items are stored in the list may not be clear to the user at all times.
We propose to sort them according to the distance to the center of the selection mask.
While this has the advantage that items close to the current cursor position are highlighted first, it may have the disadvantage that items with a similar distance are positioned at opposite sides and that their highlighting order may confuse the user.
A local zoom lens can be activated at the current gaze position to faciliate target selections.
We refer to this approach as gaze-supported expanded target selection.
The lens activation can be done via a manual command, e.g., by pressing a button, issuing a touch event, or performing a gesture.
Within the magnified area the user can select items more accurately with his/her gaze.
Inspired by the gaze-based fisheye lenses from Ashmore et al.
The lens follows the user's gaze.
Thus, the cursor remains always at the center of the lens.
After activating the zoom lens by tapping on the touch device, the user can move the lens based on his/her gaze position.
A target can be selected in the previously described way by touching the selection area at the bottom of the mobile screen .
To decrease jittery movements because of the target expansions, the gaze cursor is further stabilized .
The magnification level can be altered using a vertical slide gesture on the mobile screen .
A zoom lens is activated at the user's point-of-regard, and the cursor can be freely moved within the lens using eye gaze.
The lens does not move itself until the user looks beyond its boundary.
In this case, the lens is dragged towards the current gaze position.
Similar to the eye-slaved zoom lens, a vertical slide gesture can be used to change the magnification level.
Furthermore, we suggest a rate-based control for moving the lens while looking outside its border: the further the distance between the gaze position and center of the lens, the faster the lens will move.
The proposed zoom lenses have the advantage of improving the visibility of small targets.
However, a local magnification may not necessarily improve pointing accuracy, if the cursor speed remains at the same level.
This means that the cursor movement may become more jittery when further zoomed in.
Thus, target expansions may facili-
We conducted a user study testing the five described gazesupported selection techniques for different selection conditions.
In particular, we were interested in the performance and suitability of each technique with regard to different target sizes and distances, which were both used as independent variables.
Tasks ranged from selecting targets from an abstract grid of items to a more realistic desktop-like setting.
This aims at better assessing the suitability of the developed techniques for different situations and how they can be further enhanced.
Besides investigating the performance of each technique, we put a particular emphasis on qualitative user feedback for gaining better insights, identifying potential problems, and possible solutions for them.
We decided to test C, the two M and the two Z conditions together in a counterbalanced order to prevent the influence of order effects.
This was done since Mtch and Mtab are very similar and belong to the same selection type  and Zes and Zsf respectively .
Twenty-four paid volunteers  participated in the study, aged 22 to 31  = 26.3 with normal or corrected-to-normal vision.
In an initial questionnaire we asked participants about their background and to rate several statements on a 5-point Likert scale from 1 - Do not agree at all to 5 - Completely agree.
Based on this, participants stated that they mainly use mouse and keyboard for computer interaction =0.50.
While participants are interested in novel input devices , many participants do not frequently use multitouch devices  .
Finally, while all participants use computers on a daily basis, only eight had already used an eye tracker for interaction before.
For gathering gaze data we use a Tobii T60 table-mounted eye tracker: a binocular eye tracker is inte-
The gaze position is stabilized using the speed reduction technique .
Based on initial tests before the user study, we use a ratio of 8% of the current with 92% of the previous gaze position.
The described gaze-supported selection techniques have all been implemented as suggested in the respective Interaction design paragraphs.
We use a similar system setup for the gaze-supported multimodal interaction as proposed by .
An iPod Touch is used for the interaction on a mobile touchscreen.
The GUI on the iPod is designed according to the screen prototype illustrated in Figure 2.
The user study started with a brief introduction and an initial questionnaire about participants' background .
Participants were seated approximately 60 cm from the eye tracker display and were instructed to sit fairly still without restricting their movement.
For each selection technique the same procedure was followed.
First, a 9-point eye tracker calibration sequence was performed.
Then, one selection technique at a time was described and the user could directly play around with it.
The participant could test the technique until s/he felt sufficiently acquainted with it .
Three task blocks had to be completed in the same order with each selection technique.
An overview is presented in Figure 5.
The overall task was to select a single target from a set of given objects, whereby the alignment and size of objects and their distances differed among the task blocks: T1 T2 T3 Non-overlapping 2D items aligned in a grid  Overlapping 2D items aligned in a row  Desktop mockup: Overlapping items varying in size 
Thus, assuming a distance of 60 cm to the eye tracker screen, the visual angle of targets ranged between 3.3  to 0.3 .
Item sizes and distances were differed across runs but not within the same run .
The same order was used for alternating target sizes and distances: First, all large targets  were tested with differing distances to distractors; then, this was repeated for the other target sizes as well.
At the beginning of each run, participants needed to look at the center of the screen and touch the mobile device to confirm readiness.
This was meant to improve the comparability between selection times.
Targets had always the same distance to the screen center, however, they were positioned at alternating corners depending on the respective run.
For task block T3, a prototype was used that should resemble a desktop environment containing windows and icons.
Participants had to select five targets as illustrated in Figure 5.
The targets always had to be selected in the same order, starting with the largest  and finishing with the smallest and most difficult one .
Our quantitative measures included logged target acquisition times and error rates .
Furthermore, as previously pointed out we aimed for substantial user feedback for a better assessment of the individual selection techniques.
An intermediate questionnaire was handed out after T1-T3 have been completed with a single selection technique.
For task block T1 and T2, the object sizes and distances varied.
The distances ranged from 1  to 0.5, 0.25 and 0 .
The size and distance values are based on an internal virtual unit.
For the evaluation, we were particularly interested in qualitative feedback about how the developed gaze-supported selection techniques were experienced by the participants.
The quantitative measures such as target acquisition times and error rates give a good indication about the practicality of these techniques.
A repeated-measures ANOVA  was used to investigate differences in task completion times and usability rankings.
Post-hoc paired samples t-tests with a Bonferroni correction were used to further investigate which particular values differed significantly .
Mean selection times and error rates are shown in Figure 6 and 7 with error bars indicating 95% confidence intervals .
They will be further discussed in the following based on the independent variables distance , target size , and selection technique.
The influence of D and S on the mean selection times for T1 and T2 are summarized in Table 1.
Selection times were significantly worse for S1 than for S5 and S10 for each selection technique.
Large targets  on the other hand could be quickly selected with all five techniques with no significant performance differences.
Selection of small targets using C was very difficult and partly not realizable.
As a result, selection times for C were significantly worse than for all other techniques for S1 , except in comparison to Zes for T2.
In addition, Mtab performed significantly faster than the other techniques for S1 in T2.
A dependence between increased error rates and decreased target sizes exists for C and in an alleviated way for Zes .
The error rates for Mtch , Mtab , and Zsf do not significantly differ with respect to S. While the mean selection times and error rates were not significantly influenced by varied distances  for S10 and S5 , D had a significant effect on the task completion time for the smallest target size S1 both for T1 =20.95, p<0.001 and T2 =12.33, p<0.001.
The error rates for Mtch , Mtab , and Zsf remained relatively stable for varied distances.
Selecting target 5 was considerably harder than the other targets: a significant increase in target acquisition time occurred for all selection techniques.
In this regard, precise and fast selection of small targets was particularly wearisome for C, Zes , and Zsf , as participants sometimes had to look at a location near or beside a target to actually select it.
This is reflected by high selection times and error rates especially for C, which partly prevented users from selecting a small target at all.
Participants preferred the MAGIC techniques , for which a rough gaze cursor position is sufficient and manual fine adjustments can be made.
Seven participants mentioned that this gave them more control.
Although participants liked the virtual buttons on the touchscreen to confirm a selection , this was sometimes error-prone.
Especially for Mtch some users accidentally hit the selection button when trying to manually position the cursor.
Instead, a selection could, for example, be performed with a double tap, and the selection mask could be deactivated by looking away from it for a specific amount of time.
Although the gaze-directed cursor C was not suitable for the selection of small targets , 19 participants emphasized its high potential for very easy, fast, and intuitive selections of large objects .
Participants appreciated that items are highlighted as long as touching the display .
In general, the idea to manually position the cursor precisely via touch for Mtch and Mtab was found very good , because it is more relaxing for the eyes .
In this respect, the MAGIC techniques were assessed very positively with respect to several usability aspects that are listed in Figure 8.
Furthermore, the selection mask was found very helpful to confine the cursor movement or the amount of selectable targets respectively .
The current implementation of the relative  and absolute cursor positioning  was not particularly liked by participants.
The relative cursor movement was sometimes too fast, which led to overshooting problems.
This was especially a problem for very small targets, which 14 participants described as tedious.
The absolute positioning was regarded as most suitable for quickly selecting large targets.
For each selection technique, users rated how satisfied they were with different usability aspects , such as how intuitive the technique felt and whether tasks could be achieved as anticipated .
The results are summarized in Figure 8.
C received significantly lower ratings for task-driven use, accuracy, and ease of use which reflects the low task performance of C .
On the upside, users assessed C significantly easier to learn.
Finally, Figure 8  presents how participants rated the individual selection techniques in the final questionnaire after having tested each of them.
C was assessed significantly worse than the other gaze-supported selection techniques.
Although the other techniques were rated very similar, MAGIC tab received the best overall rating.
In the following, we discuss the gaze-supported selection techniques based on the user feedback to Q2 and Q3  and the previously presented results.
In general, the combination of gaze- and touch-based input for the interaction with a distant display was found very useful .
As one participant summarized well: A prolonged working with the eyes will be wearisome, if the gaze input demands high precision and concentration from the user.
Participants liked the discrete selection of targets very much , as it was fast and precise.
Particular advantages of Mtab include its robustness against inaccurate gaze data, its suitability to select small targets, and that it is more relaxing for the eyes.
However, the task performance is affected by the amount of targets intersecting the selection mask.
Hence, it is important that users can easily narrow down the amount of selectable items, as this was criticized by participants.
The size of the selection mask changed too slowly and was therefore considered as less useful .
In general, the slide gestures were described as slow and imprecise , because they were sometimes not immediately recognized due to the current implementation.
In addition, five participants complained that the tab order was unintuitive.
It was not clear where the target was in the item collection and whether going forward or backward would be faster to reach it.
Instead of using slide gestures, backward and forward buttons may offer more control.
It was also suggested that the selection could be based on the gesture's direction to perform more systematic selections, for example, by flicking the cursor towards a desired target and snapping the cursor to it.
However, this may cause problems if objects are positioned behind or close to each other.
Another idea is to automatically adapt the initial size of the selection mask depending on the current context, for example, based on the number, size, and distance of targets.
The combination of an enlarged view and a slowed movement of the cursor/lens are considered helpful for selecting small targets more precisely.
However, they do not overcome eye tracking inaccuracies completely, which becomes apparent for small and closely positioned targets .
In addition, participants liked the vertical slide gesture to adjust the zoom level .
However, especially for the zoom lens techniques it showed that users desired more possibilities for customization, for example, of the lens size, the maximum zoom level, and the lens speed.
This is reflected in highly diverging ratings in the questionnaires.
For example, the decreased speed, although preferred by several users, received an overall moderate rating .
In this respect, five participants explained that they found the lens speed too slow and three not slow enough.
While Zes was on the one hand described as fast and intuitive , it was also characterized as distracting, laborious, and imprecise, because it does not take eye tracking inaccuracies into account.
Zsf was found less irritating than Zes , however, it was considered imprecise .
It was positively mentioned that Zsf provides more stability  while still being flexible to be panned if the user looks outside the lens .
Thus, it was suggested that the lens should already move when looking at the lens's border from within so that there is no need to leave the lens with the gaze.
All in all, each selection technique has its unique advantages as described above.
Even though C achieved the lowest performance and overall user rating, it was assessed very positively for the selection of large targets.
In this respect, a seamless integration of the proposed selection techniques into one complex interaction set seems highly promising to cater for diverse selection contexts.
While large targets can be selected with a simple gazedirected cursor , smaller items could be quickly selected with a gaze-supported zoom lens and/or manual selection  technique.
Providing more customization features for the zoom lenses, a user could dynamically adjust how fast the lens follows the gaze and with that a smooth transition between Zes and Zsf could be achieved.
In summary, the combination of gaze- and touch-based input for the interaction with a distant display was perceived as very promising.
However, since this study was restricted to a desktop setting, the techniques need to be further investigated and adapted to distant large displays.
In this paper, we contributed several novel and practical ways for combining gaze and touch input from a mobile touchscreen for selection tasks on a distant display.
According to the principle gaze suggests and touch confirms, we described five gaze-supported selection techniques ranging from a simple touch-enhanced gaze-directed cursor to novel gaze-supported local zoom lenses and our MAGIC touch and MAGIC tab techniques.
While MAGIC touch allows for manually positioning the cursor via touch input, MAGIC tab allows for discretely going through a list of objects that are spatially close to the user's point-of-regard via a slide gesture.
These techniques were evaluated in a user study providing important insights for further improvements.
With the presented techniques we could overcome prevalent problems associated with gaze-based interaction, such as the Midas touch problem.
Especially with our MAGIC techniques we could compensate eye tracking inaccuracies well.
In this respect, the MAGIC techniques excel with their robustness against inaccurate gaze data, a high comfort as they are less straining for the eyes, and a high overall performance and perceived usability.
In particular MAGIC tab shows a high potential for further investigations as it is suitable for quick and reliable selections of small, closely positioned and overlapping targets.
For future work, we will enhance the proposed gazesupported techniques based on the collected user feedback and investigate possibilities to seamlessly combine them to take advantage of their particular strengths for different conditions.
Furthermore, the revised techniques will be tested with large distant displays and in comparison with traditional input devices.
Finally, the proposed selection techniques may also benefit target acquisition within virtual 3D environments, in which several objects may be positioned behind each other.
