Accelerometers are common on many devices, including those required for text-entry.
We investigate how to enter text with devices that are solely enabled with accelerometers.
The challenge of text-entry with such devices can be overcome by the careful investigation of the human limitations in gestural movements with accelerometers.
Preliminary studies provide insight into two potential text-entry designs that purely use accelerometers for gesture recognition.
In two experiments, we evaluate the effectiveness of each of the text-entry designs.
The first experiment involves novice users over a 45 minute period while the second investigates the possible performance increases over a four day period.
Our results reveal that a matrix-based text-entry system with a small set of simple gestures is the most efficient  and subjectively preferred by participants.
Researchers have previously designed accelerometer based text-entry techniques, such as TiltText , and techniques for distant text entry on large displays .
However, these systems rely on auxiliary input mechanisms or devices to aid text-entry.
For example, TiltText requires a keypad in conjunction with the accelerometer.
Shoemaker et al  required an IR-enabled device with accelerometers for carrying out text-entry.
While these systems target specific hardware platforms, an accelerometer based text-entry system should ideally be hardware independent.
This would allow for a wider range of devices, such as tilt-enabled watches or remote controls to facilitate the text-entry task.
The ubiquity of text entry warrants an investigation of this task on accelerometer-only devices.
Text entry is, fundamentally, a series of selections from a set of options.
The findings of this research therefore go far beyond this task and can be applied to any accelerometer-based selections.
Embedded motion sensors are becoming increasingly common in many devices, ranging from generic consumer electronics to more specialized hardware dedicated to specific tasks.
It is not surprising that 2007 was coined the year of the accelerometer .
With a five-fold drop in sensor costs , the ubiquitous accelerometers have allowed the general population to become familiar and comfortable with their use for a variety of tasks.
While devices with accelerometers are geared toward supporting primary tasks, they are also used for auxiliary functions such as entering text.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
This paper starts by analyzing the factors that influence the design of accelerometer-only text-entry systems.
It then reports the results of two pilot studies that identify the extent to which designers can harness accelerometers within the limits of human wrist motion.
These results assist with the design of GesText, a class of novel accelerometer-only text-entry systems.
In addition this enables GesText systems to be distance independent  and potentially `eye's free'.
In two formal experiments it compares a matrix-based gesture layout and tri-center layout  and shows that users can reach up to 5.4wpm when using a design that uses multiple simple gestures for character selection, as exemplified by the matrix-based layout.
Research into accelerometer based input can be classified into two categories: the utility of accelerometer-enabled devices in higher level applications, and low level studies that identify compelling properties of tilt for interaction.
Rekimoto's work was one of the earliest systems to propose accelerometer-based input as a means of controlling virtual objects on the screen .
Rekimoto demonstrated that designers can map angular tilt input in either a continuous or discrete manner.
As a result, tilt values could be effectively used for carrying out actions such as selecting items from pull-down menus, interacting with scroll bars, panning or zooming around a spatial workspace, or even to perform more complex tasks such as 3D object manipulations.
Other high-level applications for tilt include interacting with menus , controlling object placement on a digital tabletop , or entering text .
Researchers have also studied properties of tilt input at a more basic level, such as the effect of feedback on tilt input , the minimal achievable resolution with tilt , and various methods for discretizing and controlling tilt values .
For instance, Oakley and O`Modhrain  describe a tilt-based system with tactile augmentation for menu navigation and show that tactile feedback can be more effective than visual feedback, particularly when the device is turned away from the users' field of view.
They showed that error rates dropped significantly when the targets selected had a width of 9 or more.
A systematic study by Rahman et al.
Their results reveal that users had the most control on the axis of pronation/supination and can achieve a resolu-
Researchers have demonstrated the effectiveness of tilt input for text-entry systems .
With TiltType  and TiltText , users enter a character by pressing a keypad button and then tilting the device in the right direction to enter the letter.
Their study was motivated by the need to enter text on large wall based systems at a distance away from the medium.
Shoemaker et al's systems do not consider the possibility of entering text when only relying on accelerometers and achieved input rates between 6 and 10wpm.
The design of Unigesture was also motivated by the need for small devices to perform text-entry, even in the absence of additional input buttons.
The Unigesture layout divides an alphabetic list into seven zones.
Users enter text by tilting to the appropriate zone, analogous to rolling a marble on a plate.
Once a few letters are entered, the system infers the possible word.
As a result, Unigesture is dependent on a dictionary.
Users achieved a text-entry rate of 3.33wpm with Unigesture.
Tilt input is largely gesture based and this form of input has shown relative success for several types of tasks.
For example, gesture-based menu interaction, such as marking menus, allow for item selection via pen strokes in the direction of a desired menu item .
The interaction becomes even more powerful as users grow more familiar with the strokes, thus relying less on any visual feedback or markers .
Selection is scale invariant so that a menu item is selected only based on the direction of the stroke.
Studies have shown that expert users can perform menu selection 3.5 times faster with gestures, such as with marking menus, than when using regular pie menus .
Since marking menus largely depend on wrist movement, albeit in 2D, we inherit some of the design properties from this form of menu interaction for candidate designs of GesText systems.
Gestures that require large or complex movements should be avoided as they increase the likelihood of physical and mental fatigue .
GesText depends to a large extent on wrist motions.
These motions are limited by the range-ofmotion  along three axes, which vary considerably from one axis to another .
A study on the dexterity of wrist motions with tilt input by Rahman et al  suggests that tilt input should rely more heavily on the axes that afford the highest level of ROM, namely the axes of pronation/supination and flexion/extension .
Complex gestures may be unreliably reproduced between different users or even by a single user over time .
This necessitates a trainable, evolutionary style of recognition system to achieve an acceptable error rate.
Conversely, immutable gesture sets should therefore contain only intuitive, axial gestures.
This makes gesture sets comprised of basic tilting and rotating movements highly suited to public, walk-up and use systems where no user training is required.
In systems that use unique gestures for each character item, there is a learning-recognition trade-off between gesture sets and optimal gesture recognition.
Ideally, the simplest gestures afford a higher degree of learning  and should be employed whenever possible.
Tablet-based handwriting recognition does not suffer from this issue as the contact of the pen with the surface indicates input.
The explicit pen-up signal, along with micro-pauses between character entry and the physical pen location can be used to delimit the start and end of characters.
These spacing and lifting techniques are not easily transferable to accelerometer based gestural input.
These may take the form of a delimiting gesture , default position/orientation or an intentional pause.
The granularity of motion detectable by accelerometers means that shape-drawing input methods are potentially viable.
However, since motion  is calculated from acceleration, sudden movements can cause changes in the tilt readings.
Furthermore, a rapid gesture in one direction can generate a counteractive force in the opposite direction complicating interpretation .
Due to this difficulty in reliably differentiating velocity and range of motion, tilt gestures should be separated from other movements to improve detection.
Relative positioning is also challenging and its convoluted calculation method means that it is prone to cumulative  errors.
Accurate relative positioning in 3D space can be achieved using additional sensors .
Absolute positioning requires calibration through an additional external input, such as an initial or continuous geospatial position.
Direct mappings are 1:1 encodings of a gesture to a character.
Selection techniques involve the reuse of a limited set of gestures in order to navigate to the desired input character on a virtual keyboard.
Most commonly, the characters on a virtual keyboard are arranged in a fixed layout; however, it is also possible to use moving target selection techniques.
Mapping letters to simplified recognition-optimised gestures facilitates improved recognition rates, but requires teaching and memorization of the specialized character set, prior to use.
This learning requirement is impractical for "walk-up and use" systems.
Virtual keyboards provide a suitable compromise between reliable recognition and minimal user learning.
Text can be input using a virtual keyboard by navigating to characters using a small set of consistent navigational gestures.
To enable high performance with untrained use of virtual keyboards, there should be an intuitive mapping of gestures to navigation commands.
The primary disadvantage of virtual keyboards is the requirement for a visual interface.
This potentially reduces the mobility of the device due to the required display's footprint.
The user may also require their visual attention to be focused elsewhere.
As with marking menus, once the user is familiar with the gesture set and relative locations of characters, less visual attention may be required .
One-to-one mappings of characters to keys on a virtual keyboard can result in a larger interface footprint than desired.
A large keyboard can also lead to longer acquisition times as the navigation distance increases.
One way to resolve this problem is to organize characters into sub-groupings or hierarchies, resulting in a many-to-one mapping of characters to virtual keys.
Characters are subsequently selected through a divide and conquer technique.
Six volunteers  participated in the experiment, two of whom were left-handed.
Participants used a standard Wiimote to perform the gestures as it is a generic tiltbased device.
Only the accelerometer was used for sensing the user's actions.
Three of the participants had previous experience using accelerometer-based controllers .
The study lasted around 30 minutes.
Familiar layouts such as alphabetical or QWERTY are best suited for walk up and use situations.
These layouts require less visual scan time, providing more efficient operation.
Optimizing the layout based on the frequency of letter selection may improve the performance of expert users as frequently used characters can be positioned such that travelling distance is minimised and/or less error-prone gestures are required for selection.
Open loop feedback only allows identification of errors once they have occurred.
Visual feedback is the typical channel for this communication; however, it may be enhanced or replaced by auditory and haptic feedback depending on the context of use.
Mouse cursor style immediate, continuous visual feedback should be avoided due to the technical limitations of accelerometers .
When remote visuals are provided, designers should be aware of the potential problems of users exhibiting a split focus of attention and a decreased ability to correlate their motions with the visual representation.
The experimental interface  had a size of 640x640 pixels and was displayed on a 32 inches widescreen monitor .
Participants stood 2m from the screen.
Each trial begins with the target position highlighted with a blue circle.
A timer counts down from three to one over a period of one second.
During this time, the participant is required to hold the Wiimote still, at the center or neutral position.
Once the countdown completes, the user performs the required gesture .
The participant is automatically moved to the next trial after completing the gesture .
The experimental interface cued all tasks and logged all task completion times.
To complete vertical gestures, the user tilted their wrist along the y-axis , dwelled in that position for at least 20ms  and returned to the center.
Horizontal gestures were made by rotating the wrist around the z-axis  and completing a similar dwell and return routine.
Gestures that did not fall on the horizontal or vertical axes were made using combinations of these base gestures.
The required gestures were indicated on-screen using a circular layout with the target highlighted using a blue circle .
Error rates for the three layouts are summarized in Figure 4, right.
The target acquisition time and the error rates increased as the separation angle decreased .
There was also a significant difference for block number, with the first block having a mean error rate of 18% and the last block 12% .
Participants subjectively rated the 90 layout the least effort  and best performing.
Figure 3: Wrist movements  neutral position - holding wrist in a horizontal orientation to target the center point;  tilting along the y-axis to target characters above or below the center point;  rotating the wrist around the zaxis to target characters to the right or left of the center point.
The experiment was divided into three parts, one for each of the three layouts described above.
For each part, the participants first completed a training session, consisting of a freeform practice session  and then a cued practice session, where the user completed one block of gestures, such as would be encountered during the timed sessions.
All data from the practice sessions was discarded.
Then, each user completed six blocks with a particular layout.
Each block required the user to perform all gestures for that layout, three times, in a random order.
Once the user had finished all blocks for one layout, a NASA-TLX form was completed for that layout and the procedure repeated for the next layout.
The order the layouts were provided to the users was also randomised.
The task-time dependent measure was analysed using a 3x6 repeated measures analysis of variance , with the two factors: depth  and block number .
Six volunteers  participated in the experiment.
The results from the first pilot study are summarised in Figure 4, left.
A total of 2604 selections were made during the experiment, 115 of these were discarded as they took longer than +3 s.d.
The block/layout interaction  is caused by the differing rates of learning through the blocks of each layout.
There was no significant main effect for the factors layout or block number on task completion times.
These are summarized in Figure 6, left.
There was a significant difference in error rates between the layouts.
Further increasing the dwell time required before selection would further increase the percharacter selection time.
Cumulatively these severalhundred millisecond pauses could noticeably impact the speed obtainable by a gestural text entry system.
Increasing the number of gestures by including depths and diagonals significantly increased the number of errors.
Pilot study one found that single horizontal and vertical gestures were considerably quicker and less error prone.
However, an text-entry system must provide selection possibilities for all alphabetic characters , a space and backspace character .
To do this, a mechanism for providing these extra selection points must be chosen.
We considered two possibilities: adding depth and adding diagonals.
When participants were required to select between multiple depths along an axis, outside points  generally fared better than the inner and central points.
One possible solution is to increase the selection area for the internal points.
This would provide a similar effect to that of increasing the target size in a Fitts' task.
Examining the selection path data for diagonal gestures reveals the extent of the problem.
Figure 7 shows the gesture paths for three different participants.
The black diagonals show the idealised gesture directions.
For the righthanded participants , the bottom right and top left positions more closely adhere to the diagonal axes than the opposite two diagonals.
Interestingly for left-handed participants this result is mirrored .
Difficulties in selecting diagonal targets are potentially due to the motion that is required along multiple axes, simultaneously.
The radial nature of the layout means that the selection area of vertical and horizontal gestures is reduced by the inclusion of diagonals.
Inaccurate axial movements can subsequently be interpreted as diagonal gestures, resulting in an increased error rate.
Where these selection errors detrimentally impact the overall text entry rates it may prove beneficial to transform these gestures into successive motions, forming compound gestures.
Compound gestures are discussed further in the design of our GesText system.
Figure 7: Gesture path plots.
Solid black diagonals represent idealized positions.
The bottom right and top left are easier.
The bottom-left and top-right are easier .
We also frequently observed participants having difficultly returning to the central position after performing a gesture.
This may partially be due to the spilt-focus required: users were concentrating on the screen which displayed instructions, meaning they were unable to look down at their hand to accurately locate the horizontal position.
The feedback provided for locating the central position therefore impacts on the ability of users to accurately perform the required gestures.
A viable solution to this problem is to increase the size of the central area.
Mirroring of difficult gestures is also visible between right- and left-handed people.
Using these findings as input, and considering the design factors outlined earlier, we designed two contending GesText systems.
Both designs incorporate the three fundamental lessons derived from the pilot studies, and are detailed below.
Multi-center layouts offer an attractive solution to the higher error rates and acquisition times found in the pilot studies for depth-based and angular gestures.
Users can still employ the quick and  error-free vertical and horizontal position selections, and can instead employ multiple gestures to select a character.
The primary disadvantage of this technique is the increased selection time .
However, this is an ac-
Restricting gestures to axial movements around multiple centers is an accurate yet inefficient way of meeting the demand for gestures.
Including diagonal selections significantly reduces the number of centers required.
However, the pilot studies showed that the selection of diagonal gestures was more error prone than the horizontal and vertical axes.
To resolve this issue we employ compound gestures in the text entry systems as a method of selecting diagonal items.
A compound gesture breaks a diagonal movement into its constituent components: a vertical gesture and a horizontal gesture.
In-house testing showed this method to be more accurate than the single diagonal gesture.
Any blank square can be used as an `escape' to return to the center.
On character selection, the cursor returns to the matrix's center.
Diagonal gestures are achieved using compound gestures, as described earlier.
The selection of backspace and space characters requires only a single gesture.
The tri-center layout  uses three centers for character layout and selection.
It uses single depth compound diagonal gestures and a combination of single and double depth horizontal and vertical gestures for character selection.
Character selection begins in the middle center-point .
From this position, the user can select any of the nine characters immediately surrounding the center-point, or the vertical double-depth functions of space and backspace.
To select characters centered around the left-hand center, the user makes a double-depth motion to the left, moving the center-point.
The user can now select characters as per the middle center-point.
This layout supports center selection correction , but does not support correction once a gesture towards a character has begun.
We propose a rapid-return-to-center gesture as an alternative to dwelling for selection.
This resolves the lack of error correction and increased selection time, present in the latter.
All of the investigated gestures require the user to begin at a central point, gesture to the correct location and return to the central point.
This action of returning to the center can therefore be used as a gesture delimiter; once a user has highlighted their desired target.
This has the triple advantage of eliminating dwell time delays, making it easier for users to correct from overshoot mistakes and ensuring that the user automatically returns to center; enabling the rapid performance of successive gestures.
Both of the GesText systems incorporate a virtual keyboard from which characters are selected using wrist-based navigational gestures.
The character set comprises the 26 English alphabet characters, along with space and delete.
The layout of the two systems vary due to the different techniques used to accommodate the 28 characters.
Character disambiguation is achieved manually using additional gestures rather than through predictive text engines, due to the intended context of use; novice users entering nondictionary words .
The alphabetical arrangement of characters for both systems is also implemented as an optimization for novice users.
Fifteen volunteers  participated in the experiment, three of whom were left-handed.
As per the pilot studies, participants used a Wiimote for gesture entry .
Only one participant indicated that they had no prior experience using accelerometer-based controllers.
Participants entered text using the two gesture-based interfaces described in the previous section.
The matrix-based interface was 517 x 515 pixels, while the tri-center interface was 710 x 325 pixels.
They were displayed on the 32 inches monitor 2m in front of the participants.
The experiment lasted approximately 45 minutes.
The matrix-based layout primarily uses single depth vertical and horizontal gestures and single depth compound diagonal gestures for character selection, as shown in Figure 1, left.
The complete character set is provided by using multiple gestures for each character.
Character selection begins in the central square.
For alphabetic characters, the user first gestures towards the subset square that contains the required letter .
The cursor then moves into the required subset.
Each task required the user to enter the provided sentence with the given interface.
The participants were given 10 seconds to memorise the sentence before the interface was revealed, with the sentence the participant was required to enter displayed above the interface.
The entered characters appeared immediately below the required sentence.
Incorrect entries caused a system `beep' and the entered character to be shown in red.
Participants were required to correct their mistakes before the task could end.
Two sets of 17 sentences were randomly selected from MacKenzie and Soukoreff's publically available corpus .
The first set was always used first, regardless of the interface.
Sentences were presented in a random order to the users.
The input time was measured from the beginning of the first gesture to completion of the final character.
Typing speed is analysed with a repeated measures ANOVA with interface type as the factor .
The number of errors per sentence was also recorded and analysed separately.
The experiment was divided into two parts, one for each of the two layouts described earlier.
For each interface, the participants first completed a training session.
The training consisted of a freeform session where the user could select any character they wished  and then two cued sentences, similar to that which they would complete during the recorded tasks.
All data from the practice sessions was discarded.
Following the practice sessions, each participant `typed' fifteen sentences using their first interface.
A subjective feedback form, including a NASATLX evaluation was completed at this point.
The procedure was then repeated for the remaining interface.
The order the interfaces were provided to the users was counter-balanced, with eight using the matrix layout first and seven using the tri-center layout first.
After completing the tasks with each interface, the participants completed NASA-TLX evaluation forms.
At the conclusion of the experiment overall preference rankings were also gathered.
The results of the NASA-TLX surveys are shown in Figure 9.
In all categories, except mental demand, the matrix-based interface out-performed the tri-center interface.
Preferences for the two interfaces were significantly different, with the matrix-based interface preferred by 11 participants, the tri-center interface preferred by one participant and three could not decide: Wilcoxon z = 2.35, p< .01.
The results of experiment one showed that the matrix-based layout was significantly faster and significantly less errorprone than the tri-center layout.
Subjective results indicated that participants also preferred the matrix-based layout.
This study has shown for novice, `walk up and use' situations, the matrix-based interface provides the best performance, with the least number of errors.
To explore whether this result holds over longer periods of time, we conducted a study with repeated use over several days.
The results from experiment one are summarised in Figure 8 .
A total of 225 sentences were `typed' during the experiment.
This experiment investigates how the performance  of the two text-entry systems is affected by experience.
Participants were asked to use the two GesText systems daily for a period of four consecutive days.
The apparatus and experimental design are the same as for experiment one with the participants and procedure varying as described below.
The error rates for the two interfaces are summarized in Figure 8 .
The number of errors per sentence were also recorded and analysed separately.
This experiment followed the same procedure as experiment one, with the exception of the collection of subjective data: participants were only asked to complete this information on the first and last days.
No other factors showed significant differences.
The error rates for the two interfaces are summarized in Figure 11.
For both interfaces, the error rate was the highest on the first day and was variable for the remaining days.
Significant variation of the percentage error rates per-character and the incident rate of wrongly selected characters is apparent for the tri-center layout, as shown in Figure 11.
All five participants indicated that they preferred the matrix-based interface.
The NASA-TLX results showed that participants also thought that their performance increased over the four days.
On the final day of experiments, participants expressed that the tri-center layout "improves significantly with use" explaining that "the knowledge of how to use the system is intrinsic, but the feel for it takes time".
The matrix-based layout is more suitable for novice users than the tri-center layout, as the error rate of the tri-center layout remains comparably higher.
An investigation into the causes of the various observed errors and difficulties exhibited by participants highlighted a few specific areas, where if improvements were made, considerable performance benefits could be expected, particularly for the tri-center layout.
These are discussed in this section.
Examination of Figure 11 reveals that in the tri-center layout, where gestures incorporated a multi-depth selection, errors rates were significantly higher.
An example of this is between characters E and space and also D and F, due to the first step of the compound gesture.
Participants found double depth "irritating because you have to stop and be careful ... therefore it is frustrating as it interrupts the rhythm of inputting text".
A summary of the rate of word entry for the two interfaces, over the four days, is shown in Figure 10, top.
If the problem of having to gesture slowly to avoid overshooting was eliminated, a higher WPM rate could be achieved.
For experienced users, breaking the alphabetical layout to place frequently used characters around the main central point could then subsequently further improve the performance of the tri-center layout.
Following the difficulties finding and maintaining the center position observed in the pilot studies, the centers were enlarged to reduce random motion errors.
This aim was successfully achieved; however this appears to have exacerbated the multiple depth problem.
The discrete visual feedback employed in both layouts  coupled with the size of the center, meant that a noticeable displacement gesture could be performed whilst the cursor was still at the center.
The temporary non linear mapping of gestures that occurs when the cursor warps back to the main center following the selection of characters from the outer edges could have caused confusion resulting in these errors.
Disregarding a few sensor readings immediately preceding such gestures may be a viable solution to this problem.
This paper has provided a description of the three categories of factors that influence the design of accelerometer -based text-entry systems; human, accelerometer and interface considerations.
The limits of human ability to perform wristbased gestures with accelerometers were evaluated, leading to the establishment of GesText; a class of novel text-entry designs using a set of simple tilt based gestures.
We evaluated the performance of two GesText systems with respect to user efficiency and subjective satisfaction, demonstrating that accelerometer only text input is viable.
Overall, the matrix layout was found to be most suitable for novice users, demonstrating a significantly higher WPM rate, user preference and a lower error rate.
The performance of the tricenter interface improves with experience, as users learn the scale of gesture required to select double depth characters.
This style of interface is best suited to situations where people are willing to put in the effort required to become proficient.
