Wearable sensors are revolutionizing healthcare and science by enabling capture of physiological, psychological, and behavioral measurements in natural environments.
However, these seemingly innocuous measurements can be used to infer potentially private behaviors such as stress, conversation, smoking, drinking, illicit drug usage, and others.
We conducted a study to assess how concerned people are about disclosure of a variety of behaviors and contexts that are embedded in wearable sensor data.
Our results show participants are most concerned about disclosures of conversation episodes and stress -- inferences that are not yet widely publicized.
These concerns are mediated by temporal and physical context associated with the data and the participant's personal stake in the data.
Our results provide key guidance on the extent to which people understand the potential for harm and data characteristics researchers should focus on to reduce the perceived harm from such datasets.
By applying sophisticated machine learning algorithms on these data, rich inferences can be made about the physiological, psychological, and behavioral states and activities of people.
Example inferences include dietary habits, psychosocial stress, addictive behaviors , exposures to pollutants, social context, and movement patterns.
These inferences, particularly when made continuously as people go about their daily lives, have many uses, such as sharing activity and context information with friends and family , self-monitoring health for behavior change , and scientific study of human physiology, psychology, and behavior .
While useful, such sensing systems raise significant new privacy concerns.
Seemingly innocuous data shared for one purpose can be used to infer private activities and behaviors that the individual did not intend to share.
For example, inertial sensor data  shared with caregivers for evaluating and improving gait or physical activity levels could also be used to track location and movement patterns by sophisticated tracking algorithms  that make use of public information such as street maps and an individual's work locations.
Inertial data may also reveal sensitive medical conditions, such as seizures , that one may wish to keep private.
As another example, respiration and location data, combined with publicly available pollution maps, could be shared to measure an individual's exposure to air pollution, where respiration measurements are used to estimate the amount of air inhaled.
However, this same data can also reveal the timing and duration of conversations  or even smoking .
In some cases, inferences from sensory data  combined with other public information  can also re-identify an individual.
Privacy research has traditionally dealt with reidentification from quasi-identifiers , search histories , movie ratings , and social networks .
Some work exists on understanding the privacy concerns emerging from sensory data, such as location traces , but little work has investigated the new privacy concerns that emerge from the disclosure of measurements collected by wearable sensors.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
While the privacy risks emerging from sharing data collected by personal sensors are poorly understood, their adoption is growing rapidly.
One example of such a system is AutoSense, an experimental, unobtrusive wearable sensor suite capable of capturing physiological data and using it to infer the wearer's behavior and psychological state in realtime .
It can be worn for weeks at a time and is thus capable of collecting significant amounts of personal data from the mobile environment of its wearer.
To date, AutoSense has been used by 60+ participants in behavioral science field studies totalling 1,000+ hours, where it collected ECG, respiration, accelerometer, temperature, and skin conductance data; and from this data made continuous inferences of physical activity, posture, stress, conversation, and commuting.
This rich and unique dataset, however, can not be readily shared with other researchers out of fear of similar unknown threats to privacy.
This is because seemingly innocuous personal datasetes have previously been shared, only to learn later of hidden privacy threats within them .
We note that commercially available commodity devices capture similar information .
To see continued growth and adoption of such devices, the privacy challenges associated with them must be addressed.
Restricting or abstracting the disclosure of temporal context  had a significant effect on reducing privacy concerns for a variety of contexts and behaviors.
Lastly, the risk of reidentification doubled concerns regarding disclosure of the data; the effect was largest for disclosure to the general public.
Our results call for deeper investigation into the new privacy issues emerging in the domain of personal sensing.
Via a user study, we study the privacy concerns associated with the disclosure of data collected by wearable sensors in the mobile environment.
We first develop a conceptual framework for examining the privacy issues associated with the disclosure of continuously-collected physiological, psychological, and behavioral data.
Next, we create a new privacy survey to assess how concerns about privacy threats change as various behaviors and contexts are restricted and abstracted.
Lastly, to analyze how concern level changes as the respondents' stake in the data is increased , we administered the survey to two groups.
The first group, Group N S , completed the survey and had no personal stake in the data described.
The second group, Group S , was composed of participants in an AutoSense field study who had direct personal exposure to the collection of sensitive datasets.
They wore the AutoSense system for 3 days.
Group S completed the privacy survey twice, after the 3 day collection period, and again after they were visually presented with behavioral and contextual inferences derived from their data 
We analyze the data from three perspectives.
First, we assess how disclosure of different behaviors and contexts affect participant concern levels as their stake is increased in the data.
Second, we evaluate the impact of applying various restrictions and abstractions on concern level.
Third, we assess the impact of reidentification on the concern level as the role of the data consumer  is varied from the research team to the general public.
We find that participants are most concerned about release of conversation and commuting behavior, and release of stress, a psychological state.
Understanding and Awareness of Privacy Risks: Several well-publicized privacy breaches have contributed significantly to awareness of privacy risks .
This awareness has led to studies that seek to understand the privacy concerns of users when sharing personal data, such as location , calendars , and more generally sharing over online social networks .
Their insights do not necessarily apply to the rich set of sensors in wearable devices and mobile phones that enable non-obvious inferences about users' behaviors and activities.
Some recent papers have examined awareness of non-obvious privacy threats from personal sensing applications and WiFi data.
Using the Personal Audio Loop, Iachello et al examined privacy concerns regarding audio capture of conversation .
However, they focus on the privacy concerns of secondary stakeholders and third parties, who may be inadvertently recorded by the system.
Our work focuses on the concerns of the primary user whose conversation episodes may be inferred without any recording of the audio, i.e., from an innocuous respiration sensor.
Klasnja et al  investigate privacy concerns by interviewing participants using a physical fitness system.
Concerns were evaluated for activity, GPS, and audio data.
Our work is complementary, in that it studies concerns for physiological sensors and the psychological and behavioral inferences made from them.
Klasnja et al  and Consolvo et al  investigate privacy problems associated with daily WiFi use.
This work employs similar methods to assess privacy concerns, i.e., we also show participants their own potentially-sensitive data and then assess how exposure to the data changes awareness and concerns.
However, we apply it to expose concerns associated with the use of innocuous physiological sensors.
To our knowledge, we are the first to highlight privacy concerns associated with behavioral and affective inferences that can be made from seemingly innocuous physiological data .
Historically, these sensors have not been viewed as sources of privacy threats.
We also identify disclosure of psychological state  as concerning to users.
Reidentification attacks exploit quasi-identifiers such as age, zip code, etc., which cannot be entirely suppressed due to their utility to the end-user, but can reveal identity when combined with background information.
Current techniques, mostly explored in context of relational data, aim to preserve an individual's identity in a population.
While analogous techniques have utility for sensor time series data , anonymizing sensory information is often harder and different because sensor data  may be both sensitive and quasi-identifying, making it harder to achieve privacy without affecting utility, and  may need to be shared with identity .
Thus, existing anonymization techniques alone cannot be used to protect individuals sharing personal sensor data.
New approaches are needed to preserve behavioral privacy when identity cannot be removed from the dataset.
This section describes a conceptual framework for examining the privacy issues associated with physiological, psychological, and behavioral data captured by personal sensors .
Our framework is primarily concerned with what  calls the disclosure boundary, i.e., where privacy and publicity are in tension.
We focus specifically on the choices data producers can make that displace this disclosure boundary for personal sensory data.
In addition, the threats highlighted by our framework touch upon the identity boundary, and how unintended disclosure of an individual's behavior may change how others view the individual.
Our framework is composed of six elements: measurements, behaviors, contexts, restrictions, abstractions, and privacy threats.
Data producers capture measurements with sensors, which can then be processed to infer behaviors and contexts.
To control access to the data, data producers can put restrictions in place that prevent sharing of specified measurements and/or behaviors and contexts derived from them.
To control the level of detail in the shared data, abstractions can be applied to unrestricted measurements, behaviors, and contexts.
The set of disclosed behaviors and contexts, and the level of abstraction applied to each, ultimately decide the types and significance of the resulting privacy threats.
We motivate each element of the framework using a fictional AutoSense study participant, Jane Doe.
Jane is participating in a study to examine physiological, psychological, and behavioral factors associated with stress.
Jane is nearing the end of the third of seven consecutive days wearing the AutoSense system.
She had a stressful day at work, so she decided to run home for exercise and stress relief.
Her husband, John Doe, met her in the middle of the run, and they completed the run together.
Jane has a medical condition that causes ocassional seizures, and after the run, Jane experienced a seizure.
AutoSense recorded the following information about Jane's run, among others:
The route taken was also recorded.
She was relaxed from 5:51pm until the start of her seizure at 6:15pm.
She was terrified from the start of her seizure at 6:15pm until she removed the sensors at 6:30pm.
Measurements are the raw data captured by wearable sensors, such as ECG, respiration, and accelerometer.
Behaviors are actions taken or experienced by the data producer, and are inferred from measurements.
In Jane's case, both the running episode and the seizure could be inferred from her accelerometer measurements.
Jane was aware of the system's ability to detect running because it was mentioned in the study's informed consent document.
However, the document did not mention that accelerometer data could also capture the motion signature of her seizure.
The study designers anticipated behaviors that are not of interest might be captured by the sensors, but they did not anticipate that seizures, a sensitive medical condition, could be captured.
Other similar accidental captures of private behavior are possible.
For example, heart rate and respiration, measurements used to infer stress level could also be used to infer cocaine, heroin, and other illicit drug use, all of which affect these measurements in extreme ways.
Smoking, alcohol consumption, conversation, and commuting can also be cap-
The list of behaviors that can be inferred from sensory data is growing rapidly.
Paraphrasing , we define context as any information that can be used to characterize the situation of a behavior.
As with behaviors, contexts are explicitly stored in, or inferred from, measurements.
The example contains four types of contexts which are representative of the capabilities of today's personal sensing systems: temporal, physical, psychological, and social.
Temporal contexts describe characteristics related to the timing of a behavior, such as the exact start time of a behavioral episode.
In the example, the temporal properties include the start and end timestamps of Jane's run, seizure, emotional states, and time with her husband.
Physical contexts describe the physical environment in which a behavior occurs, such as location and objects at a location.
In the example, the addresses of the start and endpoint of the run, and the route taken between them, define these properties.
Psychological contexts describe the psychological state of the user during the behavior.
In the example, Jane experienced four psychological states: angry, stressed, and relaxed while running and terrified during the seizure.
There is a wide gamut of emotions a person can experience, all of which fall under this category .
Social contexts describe the social environment in which a behavior occurs, and could include who the user was with when the behavior occurred and whether the user was interacting with that person.
In the example, Jane's social context was initially empty.
Later, her husband John entered the social context.
Note that contexts cannot only be associated with behaviors but also with other contexts.
This is demonstrated in the association of timestamps , with Jane's locations , emotions , and the presence of her husband .
Physical threats are threats to personal safety that may result in physical harm to the data producer.
To reduce the probability of these threats when identity privacy cannot be maintained, we must maintain behavior and context privacy using restrictions and abstractions.
Restrictions remove data from a dataset before it is shared to reduce the potential privacy threats in the data.
Measurements, behaviors, and contexts can all be restricted.
For example, to prevent her husband's identity from identifying her own, Jane could restrict access to her bluetooth data.
Additionally, to make it difficult to infer the seizure, Jane could restrict access to accelerometer data.
Note, however, that this would also prevent the AutoSense team from accessing Jane's physical activity data, an important factor in moderating stress that may be of scientific interest.
Restricting all measurements, behaviors, and contexts would effectively empty the dataset, making it useless.
On the other hand, sharing all the data is not desirable either.
Abstractions provide a middle ground.
They operate on measurements, behaviors, and contexts to reduce the extent of the exposure in the dataset.
Jane's privacy could be better protected using several abstractions.
John's presence during Jane's run and seizure  could be abstracted into "family member present."
Jane's terrified state during the seizure  could be abstracted into a less specific emotion such as "stressed."
This would not remove the seizure from the dataset, but would make it more difficult for an adversary to search for unusual emotional events like the seizure.
The examples above imply that one can mitigate the threats associated with a deanomymized dataset by restricting or abstracting behaviors and contexts.
Privacy threats are the risks or harms that could come to the data producer if the his or her identity is associated with the data.
If identity were removed from the dataset, the number of threats to the data producer decreases.
Unfortunately, maintaining identity privacy is not always feasible with sensory datasets for two reasons.
First, identity is sometimes needed to retain the utility of these datasets.
A caregiver could develop a personalized treatment for a patient from his/her dataset, but would not know who to give the treatment to without the patient's identity.
Second, identity is often intertwined with useful information that the data producer would like to share.
In Jane's case, reidentification is possible from the GPS data  or from the presence of John's identity in the dataset.
Once a dataset is re-identified, there are three types of threats that could emerge: financial, psychological, and physical.
Financial threats are threats that lead to loss of assets or property.
It includes professional threats, such as the loss of a job or damage to one's business reputation.
Psychological threats affect the data producer's emotions.
To study the privacy concerns associated with the disclosure of measurements collected by wearable sensors in the mobile environment, we designed a user study with three goals.
Goal 1: Assess the privacy concerns of real people regarding disclosure of continuously-collected physiological, behavioral, and psychological data.
We also assess the change in concern levels as personal stake in the data is increased .
Goal 2: Use the proposed privacy framework and examine how restrictions and abstractions applied to various behaviors and contexts change concern levels.
66 participants were recruited from the student population at a 20,000+ student university in the United States.
Participants were recruited using flyers and word-of-mouth.
Participants volunteered to join one of two groups, a group with no personal stake in the data  or a group with a personal stake  in the data.
Table 1 summarizes the demographic characteristics of the groups.
Participants in Group S first provided informed consent.
As part of the informed consent process, Group S was informed about the data collected by the AutoSense sensors.
After consent was given, they completed the demographics questionnaire.
Then the field protocol was explained to the participant.
After explaining the protocol, the study coordinator demonstrated how to put on the sensors and then assisted participants in putting them on if necessary.
The study coordinator verified the sensors were working properly by visually examining the streaming sensor data using an oscilloscope program.
Once the sensors were verified as working, the participant was sent into the field.
Participants wore the AutoSense sensors for 3 consecutive days during their awake hours.
They were instructed to take the system off at night and put it back on in the morning.
Participants also carried the AutoSense mobile phone .
Periodically, the phone would ask participants to complete questionnaires.
Participants earned micro-incentives  for each question they completed.
On the last morning of the study, the participant returned to the lab where he/she completed the privacy questionnaire.
After the questionnaire, the participant reviewed graphs depicting the data he/she collected over the 3 days , and then completed the questionnaire again.
Lastly, participants were debriefed to collect subjective comments about their data and any concerns they had about it.
To capture the concerns of people with a personal stake in such data, we integrated this study into an existing study whose participants wore the AutoSense system for three consecutive days .
The existing study examined the use of micro-incentives for scientific data collection and the effect of interruptions on user stress level.
The data on microincentives and interruptions is outside the scope of this article and will be reported separately.
For three days, Group S participants  collected physiological, behavioral, and psychological data using the AutoSense sensor system as they went about their normal everyday life.
At the end of the three-day period, Group S participants completed a privacy questionnaire assessing their concern regarding disclosure of selected behaviors and contexts with various restrictions and abstractions applied.
Next, they reviewed graphs depicting the various behavioral and contextual inferences that were derived from their data 
Finally, they completed the privacy questionnaire again.
This within-subjects, repeated measures design allowed assessment of Group S concern level  at both a low  and high  level of personal stake in the data .
To distinguish between these two levels, the rest of this article refers to Group S before the review as Group S -P re and Group S after the review as Group S -P ost.
Group N S participants  had no exposure to continuous physiological, behavioral, and psychological data collection.
They did not wear AutoSense and did not review any data collected by it.
This allowed a between-subjects comparison of concern levels between participants with no personal stake in the data, Group N S , and participants with a personal stake in the data, Group S .
We developed the Aha visualization system for reviewing data collected by the AutoSense system from natural environments.
Aha is designed to help participants examine their daily behaviors and contexts at low, medium, and high levels of abstraction.
The Day at a Glance visualization depicts an overview of behaviors performed by individuals in their daily life.
For example, the visualization presents the fraction of time  spent commuting or in conversation.
Psychological context is also depicted here as durations of stress in a day .
The Stress at a Glance visualization depicts the fraction of time  participants are stressed  during behaviors such as commuting or walking.
The Stress at Places visualization depicts the fraction of time  at places, such as home and work , when participants are stressed .
Lastly, the Daily Timeline  visualization plots behaviors on a detailed timeline .
An analysis of 100 hours of AutoSense data found that the visualizations in Aha were approximately 80.16% accurate.
Group S participants reviewed the data they collected using Aha for 10-15 minutes.
The last question for all behaviors, stress, and place asked participants to write in an explanation for ratings of at least moderately concerned.
The latter requirement was particularly important given the already high burden placed on Group S participants, who filled out the privacy questionnaire after 3 days of wearing sensors and answering field questionnaires.
Section 7 asks participants what information about their daily life they are particularly concerned about sharing.
Sections 8 and 9 assess how concerned the participant would be if the data were shared, with or without identity, with the study coordinators, other study participants, other scientists and researchers, and the general public .
Concern is rated on a five-point scale: Not concerned , A little concerned , Moderately concerned , Concerned , and Extremely Concerned .
The questionnaire consists of 9 sections.
Sections 1 through 6 ask participants to rate their concern level for disclosure of places, smoking, stress, conversations, commuting, and exercise habits, all of which were depicted in the Aha visualizations.
However, all participants reported they did not smoke, so the smoking section was automatically skipped by the questionnaire software.
Within each section, the participant is asked to rate the section's behavior or context if it were released with no context, temporal context, physical context, and both temporal and physical context simultaneously.
The abstractions of temporal context on the questionnaire are timestamp, duration, and frequency.
Physical context is assessed at one level of abstraction, place.
Questions that ask about both temporal and physical context simultaneously ask about place and timestamp and place and duration.
We analyzed participant data with respect to the three goals of the study as discussed in the preceding section.
We now discuss the results and their interpretation from comments provided by the participants during debriefing.
Unless otherwise noted, two-tailed t-tests were used to test for significant differences .
In comparisons between Groups N S and S -P re , unequal variances were assumed.
In comparisons between Groups S -P re and S P ost , a paired t-test was used.
ShapiroWilk tests confirmed the normality of the data.
Where space allows, p-values are reported with the means and standard deviations of the corresponding distributions.
To analyze participant concerns and the effect of personal stake on those concerns, per-participant averages of concern were calculated for conversation, commuting, exercise, stress, and places for all three groups.
Figure 5 depicts these summary measures for all three groups.
Group N S and Group S -P re provided similar concern ratings for exercise and place, and no significant differences were found between their distributions.
Group S -P ost had higher concern ratings than Group S P re.
Furthermore, across Group S -P ost, 20% of stress questions were rated at "Concerned" or "Extremely Concerned", 15% for commuting, 12% for conversation, and 8% for exercise.
Disclosure of place did not see a significant increase in concern from pre- to postreview .
Taken together, the results for Groups N S , S -P re, and S P ost indicate increasing personal stake in the data helps people better estimate their concerns regarding the disclosure of behaviors and contexts.
The highest level of concerns emerged for exercise, conversation, commuting, and stress after participants observed visual depictions of their data.
Surprisingly, only 10-15 minutes of data observation was needed for these differences to emerge.
To better understand the rationale for participants' concern ratings, we examined their free-form survey responses and debriefing comments.
Two participants indicated their concern regarding commuting stemmed from an overall concern about being watched: "I am uncomfortable with someone watching over me that closely."
Two other participants expressed similar concerns: "It would be strange knowing that a person knew exactly where I was going to be, and when, at all times of the day.
I wouldn't feel like I had any privacy."
Although the survey did not have any questions about social context, some participants said they were concerned about revealing social context with conversation: "I don't like people to know whom I talked to at all...as i feel it's none of their business."
Another participant said, "I do not want to share even when I talk because people may find out with whom if it is marked with time, which can be private."
This participant felt that adding time to conversation data may provide enough information to reveal who he talks to.
Participant concerns about stress appear to stem from a fear of having one's inner thoughts revealed.
One participant said, "I have this level of concern because whenever I am stressed out, I am around my mother."
The comment implies the participant does not want to reveal to her mother that she is a source of stress.
Another participant said: '`I feel my stress data if revealed can be a professional issue as am generally hot headed."
People expect psychological states and inner thoughts to be private.
Psychological monitoring technologies threaten this expectation.
Although concerns about exercise were low, participants commented that they did not want their exercise habits revealed.
One participant explained: "I guess I'm a bit self-conscious over how little I exercise; I'd hate to have that broadcast."
Generally speaking, adding temporal context to behaviors and other contexts increased concern level, with each decreasing level of temporal abstraction leading to a corresponding increase in level of concern.
More often, adding timestamps induced a significant change in concern as compared to adding duration.
Participants were most concerned about others knowing the exact moments they experience stress.
Adding duration to stress events did not increase concern significantly, but adding timestamp increased concern by approximately 50%.
Participants were most concerned about sharing physical and temporal context together.
There was a trend of increasing concern for place and timestamp, with the lowest concern for just sharing the behavior or context, the next level of concern for reporting the place or time of the behavior or context, and the highest level of concern for reporting both the place and time of the behavior or context.
Across Group S -P ost, 9% of behavior-only concerns were rated at "Concerned" or "Extremely Concerned", 11% for behavior with place, 20% for behavior with timestamp, and 26% for behavior with both timestamp and place.
Adding both place and time simultaneously increased concerns significantly for conversation, commuting, and stress.
Indeed, one participant explicitly expressed a concern about stress, time, and place: "I'd be concerned with people being able to tie the `whens' of my being stressed with the `wheres.'
I'd rather people not know that I felt stressed at my particular job or when at my house, because they wouldn't have the whole picture."
Exercise, however, did not exhibit a significant change from adding timestamp or place alone.
Participants are likely less concerned about exercise episodes, since exercising is a positive behavior which they may be proud to share with others.
Lower concerns for sharing place alone is consistent with the growing use and acceptability of location sharing sys-
Effect of adding duration and timestamp to behaviors and other contexts .
Each cell is the result of a paired t-test comparing the particular row-column pair with that same pair, but with duration or timestamp added.
The result for duration is the first value in each cell, and the second value is the result for timestamp.
The results highlight an overall lack of awareness of the actual information contained in personal sensing datasets and the privacy threats associated with them.
Participants who had little or no personal stake in the data, representative of most of the population today, did not appear to understand the sensitive nature of the data.
Interestingly, concerns regarding place did not change after the review session.
One interpretation is that participants were already aware of the potential threats associated with sharing place, due to the growing use of location-based services and media coverage of the issues associated with them.
Participants rated their concern with respect to sharing identified and anonymous data with 4 groups: our research team, other study participants, other scientists and researchers, and the general public.
Figure 7 summarizes participant responses.
10% of participants rated releasing data to the general public without identity at "Concerned" or "Extremely Concerned."
This increased to 70% after adding identity.
Releasing the data to members of our research team with identity increased concerns, but not significantly.
There was a significant increase in concern from releasing identified data to other researchers to releasing identified data to the general public .
Note that releasing identity to the general public more than doubles concerns to 3.1, the highest average concern level reported on the privacy survey.
A trend toward significance was found between releasing identified data to members of our research team and releasing identified data to other study participants .
Releasing identified data to study participants and to other researchers generated approximately the same level of concern.
For anonymous data, increase in concern rating trended toward significance only when the recipient was changed from other researchers to the general public .
The results indicate disclosure of any data to the general public is of significant concern to participants.
When identity is added to the dataset, this concern more than doubles.
The results imply the components of the proposed framework - behaviors, contexts, restrictions, and abstractions - can be manipulated to mitigate perceived threats in a dataset.
Avoiding combinations of temporal and spatial context significantly reduced concerns about the dataset.
Likewise, the results suggest one should be extremely careful with revealing psychological states, as indicated by the high concern regarding stress.
Abstractions also clearly play a role in defining the threats in the data.
Timestamps generally raised concerns significantly when combined with other contexts and behaviors.
Increasing the level of abstraction  decreased these concerns.
Ultimately, the choice of which behaviors and contexts to release, and how they should be abstracted, cannot be decided purely based on the threats created by those choices.
Increasing abstraction may reduce the threats in the data, but it also reduces the utility of the data.
For example, a scientist cannot study daily variations in stress if only the durations of stress events are revealed.
Likewise, a caregiver cannot study a patient's gait if the patient's accelerometer data is abstracted into "moving" and "not moving" states.
Clearly, manipulations of restrictions and abstractions need to be tailored for both the data contributor and consumer.
Only by customizing privacy transformations can the privacy requirements of the data producer and the information quality requirements of the data consumer be met.
This same observation inspired Iachello and Abowd's  complementary privacy framework, which aims to balance the utility of a UbiComp application with its burden on privacy.
The results of the study imply psychological context requires the most attention from the privacy and broader HCI community.
Ratings indicated participants were more concerned about episodes of stress being revealed than any other behavior or context.
The section on stress in the privacy questionnaire garnered more written comments than any other section, both pre- and post-review.
Psychological states like stress are different from behaviors and other contexts because they cannot be observed with the naked eye.
Thus, psychological states are private by default, and remain private unless they are revealed.
Personal sensing technologies that assess psychological states change this fundamental expectation.
They enable mind-reading, a significant concern highlighted by participant comments.
However, such information can be useful, especially in the context of studying and treating psychological disorders .
Methods to mitigate the harms associated with psychological context while still preserving their utility should be developed to address these concerns.
This paper did not assess participant concerns for combinations of psychological context with other behaviors, nor did it assess multiple abstractions of psychological context.
Furthermore, it only assessed concerns for stress, which is not as specific as releasing specific emotions.
In addition, the community could examine social context's effect on a dataset.
Threats from social context are different than the others assessed here because they affect not only the user wearing the sensors, but also potentially any person interacting with the user.
Lastly, this study only examined privacy concerns from the perspective of data producers.
The community should also examine how data consumers perceive privacy issues and what aspects of the data make it useful.
Such a study would provide a better understanding of how to tradeoff behavior privacy and utility for physiological, psychological, and behavioral data collected by personal sensors.
Either variable alone or a mix of both could have increased concerns.
A future study will address this, wherein Group N S will be shown visual depictions similar to those shown to Group S , except the depictions will not represent the participant's personal data.
Assessing privacy concerns before and after exposure to the non-personal data will allow separating the effect of both variables.
We acknowledge the contributions of the following individuals.
Anind Dey, Mustafa al'Absi, and Deepak Ganesan contributed to the study design, while Mary Read, Kelly Peck, Andrew Hoff, Ken Ward, and Satish Kedia helped conduct the study.
Emre Ertin, Nathan Stohs, and Siddharth Shah developed the AutoSense sensor suite.
Dan Siewiorek, Asim Smailagic, Patrick Blitz, Brian French, and Scott Fisk contributed to the study smartphone software.
Context detection algorithms used in the visualization software were developed by Somnath Mitra , Md.
Mahbubur Rahman and Amin Ahsan Ali , and Kurt Plarre .
Data collected by wearable sensors provide personal and societal utility, but also contain many unknown threats.
Our study provides three key insights into the perceptions of threats from such datasets and how those threats can be mitigated.
First, our results indicate people cannot understand the potential threats in the data unless they have a personal stake in it.
Of the behaviors and contexts we examined, people were most concerned about revealing conversation, commuting, and inherently private psychological states .
Second, adding physical and temporal context increases concerns about the data, but those increases can be mitigated through restriction and abstraction.
Third, people are willing to share such data - even with their identity - with us, other study participants, and other researchers.
However, participants have significantly more concerns regarding sharing with the general public , especially when released with identity.
Given the concerns about psychological context, and our growing ability to capture psychological states, the com-
AutoSense: A Wireless Sensor System to Quantify Personal Exposures to Psychosocial Stress and Addictive Substances in Natural Environments.
M. Barbaro, T. Zeller, and S. Hansell.
A Face is Exposed for AOL Searcher No.
L. Barkhuus and A. Dey.
Location-Based Services for Mobile Telephony: a Study of Users' Privacy Concerns.
Exploring end user preferences for location obfuscation, location-based services, and the value of location.
S. Consolvo, J. Jung, B. Greenstein, P. Powledge, G. Maganis, and D. Avrahami.
Activity sensing in the wild: a field trial of ubifit garden.
S. Consolvo, I. Smith, T. Matthews, A. LaMarca, J. Tabert, and P. Powledge.
Location Disclosure to Social Relations: Why, When, & What People Want to Share.
A. Dey and G. Abowd.
Towards a Better Understanding of Context and Context-Awareness.
FieldStream: Network Data Services for Exposure Biology Studies in Natural Environments.
PoolView: Stream Privacy for Grassroots Participatory Sensing.
AutoWitness: Locating and Tracking Stolen Property while Tolerating GPS and Radio Outages.
AOL Removes Search Data on Vast Group of Web Users.
G. Iachello and G. Abowd.
Privacy and proportionality: adapting legal evaluation techniques to inform design in ubiquitous computing.
Prototyping and sampling experience to evaluate ubiquitous computing privacy in the real world.
Privacy Implications of Personal Locators: Why You Should Think Twice before Voluntarily Availing Yourself to GPS Monitoring, The.
P. Klasnja, S. Consolvo, T. Choudhury, R. Beckwith, and J. Hightower.
Exploring privacy concerns about personal sensing.
P. Klasnja, S. Consolvo, J. Jung, B. Greenstein,  L. LeGrand, P. Powledge, and D. Wetherall.
When I am on Wi-Fi, I am fearless: privacy concerns & practices in everyday Wi-Fi use.
Inference Attacks on Location Tracks.
Stress and emotion: A new synthesis.
N. Li, T. Li, and S. Venkatasubramanian.
Mercury: A Wearable Sensor Network Platform for High-Fidelity Motion Analysis.
A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam.
Price, L. Jedrzejczyk, A. Bandara, A. Joinson, and B. Nuseibeh.
From spaces to places: emerging contexts in mobile privacy.
Respiratory Markers of Conversational Interaction.
Peir, The Personal Environmental Impact Report, as a Platform for Participatory Sensing Systems Research.
A. Narayanan and V. Shmatikov.
Robust De-Anonymization of Large Sparse Datasets.
A. Narayanan and V. Shmatikov.
S. Oliveira and O. Zaiane.
Privacy Preserving Clustering by Data Transformation.
L. Palen and P. Dourish.
Unpacking Privacy for a Networked World.
S. Patil and J. Lai.
Who gets to know what when: configuring privacy permissions in an awareness application.
Achieving k-anonymity Privacy Protection Using Generalization and Suppression.
International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10, 2002.
Empirical Models of Privacy in Location Sharing.
Generating default privacy policies for online social networks.
