Increasing use of automation in computer systems, such as advanced cockpits, presents special challenges in the design of user interfaces.
The challenge is particularly difficult when automation is intermittent because the interface must support smooth transitions from automated to manual mode.
A theory of direct manipulation predicts that this interface style will smooth the transition.
Interfaces were designed to test the prediction and to evaluate two aspects of direet manipulation, semantic distance and engagement.
Empirical results supported the theoretical prediction and also showed that direet engagement can have some adverse effeets on another concurrent manual task.
Generalizations of our results to other complex systems are presented.
An important question in designing the user interface of modem cockpits is how to handle automation.
Our researeh is part of a larger researeh program in adaptive automation whose role is to allocate tasks between the pilot and the computer system in an optimal manner .
In adaptive  automation, the pilot performs a task only intermittently.
Given a dual task situation, arise in the level of difficulty of one task causes automation of the second task.
Having the computer system take over the second task allows the pilot to focus his efforts on the increased difficulty task.
Once the difilculty level of the fmt task returns to normal, the pilot resumes control of both tasks.
Such an approach to automation is expected to result in better overall pilot/system performance .
Because the pilot only performs the first task intermittently, a challenging problem, and the problem that this paper addresses, is how to design an interface that supports a smooth transition from automated to manual mode.
This paper presents the results of our empirical research on interface styles for adaptive automation.
Our research is designed to test predictions from a theory of direct manipulation.
A fundamental goal of the research is to determine whether a direet manipulation interface has performance benefits in adaptive automation; i.e., does direet manipulation lead to improved performance when a pilot must quickly resume a task that has been previously automated?
A related goal is to separate and evaluate two aspects of direct manipulation identified by the theory, In this paper, we namely, distance and engagement.
We conclude with a discussion of the implications of our results.
However, modem cockpits differ from traditional office systems in several fundamental ways.
FirsL unlike office systems, they often include sophisticated automation, such as the ability to fly on automatic pilot.
Moreover, unlike office applications, the cockpit application is dynamic and complex.
The pilot must not only handle large quantities of red-time, often continuous, input daw he must also perform several demanding tasks concurrently, usually under severe timing constraints, Finally, unlike users of office systems who typically communicate via electronic mail, the pilot of a modem cockpit communicates in real-time via networked voice and data links.
Given these differences, the cockpit interface presents many design challenges that the developers of office systems seldom encounter.
We define situational awareness as the extent to which the pilot has the knowledge needed to perform a specified task or tasks.
Clearly, this knowledge depends upon the specific state of the aircraft and selected aspects of the aircraft environment.
In adaptive  automation, the pilot shifts from manually performing a task to monitoring its automated performance and then back to manual operation.
In this situation, the key to assessing situational awareness is how well the pilot can resume a task that has been previously automated.
We claim that a critical factor in achieving a smooth transition from automated to manual performance of a task is interface style.
Hutchins, Hollan, and Norman  have developed a theory of direct manipulation .
They characterize direct manipulation interfaces according to a model world metaphou the user interacts with an interface that represents the task domain itself, the domain objects and the effect of user operations on those objects.
Command language interfaces behave according to a conversational metapho~ the user and the interface have a conversation about the application domain.
The interface acts as an intermediary between the user and the domain.
Although typically associated with office computer systems, direct manipulation is also being considered for large safetycriticaI systems, such as nuclear power plants .
HHN concluded that `two aspects of direct manipulation account for its performance advantages, low distance and direct The first aspect is the "information engagement.
Performance advantages come with less distance, because there is less cognitive effort needed to understand and manipulate the domain objects.
HHN call such an interface semantically direct and claim that it can be achieved by "matching the level of description required by the interface language to the level at which the person thinks of the task".
Distance is of two types, semantic and articulator.
Semantic distance is the difference between the user's intentions and the meaning of the expressions available in the interface, both expressions that communicate the user's intentions to the computer and expressions whereby the computer system provides user feedback.
For example, if the user wishes to delete all files whose names end in text and the computer system  has no single expression for this purpose, then significant semantic distance exists between the user's intentions and the expressions available in the interface.
Articulator distance is the difference behveen the physical form of the expressions in the interface and the user's intentions.
For example, when a Unix user wants to display a file and to do so he must invoke a command named "cat", significant articulator distance exists between the name of the Unix command and the intended user operation.
Our studies have focused on semantic distance.
We have proposed followup studies to investigate issues concerned with articulator distance.
The second aspect of direct manipulation is engagement, i.e., the involvement that comes when the user is able to interact directly with the application domain and the objects within it rather than interacting through an intermediary, The key to direct engagement is interreferential I/0, which permits "an input expression to incorporate or make use of a previous output expression".
For example, if a listing of file names are displayed on the screen, one of these names cart be selected and operated on without entering the name again.
In Draper's view , the important aspect of interreferential I/O is that the user and the computer system share a common communications medium.
This takes the notion of interreferential I/O beyond the Unix concepts of In direct manipulation, the shared channels and pipes.
Contrary to expectations, iconic interfaces were inferior to menu systems and command language interfaces for new and transfer users.
Mom recent studies have generally shown advantages for direct manipulation over command language interfaces .
For example, Karat  found consistently faster times for several file management tasks in a direct manipulation interface that used pointing and dragging operations on iconic representations of fdes.
However, Karat did find an advantage for the command language interface on one particular type of file management task.
Thus, evaluations of interface styles need to be sensitive to task-specific effects.
Along this line, Elkerton and Palmiter  suggest that the basic principle of direct manipulation lies in the replacement of complex cognitive operations with perceptual and motor activities.
Thus the advantage of direct manipulation may lie in tasks with complex cognitive operations that can be transformed into motor and perceptual operations.
Research on direct manipulation has been mostly on conventional applications, such as word processing and fde management.
A notable exception is a study by Benson and her colleagues  which compared a conventional interface to a direct manipulation interface for a pacts manufacturing system.
The conventional interface used menus, function keys, typed commands, displayed textual information, and paged displays.
The direct manipulation interface used a mouse as the only input device and provided a continuous display of important information.
The evaluation of these interfaces used performance measures relevant to manufacturing, such as cost, inventory levels and status, and late deliveries.
Performance with direct manipulation was superior on three of five dependent measures.
All previous research on direct manipulation has not attempted to tease apart semantic distance and direct engagement, and determine which is important in user performance.
The effectiveness in the cockpit of a direct manipulation interface and its two aspects remains an open question.
Some studies suggest that navigation displays should present a model world to the pilot.
For example, in 1987, Marshak, Kuperman, Ramsey, and Wilson  found that moving-map displays in which the viewpoint is similar to what would actually be seen by looking outside the plane led to improved performance.
However, in other kinds of displays, a graphical representation of the model world does not provide an advantage.
For example, Reising and Hartsock  found that in warning/caution/advisory displays, a schematic of the cockpit showing the controls that were needed to handle art emergency did not improve performance.
The important factor in improved performance was a checklist of the required procedures .
Ironically, in modem flight control systems, some trends have been away from direct manipulation.
For example, fly-by-wire systems remove the pilot from direct control of wing surfaces.
Bemotat  argues against this trend, suggesting that, in such systems, the pilot needs direct sensory feedback about the aircraft's performance.
Such feedback is consistent with the notion of direct manipulation.
Other trends in cockpit controls suggest a move toward direct manipulation, e.g., the incorporation of touchscreen displays.
However, the incorporation of pointing devices into the flight deck needs to be carefully evaluate~ e.g., what is the effect of the pilot's use of two pointing devices concurrently ?
Direct Manipulation In Intermlttant Automation An issue in interface design for intermittent automation is automation dejicit, the initial decrease in pilot performance that occurs when a task that has been previously automated is resumed.
This deficit may reveal itself in several wayx slower human response, less accurate human response, subjective feelings of not being in control, subjective feelings of stress, etc.
Some previous studies have shown an automation deficit for manual control tasks, while others have not .
In our research we are interested in automation deficits in response time and the effect of interface style on automation deficit.
Our hypothesis is that direct manipulation interfaces lead to a reduction in automation deficit that is reflected in decreased response times right after automation ceases.
The rationale underlying this hypothesis is that decreased semantic distance and improved direct engagement enhance a pilot's ability to monitor a task that is automated and then to quickly resume the task.
Each time the difficulty of the tracking task rose to high, the tactical assessment task was automated, and the subject performed the tracking task only.
The display screen used in the experiment was partitioned into two nonoverlapping windows, one for the tracking task, the other for the tactical assessment task.
The target on the display was a graphical representation of an enemy aircraft.
The tracking control was a self-centering, displacement joystick.
The two levels of tracking difficulty, high and moderate, were produced by changing the movement rate of the target.
Performance measures included RMS amplitude calculated for each axis.
In addition, the target's and the subject's movements were recorded for later analysis.
Our hypothesis was tested on interfaces for the tactical assessment task.
The simulated tactical situation included three classes of targets - fighters, aircraft, and ground-based missiles - and contacts on the targets by sensor systems.
The targets frost were designated as possible threats using black color coding, but as they got closer to the owrwhip , they were designated as neutral, hostile, or unknown, using blue, red and amber color coding, respectively.
The subjects were told that simulated sensor systems were assigning these designations.
The subjects were required to perform two operations, confirm and classify.
If the system designated a target as neutral or hostile , the subject had to confirm the designation by picking the target and then indicating the proper designation, i.e., neutral for blue targets and hostile for red targets.
Thus, confii decisions only required the subject to discriminate colors.
If the system designated the target as unknown , the subject had to classify the target as hostile or neutral based on its behavior.
Table 1 provides the rules for designating a target as hostile or neutral.
The target class determines what target attribute the subject uses to determine the target's designation.
The words `HOSTILE and `NEUTRAL' in the two side strips are colored red and blue, respectively.
For classify decisions, the subject needs to observe the behavior of the graphical symbol that represents the target to determine the For confirm decisions, the proper target designation.
Rules for tactical assessment of targets To classify the amber targets, the subject needed to monitor heading for fighters, speed for aircraft, and projected laterai distance for ground missile threats.
The responses were timed and analyzed to produce measures of accuracy and response time.
Training was provided on each task alone and on the dual task without automation.
A total of twenty subjects were tested on the intermittent automation, five on each of the four interfaces.
Twelve subjects were retested four months later.
They received a 3 minute retraining session on both tasks.
Further details of the experiment are presented elsewhere .
These four interfaces, which include a direct manipulation interface, a command language interface, and two hybrid interfaces, represent the four combinations of semantic distance and engagement shown in Figure 1.
Below, we briefly describe each interface and discuss how each implements some combination of semantic distance and engagement.
The direct manipulation inte~ace  has direct engagement and low semantic distance.
It uses a shared both the subject and the communications medium: computer use the entire tactical assessment window to communicate.
The symbol us~d to represent a target is an intuitive graphical representation of the target class.
Each target symbol is initial y colored black but changes to red, blue, or amber once d e system assigns the target a designation.
A touchscreen overlays the display.
The subject confirms or classifies a tii.rget by picking a target symbol on the display and seledtirtg one of two strips, labeled HOSTILE and NEUTRAL, located on either side of the display.
The command language interface  has indirect engagement and high semantic distance.
This interface uses a split visual medium: the tactical assessment window is partitioned into a top portion, which displays a table of target names and attributes, and a bottom portion, which is for subject input and error feedback.
Each entry in the table describes a single target, providing the target's name , the target's class, and continuously updated data about the target.
Thetable isdecluttered i.e., itonly presents thecritical attribute forthe given target class.
After the subject has completed a classify or a confii operation on a target, the system removes the target entry from the table by scrolling the table.
The subject uses a keypad to invoke a confirm or classify operation.
For each operation, two sequential keypresses are required, one designating hostile or neutral, a second indicating the target number.
For classify decisions, the subject needs to interpret the data in the table to determine the appropriate target designation.
For confirm decisions, the subject needs to interpret the color of the word identifying the target class.
One important difference between the command language interface described above and the command language interfaces associated with more traditional office systems is that the table of target data is updated continuously.
Such an approach is dictated in an aircraft context by the impact of external factors on the domain objects  and the real-time demands of the tactical domain.
The approach makes less sense in an office system where, in most cases, changes to domain objects are made solely by the user and rapid response times are not as crucial.
The third interface , the graphical/keypad inte~ace, combines the low semantic distance of the first interface with the less direct engagement of the second interface.
Like the command language interface, this interface splits the tactical assessment window into two portions.
The top portion contains the simulated radar display; the bottom portion is for subject input and error feedback.
The subject uses the keypad to enter his classify and confmn decisions.
Finally, the fourth interface , the tabzdar/pointer interface, combines high semantic distance with direct selection of the tactical targets on the display using a touchscreen.
The subject picks a target by touching the appropriate table entry.
He designates the target by touching either the HOSTILE or NEUTRAL strip at the sides of the display.
This last interface is similar to a menu interface, except that the table items are updated dynamically.
Scrolling in this interface occurs just after the subject completes entry of the confirm or classify decision and is thus associated with the completion of a user action.
To support the second goal, all relevant information about each target was encapsulated by this graphical representation.
The high distance display was designed to support only the second goal, user performance of the assigned task.
In developing the high distance display, considerable effort was required to design a table that effectively supports the assigned task.
For example, the target's spatial coordinates  were not provided because they are not relevant to the task and would have made the table harder to inteqret, Moreover, the color code indicating the type of decision required was shown in the class column only, thus separating the system-assigned designation from the target attribute information.
Finally, the columns were arranged to support efficient eye movements.
The levels of engagement can also be considered from several perspectives.
We provide a pointing device  for high engagement and a keypad for low engagement.
The keypad uses a mode shift for two keys in order to preserve a common aspect of command language interfaces and to avoid introducing direct engagement with labelled keys for each action and object, a feature that Shneiderman associates with direct manipulation .
The theoretical difference between the levels of engagement in the interfaces is based upon the notion of a shared medium.
In the direct engagement interfaces , both the user and the computer system use a shared communications medium, that is, they both operate on the same objects.
In the direct manipulation interface, the shared medium is the spatial display.
The objects to be operated on are the target symbols and the strips labeled `HOSTILE' and `NEUTRAL'.
In the tabular/pointer interface, the shared medium is the table, and the objects to be operated on are the table entries.
In both direct engagement displays, the objects to be operated on and the strips sham the same color code.
Thus, for example, red in either the spatial display or the table of target attributes indicates that the subject should select the Srnp with the red wording.
In the indirect engagement interfaces , the computer communicates to the user through one medium  and set of objects, while the user communicates to the computer through another medium  using a different set of objects.
Thus there is a separation of the user input and computer output.
In the Interfaces Distance and Engagement Although the four interfaces intuitively represent different combinations of semantic distance and engagement, it is important to understand the theoretical rationale for the level of distance and engagement in each interface.
Metaphorically, the direct manipulation interface qresents a model world of the task domain, the command language interface a verbal description.
A graphical representation more closely matches the way that a pilot thinks about the tactical situation.
More importantly, these two interfaces support the user's goals differently.
Similar results were found in the initial testing with the larger set of 20 subjects.
As shown in Figure 3, with the direct manipulation interface, initial performance was as good as later performance.
In other words, virtually no automation deficit was found with the direct manipulation interface.
In contras~ automation deficit was Later clearly present in the two hybrid interfaces.
This is shown by the reduction in response time for the later response in the two hybrid interfaces.
If neither component of direct manipulation was present, as in the command language interface, both initial and later performance were poor.
Further analysis has suggested that there may still be a deficit after a minute or so in handling events at a high rate with the command language interface when the tactical task is completely automated .
Accuracy was related to the type of decision and the type of information that had to be interpreted Accuracy for the confirmation decisions was 95% and for the classification decisions was 7870.
Accuracy was lowest for classification decisions which depended upon monitoring whether a number was changing.
This occurs when the subject monitors the bearing of a fighter.
In a multiple task domain, the interface for one task might have effects on other tasks.
An interesting intra-task effect of engagement  was found when the performance on the fracking task was examined.
Those using the keypad for the tactical assessment task had better tracking in the initial phase of resuming the tactical assessment task than those using the touchscreen.
To understand this R.SUIL it is useful to consider touchscreen usage as a form of tracking, and initial performance of this additional tracking task may interfere with making required adjustments to the other  tracking task.
This result suggests that the touchscreen in the tactical assessment task induces an automation deficit in the tracking task.
This occurs even though the subjects have been continually doing the tracking task.
We also analyzed the effect of the type of decision and fhe type of display on automation deficit.
We found that automation deficit was related significantly to the interaction between the type of decision and the type of display, F = 7.89, p < .02.
On classification decisions, automation deficit was greater with the tabular displays.
On confirmation decisions, the deficit was greater with the graphical displays.
The interaction is best illustrated by calculating the difference between the fiit response and the seventh response .
This pattern was also seen in the retesting four months later, although it was not as strong.
Twenty-four rating scales were used to obtain subjective judgments about feelings of control, feelings of awareness, preferences for the interface, judgments of the difficulty in Ieaming and performing the tasks and specific aspects of the tasks, and ability to anticipate the changes in automation.
Significant results were found on five scales.
Those with the graphical display felt that they were able to anticipate the changes more often than those with the tabular interface.
Furthermore, those with the graphical display felt that they were signifkantly more aware of the tactical situation at the end of automation.
Debriefing confirmed that subjects with the graphicat interface noticed the ebb and flow in activity during automation , but those in the two tabular interfaces did not.
The reason is that responses with the non-dirwt manipulation interfaces improved during periods when the event rate was lower .
Thus the four interfaces supported comparable performance in "normal" operation.
It is likely that the confiiation decisions wete best supported by the tabular display because the user did not need complete information about the object but simply needed to know the value of a single parameter.
If the model world metaphor is implemented faithfully, then different representations for different decisions are not directly possible.
Thus an extension of the theory should be considered to support different levels of representation for different requirements.
Second, we found that the theory does not always help with detailed aspects of interface design.
Our goal was to evaluate interfaces that had different levels of distance and engagement.
The iterative design prwess we used forced many decisions about details of each of the four interfaces.
Many of these decisions were based upon performance considerations and could not be based upon logical derivations from the tenets of the theory.
Furthermore, the performance constraints were related to the specific application.
For example, the relative placement of the two windows  had an impact on how easy it was to use hands dedicated to the two tasks.
This is a stimulus-response compatibility issue that the theory does not address.
In essence, the theory is not performance based as are other formal models such as GOMS.
It is most relevant in dealing with aspects of the interface that relate to its cognitive complexity.
Finally, we found that distance and engagement are difllcult terms to define operationally and to evaluate.
Our experiment required interfaces that combined different levels of distance and engagement.
In other words, these were design requirements for the interfaces.
One of the problems is how to distinguish between distance and engagement.
Our empirical results suggest that they = not independent, in that the degree of automation deficit in the command line interface was not a combination of the deficits in the two hybrid interfaces, which each lacked an aspect of direct manipulation.
HHN themselves point out that engagement is only present when both semantic and articulator direcmess is present.
The interfaces that we produced represented combinations of different levels of distance and engagement.
What is not clear is how much distance and engagement were actually present.
It is apparent that any interface that allows the person to perform a task successfully has bridged the distance of the gulfs of execution and evaluation as HHN discuss them.
The command language interface we produced supported the user's goat of performing the task and therefore reduced semantic distance to a greater degree than an interface which would not support this goal.
And yet, it did not provide a view of the model world as a pilot would normally think of it, so considerable distance still remained.
Better precision about the degree of distance and engagement in an interface would be helpful.
These results show that subjects using the graphical display were able to monitor events during automation.
Their ability to anticipate the changes could have produced improved performance, at least for those who had the graphical display and the touchscreen.
Several other questions were asked about activity during automation, and subjects accurately described some global characteristics of what had occurred during automation , but not details.
The theoretical implications are based upon both empirical results as well as observations we made during the course of developing the interfaces and conducting the experiment.
On the positive side, we found that the theoretical predictions that we made were generally supported.
This result is noteworthy for several reasons.
First, this research is a rare example of designing interfaces to test a theory explicitly.
Previous studies of direct manipulation and command language interfaces have used interfaces for established applications which may not fairly represent the theoretical concepts.
Second, our predictions concern a specific aspect of performance  in a complex, multitask situation.
Either challenge--specificity of prediction or complexity of context---would put demands on a theory.
Both were present in this research, which makes the successful predictions of the theory especially impressive.
However, we also found that the theory has limitations.
First, the theory does not address interfaces which include a mixture of interface styles and which are probably the rule more than the exception in complex applications.
The reason is that complex applications involve different types of tasks.
A single interface style may not support all tasks in an optimal manner.
In the HHN theory, a general interface for the application is assumed.
This requires choosing a representation that is suitable for most tasks.
But it may not be optimal for certain tasks.
Thus choosing a single interface style for a complex application may produce suboptimal performance on some aspects of the application.
This point is important because it is based not only on observation but on empirical results.
In our data we found evidence that the optimal display for reducing automation deficit depends upon the type of decision.
Simple deeisions were served better by the tabular display, complex decisions by the graphical display.
In terms of theoretical predictions, the shortcoming of the HHN theory is that it  did not make predictions about the simple decisions.
Although our results were found in a cockpit application, extension to other systems is appropriate, particularly systems in which the operator is intermittently moving from one task to another.
To envision potential generalizations, it is helpful to characterize our application in abstract terms.
The cognitive complexity of the intermittent task was manipulated by changing the interfaces and by changing the decisions.
The results can be interpreted at an abstract levek incnmses in the cognitive complexity of an interface adversely affect the resumption of its use after a period of automation.
This principle certainly holds for systems that include the two types of tasks.
The principle would probably hold for systems which have greater complexity on the continuous task.
In fac~ the effects of interface would probably be greater, The key to appropriate generalization is that there was relatively little cognitive interaction between the two tasks.
There was some manual interaction as noted below.
Generalization may not be warranted if the system includes multiple tasks which use similar cognitive processes.
In a multitask application, there may be different forms of expressions to the various tasks; the interaction of these expressions is an important issue.
Direct engagement in particular may introduce incompatibilities, We found that tracking performance was adversely affected in the initial seconds of resuming pointing with the touchscreen.
The cause was an incompatibility between the two forms of manual manipulation.
The important issue is whether direct manipulation interfaces to different tasks could compete.
According to Wickens , the answer is yes, In his resource theory, competition for attentional resources occurs whenever information to the user is' in similar modalities or is in a similar code .
Competition also occurs whenever responses are similar.
Thus two direct manipulation interfaces which both have spatial graphical displays, and which both require pointing devices could produce competition for attentionai resources.
Thus the generalization of our results to other multiple task systems should be made with consideration given to possible competition between aspects of the direct manipulation interface.
Beltracchi, L. A direct manipulation interface for heat engines based upon the Rankine cycle.
IEEE Transactions on Systems, Man and Cybernetics.
Benson, C. R., Govindaraj, Krosner, S. P. Effectiveness of direct manipulation interaction in the supervisory control of FMS parts movement.
IEEE International Conference on Systems, Man, and Cybernetics.
Bemotat, R. K. Man and computer in future on-board guidance and control systems of aircraft.
Man-computer interactwn: Human factors aspects of computers and people.
Draper, S. W. Display managers as the basis for usermachine communication.
Erlbaum Associates, Hillsdale, NJ, pp.
Erlbaum Associates, Hillsdale, NJ, pp.
Proceedings of the Human Factors Society 31st Annual Meeting.
Human Factors Society, Santa Monica, CA, pp.
D. E. An overview of human-computer 8.
Journal of the Washington Aca&my of Sciences.
Marshak, W. P., Ku~erman, G., Ramsey, E. G., and Wilson, D. Situatbn awareness in map displays.
Proceedings of the Human Factors Socie~ 31st Annual Meeting.
Human Factors Society, Santa Monica, CA PP.
Parasuraman, R,, Bahri, T,, Deaton, J. E,, Morrison, J. G. and Barnes, M. Theory and design of adaptive automation in aviation systems.
Cognitive Science Laboratory, The Catholic University of America, Washington, D. C. .
Reising, J. M. and HartSock, D. C. Advanced warning/caution/advisory displays for fighter aircraft.
Proceedings of the Human Factors Society 33rd Annual Meeting.
Human Factors Society, Santa Monica, CA.
Whiteside, J., Jones, S., Levy, P. S. and Wixon, D. User performance with command, menu, and iconic interfaces.
ACM CHI'85 Human Factors in Computing Systems.
Codes and modalities in multiple resources: A success and a qualification.
Ziegler, J. E. & Fahnrich, K. P. Direct manipulation.
Elsevier Science Publishers, North-Holland, pp.
We acknowledge Rob Jacob for contributing to the initial idea for the hypothesis and Rob Carter and Diane Dames for contributions to the design.
We thank the subjects for their participation.
This research was supported by the Office of Naval Technology.
