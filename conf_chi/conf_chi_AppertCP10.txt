Focus+context interfaces provide in-place magnification of a region of the display, smoothly integrating the focus of attention into its surroundings.
Two representations of the data exist simultaneously at two different scales, providing an alternative to classical pan & zoom for navigating multiscale interfaces.
For many practical applications however, the magnification range of focus+context techniques is too limited.
This paper addresses this limitation by exploring the quantization problem: the mismatch between visual and motor precision in the magnified region.
We introduce three new interaction techniques that solve this problem by integrating fast navigation and high-precision interaction in the magnified region.
Speed couples precision to navigation speed.
Key and Ring use a discrete switch between precision levels, the former using a keyboard modifier, the latter by decoupling the cursor from the lens' center.
We report on three experiments showing that our techniques make interacting with lenses easier while increasing the range of practical magnification factors, and that performance can be further improved by integrating speed-dependent visual behaviors.
Users do not necessarily need to navigate through the entire scale range at one given time, but still, they need interaction techniques that will allow them to fluidly navigate between focused and contextual views of large datasets.
Such techniques are typically based on the following interface schemes : overview + detail, zooming, focus + context; none of which offers an ideal solution.
The task determines which technique is most appropriate, taking scale range, the nature of the representation, input device, available screen realestate, and of course, the user's preferences, into account.
This paper introduces techniques designed to improve lensbased focus+context interfaces.
Our goals are to extend the range of practical magnification factors, which is currently very limited, and to make low-level interactions easier.
For the sake of clarity, we illustrate all of our techniques with one common type of lens: constrained magnification lenses .
However, our improvements are generic and apply to all types of lenses.
They can also be adapted to other focus+context interfaces, including hyperbolic trees  and stretchable rubber sheets .
Although display technologies continue to increase in size and resolution, datasets are increasing even faster.
Scientific data, e.g., telescope images and microscope views of the brain, and generated data, e.g., network visualizations, geographical information systems and digital libraries, are too big to be displayed in their entirety, even on very large wall-sized displays.
In Google Maps, the ratio between extreme scales is about 250,000.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
This quantization problem has limited the range of magnification factors that can be used in practice; the upper limit reported in the literature rarely exceeds 8x, a value relatively low compared to the ranges of scale encountered in the information spaces mentioned earlier.
In this paper, we introduce techniques that make it possible to perform both fast navigation for focus targeting and highprecision selection in the focus region in a seamless manner, enabling higher magnification factors than those allowed by conventional techniques.
After an overview of related work, we introduce our techniques.
Speed continuously adapts motor precision to navigation speed.
Key and Ring use a discrete switch between two levels of precision , the former using an additional input channel, the latter by decoupling the cursor from the lens' center.
We then report the results of two controlled experiments that evaluate focus targeting and object selection performance.
Finally, we iterate our designs by integrating speed-dependent visual behaviors from the Sigma Lens framework .
The resulting hybrid lenses further improve performance, as shown in a third controlled experiment.
Early implementations of magnification techniques only magnified the pixels of the context by duplicating them without adding more detail, thus severely limiting the range of useful magnification factors .
Newer implementations  do provide more detail as magnification increases.
Theoretically, this means that any magnification factor can be applied, if relevant data is available.
In practice, this is not the case as another problem arises that gets worse as magnification increases: quantization.
Lenses are most often coupled with the cursor and centered on it.
The cursor, and thus the lens, are operated at context scale.
This allows for fast repositioning of the lens in the information space, since moving the input device by one unit makes the lens move by one pixel at context scale.
However, this also means that when moving the input device by one unit , the representation in the magnified region is offset by M M pixels, where M M is the focus' magnification factor.
This means that only one pixel every M M pixels can fall below the cursor in the magnified region.
In other words some pixels are unreachable, as visual space has been enlarged in the focus region but motor space has not.
This problem is illustrated in Figure 1: between  and , the lens has moved by 1 unit of the input device, corresponding to 1 pixel in the context, but the magnified region is offset by 12 pixels.
Objects can thus be difficult or even impossible to select; even if their visual size is above what is usually considered a small target .
The square representing Arlington station in Figure 1 is 9-pixel wide, yet its motor size is only 1 pixel.
Figure 2 illustrates the problem with a space-scale diagram : the center of the lens can only be located on a pixel in the focus window that is aligned - on the same ray in the space-scale diagram - with a pixel in the context window.
Since the focus window is M M 2 larger than the context window, and since the cursor is located at the lens' center, only one out of M M 2 pixels can be selected.
Figure 2 shows that as M M increases, more pixels become unreachable.
Beyond the general problem of pixel-precise selection in the magnified region, quantization also hinders focus targeting, i.e., the action that consists in positioning the lens on the object of interest .
This action gets harder as the magnification factor increases, even becoming impossible at extreme magnification factors.
Zooming interfaces, e.g.,  display a single level of scale and therefore require a temporal separation to transition between "focus" and "context" views.
They usually do not suffer from quantization effects, but both views cannot be observed simultaneously.
Overview+detail interfaces  show both views simultaneously using spatial separation, still requiring some mental effort to integrate the two views.
They usually allow pixel-precise selections in the detail region, but focus targeting is also subject to quantization problems in conventional bird's eye views.
Focus+context techniques "aim to decrease the short term memory load associated with assimilating distinct views of a system"  by integrating the focus region inside the context.
This integration, however, limits the range of magnification factors of practical use.
Basic magnifying glasses occlude the surroundings of the magnified region .
To address this issue, distortion oriented techniques provide a smooth transition between the focus and context views.
Distortion, however, causes problems for focus targeting and understanding of the visual scene.
Gutwin's Speed-coupled flattening lens  cancels distortion when the lens is repositioned by the user, thus removing a major hindrance to focus targeting.
The Sigma Lens framework  generalizes the idea of speed-coupling to a larger set of lens parameters.
For example, the Speed-coupled blending lens makes focus targeting easier from a motor perspective by increasing the focus region's size for the same overall lens size, using a dynamically varying translucence level to smoothly transition between focus and context.
Although their primary goal is different, focus+context interfaces share issues with techniques designed to facilitate pointing on the desktop.
The decoupling of visual and motor spaces plays a central role in techniques designed to facilitate the selection of small targets, e.g.,  - see  for a detailed survey.
Not designed for exploratory multi-scale navigation, but closer to our problem are pointing lenses , which punctually enlarge both visual and motor space to facilitate small target selection through stylus input.
However, visual space is enlarged by duplicating the pixels of the original representation.
The popup vernier  enables users to make precise, sub-pixel adjustments to the position of objects by transitioning from coarse to fine-grain dragging mode through an explicit mode switch.
The technique provides visual feedback based on the metaphor of vernier calipers to make precise adjustments between both scales.
The quantization effect is due to the mismatch between visual and motor space precision in the focus region.
This mismatch, in turn, is caused by the following two properties of conventional lenses:  the cursor is located at the center of the lens, and  the cursor location is controlled in context space.
These properties cause problems with the two low-level actions performed by users: focus targeting, and object selection within the magnified region.
In this section we introduce three techniques that address these problems by breaking the above properties.
For all our techniques, lens displacements of less than M M focus pixels, corresponding to displacements of less than 1 context pixel, are achieved by slightly moving the representation in the focus region while keeping the cursor stationary .
The Key technique represents a simple solution.
However, as the selection tools based on Magic Lenses , an additional channel is required to make the explicit mode switch.
Bi-manual input techniques are still uncommon.
Modifier keys tend to be used for other purposes by applications, and their use often results in a "slightly less than seamless interaction style" .
The next two techniques we propose do not require any additional input channel.
Following recent works that successfully used speed-dependent properties to facilitate pointing  and multi-scale navigation , our first idea was to map the precision of the lens control to the input device's speed with a continuous function, relying on the assumption that a high speed is used to navigate large distances while a low speed is more characteristic of a precise adjustment .
The black line  in Figure 3 illustrates the behavior of our speed-dependent precision lens.
Cursor instant speed s is computed as the mean speed over the last four move events.
The first approach to address the problem is to provide a way of controlling the location of the lens in focus space .
We immediately discard the solution that consists in solely interacting in focus space because of obvious performance issues to navigate moderate to large distances .
The simplest technique uses two control modes: a context speed mode and a focus speed mode.
This requires an additional input channel to perform the mode switch, for instance using a modifier key such as SHIFT.
Users can then navigate large distances at context speed, where one input device unit is mapped to one context pixel, i.e., M M focus pixels, and perform precise adjustments at focus speed, where one input device unit corresponds to one focus pixel.
Figure 3 illustrates this technique, called Key: the first case  is represented by the topmost grey line; the second case  by the bottommost grey line.
When SHIFT is pressed,  is broken.
A similar "precision mode" is already available in, e.g., Microsoft Office to freely position objects away from the intersections formed by the underlying virtual grid using a modifier key.
The last technique is inspired by Tracking menus .
Consider a large rigid ring  on a flat surface .
The ring can be moved by putting a finger inside it and then moving that finger while keeping it in contact with the surface to pull the ring.
This is the basic metaphor used to interact with the Ring lens: the ring is the lens' focus region  and the cursor is the finger.
The Ring lens breaks property : it decouples the cursor from the lens center; the cursor can freely move within the flat-top at focus scale, thus enabling pixel-precise pointing in the magnified region  in Figure 3.
Figure 5 illustrates the lens behavior when the cursor comes into contact with the ring: the segment joining the lens center  to the contact point  is progressively aligned with the cursor's direction.
Decoupling the cursor's location from the lens' center has a drawback when changing direction: because the user has to move the cursor to the other end of the flat-top before she can pull the lens in the opposite direction.
We tried to address this issue by pushing the physical metaphor: we introduced friction in the model to make the ring slide when the cursor stops, with the effect of repositioning the lens' center so as to match the cursor's position.
We were not able however to get a satisfying result, and abandoned the idea.
We conducted two experiments to compare the performance and limits of the three lenses described above.
Participants were asked to perform a simple task: selecting an object in the magnified area.
The targets were laid out in a circular manner and the order of appearance forced participants to perform the task in every direction, following the recommendations of the ISO 9241-9 standard .
Only one target was visible at a time so that participants could not take advantage of the layout to facilitate the task: as soon as the participant clicked on one target, the next target appeared.
The recorded movement time is the interval between the appearance of the target and a click on it.
The target is presented as a yellow circle on a gray background, and is always surrounded by a 10-pixel red square clearly visible in the context view.
The background is also decorated by a grid to help participants understand the transition between context and focus view, and to minimize desert fog effects  that can occur with scenes that are too uniform.
The final cursor pointing task mainly depends on the area of the target in focus space that intersects the flat-top after the focus targeting task.
The larger this area, the easier the cursor pointing task.
We can at least consider the best case, i.e., when the target is fully contained in the flat-top.
In this case, the difficulty of the cursor pointing task can be assessed D by the ratio Wf where Df is the distance between the curf sor and the target, and Wf is the motor size of the target when magnified in the flat-top.
Note that for regular lenses, the value of Wf is actually the size of the target at context scale because the target is only visually magnified.
With our lenses however, since pixel-precise selections are possible, Wf is the magnified size of the target .
We provide additional details about the division between the two subtasks in the following sections.
The first experiment tests pointing tasks with an average level of difficulty, while the second one tests pointing tasks with a very high level of difficulty, involving targets smallerthan-a-pixel wide at context scale.
Our experimental design involves the three factors that determine the pointing task difficulty introduced above: the distance to the target , its width , and the lens' magnification factor MM.
A pointing task with a lens is typically divided in two main phases:  focus targeting, which consists in putting a given target inside the flat-top of the lens  and  and  cursor pointing to precisely position the cursor over the target  and .
This formula clearly shows that difficulty increases as distance increases, as the size of the flat-top decreases, and as the size of the target decreases.
Indeed, the size of the flat-top is fixed in terms of focus pixels, so the higher M M , the smaller the size of the magnified area in context pixels .
The goal of the first experiment is to test whether any of the three techniques we introduced in the previous section degrade performance when compared with regular lenses .
We expect them to improve overall performance because the overall task difficulty is theoretically lower.
On the one hand, the focus targeting task should not be harder: since we test small targets with lenses having the same flat-top size, the distance in context space is the main factor contributing to difficulty.
On the other hand, cursor pointing should be easier since the difficulty of this second phase mainly depends on the target's motor width in focus space.
Since all of our lenses allow to navigate at focus speed, they can take benefit of the magnified target size whereas this is not the case with a regular lens: even though it is magnified, the target size in motor space is the same as if it were not magnified.
Sixteen unpaid volunteers , age 20 to 35 year-old , all with normal or corrected to normal vision, served in Experiment 1.
Experiment 1 was a 4 x 2 x 2 x 3 within-subject design with the following factors: * Technique: T ECH  {Speed , Key , Ring , Reg } * Magnification: MM  {4, 8} * Distance between targets : D C  {400, 800} * Target width : W C  {1, 3, 5} We grouped trials into four blocks, one per technique , so as not to disturb participants with too many changes between lenses.
The presentation order was counterbalanced across participants using a Latin square.
Within a T ECH block, each participant saw two sub-blocks, one per value of magnification factor .
The presentation order of the two values of MM was also counterbalanced across techniques and participants.
For each T ECH x MM condition, participants experienced a series of 12 trials per D C x W C condition, i.e., 12 targets laid out in a circular pattern as described earlier.
We used a random order to present these 2 x 3 = 6 series within a sub-block.
We removed the first trial of each series from our analyses as the cursor location is not controlled when a series begins.
To summarize, we collected 4 T ECH x 2 MM x 2 D C x 3 W C x  replications x 16 participants = 8448 trials for analysis.
Before each T ECH condition, the experimenter took 2-3 minutes to explain the technique to be used next.
Participants were told each time the value of MM was about to change, and had to complete 4 series of practice trials for each new T ECH x MM condition.
A Tukey post-hoc test shows that Reg is the significantly slowest technique and that Key is significantly faster than Ring.
Note that there is no significant difference between Ring and Speed, nor between Speed and Key.
Participants also made more errors with Reg than with our techniques.
We expected Reg to perform worse since, as we already mentioned, the target's motor size is in context pixels for Reg whereas it is in focus pixels for Key, Speed and Ring.
The target is thus much harder to acquire in the CPT phase.
Figures 6- and  respectively show the time MT and error rate ER for each T ECHxW C condition.
The higher the value of D C, the harder the focus targeting phase.
Our techniques do not seem to be at a disadvantage in this phase compared to Reg since the effect of D CxT ECH on FTT is not significant .
This clearly shows that a high MM leads to high FTT since the flat-top size in context pixels directly depends on MM, as explained in the previous section.
A higher MM also means a larger target width in focus pixels.
This can explain the effect of MM on CPT: CPT decreases as MM increases.
Movement time  and error rate  per T ECH x W C.  Movement time per T ECH x MM.
For  and , the lower part of each bar represents focus targeting time, the upper part cursor pointing time.
Indeed, as we expected, the smaller W C, the higher the focus targeting time .
Also, the larger W C, the larger the target in focus pixels to improve focus pointing time .
This is a simple interpretation that explains the difference in means that we observe; but we have to refine it to reflect the more complex phenomenon that actually takes place.
Coming back to the effect of T ECH, we also observe two significant interaction effects that involve T ECH on MT.
A Tukey post-hoc test shows that for MM = 4, Speed, Key and Ring are significantly faster than Reg but this test also shows that for MM = 8, only Key and Speed are significantly faster than Reg .
A closer look at the focus targeting phase explains why Ring seems to suffer from high magnification factors.
We know that FTT increases as MM increases.
We can observe on Figures 6- and  that Ring is actually slower than the other techniques for this FTT phase.
This is probably due to the cost of repairing overshoot errors during this phase: changes in direction are costly with Ring since the user first has to move the cursor to the opposite side of the flat-top before being able to pull the lens in the opposite direction.
A Tukey post-hoc test shows a significant difference in mean for W C=1 between Reg and the other techniques, while this difference is not significant for W C=3 and W C=5.
To better assess the interpretation of such a result, we consider finer analyses on CPT.
Figure 7 shows CPT for each T ECHxMMxW C condition.
Analyses reveal significant effects of T ECH, MM and W C and significant interactions T ECHxMM and T ECHxW C  on CPT.
Tukey post-hoc tests show that Key, Speed and Ring are globally faster than Reg for cursor pointing.
This is not surprising since the motor size of the target is smaller for Reg than for the others, as we said earlier.
However, this significant difference holds only for W C=1 and W C=3, not for W C=5.
In the latter case, only Speed is significantly faster than Reg.
Moreover Ring is faster than Key for W C= 1, while Speed is not.
These results suggest that Ring is particularly efficient for very small targets and that Speed is more appropriate for larger ones.
The latter observations suggest that modeling the movement time MT as the sum of FTT and CPT  may be too naive to explain the subtle differences between techniques.
For instance, this model does not explain the differences between Ring and Speed that depend on W C. In the same spirit, we observe that the difference between Reg and other lenses for W C=5 is very small considering that the target's motor size is 5 for Reg and 20  or 40  for Key, Speed and Ring.
The additive model based also fails to explain the following observation: Speed features significantly higher FTT values than Key and Reg for MM=8 only.
We tentatively explain this by the increased difficulty of controlling a lens with speed-dependent precision when the slope of the mapping function is too steep .
We tried several variations that, e.g., depend on the difference between these two speeds, without success.
Using a gentler slope is frustrating because of the stickiness caused by the large movements required to reach the M AX SP EED threshold.
The more subtle differences we reported in the second part of this section may be explained by the fact that a transition phase between the focus targeting phase  and the cursor pointing phase  actually exists for our lenses: pressing a key for Key, stop pulling the flat-top for Ring, performing speed adjustments with Speed.
At the end of the experiment, participant were asked to rank the lenses  using two criteria: perceived usability and performance.
These two rankings were almost the same for all participants.
All but one ranked Reg as their least preferred technique .
There was no significant difference among other lenses.
Speed first, 3 ranked it second; 6 participants ranked Key first, 5 ranked it second, and 5 participants ranked Ring first, 7 ranked it second.
We also asked participants to comment on the techniques.
The main reason for the bad ranking of Reg is the great difficulty to acquire small targets, related to the cursor jumping effect due to quantization.
Regarding Speed, most participants found the technique "natural"; some found the speed "difficult to control".
The participants who ranked Key high justified it by a "transparent control"; other participants complained about the need to use two hands.
Regarding Ring, the cursor pointing phase was found easier because the lens is stationary, but participants also raised the overshooting problem discussed earlier.
To summarize, in comparison with regular lenses, precision lenses increase pointing accuracy.
They also increase selection speed for small targets and are as fast for larger ones.
This second experiment evaluates our techniques on extreme tasks: very small target sizes and high magnification factors.
We discard the Reg technique as it is not capable of achieving sub-pixel pointing tasks, i.e., involving targets that are smaller-than-a-pixel wide in context space.
Another difference with Experiment 1 is that we use W F as a factor instead of W C. This allows us to isolate the effects of W F and MM.
Indeed, since W F = W C x MM, two values of MM correspond to two different values of W F for the same W C value.
As expected, MT increases as W F decreases, as MM increases and as D C increases.
There is an interaction effect T ECHxMM on MT : Tukey post-hoc tests show that Ring and Key are significantly faster than Speed but only for MM=12 while these differences are not significant for MM=8.
Figure 8 shows that this large difference at MM=12 is due to a sharp increase of focus targeting time  for Speed.
Comments from participants confirm that the speed dependent control of motor precision is too hard when the difference between context scale and focus scale is too high, resulting in abrupt transitions.
With Speed, participants did not succeed in controlling their speed: either they overshot the target  or spent a lot of time putting the target in focus .
Therefore, Speed does not seem to be a suitable lens for pointing with a very high magnification factor: at MM=12, the linear function linking focus speed to context speed is too steep to be usable.
Figure 8 shows that focus targeting performance of Ring degrades as MM increases.
However, good cursor pointing performance compensates for it, resulting in good overall task completion time.
Figure 9 shows CPT for each T ECH x MM x W C condition.
As mentioned earlier, the larger W F, the easier the cursor pointing task.
CPT is higher when MM=12 than when MM=8, Ring is faster than Key and Speed, and the difference between Ring and both Key and Speed is larger when MM=12 than when MM=8 .
A plausible explanation for these effects lies in the differences in terms of Control-Display  gain among tech-
As in Experiment 1, trials were blocked by technique, with presentation order counterbalanced across participants using a Latin square.
The experimenter explained the technique to be used during 2-3 minutes before each T ECH condition.
For each T ECH, participants saw the two values of MM, grouped into two sub-blocks .
Each sub-block contained 6 series of 8 trials, 1 series per D C x MM condition, presented in a random order.
To summarize, we collected 3 T ECH x 2 MM x 2 D C x 3 W C x  replications x 12 participants = 3024 trials for analysis.
As in Experiment 1, participants were alerted by a message each time the MM value changed and had to complete 4 practice series for each T ECH x MM condition.
Figure 10 illustrates the difference in terms of control-display gain among lenses, all in high-precision mode.
During the cursor pointing phase, Ring is stationary; only the cursor moves inside a static flattop.
This is not the case for Key and Speed for which highprecision cursor pointing is achieved through a combination of cursor movement and flat-top offset.
In Figure 10, to achieve a mouse displacement of 15 units, the cursor has moved by 1 context pixel  and the representation has moved by 7 focus pixels to achieve an overall displacement of 15 focus pixels.
As a result, the controldisplay gain is divided by MM for Key and Speed.
This might be the cause for the observed performance degradation.
This interpretation is consistent with the stronger degradation for Key and Speed than for Ring from MM=8 to MM=12.
Note, however, that there is still a small degradation of CPT from MM=8 to MM=12 for Ring, that we tentatively explain by a harder focus targeting phase when MM=12 that influences the transition from focus targeting to cursor pointing.
To summarize, when pushed to extreme conditions, the Speed lens becomes significantly slower than the other precision lenses while Ring remains as fast as Key without requiring an additional input channel for mode switching.
The other design  is a variation on Gutwin's original Speedcoupled flattening .
The lens flattens itself into the context as its speed increases so as to eliminate the problems caused by distortion.
Figure 11 illustrates both behaviors.
We designed four new techniques that result from the combination of one of the above two visual behaviors with either speed-dependent motor precision  or cursor-in-flattop motor precision .
Key was discarded because it proved awkward to combine explicit mode switching with speed-dependent visual properties.
Speed + Flat: this lens behaves like the original Speed design, except that the magnification factor decreases toward 1 as speed increases .
The main advantage is that distortion no longer hinders focus targeting.
Additionally, flattening provides indirect visual feedback about the lens' precision in motor space: it operates in context space when flattened, in focus space when not flattened.
Ring + Flat: This lens behaves like the original Ring design, with the magnification factor varying as above.
As a consequence, the flat-top shrinks to a much smaller size , thus making course corrections during focus targeting easier since the cursor is still restricted to that area.
As above, distortion is canceled during focus targeting.
Ring + Blend: This distortion-free lens behaves like the original Ring design, except that the restricted area in which the cursor can evolve  is larger .
As speed increases, the flat-top fades out, thus revealing the context during the focus targeting phase .
An inner circle fades in, representing the region that will actually be magnified in the flat-top if the lens stops moving.
The cursor is restricted to that smaller area, making course corrections less costly.
Speed + Blend: This lens behaves like the original Speed design without any distortion.
As above, the flat-top fades out as speed increases and fades back in as speed decreases.
Again, the larger flat-top reduces the focus targeting task's index of difficulty.
In a way similar to Speed + Flat, blending provides indirect visual feedback about the lens' precision in motor space: it operates in context space when transparent, in focus space when opaque.
Previous experiments show that techniques with advanced motor behaviors enable higher-precision focus targeting and object selection while increasing the upper limit of usable magnification factors.
The Sigma Lens framework  takes a different approach at solving the same general problem by proposing advanced visual behaviors.
We now explore how to combine these two orthogonal approaches to create hybrid lenses that further improve performance.
The two Sigma lens visual designs reported as the most efficient ones in  can be directly combined with our motor designs.
The first one is the Speed-coupled blending : it behaves as a simple magnifying glass whose translucence varies depending on lens speed.
Smooth transition between focus and context is achieved through dynamic alpha blending instead of distortion.
Behavior of two Sigma lenses during a focus targeting task ending on East Drive in Central Park.
The inner circle delimits the region magnified in the flat-top.
The inner circle fades in as the lens fades out; it delimits which region of the context gets magnified in the lens.
The magnification factor remains constant.
Experiment 2 revealed that problems arise for the difficult tasks.
We thus consider here difficult conditions in terms of magnification and target size.
To reduce the length of the experiment, we discarded the D C factor  as it did not raise any particular issue for any of the techniques.
Twelve participants from the previous experiments served in Experiment 3.
Experiment 3 was a 2 x 3 x 2 x 3 withinsubject design with the following factors: * Motor precision technique: T ECH  {Speed , Ring } * Visual behavior: VB  {Blend , Flat , Static } * Magnification: MM  {8, 12} * Target width in focus pixels: W F  {3, 7, 15} Trials were grouped into two main blocks, one per technique .
These blocks were divided into three secondary blocks, one per visual behavior.
The presentation order of T ECH main blocks and VB secondary blocks was counterbalanced across participants using a Latin square.
Within a T ECHxVB block, each participant saw two sub-blocks, one per magnification factor ; presentation order was counterbalanced as well.
For each T ECH x VB x MM condition, participants experienced 3 series of 8 trials, one per value of W F, presented in a random order.
As with the other two experiments, participants received a short explanation before each T ECH x VB condition and performed 3 practice trial series per T ECH x VB x MM condition.
Even if CPT is significantly degraded, the gain in FTT is strong enough  to decrease MT .
The degraded cursor pointing performance observed here is not surprising.
It can be explained by the time it takes for a speed-coupled blending lens to become opaque enough or for a speed-coupled flattening lens to revert to its actual magnification factor.
The performance gain measured for the focus targeting phase is consistent with previous experimental results .
Overall, the gain in the focus targeting phase is strong enough to improve overall task performance.
The effects of W F and MM on MT are consistent with the previous two experiments: MT increases as W F decreases and as MM increases.
Even if visual speed-coupling improves the performance of Speed more than that of Ring , Ring remains faster than Speed for each MM.
Note that we do not observe a significant advantage of Blend over Flat as reported in .
The main difference is that our targets are much smaller than those tested with Sigma lenses .
Small targets probably cause more overshoot errors that are more expensive to repair with Blend than with Flat: if the larger flat-top of Blend is supposed to make focus targeting easier under an error-free hypothesis, it also causes an area of occlusion that is a significant drawback when trying to correct overshoots.
Our participants actually reported that observation; in case of an overshoot they often left the target zone completely to perform a new focus targeting task.
However this interpretation should be taken carefully since we did not record the number of overshoot errors.
We only measured ER, the percentage of clicks outside the target .
As in Experiment 2, the only factor that has an effect on error rate is target width W F.
Large differences in scale between focus and context views cause a quantization problem that makes it difficult to precisely position lenses and to acquire small targets.
Quantization severely limits the range of magnification factors that can be used in practice.
We have introduced three highprecision techniques that address this problem, making focus targeting and object selection more efficient while allowing for higher magnification factors than regular lenses.
This is confirmed by the results of our evaluations, which also reveal that some lenses are more robust than others for extreme conditions, with the Ring technique performing the best.
Our high-precision techniques can be made even more efficient by combining them with speed-dependent visual behaviors drawn from the Sigma lens framework, as shown in the last experiment.
We analyzed our observations based on a model for target acquisition that sums the focus targeting and cursor pointing time to get the overall task time.
Our results suggest that this model is too simple as it ignores the transition period between the two subtasks.
This is especially true for lenses with a speed-dependent behavior, because of the delay to revert back to their stationary configuration.
As future work we plan to refine the additive model to better account for these transitions.
We also plan to adapt our techniques to other focus+context interfaces and investigate non-circular focus shapes.
