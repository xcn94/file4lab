Blur in images can create the sensation of depth because it emulates an optical property of the eye; namely, the limited depth of field created by the eye's lens.
When the human eye looks at an object, this object appears sharp on the retina, but objects at different distances appear blurred.
Advances in gaze-tracking technologies enable us to reproduce dynamic depth of field in regular displays, providing an alternative way of conveying depth.
In this paper we investigate gazecontingent depth of field as a method to produce realistic 3D images, and analyze how effectively people can use it to perceive depth.
We found that GC DOF increases subjective perceived realism and depth and can contribute to the perception of ordinal depth and distance between objects, but it is limited in its accuracy.
Fortunately, other depth cues exist that can also be used to convey depth.
One such depth cue that is also thought to influence the perception of depth is defocus blur from depth of field .
In this work we explore the value of simulating DOF to induce the perception of depth.
This is due to the constrained depth of field of the human eye; objects appear more or less blurred depending on their position, and the position of the focal plane .
Blur patterns can affect the perceived distance between objects  and convey information about the distance of the focused object from the observer .
If a display system has up-to-date information about the gaze location of the viewer, it can recreate the blur pattern generated by an optical system: objects in the same plane as the object being looked at will be rendered sharply, while objects at different distances will be blurred to varying degrees.
This approach, that we call gaze-contingent depth of field , has been suggested for virtual reality to increase realism ; however, it is not clear whether GC DOF can affect the perception of depth.
Existing evidence is not in complete agreement , and to our knowledge nobody has conducted an experiment quantifying the degree to which GC DOF can convey depth information.
In this paper we contribute to the understanding of GC DOF, its influence on perceived depth and realism and to what extent it conveys quantitative depth information.
Our findings show that GC DOF can enhance subjective perceived realism and depth and can contribute to the perception of ordering and distance between objects.
However, although GC DOF blur could be a useful addition to systems trying to convey realistic scenes, e.g., games, it is probably not sufficiently reliable to serve as an additional visual variable in information visualization applications that require accurate perception.
Representing 3D information in flat displays can be valuable to improve the realism of scenes and to increase the amount of visual information that is conveyed to viewers .
One common technique for providing depth in flat displays is through binocular disparity: different images are presented to each eye, and the visual systems derives depth information from the differences .
Although binocular disparity is commonly used in current 3D technologies, it has significant problems: a substantial percentage of people  have difficulty using it and it can lead to increased visual fatigue, especially when conflicting with other depth cues , which is commonly the case with flat displays .
This is the author's version of the work.
It is posted here for your personal use.
In the following sections we present the current state of knowledge about the role of blur in depth perception and how it is applied in research display systems.
We focus on approaches that emphasize depth perception or perceived realism rather than on approaches that use it, e.g., for directing the viewer's attention .
Finally, stereoscopic displays relying on binocular disparity have been combined with blur to reduce eye strain and discomfort.
The simplest approach applies blur to static scenes producing a fixed and static DOF .
Other approaches allow dynamic modification of the DOF position via manual input  or utilize GC DOF .
Blur has been studied in the perceptual literature as an isolated depth cue by means of static textures.
Blur is thought to only provide information about ordinal depth.
For example it has been shown that relative blur differences affect the judgment of ordinal depth but not quantitative depth .
However, the presence of blur has also been shown to alter quantitative depth perceived on the basis of other cues such as perspective or binocular disparity .
The overall pattern of blur created by DOF is a quantitative cue for perceived egocentric distance  and perceived size of objects .
The egocentric distance information provided by defocus blur may also serve to scale other depth cues .
Finally, blur can contribute to speed and accuracy in depth perception  and in some situations, blur differences between objects separated in depth are more discriminable than binocular disparity between them .
To create gaze-contingent depth of field we use two main elements: a sequence of images with a range of focal points, and an algorithm to present the images depending on gaze location and timing.
In this section we describe how the different elements come together to simulate DOF.
We assume the use of a flat display and an eye tracker that can provide the location of gaze within the coordinate space of the display with appropriate accuracy and timeliness.
Details of the hardware that we used can be found in the apparatus sections of the experimental descriptions.
In order to achieve GC DOF the display will present an image that is sharp at the viewer's gaze location and appropriately blurred in other regions.
A different image is presented for each of the distances of the objects within the scene.
These images can be rendered in advance, generated on the fly from a 3D model , or obtained photographically from a real scene, e.g., through a sequence of exposures with different lens adjustments  or through a lightfield camera .
In our system we can use both photographic and rendered images, but we use pre-rendered images for our experimental setup since they provide us with more control over the scene.
We do not render the images in real-time to minimize delays in the presentation of the image; generating highly realistic 3D scenes with correct DOF  in real time requires significant processing power and/or specialized hardware.
The presentation algorithm determines which of the images has to be presented for a given gaze location.
For this, it uses a mapping that provides the index of the correct image for each possible gaze location.
This mapping can be derived from the depth map of the scene, which is easy to generate from a synthetic 3D scene as depth information is usually readily available .
However, mapping the values from the depth map directly to an image suffers from problems caused by two factors: the inherent delay of focus changes  in the human eye, and the lack of a precise gaze point delivered by the eye tracker.
In the human eye, a process called accommodation changes the lens properties in order to focus.
This process is not instantaneous but takes different amounts of time, depending on various internal and external factors  .
If this process is ignored or the required time is underestimated this can negatively affect the viewing experience .
While it is beyond this work to create a comprehensive simulation of the human visual accommodation process, our algorithm alleviates this problem by gradually adapting the focus using a transition function.
Sun and Holliman  investigated the differences in depth perception quality of three non-gaze-contingent blur conditions: a movie scene with no blur, the same scene with a fixed focal plane, and scene where the focal plane changed in a pre-determined fashion.
A Likert questionnaire was used for evaluation and they found that the scenes with the focal plane changes and the all-on-focus scene were not statistically different, but the fixed focus scene was worse.
The effect of DOF was evaluated using a spatial dexterity task.
The results showed an improvement in task completion time for some participants, but no overall improvement was reported.
The first was a dynamic adaptation of focus to the object in the screen center.
The other used the gaze position of users, an idea first suggested by Rokita .
While not the main goal of their investigations, they found participants anecdotally reporting an increased sense of depth due to the DOF.
Others have also found subjective increases of realism or immersion using GC DOF .
They evaluated real-time rendered gaze-contingent DOF with a self defined questionnaire.
They found that using the GC DOF led to an increase in perceived depth but also a decreased sense of realism.
None of the studies above have reported objective measures of depth perception accuracy.
To stabilize the participants' head we provided a chin rest to keep their face at constant 60 cm and perpendicular to the screen.
The participants provided responses through a keyboard placed in a comfortable position close to the resting position of their hands.
2 for an overview of the setup.
We implemented the DOF algorithm as described earlier with Matlab and OpenGL.
To assure timely delivery of the images, they were pre-loaded as textures before each trial.
These can lead to ambiguity about which part of this area is currently focused on, especially at edges between areas with different depth.
If the assumed focus is rapidly changing between areas, e.g., due to noise in the gaze point, or if the wrong area is chosen, the viewing experience is disrupted.
To solve these problems we use a form of hysteresis: we can assume that small changes in gaze location are probably local and should not introduce a change in focus depth.
Only larger changes should be taken into account.
Therefore our algorithm uses mappings that are state dependent and initiate a change of focus only if the current area is left by a certain threshold.
This is achieved by dilating the active depth region in the map using a circular kernel with a 20 px radius.
The models were pre-rendered using POV-Ray at a resolution of 1152 px x 864 px.
The virtual camera positions and scene scales were adjusted to produce a rendering compatible to what a camera would record if placed central in front of a window of the screen's size and looking at a real scene with normal-sized objects.
The aperture was set at a value of 0.8 for the kitchen and 1.0 for the patio scene.
The objects in the kitchen scene are located in an area between 70 and 146 units from the camera.
For the patio scene we used larger distances between 286 and 900 .
We rendered 30 different focal images for each scene by changing the focal point in the POV-Ray camera model.
The focal points are spaced throughout the scene using an exponentially distributed density function, with higher density in the foreground, where there is more detail in the scene, and lower density at the back.
This corresponds to the geometry and perceptual characteristics of blur--blur does not change linearly with distance and produces more noticeable differences for closer focal planes .
We designed our first experiment with two main objectives.
First, we needed to validate our implementation of gazecontingent DOF to show that our setup is able to produce at least similar results as seen in related systems.
Second, we wanted to provide a more reliable replication of previous work by using validated methodology and solid statistical analysis.
For this, we designed an experiment that compares the presentation of realistic scenes presented through GC DOF and a baseline all-on-focus image.
The subjective measurements and procedure were modified from previous studies in the psychology literature that compare subjective perceptions of realism and depth produced by different  stereoscopic presentation techniques .
15 participants  took part in the experiment for a 5 compensation.
After providing written consent, participants sat in front of the display and the experimenter performed the eye-tracker calibration with them.
The experimenter provided initial instructions about the experiment and answered questions.
During each trial the participants had access to the two conditions by pressing the left and right control keys, which they could reach without having to look at the keyboard.
One of the keys  would activate GC DOF, the other would show a static all-in-focus image of the scene.
To reinforce the transition between conditions, the system emitted a beep for every condition change.
Participants were instructed to view each condition for at least five seconds, to switch at least five times, and to look for differences in depth.
If no key or two keys were pressed the screen would remain blank.
The display is a Iiyama HM204DT 22 inch CRT display with a resolution of 1280 px x 1024 px running at 100 Hz.
After viewing both conditions participants reported whether they perceived any difference and rated a battery of statements displayed on the computer screen  by using the same control keys.
The statements did not refer directly to "gaze-contingent" and "static" conditions; instead, they referred to the "Left" or "Right" conditions, which corresponded to the key used for activation.
The first trial was followed by a second trial depicting the other scene.
After they finished the experiment, participants performed an acuity and stereo vision test.
The participants assessed the perceived 3D-effect and realism of the conditions by rating the statements listed in Tbl.
1 on a seven point Likert-scale from "Strongly Agree" to "Strongly Disagree".
Each statement  was included twice, once referring to the "Left" and once referring to the "Right" condition.
The statements are based on a battery of statements from Vishwanath and Hibbard , and include four catch-statements  referring to properties that are not related to depth perception .
This allowed us to assess whether participants are biased towards ascribing properties to one of the conditions independently of whether these are related to depth perception.
The questions were presented in four randomized orders balanced over all participants.
Results of the statistical tests are summarized in Tbl.
1; p-values were corrected for multiple comparisons using the Bonferroni-Holm method.
The tests for S10 and S11 are slightly different  as they compare both conditions within the same question and are designed only to detect departures from the neutral response.
As can be seen also in Fig.
4, the results of the tests provide strong evidence that, in the gaze-contingent condition, participants judged objects to be more "like looking at real objects", appear to "stick out of the screen", have a more defined depth, and appear more separate from each other.
The results for the catch statements also show that, with the exception of S8 , participants did not generally think that objects appeared larger, more transparent and translucent, or that objects had different shapes in the two conditions.
The results suggest that gaze-contingent DOF allows viewers to perceive images more realistically and with an increased sense of depth.
There is also a strong distinction between the answers of the assessing statements and the catch statements.
While S8, one of the designated catch trials, shows a difference between conditions, we are not concerned about the overall validity of the results.
Examining the distribution of the ratings it becomes clear that there was no strong trend to identify the gaze-contingent condition with the presented concept , merely indecision and a strong opinion about the absence of distortion in the static condition.
The results of this experiment are useful in two main ways.
First, they provide replication of previous research indicating that GC DOF increases impression of depth and realism compared to regular images.
Importantly, the experiment uses a set of questions validated in previous research and statistical analyses that build upon existing evidence.
Second, the replication of these effects with our own setup provides necessary validation of our implementation and algorithms.
This allows us to confidently go beyond previous research and test new aspects of the gaze-contingent DOF technique to produce depth perception without having to doubt whether the results are simply due to differences in the used hardware or implementation.
The hardware used for this experiment was the same as in Exp.
The distance between headrest center and screen was 40 cm.
Additionally, we darkened the room for the trials and covered the area surrounding the display with a flat black surface to avoid visual distraction and possible interference due to the surrounding area of the monitor.
Input responses required recording estimated relative positions of two objects in the visual field; for this we replaced the keyboard with a custom-built Phidget device with two sliders that record positions in a fixed range, and a capacitive plate that allows participants to indicate the end of a trial.
We reimplemented the same DOF algorithm but with two differences:  we disconnected the hysteretic adjustment of the depth map because with the simpler, widely spaced objects it is not required,  the gradual adjustment of blur in accommodation was exponential instead of sinusoidal .
The new implementation was programmed in Python 2.7 using PsychoPy 1.75.013 ; gaze information was retrieved using the Eyelink Python API .
Experiment 1 provides assurance that there is a subjective effect of gaze-contingent DOF for the perception of realistic scenes.
However, this does not answer the question of whether GC DOF is an effective and reliable method to convey depth information.
In other words, can people extract depth information accurately from GC DOF?
This is important for applications where precise information is important .
To answer this question we designed a quantitative, controlled experiment that investigates depth perception accuracy through a depth comparison task .
The experiment is closer in nature to experiments in the perception literature, and uses abstract objects in a synthetic space to control for possible confounds .
Participants were presented with two abstract square objects that contain a black-and-white circle pattern that was different between tiles, but contained the same number of circles of equal size to make sure that both tiles have equivalent perceived brightness, contrast and information from blur.
The objects are perpendicular to the screen plane, aligned with the screen edge, and are floating in mid air.
The main task was to estimate the positions of the two objects.
Participants used the left and right sliders of the purpose-build Phidget input device to indicate the relative positions of the left and right objects.
The end ranges of the sliders represented the front and back ends of the tunnel.
We chose relative indirect input for this task because of the possible effect of overlapped motor activity and because seeing their own hand could affect perception of distance .
As in the previous experiment, the virtual scene was created to match the physical setup, i.e., objects were displayed at the same distance from the virtual camera as they were supposed to be located from the user.
Each scene had 20 different focal points spaced throughout the area of interest, rendered through Blender v2.64 using the built-in Cycles ray-tracing engine.
We chose to switch to Blender due to its more efficient GPU rendering capabilities.
Since depth values in Blender are calculated as distance from the camera position, a correction was applied to produce distances from the image plane instead.
While the tiles were displayed at varying depths, their size was always adjusted to occupy the same visual angle to avoid introducing size as a depth cue confound.
The elements in the screen  did not overlap and were rendered separately.
To avoid the interference of binocular cues participants wore a one-eye patch on their non-dominant eye.
16 participants  took part in the experiment.
After filling a demographic questionnaire, testing for eye dominance  and acuity  participants put on an eye patch to eliminate binocular depth cues.
They then learned the task and performed one block of practice trials and one block for all four experimental conditions.
There were ten practice trials, in which two tiles were shown in the tunnel, close to the opposite extremes  and blur was adjusted according to gaze.
The other four blocks consisted of variations of two factors with two levels each: gaze-contingency  and background visibility .
We obviously introduced the gaze-contingency factor to compare it to the baseline.
However, the introduction of the background factor corresponds to a less obvious goal.
There is a running discussion in the blur perception literature regarding the nature of the type of information provided by defocus blur in natural perception  and whether the context blur of the field/scene provides valuable information .
Notice that an appropriately blurred background does contain information about the depth of an object .
Notice also that, unlike in Exp.
1, the static conditions did have blur .
Making the non-gaze-contingent conditions all-in-focus would make no sense, since the objects' location would be indistinguishable regardless of their depth.
Trials differed in the position of the two stimulus objects .
Each configuration was shown twice, which resulted in 36 trials per condition.
In static  conditions, each of the two trial repetitions for a given position and distance had a different object on focus.
The condition presentation order was balanced but the first two conditions would be either both gaze contingent or nongaze contingent to avoid unnecessary switches and calibration problems .
Participants had no time limit to complete a trial, and indicated the end of each trial by tapping on a large capacitive sensor attached to the near end of the slider input device.
Participants were allowed to rest between blocks.
The main raw measurement taken during the experiment is the position of the sliders when participants indicated the end of a trial .
For practical purposes we report slider positions on a range from 0 to 100 .
However, since participants used a variable range of positions in the sliders, and because our measures are relative anyway, we chose to normalize each participant's measurements so that their closest and furthest reported locations over the whole experiment correspond to 0 and 100 respectively.
From the raw position data we derived several additional measures: The difference between the input sliders , the absolute difference between the input sliders , and the perceived ordering in depth of the tiles .
These additional measures are useful to determine whether the perceived depth information in a condition is ordinal , quantitative , or both .
The experiment has a general repeated measures design with two factors that have two levels each: gaze contingency   and the presence of background  .
Tests that do not show an effect related to the position factor  are of little interest to us, since they do not relate to the participant's ability to use the stimuli information to discriminate different depths and thus we omit discussing them.
Repeated trials in the same cell were averaged before analysis, and when the assumption of sphericity was broken  we applied the Greenhouse-Geisser correction.
Error bars in plots represent 95% confidence intervals unless otherwise stated.
Our first test is designed to estimate whether participants estimated positions differently depending on the virtual positions of the stimuli; in other words, whether participants were able to extract information from the stimuli to judge the absolute positions of the tiles.
For this, we performed a 2 x 2 x 5 repeated-measures ANOVA of the individual position responses for the tiles.
We expected to see strong main effects of the stimuli positions on the perceived positions of objects, if not in all cases, at least for certain conditions.
These results suggest that some absolute information is extracted from blur overall: Stimuli representing objects that are further result in further judgments, with a slight improvement when there is background.
However, there are no significant effects of gaze contingency, and a quick look at Fig.
7 shows that the linear relationship is not very pronounced and is indeed small compared to the amount of perceptual noise  for both background and non background conditions.
8 and the interaction reveals that gaze-contingent conditions resulted in a smaller range of responses: static representations of blur result thus in a stronger impression of depth.
The absolute measurements could have been affected by the indirectness of the input device, and since many perceptual effects are relative, we tested also the difference in the judgments of the left and right object as compared to the difference in position of the tiles.
This analysis is analogous to that of the previous section, but position is instead encoded as relative differences in object position in both the dependent variable, and the DiffS factor.
This indicates that participants were even worse at comparing the distance between objects.
Since the previous test eliminates the ordinal information, it makes sense to run its counterpart to test whether participants could extract pure order information .
We first converted the positions of objects to a binary location factor  as well as the participant responses , and determined the agreement between them.
For this analysis we excluded trials for which DiffS = 0, as these do not allow a correct ordering decision.
It is important to notice, however, that the correctness rate for GC is not much above chance.
It is also possible that beyond the overall effect of the different conditions, individual differences between observers might explain partly the fragmented results; in other words, could certain participants extract information very effectively while others could not?
To test this we evaluated the accuracy of individual participants at depth ordering.
The results are displayed in Fig.
These results show that the ability to perceive depth ordering with GC DOF is not universal, and at least one person interpreted the information in a consistently erroneous way.
We also found that the presence of GC DOF yields a better correlation between observed object distances and actual object distances in the presence of contextual information.
With background, gaze-contingent depth of field yielded greater absolute magnitudes of perceived depth.
These results are consistent with previous findings reported in the perception literature reporting that blur difference between isolated objects is by itself insufficient to convey quantitative depth , but now we know that blur can influence perceived depth in the presence of context without the need for a shared border.
However, this result needs to be followed up by further experiments as the backgrounds we used all had defocus blur consistent with the viewing parameters.
To know whether the advantage is caused by the background blur pattern, or just merely the presence of an additional context, we need to compare an additional condition with unblurred background.
As a complementary post-hoc analysis to the initial investigation of estimated depth difference, we analyzed the correlations between estimated depth difference and displayed depth difference in the different conditions.
We conducted a 2 x 2  RM-ANOVA over each participants' coefficient of determination R2 between |DiffI | and DiffS in each of the conditions.
We chose R2 over r as we are more concerned about consistency and explanatory power than sign of the relationship.
Further analysis revealed that depth from blur is not universal; although some participants were particularly good at deriving depth from the GC DOF, others seemed to be unable to extract information from it.
It is unclear whether this can be taught or, as in binocular displays, it is inherent to the perceptual processing capability of some people .
Our findings indicate that gaze-contingent blur contributes to depth perception; this is the first quantitative evidence of depth perception from blur with a GC DOF setup.
When considering both static and gaze-contingent blur, we found a general contribution of blur to the perception of absolute depth, and to the perception of depth differences between objects if we only consider magnitude.
GC DOF also allows better than chance discrimination in the spatial ordering of objects, although static blur seems to have a more pronounced effect in the detection of depth differences between two objects.
The effects, however, are small.
When considered across all participants and conditions, the ability of people to judge ordinal depth is significant above chance, but just by 5 percentage points.
Similarly, quantitative judgments of depth are also statistically significant, but small if compared with the degree of variability seen in the accuracy of the depth judgments.
The results from our two experiments are complementary.
The first experiment confirms previous results that gazecontingent DOF provides a subjectively more complete perception of 3D than images with static monocular cues.
This results suggests that GC DOF could be a viable technology to enhance 3D perception, or to complement other existing ways of providing 3D such as binocular disparity.
Our results have important implications for possible applications of GC DOF: although the results from previous literature and our first study might seem strong support for the application of this 3D cue in more displays, the second experiment also shows that, while GC DOF provides some information of depth  the low levels of accuracy make it undesirable for applications that require accurate perception of depth information.
This means that, while depth of field has been used for benefit in information visualization for focus+context tasks , it can probably not be reliably used to convey a quantitative visual information variable on its own.
However, games, virtual reality applications, or realistic image displays might be able to benefit from the increase in perceived depth and realism alone.
Whether static and GC DOF might be more useful in combination with other depth cues requires further research; however, it is likely that, in this case, GC DOF with a rich context will be preferable.
GC DOF provides better depth order information and, although possibly less pronounced in its magnitude than static blur, has the advantage that whatever is being looked at is automatically in focus.
Additionally, further studies are necessary to validate the results of Experiment 2 in more naturalistic scenarios including other common cues like perspective or size.
Our results provide valuable insights in how blur provides information about depth.
If we consider first only static stimuli, it is not surprising to find that blur does not provide ordering information, since it is sign-ambiguous: The more blurred object could be in front of or behind the focused object .
However the improvement in ordinal depth perception in the gaze-contingent conditions suggests that ordinal depth perception could be informed by the dynamic blur transitions between focal planes.
In this paper we presented an experimental investigation of gaze-contingent depth of field, studying its influence on qualitative and quantitative depth information.
For this purpose we conducted two experiments using a GC DOF algorithm based on pre-rendered images.
We contribute evidence that GC DOF increases perceived realism and depth and adds validity to prior research by confirming these findings using methodology validated in perception research.
We also demonstrate that GC DOF does contain information about depth order and relative spatial position of objects.
This information is, however, limited.
Our findings show that GC DOF can be a valuable addition to systems that aim to convey realistic scenes  but has to be used with caution in applications that try to communicate accurate information such as information visualizations.
