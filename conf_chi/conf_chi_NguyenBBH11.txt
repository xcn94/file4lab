In this paper, we present the results of an empirical study of perceptions towards pervasive video recording.
We describe a commonly used model for understanding information privacy, the Concern for Information Privacy  model, and present the ways that this model and its associated questionnaire can shed light on information privacy concerns about pervasive and ubiquitous computing technologies.
Specifically, the CFIP model encourages analysis of data across four facets of experience: the collection of personal data, the risk of improper access, the potential for unauthorized secondary use, and the challenge of preventing or correcting errors in the data.
We further identify areas not well handled by this model of information privacy and suggest avenues for future work, including research on how and when to notify people about recording technologies, awareness of data provenance and leakage, and understanding of and access to the data assemblage being created about individuals.
The pervasive capture of daily activities through video recording can raise concerns for information privacy .
Studies of CCTV, however, have primarily focused on the effectiveness and public support of the technology .
For example, Dixon et al.
These studies are only a first step in exploring the complex nature of those concerns.
In this paper, we present results from an empirical study about people's perceptions of pervasive video recording.
We build on Smith et al.
Our work provides three significant contributions.
First, our results indicate how people feel and make decisions about pervasive video recording.
Second, we demonstrate how the CFIP model for understanding consumer responses to information privacy can be applied to ubiquitous computing technologies .
Third, we present issues drawn from our empirical data that enrich and augment the CFIP model.
Computing applications frequently require the collection of vast amounts of data, including images and video recordings.
In particular, the class of applications known as capture and access  are built on the promise of usefully collecting and making available these data.
Video recording technologies, in the form of closed-circuit television  and others , are some of the few truly pervasive capture and access technologies in existence today.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Public opinion researchers have employed surveys to monitor overall levels of public concern for information privacy .
These surveys ask broad questions, such as "How concerned are you about threats to your personal privacy in America today?"
However, these surveys result in a paucity of evidence for the specific dimensions of concern.
In response, Smith et al.
The associated CFIP survey instrument has been adapted for and used in other contexts outside of organizational usage of personal information.
Malhotra et al., for example, used a modified CFIP instrument to understand Internet users' concerns for information privacy .
However, the CFIP model has not previously been applied to Ubicomp technologies, such as pervasive video recording.
Prior research has demonstrated the efficacy of drawing from models of privacy in other fields for use in design of Ubicomp systems.
Palen and Dourish first applied Altman's privacy framework to describe privacy as a boundary negotiation process , with Lehikoinen et al.
Hong and Landay described risk models as a means for understanding privacy in Ubicomp systems and created a toolkit for developers to support privacy-sensitive design .
Similarly, Iachello introduced the concept of proportionality as a principle to guide the design of Ubicomp technologies and developed the associated Proportionality Method for designers .
Langheinrich used fair information practices as the basis for privacy design principles in Ubicomp .
However, these prior works do not tease apart the issues related to different dimensions of information privacy concerns.
Thus, in this work, we turned to the domain of Information Systems, using the CFIP model as a basis to understand situated concerns about information privacy.
Particularly related to our work is Massimi et al.
Both our study and theirs used the Day Reconstruction Method  to contextualize the inquiry .
However, their study examined all recording technologies; our study built on that work by focusing on and drawing out the nuances surrounding one particular recording technology--video recording.
By using both DRM and CFIP, we were able to not only quantify participant concerns, but also use the CFIP model to provide a means for unpacking privacyrelated concerns that Massimi et al.
Additionally, previous work indicates that perceptions of and responses to tracking and recording technologies are highly contextualized  and often impacted by the ways in which they are introduced .
Even if not explicitly told about a positive purpose for a tool, individuals may assume one, such as cognitive therapy for a person with a disability .
At times, this positive feeling about a technology may even come from something as simple as the name of the technology.
For example, the Whereabouts Clock invoked in participants feelings of family togetherness over any concerns about location tracking, potentially due to its whimsical Harry Potter-inspired name .
End users make nuanced decisions about sharing private data based on a wide variety of contextual factors, such as knowing who wants the information, why, and at what level of detail .
These views of privacy support the notion of the privacy management process as highly contextualized and corroborate our interest in situated inquiry here.
A full review of the work on privacy within interactive systems is beyond the scope of this paper; instead, we refer the reader to Iachello and Hong's extensive review  and here have focused on only the most related works.
We used the DRM to elicit grounded and contextualized responses to video recording .
The DRM involves asking participants to recollect a full day  from the recent past--typically the day before.
It has been used to study other surveillance technologies, such as cameras, browser cookies, and so on .
The DRM includes a combination of written recollection of experiences and a complementary interview.
Participants were instructed to complete a time sheet for the previous day, crossing out any times they wanted excluded from discussion.
No participant exercised this option.
After completing the DRM timesheet, participants were asked general questions including if they had any difficulty remembering certain time ranges and how typical their days were.
Six participants reported having atypical days .
Recollected days included both weekdays and weekends.
Using the completed timesheet as a guide, we interviewed participants about each activity and time segment for details about the potential presence of video recording technologies.
In addition, participants were asked to explain how they determined the presence or absence of video recording, the purposes of it, and who might have access to the data.
The interview closed by giving participants an opportunity to discuss any general feelings they have about video recording systems.
Two interviewers recorded and transcribed the interviews with participant consent.
Interviews lasted between 30 and 70 minutes.
Participants also completed a questionnaire that included demographic questions and the validated CFIP instrument .
The parsimonious, 15-item CFIP instrument measures information privacy concerns along four dimensions:  Collection of personally identifiable data;  Improper access by people not properly authorized to view or work with the data;  Unauthorized secondary use of information collected for a particular purpose but used for another; and  Errors in collected data.
The questionnaire measures information privacy concerns on a scale from 1 to 7, with 1 indicating very low concern and 7 indicating very high concern.
We counterbalanced the delivery of this questionnaire, administering it to half of the participants before the DRM and to the other half afterwards.
There were no significant differences in responses between those two groups.
Participants were compensated 40 USD for their time.
In other words, concerns for unauthorized secondary use and improper access were higher than concerns for collection and errors.
In the following sections, we present the results from our qualitative analysis of the interview data as they relate to each of the elements of the CFIP model: collection, improper access, unauthorized secondary use, and errors.
When used together, CFIP and DRM enabled us to better understand people's perceptions of video recording technologies.
We used CFIP and DRM to break down generalized concerns in a contextualized manner.
The results from analysis of data collected through the DRM provided a situated understanding of people's perceptions.
CFIP provided a model to unpack that situated understanding.
In particular, CFIP and DRM used together provide a detailed opportunity to deconstruct the misalignment of privacy behavior that has been seen in previous research .
Specifically, although concern for information privacy was reported as high, the DRM was able to show how these concerns were nuanced and how they could be played out in everyday use.
By combining the CFIP model and the DRM, we are able to address two related issues.
One, the CFIP instrument provides a quantitative measure of how concerned participants are in relation to traditional validated definitions of information privacy.
Two, using the DRM, we can then compare the CFIP measurements with participants' everyday practices.
Furthermore, we can use the CFIP model as an analytical lens to unpack participants' concerns and practices involving video recording, as seen in the data provided by the DRM.
Using the four dimensions of the CFIP model as an initial coding scheme, two researchers independently coded and analyzed the transcribed interviews.
Additional passes expanded the coding scheme inductively along each of the four dimensions that were coded in the initial pass.
The intent of the subsequent passes was to unpack Smith et al.
These subsequent passes gave us a more nuanced understanding of the four broad dimensions of concerns as they manifested in the situated data, demonstrating how the model needs to be expanded to account for novel interactive technologies.
The CFIP questionnaire operationalizes these concerns in two ways.
First, people might be concerned that organizations are asking for and collecting too much personal information.
Second, people may express concern that too many organizations are requesting and collecting this information .
In our data, concerns about collection focused on how resources were used and how participants were notified and asked for consent.
Using the CFIP instrument, participants responded with a mean of 5.54 in the collection dimension ,
Overall concern as measured by the CFIP instrument is high, but not significantly different from Smith et al.
In comparing the dimensions with each other , collection and errors were not significantly different from one another, nor was unauthorized secondary use significantly different from improper access.
However, when explicitly asked to consider video recording--a potentially rich source of personal data--they expressed nuanced, multi-faceted rationale around when collecting video data was acceptable and when it should be prohibited.
One of the primary reasons repeatedly expressed for tolerating, or even embracing, pervasive video recording is the benefit these kinds of technologies can provide.
I feel like with the added security that there's a camera there...
If somebody comes into the restaurant with a shotgun or something and holds us up... we'll get the money back because we have it on video tape.
As P19 noted, "We're going to lose some privacy but we're going to get also other things out of it..."
The need to notify people in an unobtrusive way that still garners their attention and consideration is a significant problem for HCI.
In particular, as more recording happens in mobile and ubiquitous computing applications where the technology is designed to "disappear" into everyday objects, notification will become both more difficult and more important for user adoption and acceptance.
Perceptions of the resources required to collect and store large amounts of data can complicate considerations about recording.
For example, a project manager who expressed concern about data collection in the office was less concerned about collection of data in the home because it was "expensive" and "useless"--therefore improbable:
Although participants often reported being comfortable with collection of video data, they described being most concerned about recording activities to which they did not consent prior to the recording, particularly when at home or in another space generally considered to be "private."
Of course, the first step to consenting to recording is to become aware of it.
Concerns can be raised over the potential for databases to combine personal information from various sources, creating a "mosaic effect" .
This issue is present throughout the literature  but came up very little in our data.
One notable exception to this trend was an individual whose office workplace had extensive surveillance to monitor employees.
She noted that she was careful about the departments she visited and how often she went to the bathroom based on the idea that her employer could assemble information from all of the various cameras to provide a comprehensive view of her day.
Notification of recording can occur in a variety of ways, such as seeing the camera itself, observing a video feed from the camera on a large display, reading a notice at the site of recording, or being notified of recording prior to installation.
However, participants reported becoming acclimated to both the recordings and notification about them.
Furthermore, in public places, people described having limited concerns about video recording despite substantial concern over collection of data generally .
Typically, this tolerance stemmed from both acclimation over time and a general lack of awareness of the recording activities.
For example, referring back to a therapy training program during which sessions were video recorded, a school counselor commented:
This issue inherently brings up technological issues  as well as policies and social norms.
The CFIP questionnaire operationalizes these concerns as deficits in the willingness or ability of organizations to prevent unauthorized access, including the often-expensive protection of databases and ensuring appropriate access control to prevent unauthorized people from accessing personal information on their computers .
Participants expressed a mean response of 6.16 in the improper access dimension , meaning they were highly concerned about their personal data being accessed by the "wrong" people.
However, in our interview data, considerations of improper access included accidental sharing, in addition to intentional or malicious sharing.
At times, concepts of improper access also related to unknown access, in which participants struggled to describe who might have or want access and why.
In this section, we detail the ways in which these concerns about improper access emerged.
So, you can't... have a lower-level employee who is looking at the data and like, stalking someone.
The policy of having the manager for the next shift replace the tape from the previous shift limits the potential for a manager to take identifiable data out of the restaurant, fraudulently replace a tape showing misconduct, and so on.
A particularly strong type of policy, legal protection of video data, can include the requirement for a court order, the inadmissibility of evidence in court, and laws surrounding libel and slander .
That store...it strikes me as a well-run business, so therefore, I'm taking that same leap that they are taking then certain precautions that a good business would.
In other words, everything being consistent, I would guess that they would have very limited access, probably locked up in a manager's office.
The repercussions of this kind of data leakage may not be as severe as identity theft and other risks of improper access to personal data.
However, improper access can ruin relationships and disrupt careers, which for many individuals are more "real" concerns than identity theft and other commonly described risks.
For example, describing recording from a job interview, a university counselor noted,
Within some organizations, trust and close relationships among the people in them can be considered adequate protection without specific policies.
For example, in describing recording and broadcasting of church sermons for which there are no hard policies about data protection, a churchgoer nonetheless viewed herself as adequately protected based on the accountability to those with access to one another and to the church leadership.
Despite concerns about improper access, in many cases, people expressed ambiguity about who does have authorized access to the records.
For example, in describing traffic cameras, P14 noted "I'm assuming the Caltrans department, possibly the police, maybe California Highway Patrol.
I'm hoping those are the only departments."
Adding to the ambiguity over who can access records is the concern about how they might access them.
Monitors in unsecured locations, such as a security officer's desk in the main lobby of a building or even mounted from the ceiling in a store to deter theft, are often used to view recordings.
These monitors can be viewed and perhaps recorded surreptitiously using another video camera.
The laws and policies regulating access to personal information do not yet completely account for video recordings.
For example, the Family Educational Rights and Privacy Act1 in the United States requires that guardians of a child must be given access to all school records for that child upon request and that no such records will be given to anyone else without their consent.
If video or images of multiple children are captured and stored at a school, it is not clear how the school is meant to handle the case in which the guardians of one child demand a copy of the image and the guardians of another refuse its release.
Interview participants hypothesized about a variety of potential internal unauthorized secondary uses in which organizations that collected data might reuse it without permission.
Participants often noted that it was unclear what the data collectors might do with these recordings after they were captured.
For example, when asked about other uses of video recording in a mall, P14 posited that they might be used to "count foot traffic and seeing how busy the shopping center is.
I guess they could be doing racial profiling or something..." When participants described being more confident about the potential for internal secondary uses, these uses were nearly always attributed to the idea that employers and business owners might observe the activities of their employees.
For example, as a shopper, P10 described how records from security cameras might be used in a store if he were the owner of that store:
Secondary use implies that information collected for one purpose is used for another.
This kind of secondary use is central to argumentation about many recording applications in HCI, for example, any data mining application that uses data collected for one purpose to reveal connections that were not previously known.
Likewise, it is common for corporations to find additional uses for data already collected, such as utilizing research data for marketing purposes .
When secondary use occurs without the permission of the person whom the data describe or without deidentifying these data, it is considered unauthorized secondary use.
Unauthorized secondary use can be internal--access and use by a party internal to the organization that originally collected the data--or external.
Both kinds of "information release"  can intensify concerns over unauthorized secondary use .
The CFIP instrument operationalizes beliefs about unauthorized secondary use in terms of agreement with two principles.
First, organizations should not use personal information for any other use than originally intended without permission .
Second, companies should not sell personal information nor share it without permission .
Participants reported a mean response of 6.23 in the unauthorized secondary use dimension , meaning they were generally highly concerned about their personal information being used for unauthorized purposes.
As noted in the quote above, the legality and underlying ethics of such a choice are not always immediately clear.
Particularly in public or semi-public locations like a store, the expectation of privacy might be so minimal that secondary uses, even those that are unauthorized, are not well protected under current law.
Instead, corporate policies may be more likely to govern these kinds of uses.
Even in those places where employee surveillance is commonplace, however, these uses have often emerged from an infrastructure originally installed for security or some other purpose.
For example, one participant described her workplace, in which surveillance is the norm:
For example, CCTV systems installed for security purposes in restaurants can be used to help recognize customers looking for service.
I would hope it's not some other purpose!"
In particular, after determining a logical primary use in situations in which the actual use is unclear, participants struggled to articulate other potential uses and therefore objected to these unknown secondary uses.
For example, after describing being comfortable with traffic cameras for "traffic and safety purposes", P01 went on to describe disagreeing with other purposes, such as "general surveillance that have no particular law enforcement use per se, just for spying on people who haven't done anything."
When participants were unsure about the potential uses of the data, they also described being unsure of archival policies, analysis methods, and access practices.
The intentional sharing or unintentional leaking of data beyond the organization that originally collected it constitutes external unauthorized secondary use.
In our study, concerns were raised about particular parties who might gain access and use the collected personal information.
There was also a general sense of "other" parties, in which that "other" gaining access would be undesirable.
When probed about this kind of external access, however, many individuals reverted to a "need to know" articulation similar to the policies surrounding improper access, with the underlying position that protections would be in place to keep data from any external parties who have no explicit need to know this information.
Furthermore, participants at times described being influenced by existing laws surrounding data protection that take a similar position: "...being that Disney is a Fortune 500 company, they wouldn't want to be violating any of those laws."
In the CFIP instrument, concerns about errors are operationalized as deficits in two areas: organizational investments in time and effort to ensure the accuracy of their databases and organizational procedures to correct errors .
Although there are examples of researchers--even Smith himself--describing the inspection and correction of errors as solutions for addressing these concerns , Smith goes on to describe how errors can be "stubborn" and "snowball in spite of such provisions" .
He asserts that these errors may be malicious or accidental, noting in fact that a large source of errors might be the presence of data that have changed or fallen out of date and thus should be deleted .
From the CFIP instrument, participants provided an average response of 5.33 in the errors dimension , meaning they were generally concerned about the errors that could occur in the records about them.
More specifically, in our data, individuals described cases in which expectations of the presence of data influence the required accuracy of those data.
Thus, in this section, we describe concerns and expectations about accuracy in terms of both accidental and malicious errors present in both the presence and the absence of data.
In the cases in which specific "others" are of concern, it may be that respondents' identities are carefully crafted for those individuals 
In other cases, however, the issue at hand is the persistence of the activity, rather than its initial experience.
For example, after describing his amusement at the idea of replaying embarrassing moments in his friends' lives, a restaurant manager was explicit in his desire not to have such moments replayed about him:
Seeing is believing, as the old adage goes.
Often video "evidence" is believed to be "reality."
However, every video recording is selective in some way, whether it be the angle, the quality of the recording, or the control of when recording occurs and how long it is archived.
Recording can enable the recreation of "truth" from video within "socially situated, historically constituted" bodies of practice .
Video records have the appearance of "realness," but without the extra context of lived experience, people construct a variety of explanations for any particular video record.
Given the potential for video to provide a kind of "truth" that other records are often perceived not to have , respondents described using video evidence to correct errors in other records.
For example, when asked about the uses of video recording in a parking lot she frequents, P05 noted "If someone's car was being towed because they didn't have a permit, but on the video it shows they have a permit, "
Similarly, P08, a restaurant owner uses a substantial CCTV recording system in his business.
He described being concerned about someone fraudulently filing a claim in his restaurant and how he might use the records in such a case:
Like somebody slipping, like fabricating a story about slipping in the restaurant and suing us while we have it on camera that they cause their own slippage... it serves as evidence.
To me, video cameras don't lie.
I mean it shows the date, the time, and it shows you were at a specific place at a specific time."
Despite comments about the potential for errors, few participants described these kinds of severe consequences  or wanting to prevent video recording from taking place in most places outside their own homes.
However, stories in our interviews resonated with reports of public surveillance projects being abandoned due to citizen concerns as well as consumer choices, such as boycotting a store.
For example, P10 noted his discomfort with constant reminders of surveillance cameras in stores that make him feel distrusted: "...if it's a place where all I'm reminded of is `you could be a criminal', then I'd probably go somewhere else."
These kinds of beliefs in the relative infallibility of video recording may explain in some part why eight participants tested very high  on Smith et al.
Concerns over accuracy of the interpretation of data in video records do, however, abound in our interview data.
In particular, video records can give the appearance of a truth of sorts and yet be flawed.
At times, these flaws can undermine a sense of safety that justice will prevail due to the recording.
For example, although nearly every interview participant noted at some point that records can be used by police to oversee traffic accidents, prosecute criminals, and so on, P20 described a real-world example in which the video records were not of a high enough quality to meet her expectations:
Video records can often be accurate in form and content but inaccurate in terms of how people perceive a particular occurrence or activity.
Human judgment is filled with nuance, responses to contextual cues, and so on.
These capabilities can be hindered when the records are created and acted upon without substantial human intervention.
Most frequently, this concern surfaced from interviewees describing traffic and "red-light" cameras.
For example, when describing a traffic citation his father received, P14 noted, "...they gave him a ticket, because he ran the red light, but it was safe...I don't think he should have gotten the ticket just because it was a safe judgment..." People are accustomed to discussing the nature of a traffic citation with the officer delivering it, and in many cases, convincing the officer to reduce the citation to a warning or some other lesser charge.
Increasingly, however, the camera is the decision-making agent with which negotiation is impossible and through which citations are easy to issue.
Even when there's no one there, you're being filmed at an intersection, and it's one thing if you blew right through a red light or something that you deserve...but a lot of this persnickety little stuff--like, you didn't stop or you went through when it was yellow--I just think it gives too much power... 
Concerns over information privacy are "neither absolute nor static, since perceptions of advocates, consumers, and scholars could shift over time" .
Indeed, such shifts have been demonstrated repeatedly in the literature, including the shift from a particular population wanting to be notified about CCTV  to the same population not caring , as well as the continued rising levels in reports about general anxiety over information privacy .
Similar to other populations who have been studied using the CFIP instrument, our results indicate that people continue to be concerned about information privacy as a general case.
However, the way in which concerns about collection, unauthorized secondary use, improper access, and errors are articulated with regard to pervasive video recording technologies require expanded consideration of these dimensions of information privacy.
Our results indicate that collection of large quantities of video data is nearly assumed when outside the home or other private spaces in the United States.
Similarly, search engines, tracking cookies, and other software accomplish substantial surveillance of online activity .
This repeated exposure to data collection and an overall sense of continuous surveillance helps to explain why reported concerns for collection were lower than the responses to improper access and secondary use.
When recording was not expected, people described wanting to provide informed consent.
However, the first step of that process--being informed--requires improved awareness of what is being collected and how it might be assembled during long-term, large-scale collection.
Participants described a general lack of awareness, suggesting the need to expand the CFIP model to gauge awareness and understanding of how and what personal information is collected.
Furthermore, this issue raises open questions for HCI researchers and designers who seek to develop new models and mechanisms for informing people about recording and assessing their consent to that recording.
Concerns over improper access of video recordings were articulated by participants in terms of data protection policies for video as well as the kind of accidental sharing that might come from data leakage or uncertain security protections and processes.
The lack of awareness about who has access necessitates expansion of the CFIP model to measure awareness and understanding of processes used in storing, processing, and disseminating collected personal information.
As shown in previous work on location data, who has access to collected data impacts concerns regarding information privacy .
An open challenge for HCI designers and researchers, then, is to provide awareness of not only how and what personal information is being collected, but also who may have access to data invisibly stored on servers elsewhere.
Participants described being concerned about both internal and external unauthorized secondary use of video data, most of which is unknown to the participant.
The uncertainty over who has access and what they might do with the data indicates both a challenge of using the CFIP model in HCI and Ubicomp and a challenge for HCI designers and researchers to inform those recorded about the capabilities of these technologies.
Additionally, the flexibility of these systems requires that the CFIP model be expanded to account for not only the reuse of collected data but also the repurposing of existing infrastructure and applications for other uses.
Unfortunately, it is all but impossible to account for every potential, possible use of the personal information, infrastructure, and applications involved.
Finally, consideration of concerns over information privacy and video recording in terms of errors brings to light the ways in which the expectations of the infallibility of video recordings influences views on the acceptability of these records.
In particular, expectations of the data to provide evidence and represent "reality" without the nuance and context of lived experiences may not match the potential for errors nor the capabilities of the systems themselves.
Additionally, the notion of errors is not solely technical.
Participants were also concerned about the erroneous interpretation of video recordings.
In light of these challenges, the CFIP model should include a consideration of the understanding of the technical capabilities and limitations of Ubicomp technologies.
Application of the CFIP model to video recording technologies can enable understanding of the four dimensions of information privacy concerns: collection, improper access, unauthorized secondary use, and errors.
However, the original CFIP model must be augmented to account for issues that emerge in Ubicomp technologies, particularly those related to pervasive tracking and recording, such as video recording.
In this paper, we have presented an analysis of empirical data surrounding concerns over information privacy in relation to video recording.
These results demonstrate that just as with other issues of information technology and privacy, respondents have nuanced and complex responses to these technologies.
Furthermore, our results present a variety of challenges for HCI researchers and designers including how to inform and to educate end users and those who may be recorded as new technologies are created and how to present these technologies in ways that encourage adoption and use while still being informative about potential risks.
