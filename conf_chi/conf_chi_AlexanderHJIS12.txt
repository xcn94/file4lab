Foot-based gestures have recently received attention as an alternative interaction mechanism in situations where the hands are pre-occupied or unavailable.
This paper investigates suitable real-world mappings of foot gestures to invoke commands and interact with virtual workspaces.
Our first study identified user preferences for mapping common mobile-device commands to gestures.
We distinguish these gestures in terms of discrete and continuous command input.
We investigate this issue further through three user-studies.
Our results show that rate-based techniques are significantly faster, more accurate and result if far fewer target crossings compared to displacementbased interaction.
We discuss these findings and identify design recommendations.
In most of these scenarios the user's hands can hold the device giving them visual access to information on screen but it is difficult for the user to perform precise interactions with the touch-screen or buttons required for device operation.
The use of feet for non-mobile device interaction is already common: the pedals in a modern car are often connected to a computer rather than directly to the engine, arcade games such as Dance Dance Revolution  use only the feet for interaction and home entertainment systems such as Microsoft Kinect and Nintendo Wii use foot-based interaction for gaming purposes.
Passive interaction systems such as Nike+  connect an accelerometer embedded in the shoe to a mobile device to record running or walking information.
The literature has considered various foot gestures for different contexts and one can find three emerging categories: kicking , foot tapping  and ankle rotations .
Prior work has focused on providing a thorough understanding of human abilities for input using foot interaction.
Less attention has been paid as to how these types of interactions map to real-world commands.
Foot and lower-leg gestures open a broad design space to the problem of interacting with a mobile device when the users' hands are busy or otherwise unavailable.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
This work therefore aims to understand how user would expect foot gestures to be mapped to common mobile device functions and to investigate interaction parameters associated with these mappings.
We achieve this through a guessability-style study, alike , that probes users on suitable commands.
This resulted into two major groupings, discrete and continuous foot mappings.
With accelerometers on the mobile device, their trained system could classify ten different foot gestures with approximately 86% accuracy.
In addition to tapping, which commonly occurs when the user is stationary, harnessing discrete foot input has also seen some success when jogging.
Foot-Step  can recognize a command based on the type of side-stepping  a user makes while jogging.
Foot stepping can be captured with as high as 95% accuracy but was useful for a limited input range of up to four commands.
In contrast to discrete command invocation, continuous foot input has been explored for a number of activities such as artistic expressions , game playing  and document navigation .
Paelke et al  successfully report on user satisfaction in playing games when using a mobile phone's camera  to detect foot movements and control a soccer ball.
They apply their results to kick-up menus for selecting items on a mobile device with the foot.
Drawing inspiration from work on wrist control , Scott at al.
Similar to their results on feet tapping  they show that heel and toe rotations provide a reasonable range-of-motion and can be successfully detected by a device placed in a user's pocket.
The above work provides a frame of reference and motivates our systematic study on foot gestures.
What is less known is what types of tasks map well to either discrete, continuous or both input types.
Given the dexterity of the lower limb, we explore the use of specific real-world mappings for foot based gestures, by first developing a catalog of novel foot gestures.
To find ideal mappings for foot interaction we first examine some of the salient properties of feet motion.
We then review prior foot-based interactions and group these into two camps, discrete and continuous mappings.
Finally, related to this work are methods for defining a new catalog of gestures, which we briefly review.
The human foot is a highly dexterous system with advanced movements using multiple joints that increase in movement complexity from the hip to the ankle.
Advanced movements such as balancing on one foot to kick can be mastered as early as nine months .
Each leg joint provides multiple movements with varying ranges of motion.
While the lower limb allows for a variety of maneuvers, studies comparing it to the arms show that it is not as precise as human hand and fingers .
This is particularly the case for fine and precise movements, such as moving a trackball with the foot to select on-screen targets .
However, studies have also shown that the foot is better suited at more coarse level movements, and can be quick in performing such actions .
It is for such types of coarse and less precise motions, that we investigate the use of foot gestures.
Early work in foot-based input concentrated on the design of suitable foot controls  and sensor embedded shoes for entertainment purposes .
Recent work has extended foot input to specific application contexts, such as to retrain the lower limbs in physiotherapy , for ambient awareness , and for user identification through gait  and foot imprints .
In relation to our work, we broadly categorize existing literature into discrete and continuous foot-based input.
Discrete foot input has primarily focused on command invocation.
The most common type consists of foot tapping  which was shown to provide sufficient input breadth to interact with mobile devices, even when the latter are buried in a user's pant pockets .
With accelerometers attached to the foot,
Numerous interplaying factors influence the design of any new system of gestures.
Designers have to provide optimally usable features within a fixed number of constraints, such as the type of sensing device used, limitations of human motor function, and finally the robustness and distinctiveness of the gesture set for easy and sound classification.
Researchers have proposed several methods to provide rigor to the design of gestures.
The design of most gestural syntax begins with a basic knowledge of human motion capabilities, to determine what is mechanically possible.
Alternatively, the user can be included in the process .
Participatory design incorporates users in the design process and carefully iterates on numerous proposals before settling on a given syntax.
When possible, researchers can observe user interaction with physical objects, as gestures are largely based on our interaction with items around us.
For example, observing user behavior in passing sheets of paper around a table  or rotating and dragging objects  has given impetus to the design of novel gestural techniques for tabletops.
In the absence of an implemented system, designers can get quick feedback on their newly defined catalog using Wizard of Oz approaches.
Recently, researchers have elicited user input and defined methods for converging on a given gestural mapping through metrics such as agreement scores .
Such an approach, referred to as guessability, has worked successfully in defining a gestural mapping for one or two handed input on a tabletop  and for gestures with mobile devices .
The ultimate objective of democratizing the design process is to funnel the user's input into a sound set of implementable gestures.
This provides some level of assurance that a good number of users may intuitively agree on the gesture set and above all, that these are physically possible.
We adopt this approach in identifying a suitable mapping of real-world gestures for foot-based input, one that has not been explored in this context.
Additionally, since users may provide broad mappings from task to gestures, to complete the design loop we also further refine user suggested mappings and consider how best to design for cases where the mappings may not be so straightforward.
The study primarily consisted of a researcher presenting participants with a range of mobile interaction scenarios  for which each participant was requested to perform a foot-based gesture that they believed was appropriate for the required situation.
During the study, participants stood upright on a marked area on the floor.
They wore a Texas Instruments EZ430Chronos Sports watch on their dominant lower leg, just above their ankle.
The watch contains an accelerometer that was set to sample at 400Hz.
This was used to record their movement while performing gestures, with each session also video-recorded for later verification.
Although participants only wore the watch on their dominant leg, we placed no restrictions on which leg  they could use for performing the gestures.
We choose a range of mobile-device interactions that we believed were representative of the commands that users issue to their mobile devices, as shown in Table 1.
Many of these were also used in the study by Ruiz et al.
For each of these scenarios the experimenter placed the command into context, for example, "your phone is ringing and you want to answer it.
What foot gesture would you perform to answer the phone?"
Once all gestures were performed, participants completed a questionnaire regarding their interactions.
Participants were told to consider each gesture independently and not to worry about the technical issues of gesture detection.
The categories of gestures were always presented in the same order ; the gestures within those categories were presented randomly.
The literature provides a solid understanding of the various methods of conducting foot-based interaction.
However, there is little knowledge of how these gestures may be mapped to device commands.
To investigate this we did a study to understand users' perceptions on how foot gestures should be coupled to common mobile device functions.
To capture these perceptions we wished to create a userdefined gesture set for a range of mobile device interactions.
To do this we conducted a guessability study, similar in style to Wobbrock et al.
This type of study is particularly suitable for foot-based interaction, as users currently have few, if any, preconceptions of this mapping--the gestures they provide are likely to be natural or logical mappings.
The remainder of this section describes our experimental design and results.
We observed a total of 537 commands: each of the 19 participants provided a foot gesture for the 30 commands, with 33 instances where no gesture was performed.
Based on the participants' actions, we performed a series of analyses to understand participant preferences for mapping commands to gestures.
To begin, we analysed the agreement between participants for the gesture sets they selected using Wobbrock et al's methodology .
High agreement values indicate many participants selected the same mapping, low value indicates a large diversity in the selected gestures.
Overall, we found a large diversity in the gestures participants selected for each command.
To further understand why we had observed such disagreement between participants, we performed a second, deeper analysis of the suggested gestures.
Activate gesture recognition Generalised gesture Tap foot Shake foot Tap foot Trace circle Tap Tap Tap Rightwards movement Leftwards movement Tap Tap Trace symbol % part.
In the generalised gesture set classification, we now saw a mean agreement across all commands of 0.42 , with all commands now having agreement values above 0.2 .
This approach improved our ability to create a coherent gesture set based on user inputs.
A summary of the process for gesture selection is shown in Table 1: for each command we indicate the most common generalised gesture feature, the percentage of participants who performed a gesture with this feature and then finally the selected gesture for each command.
Note that for some gestures there was still significant disagreement between participants and/or the same gesture selected for multiple commands meaning that some commands did not have a gesture assigned.
We were able to define gestures for 23 of the 30 commands in the command set.
Several specific gestures appear multiple times in the defined gesture set .
However, contextually, the gestures are unique: gestures from one set will not be used at the same time in another, for example, media control gestures will not be required while navigating maps.
In the first analysis we strictly classified gestures into sets that were identical--for example, gestures that used two foot taps and gestures that used three foot taps were classified as different.
To better understand the disagreement between participants, we relaxed the gesture classification criteria from being strictly identical to those that are `similar'.
Similar gestures were those classed as having a common property.
Examples of these properties include: the direction of leg movement and the type of movement, e.g.
For those gestures with multiple common properties, we grouped gestures in such a way that we formed the largest groups possible.
The coding of identical  and `similar' gestures was performed independently by two coders with discrepancies reviewed and corrected.
A much clearer pattern of gesture to command mappings emerged from the more generalised groupings.
Our findings suggest that many of the gestures are logical mappings from commands participants are already familiar with.
For example, tapping to select or a rightward movement to move to the `next' item, seem to be direct translations of what occurs when using touch on mobile devices.
Interestingly, we find a consistent left-to-right pattern for distinguishing commands along the time dimension.
Finally, for browser control, such as moving forward or backward, users apply spatial positioning to their taps.
These results overall reveal intuitive mappings that seem sound and logical.
In generating the mappings for common phone commands to foot-based gestures, our initial analysis resulted in a large diversity in participants' gesture selections.
By performing a generalised set analysis we could better represent the gestures of a greater number of participants.
This required generalisation and is perhaps representative of the need for gestures to be flexible and/or customisable.
If a user finds a particular gesture uncomfortable or the mapping unnatural the option should exist for reconfiguration.
Additionally, relaxing constraints for classifying the gestures led us to converge more quickly on certain types of gestures.
Such generalised classification provides a good starting point for gesture implementation.
However, all gestures still require refinement and the ability for users to customise exact input parameters.
Recognition of all gestures, except `shake foot' was successfully with rates above 80%.
The `shake foot' gesture fared poorly during recognition.
This is likely due to the ambiguity of the gesture: participants did not perform the same number of left and right shakes each time meaning the classifier misinterpreted the user's intentions.
The previous section detailed a user-defined gesture set for foot-based control of common mobile device commands.
Since users were free to derive any set of gestures, one important design aspect is to ensure that the gestures can be easily recognized and distinguished by an electronic recogniser.
To validate this intention, we used an implementation of the Protractor3D gesture recogniser  to classify foot gestures detected using an accelerometer.
The goal of this validatory study is to determine whether our set of gestures can be accurately detected and classified.
For this reason, we asked participants to wear an accelerometer strapped to their leg, combined with the Protractor3D recognizer.
While this setup would be impractical for real-world use, we were interested in determining if the gestures can be reliably classified, not whether our method is the technologically best implementation.
We chose the Protractor3D recognizer as previous studies validating its use showed high recognition rates .
Participants wore the same Chronos Sports watch as described earlier.
The data from the accelerometer was fed in real time, into the Protractor3D recognizer.
This study has shown that the selected gesture set is detectable and individual gestures are distinguishable.
These gestures must now be mapped to mobile device functions.
The mapping of discrete gestures--single actions often triggering a mode switch, such as answering the phone-- are straight-forward to implement.
However, those gestures that map to continuous actions such as panning a map or adjusting the volume of the audio require further design thought due to the increased number of parameters involved.
For example, how should a forward kick map to the size of an upward pan?
Should the kick velocity map to distance travelled?
Should the distance the foot has moved map to the distance the map moves?
Or should the kick initiate a rate-based scrolling action?
To answer these questions we conducted three additional studies that examined the mapping of foot-based gestures to continuous actions.
Five participants  with an average age of 22.2 years participated in this validation study.
The gestures were presented to users in the sub-categories introduced earlier , using the gestures selected in Table 1.
However, each unique gesture was only presented once-- identical gestures were not presented multiple times.
For each sub-category, participants were first asked to perform each of the gestures five times to train the recogniser.
Once the training was complete, gestures from the sub-category were performed, five times, until all gestures were covered.
The gestures were presented to the users in a pseudowizard-of-oz style.
For example, when the user was required to answer the phone, they were verbally told the context, reminded of the gesture, and were then asked to complete the gesture when they heard a phone ringing--the experimenter played a ringing sound from a PC and manually stopped the sound once the gesture was complete.
Many mobile device applications require continuous interaction, such as scanning music playlists, scrolling an address book or navigating a map.
Having derived a mapping of foot-based gestures to commands and validated that these gestures can be detected, we wished to examine the issue of mapping foot gestures to continuous interaction.
The over-arching goal of these studies is to answer the question: how should a kick action control continuous movement?
To answer this question we implemented four kick-tomovement mappings: one distance-based, two velocitybased and one combined distance and velocity mapping .
We then tested the viability of these techniques in three controlled evaluations.
The second looks at their suitability for acquiring targets at different distances.
While the third combines these two parameters to understand the complete context of continuous navigation.
In these studies we apply our techniques to spatial document navigation; however, we envisage our results will also generalize to other continuous interaction techniques.
With all continuous interaction systems, there is the question of whether a navigation action should move the content or move the view of the content .
To provide an informal answer to this question we implemented two versions of each of the four mapping techniques: one which had the kick actions tied to the content and one which had the kick actions tied to the view.
We gave 10 participants both versions of each mapping technique with a single target location north of the starting position.
Once they had used both mappings of each technique, they were asked to subjectively indicate which they preferred.
We implemented four mappings of kick gestures to panning actions.
These mappings are motivated by techniques found in the literature and commonly employed in desktop or tabletop environments.
Returning the leg to the vertical position does not result in the map panning in the opposite direction, instead the user must move their leg in the opposite direction past the vertical position to achieve reverse panning.
A horizontal position delineates movements in left and right directions.
The user must hold their leg in that position to maintain the panning velocity--moving their leg back towards a vertical position reduces the velocity accordingly.
In this technique, the user kicks forward to create a velocity mapping--the furthest distance the kick reaches is used as input to the panning velocity.
The user can then return their leg to the vertical position.
Kicking again changes the pan velocity, tapping their foot on the ground stops the panning motion.
We implemented a version of CMF for kick gestures.
For a forward kick, this worked as follows: when the user's leg is moving forward, CMF is in displacement mode.
When the user's leg halts or begins to return to the upright position, CMF enters flick mode, with the initial flick velocity derived from the velocity of the forward displacement.
A friction element slows the flick, which can also be terminated by the user tapping their foot on the ground.
We fine-tuned the parameters for each of the above interaction techniques through informal pilot studies involving four participants.
Participants conducted a series of kicking tasks and provided feedback on the mapping of kicks to parameters for each of the interactions.
Using a series of iterations we selected parameters that provided the most `expected' behavior for each technique.
Participants overwhelmingly preferred kicks to move the content.
This meant that a kick forward should move the map content forward, closer to a southwards target.
All participants preferred this mapping for the RH technique, and all but one preferred this mapping for the displacement and flick techniques.
Six of the ten participants preferred a kick to move the content for the rate-based continuous technique.
In all of our subsequent implementations the kick action is tied to the map--i.e.
We created a 4x8 factor within subjects experimental design with factors Gesture Mapping and Direction.
We used the four gesture mappings  described in the previous section.
We tested movement in the eight compass directions: north, north-east, east, southeast, south, south-west, west and north-west.
The study had four sections, one for each of the Gesture Mapping techniques.
These techniques were provided in a random order to each participant, with all tasks using that technique completed before moving to the next.
The experimental software automatically recorded all aspects of the interaction required to reach a target goal on a map: the task completion time, the number of kicks and the total distance and direction of all movements.
We used a spatial workspace, a map, to provide users with a familiar context in which we could test continuous foot gestures.
Participants got two minutes of freeform practice navigation with the techniques followed by four practice trials.
Each trial began by the participant pressing a button on the side of the screen and was completed when the target was placed inside the green rectangle .
We recruited 10 volunteer participants , with an age range of 18-27 years old.
All of these participants were right handed; six of whom had experience with onscreen touch gestures.
During the study, participants stood upright on a marked position and held a 7" screen  at waist level, as shown in Figure 1, right.
This displayed the experimental interface, as described below.
The participants' foot-based gestures were captured using an XBox Kinect camera mounted 0.75m from the ground and 3m from the participant.
This camera detected participants' kick gestures accurately in three dimensions, a `kick' began when the foot was lifted from the ground.
We used the skeleton tracking algorithm provided by the OpenNI  libraries and we implemented functions to translate these gestures into the interaction techniques described earlier.
These overall results are shown in Figure 4.
Visual inspection of the number of target crossings  indicates that slower techniques suffered from target crossing .
Interestingly, RC suffered badly from this issue, while RH did not.
With RH, users are already poised to drop their foot, while RC users must recognize the end-point, raise and lower their foot to stop, or perform another kick to reduce scrolling speed.
This experiment tests the capabilities of each continuous foot mapping and not the users' spatial abilities.
For this reason, all tasks began at a random  location.
Tasks did not necessarily end at a specific city or location, but were determined by the required direction of travel .
Each task began with the starting position centered on-screen .
The target was marked with a black cirlce and had concentric circles extending throughout the whole map  to indicate the required navigation direction.
Participants were required to place the target in a rectangle with a green outline  occupying the middle fifth of the screen in order to complete the task.
The system automatically detected when the map was stationary and the target was inside the required circle for one second.
However, participants took different times to complete target acquisition in the eight different directions.
This is summarized in Figure 5.
Participants took the longest to reach targets in the East  and North-East  directions, and were the quickest in the South  and South-West  directions.
There was a significant difference in the target acquisition times of different directions .
Overall, participants were more efficient at reaching targets that require kicks in the forward direction .
This is likely due to participants finding forward kicks more natural--e.g., kicking a ball is often performed with a forward kick.
This study used a 4x5 factor within subjects design, with factors Gesture Mapping  and Distance.
These positions represented a range of close and distant targets for a typical map navigation task.
All kicks were made in the same direction--towards a southern target  as this performed the best in experiment one.
We ignored all non-north/south direction in participants' kicks.
Each participant repeated the kick gestures for each technique/distance combination five times.
The pilot study and the two quantitative experiments have provided us with insights into various aspects of how kick gestures should be mapped to continuous interaction.
In this final study, we wished to compare all aspects of the two best performing systems so far: rate-based continuous and rate-based hold.
We also wished to gather subjective feedback on these two systems.
This experiment was conducted in the same manner as experiment 1 and 2, with the differences in experimental method listed here.
This study used a 2x3x8 factor within subjects design, with factors Interaction technique , Distance  and Direction .
Each participant repeated the kick gestures for each {Technique, Distance, Direction} combination, three times.
Participants performed all gestures with one technique before moving to the next .
The presentation of direction/distance pairs was randomised.
After performing all kick gestures with one technique, participants completed a subjective evaluation.
Overall preferences were gathered at the end of the evaluation.
We collected data on a total of 10 participants x 4 techniques x 5 distances x 5 repetitions = 1000 target acquisitions.
All tasks were successfully completed.
Figure 5 shows the expected result of target acquisition time increasing with target distance.
There was also a significant difference in target acquisition times for factor Distance, with closer targets always quicker than those further away .
A post-hoc Tukey test gives an HSD of 0.63  indicating that all distance pairs were significantly different.
To better understand the acquisition time differences between the four techniques we analysed the number of kicks users performed to reach the required target .
We collected a total of 1440 data points .
All tasks were successfully completed.
Overall, we found no significant difference in the task completion times between the two techniques.
RH  is more consistent around all directions of the compass.
Conversely, RC is faster in achieving southern targets but is generally slower for northern targets .
This skewedness correlates with observations during this and previous studies--participants find it harder to kick backwards than forward, with RC especially suffering.
An analysis of the number of kicks and number of overshoots for the NW/N/NE and SW/S/SE sectors showed that both techniques had higher errors and overshoots when heading towards the northern regions.
Overall, seven of the ten participants preferred RC, two preferred RH and one was undecided.
After using each system participants completed a NASA-TLX workload assessment form indicating their subjective impressions on 5-point Likert scales.
Only the `physical' category differed significantly between the two techniques: RH had a median ranking of 4.5  and RC 3.5 .
This feedback correlates with the requirement for the user to hold their leg in their air when navigation is taking place in the RH condition.
User feedback indicated this `mid-air' holding was tiring.
Our results suggest that for mapping continuous foot gestures to tasks, a rate-based approach works best.
This technique results in far fewer kicks than the equivalent displacement based techniques.
Users felt most in control, despite having to keep their feet in the air.
This finding matches that for other finger-based interactions  where rate-based navigation is more efficient in a number of tasks.
Finding that this also applies to continuous foot movement reaffirms the strength of rate-based techniques and their transferability to other forms of interaction.
In our experiments we assumed the users had visual information available to them at all times through the display of the phone.
This we believe is a reasonable assumption.
For example, when the user's hands are dirty or wet they can still hold the device on its rim without damaging or dirtying it.
In these cases the user would rather use foot gestures to interact with the device than risk touching the screen with their wet or dirty hands.
Combing continuous foot interactions with other output modalities would provide another avenue for future work.
The last experiment assessed the effectiveness of the two rate-based techniques that performed best in comparison to the displacement and flick techniques for distance and direction control.
The results overall indicate preference and efficient interaction for rate-based continuous , even though this required participants to move their leg and keep it in the air until they reached their target destination.
This result was not echoed for all directions, particularly those requiring the users to kick backwards.
The results of the studies conducted in this paper allow us to draw a number of implications for the design of footbased gestural interaction.
Context of use can ensure small gesture sets: small gesture sets are easily remembered by users and can be encouraged in foot-based interaction by using the device's context to eliminate ambiguity.
The types of gestures should not be restricted as `off the shelf' recognizers perform well.
Use Rate-Based Continuous for continuous interaction: this technique was subjectively preferred due to the less physical effort even though it suffered from greater acquisition times in the southern direction.
Physical effort has the potential to alienate large groups of users, so should be minimized wherever possible.
Avoid backwards selection: experiments 1 and 3 showed that users struggle more with backwards kicks.
Where possible, these should be avoided.
Use direction over distance: many designs require users to select from multiple choices.
When using foot-based interaction, it is desirable to have these choices made around a compass point rather than by asking users to select items at a distance.
The guessability study revealed a clear demarcation of gestures that were either discrete or continuous.
Discrete gestures were mapped to specific tasks, such as locking or unlocking a mobile phone.
Continuous gestures were mapped to tasks with a spatial component, such as moving in one direction in an information space.
Such a distinction allows designers to separate their mappings into very clear camps.
However, this also requires further design consideration for the continuous mapping, as we performed with the three studies outlined above.
We found good recognition rates of the user suggested gestures using a simple yet efficient recognizer.
This suggests that while users were given full freedom for generating best mappings, there should not be issues implementing detection algorithms.
This work focused on the selection of command-to-gesture mappings and defining parameters for continuous interaction.
Real-world deployment of a foot-based gesture system would first require consideration of context of use and social acceptability.
Context of use refers to the user's posture and movement during interaction.
For experimental accuracy and reproducibility we only considered stationary users.
A real world deployment would need to employ activity recognition algorithms to differentiate walking from phone-interaction gestures.
Consideration must also be given to the social acceptability of kick-based gestures in various contexts.
It is likely these would be unacceptable in some environments and so alternatives must be provided.
Hancock, M.S., Vernier, F., Wigdor, D., Carpendale, S., Chen, S. Rotation and Translation Mechanisms for Tabletop Interaction.
Iso, T., and Yamazaki, K. Gait analyzer based on a cellphone with a single three-axis accelerometer.
Protractor3D: A Closed-form Solution to Rotation-invariant 3D Gestures.
This paper examined the mapping of foot-based gestures to mobile device commands and deeply probed continuous interaction gestures.
A guessability study derived a mapping for common commands that could be divided into continuous and discrete interaction.
We validated that these gestures could be recognized.
Through a series of three user studies we examined the most appropriate mechanism to support continuous interaction.
Our results showed that users are faster, more accurate and prefer rate-based techniques over displacement based techniques.
We believe these results provide a solid foundation for future investigations into the design of foot-gesture interaction techniques for novel mobile applications.
