This paper studies how users perceive their own performance in two alternative user interfaces.
We extend methodology from psychophysics to the study of interactive performance and conduct two experiments in order to create a model of users' perception of their own performance.
In our studies, two interfaces are sequentially used in a pointing task, and users are asked to rate in which interface their performance was higher.
We first differentiate the effects of objective performance  versus interface qualities  on perceived performance.
We then derive a model that predicts the amount of change required in an interface for users to reliably detect a difference.
The model is useful as a heuristic for predicting if a new interface design is better enough for users to reliably appreciate the obtained gain in user performance.
We validate the model via a separate user study, and conclude by discussing how to apply our findings to design problems.
Present-day computer users are bombarded with possibilities to upgrade, modify, and switch software and hardware.
Such changes trigger them to exercise judgment on the interface being presented to them.
This could encompass the study of their perceived utility, perceived usability, or perceived user performance.
This paper focuses on the perception of performance.
Despite the obvious importance of this topic to humancomputer interaction , the question of how users perceive changes in interactive performance has not received serious attention.
For example, interface designers could focus on usability features where noticeable differences can be gained and marketers could focus advertisements of a new interface on aspects of an interface that users will most likely recognize as improvements.
Theories and designers' conceptions of user experience  make the point that it is not objective performance that matters but the user's experience and perception of it.
However, the link between the perception of user performance and user interface design is not well understood.
This paper builds on psychophysics methods to study users' perception of which of two interfaces yields better performance for the same HCI task.
In computer science, psychophysical models are used to optimize computer graphics , image processing , haptics , audio , and video .
In HCI, some psychophysics laws have gained recognition as design guidelines .
However, research has been limited to time perception, with applications to system response times and progress bars .
Our goal is to extend psychophysical methodology to interactive tasks.
One challenge is that user performance is not passive perception--it involves active engagement of the user over a longer period of time.
Another challenge is that the standard psychophysics experiments typically consider relationships between two variables .
However, user performance cannot be reduced to a single quality: users can base their judgment on two main categories of cues: perceived qualities of the interface and perceived qualities of the user's own sensorimotor performance.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Examples of the latter are the speed and number of errors when performing a task, which constitute human performance more generally .
Complicating this further is the fact that such variables are necessarily interrelated.
For example, a new design that enlarges the sizes of the targets, also affects the time and accuracy of users' target selections.
For these reasons, the standard psychophysical methodology has not been readily applicable to interactive tasks.
The method we introduce in this paper includes an aimed movement task consecutively carried out with two alternative designs: the standard , and the alternative .
A participant completes a tapping task with one design , then completes the same task with another design , and is finally asked to rate in which his or her performance was better .
This emulates the use of two interfaces that are compared retrospectively.
We regard perception of user performance as a second-order representation that is most likely constructed based on other, more directly experienced aspects of interaction.
We hypothesize the most important to be: speed and accuracy of aimed movements and visual qualities of the interface, such as the distance to and the width of the target.
The key to our experimental design is that we manipulate index of difficulty   to exert a predictable effect on user performance.
By manipulating ID , and thus the difficulty of sensorimotor responses, we emulate the fact that the alternative design changes the user performance only indirectly: by affecting the demands placed on sensorimotor control and by changing the perception of the layout.
ID is controlled in two ways in the two experiments reported here.
In Experiment 1, we keep the ID of the standard and the alternative design constant, but we change D  and W .
In Experiment 2, we use a staircasing method: the ID between the standard and the alternative interface is gradually increased until the user can reliably notice the difference in performance.
These two manipulations allow us to address two central research questions: Experiment 1: What are the criteria users use to judge that performance in one design is better than an alternative design?
Experiment 2: How large does a difference in user performance have to be for users to reliably notice it?
The two experiments map to two common decision contexts in interface design.
Experiment 1 addresses what we call a within-ID design.
In this case, the qualities of interface elements cannot be changed independently of each other.
Limited display space would pose such a scenario: the size of the elements cannot be changed independent of inter-element distance.
The second experiment addresses the between-ID case where the qualities of the new design can be changed independently.
For example, smartphones with two different screen sizes would have different average ID s.
The results from our experiments expose several previously unreported phenomena on the contribution of different factors and biases.
Based on the experimental data we develop a predictive model in the form of a mathematical function that can be used to estimate the percentage of users that will be able to identify a performance change between two interfaces with different ID s. Our model characterizes the probability of a user being able to reliably judge that there is a difference between two designs with different ID s. This model is validated via a separate task, a Whac-A-Mole type of game.
In summary, we contribute to the HCI literature by presenting a novel variation of a psychophysical method that we have adapted to interactive tasks.
This methodology can be generalized to other studies of user performance in other interactive tasks.
The presented model and obtained results are limited to target acquisition tasks.
While this is a common sub-task in HCI, for example in command selection and typing , it leaves room for future work on other task domains.
We conclude this paper with a discussion on how to use of this model and deploy the method to other task domains.
Psychophysics is the study of a human's perceived ability to distinguish a difference in physical stimuli and events.
The classic psychophysics model, the Weber-Fechner law, states that equal stimulus ratios produce equal subjective ratios .
The Just Noticeable Difference  is proportional to the size of the standard stimulus: JND = kS , where S is the size of the standard stimulus and k is a constant .
JND thresholds using the Weber-Fechner law and other models have been charted for perceptual events and for user interface qualities one can expect to be relevant: visual length and area, visual distance, visual velocity, visual flash rate, and duration .
Recently, cognitive load was found to affect time perception in an HCI task .
We are unaware of work in HCI addressing aspects of interactive tasks more broadly.
Evidence from psychology suggests that interactive performance can turn out to be special.
Studies in psychology have found time perception to be affected by the allocation of attention during the task and the structuring of the stimulus environment .
Both attention and the stimulus environment are affected by the interface.
More generally, it has been found that judgment tasks that involve action by the perceiver at times differ from passive tasks.
In such cases, JND functions are not always Weberian and complex interactions emerge.
Such tasks include the perception of motion as opposed to stationary stimuli  and visually-guided grasping .
Moreover, JND thresholds can change during an action.
For instance, a moving hand is less sensitive to external stimuli than a static hand .
Another complication is posed by multimodal perception.
Given this brief overview, we deduce two requirements for our experimental method.
First, the Weber-Fechner law, or any other known JND function, cannot be expected to apply.
Thus the experimental method should allow a function of any shape to emerge.
Knowing that aimed movements at different scales can be associated with different sensorimotor requirements3 , we sample the whole permissible range of ID s afforded by a large interactive surface .
In the two experiments, we study four base-ID conditions ranging from very small  to large but still comfortable targets .
This allows us to examine if the size of the standard ID affects JND thresholds similar to the Weber-Fechner law.
To measure perception we use a rating scale where two sequentially used interfaces are directly compared on a slider ranging from -100 to 0 to +100.
Participants are instructed to use the slider to express in which interface he or she experienced a "better performance".
Performance is explained as the combination of how quickly and how accurately they were able to do the task .
Although rating is a motor task, we do not expect this to affect the reported differences.
A measurement via a rating allows participants to use whatever criteria they deem reasonable to make the judgment.
Moreover, the scalar value allows them to express their level of certainty.
The known drawbacks of the rating scale are nonlinearity of responses, order effects, and scale non-uniformity .
We address these in our analysis by making no assumptions of linearity or uniformity, and in the experimental design by randomizing the order of interfaces.
Our research interest is exploratory and encompasses two goals: The first is to understand the factors that affect the perception of user performance and the second is to chart JND thresholds for user performance.
Within a range of commonly studied HCI tasks--such as navigation, command selection and search--we chose target acquisition  for two reasons.
First, visually controlled discrete aimed movements executed with the hand are prevalent in present-day HCI, and thus studies of target acquisition are relevant to a wide range of user interfaces.
Second, there exists a well-established predictive model that links interface characteristics with user performance for target acquisition.
TP = In Experiment 1, we hold ID constant and change D and W within an ID condition.
This means that MT should stay at the same level for the the two to-be-compared interfaces, although the interface qualities D and W change.
If judgments are based on TP , they should not favor either interface.
In Experiment 2, we manipulate ID with a so-called staircasing method .
We start from a minimum difference between two interfaces and subsequently increase the difference between the ID s  with a constant step size until participants are able to reliably tell that their performance has changed.
This exploits the prediction of Fitts' law that increasing ID increases the difference in MT as well .
Our first experiment investigated if perception of performance is affected by four variables that characterize user performance--perceivable interface qualities: the width W and the distance D between targets; and objective performance qualities: speed and accuracy.
The ID of the two to-be-compared interfaces was kept constant, but W and D were manipulated.
In order to examine if sensorimotor demands affect users' perception of performance, we investigated four different ID s.
We recruited 18 participants from a university campus.
Eight were female, ten were male.
Their sight was normal or corrected-to-normal, and they reported no motor or neural disorders.
The experiment was a within-subjects design with one independent variable, ID , with four levels: 1.2, 1.6, 2.0 and 2.4.
Sixty trials were carried out in each ID condition by each participant, each trial consisting of two interface designs.
Participants performed eight target acquisition tasks in each interface design per trial.
A trial consisted of two successive sub-trials followed by a rating task in which the participant rated which interface design  resulted in a better perceived performance.
Contributions of Time, Error, Distance, and Width on the perception of performance in Experiment 1.
Markers denote observed means for each bin, lines are regression models described in Table 1.
The x-axes show the differences between interfaces in Trial 1 and Trial 2, the y -axes show the ratings: positive values refer to a preference for Trial 2 , while negative values refer to a preference for Trial 1.
For each trial, D and W were randomly sampled from a uniform distribution covering all permissible combinations within the ID condition.
The minimum D was limited by the feasibility of the minimum width for the smallest ID , while the maximum D was based on the screen resolution.
ID conditions were balanced using a Latin square design.
The current target was highlighted with a green color.
The target areas were shaped as vertical columns, as in the original Fitts' unidimensional tapping task.
Participants were instructed to aim for the middle part of the column that contained the target.
However, the whole column was considered the target from the system's point of view.
If the participant missed the target but was still within the tolerance range defined by the column, the system accepted the touch.
Each task had two interfaces  followed by a judging task.
Each trial consisted of eight reciprocal target acquisition tasks for a particular interface.
Afterwards, the participant made a judgment on the performance of the interfaces by providing a rating on a continuous rating scale shown as a slider on the display .
The numbers "1" and "2" referred to the first and second interface with the corresponding label.
Participants were instructed to treat the slider as a continuous rating scale, where the mid point was 0% difference in the interfaces, and each extreme meant a 100% difference in favor of either interface 1 or 2.
We instructed participants to think of "performance" as the combination of how quickly and how accurately they were able to do the task.
The experiment was carried out on a Microsoft PixelSense, model Samsung SUR40.
The experimental software was developed in C# and used the Microsoft XNA framework and the Surface SDK.
The software was designed to register touches through the surface API.
A tolerance threshold was used in order to mitigate tracking errors that may occur on the apparatus for very fast movements.
The precision of the timing data was in the order of microseconds.
The dependent variable Rating was calculated from the position of the slider as positioned by the participant.
The left end of the slider was mapped to -100%, i.e.
The predictive variables were calculated as the difference between Trial 1 and 2.
Time was calculated as the difference in time taken to complete the tapping task in milliseconds, Error was calculated as the difference between the number of taps outside the target, Distance was calculated as the difference in the distance between the targets, and Width was calculated as the difference in the widths of the targets.
We here report general observations from the data and provide models for the effect of Time, Error, Distance, and Width.
Observations outside three standard deviations were removed.
Remaining ratings were grouped into bins per dependent variable, each bin containing roughly the same amount of observations.
Binning for Distance, Width and Time was done by allocating 8 bins covering the entire range for each ID condition.
The sizes of the bins increases with each ID as higher IDs span over greater ranges.
Within each ID, bins also increase in width for larger values.
For Error however, only the bins at either extreme contain more than one error value, all others contain a single error value.
In the case of ID = 1.2, participants exhibited a clear recency bias.
This may be due to the fact that perceiving a difference in D or W is relatively more difficult with larger and closer targets.
Error: Errors were not always taken into account when judging performance.
For ID s 1.6 and 2.0, participants tended to be biased towards the second interface regardless of the number of errors in each trial.
Again, ID = 1.2 is an outlier: participants tended to rate the first interface as providing higher performance even if they made more mistakes with it.
However, as Figure 2 shows, this effect was negligible.
Distance and width: Figure 2 shows that Distance and Width affected judging performance for ID s 1.6, 2.0 and 2.4 but, again, ID 1.2 did not provide reliable grounds for performance judgments.
In this case, the ratings were dominated by the recency bias.
This is because the ratings tended to be positive if any difference in D or W was present.
Overall, the average rating is close to zero, which is expected, because the ID was held constant and the order was randomized in both interfaces.
However, participants demonstrated a slight recency bias--a bias towards the second and more recent interface they used  under all ID conditions.
The bias was particularly pronounced in the lowest-ID condition.
Figure 2 shows the effects of Time, Error, Distance, and Width, respectively.
The markers represent the averaged observations per bin and the curves the fitted models listed in Table 1.
Ratings range from -100.00% to 100.00%, where negative values correspond to a perception of better performance in the first interface , and positive values to better performance in the second interface .
A rating of 0% expresses no noticeable difference.
The following observations were made on the effects of the variables on the perception of user performance: 1.
Time: Time had the strongest effect.
Figure 2 shows a strong correlation between the rating and Time.
The plot also reveals that the Time required for participants to adjust the rating was affected by the ID condition.
Table 1 lists the regression models that best explained the data.
Line plots for the models are shown in Figure 2, together with the observed points.
The regression models for D and W can explain 94-95% of the variance for ID s 1.6- 2.4.
Interestingly, these models for ID 1.2 can only explain 51% and 42% of the variance respectively.
To conclude, except for ID = 1.2, the observations reported in the previous subsection could be captured by psychometric functions.
We started comparisons from four base-ID conditions , increasing ID of the two to-be-compared interfaces gradually until the user could notice a difference with >90% reliability.
We recruited 16 participants from a university campus.
Five were female, 11 were male.
There sight was normal or corrected-to-normal, and they reported no motor or neural disorders.
None of the participants had participated in Experiment 1.
Unlike Experiment 1, in Experiment 2 participants were explicitly told that the ID of the two interfaces  were different.
This was necessary because the difference would become salient as the ID increased.
They were told that there were four different difficulty levels  and that promotion to the next level only occurred when they identified the interface that yielded the highest performance in more than 90% of the trials .
They were instructed to judge their performance using the same definition of performance provided in Experiment 1.
In the case they could not notice any difference, participants were instructed to provide their best guess and to avoid a 0% rating.
In the case of the last stair, a limit was set by the maximum ID allowed.
This limit was set because randomly sampling combinations of D and W for ID s higher than 3.2 provided too many distances that are far beyond the physical limits imposed by the resolution of the multitouch display used in the experiment.
If participants made too many target selection errors the system beeped and forced the same sub-stair to be repeated.
A participant was considered to judge correctly when he or she rated the sub-trial with the lower ID as providing higher performance, ultimately reducing judgment to a binary decision.
There are two key observations.
First, no stair showed a 100% completion rate; that is, not all participants were able to achieve a correct judgment above 90% before being pushed onto the next stair.
Second, completion rate increased through the stairs.
Figure 4 shows mean Judgment-Reliability per sub-stair for each of the base-ID s. Only sub-stairs including at least 2/3 of the participants are included in the figure, to ensure representativeness of the whole user group.
We was calculated as 4.133 multiplied by the standard deviation of all total widths for the condition , where a total width was defined as the distance from the center of the target to the x-coordinate where the user touched.
If Interface A is able to provide TP a bits/s and Interface B TP b bits/s, how likely is that difference noticeable?
We chose the 91 percent cut-off as it corresponds to the stair advance rule used by our method.
However, because we did not manipulate throughput  but ID in our experiment, we have a narrower range of TP -differences in the data.
This analysis should therefore be regarded as tentative.
The main observation is that smaller TP differences are required for larger base-TP s. In other words, when user performance is high, only a small difference is required.
However, when it is very high , TP does not predict the reliability of judgments.
Instead, users use some other criteria.
In contrast, for low-TP conditions, users require a relatively large  difference.
Comparing the curve to our analysis of ID , we conclude that ID is a more powerful predictor for this task.
Figure 5 shows the average ID by minimum JudgmentReliability.
Figure 5 confirms the already mentioned trend that the reliability increased as participants progressed "up" the sub-stairs.
Moreover, it also shows that the intuition that can be gained from the histogram in Figure 3 of approximately six sub-stairs being necessary for most participants  to reliably notice a difference holds.
Base-ID = 1.2 is the exception, for reasons observed in Experiment 1: for this base-ID , participants struggled to reliably judge differences due to the small changes in Distance and Width.
To learn about individual differences, we split participants into two groups according to the median of JudgmentReliability.
For stair-IDs 1.2 and 1.6 these participants judged correctly when the Time difference was above 500 ms.
This practice decreased in the higher stairs as the increase in distance reduced the capacity to estimate Time.
Participants instead chose shorter Distances and smaller Widths  when being unable to estimate Time.
In contrast, the worse judgers do not reveal an obvious behavioral pattern.
Participants tended to take longer to complete the pointing task on later stairs.
Figure 6 shows the average Time in milliseconds for each sub-stair that still contains at least 2/3 of the participants.
The plot shows that Time remains fairly constant for the first two stairs, but it increases steadily on the last two stairs, revealing the increased difficulty in tapping the targets.
In other words, there is a linear component as presumed by Fitts' law, and some curvature at the extremes due to changes in sensorimotor demands.
We found such a model in the form of a variant of the logistic function.
This model, referred to as the generalized model from now on, predicts the probability p that a user will judge his or her performance as different between two alternative interfaces separated by an ID step-size of x is:
Based on our data we estimate a = 8, b = 4.
The generalized model is shown with a dashed line in Figure 8 .
We evaluated this model against the actual judging data we collected in Experiment 2 and found that it had good fit for the actual observed frequencies of users being able to reliably judge their performance as being noticeably better for a given ID step-size.
Having identified the generalized model from data collected in Experiment 2, we validated its predictive power on an alternative task outside of the experimental framework that shaped its functional form.
For this purpose we created a variant of the Whac-A-Mole game for the Microsoft PixelSense, model Samsung SUR40.
In the game, the user has to hit two sets of five circular targets  in a predetermined order as quickly as possible.
Thereafter the user chooses which of the trials resulted in a higher performance.
The distance to the target was then fully determined by the fixed ID and the width of the target.
The configuration of the five targets was randomly generated within the constraints given above.
In the game, when the user hit the "Start" button a pre-determined circular target was highlighted.
The user was instructed to hit this target as quickly as possible.
Thereafter another circular target was highlighted.
When the user had hit all the highlighted targets the game was over.
The highlighted sequence guided the user through a series of target acquisitions that all had the same ID for an individual game, but all the targets had randomly generated widths and heights within the constraints given above.
We recruited 11 participants from a university campus.
Five were female, six were male.
None of the participants had participated in Experiment 1 or 2.
Using the same definition of performance as in Experiment 1 and 2, participants were asked after every two games to choose the game in which they perceived they experienced a better performance.
To make the results of our validation exercise stronger we took two measures to reduce the possibility of false-positives.
Second, we selected a single base-ID of 0.9.
This choice was made based on the observations from Experiment 2, where we noticed that participants had greater difficulty in recognizing differences in low-ID conditions.
Coincidentally, this makes the prediction task of our generalized model more challenging as the chosen base-ID for the game is lower than the lowest base-ID of 1.2, which had the worst R2 fit in Experiment 2.
In total, each user played 1 base-ID x 9 ID s x 3 pairs of games per ID comparison = 54 games .
It took a user circa 7-8 minutes to play all 54 games, which mimics a typical walk-up use-scenario for a tabletop interface.
We found that the judgments about the ID differences in the game had an R2 goodness of fit of 0.72.
This should be compared against the closest comparable base-ID we used in Experiment 2  and its R2 model fit of 0.71, which was obtained in a much more controlled setup with less noisy data and more participants.
Our good model fit for the game data indicates that our generalized model of perceivable performance gain does generalize beyond the experimental task that shaped its functional form and model parameters.
These differences are needed to improve the reliability of users' judgments.
The model was validated in a Whac-A-Mole game, which demonstrated that the model's predictions generalized to at least one other HCI task involving discrete aimed movement.
However, we acknowledge that there could be other effects influencing the user, which he or she could not perceive, especially over a prolonged period of time.
For example, when the visual layout is more complex, users will have more cues than distance and width of targets that they can use to assess the user interface.
However, we believe that the results in this paper are promising enough to motivate further research in this direction.
Psychophysics models are of paramount importance in graphics, audio, and multimedia, thanks to their ability to directly inform design and engineering.
Psychophysics could be a useful asset in the HCI toolbox, too, if it allows us to make reliable quantitative predictions about the effect of a change in an interface on users' perception of interaction.
This paper has extended the application of psychophysics in HCI from passive perception to interaction.
To this end, we have presented a method for operationalizing another important psychophysical dependent variable in HCI: a user's perception of his or her own performance in a user interface.
By definition, user performance is the efficiency of a user carrying out a task , which in HCI is measured in terms of speed and accuracy aggregated over acts exhibited in the course of a task.
We have presented an adaptation of psychophysics methodology into an interactive task.
A central insight is that a change in interface design assumes its potential effect via two interconnected routes: via overt perceivable changes and via changes in a user's objective performance.
Changing something in a layout may be noticeable via perception of the elements and/or via how it changes the speed and accuracy of a user's performance.
We registered both interface qualities and objective user performance to predict the user's judgment.
A drawback in comparison to more realistic tasks is that the two interfaces in our experiments are used immediately after one another, whereas in actual use they are probably separated more in time.
The results in this paper provide insights into users' perception of interaction.
In Experiment 1, we found that: * Users can indeed reliably judge their performance, but they also exhibit slight biases.
Low-ID conditions were markedly different from others: interfaces consisting of large targets close to each other provide no reliable ground for judging one's performance.
In Experiment 2, a staircasing design enabled us to learn what happens when ID changes between two to-be-compared interfaces.
We learned that, depending on the base-ID , users' judgment capabilities change.
In particular: * For small base-ID s, users struggle to consistently identify differences.
Currently, the models derived in this paper can be considered heuristics for interfaces where pointing is important.
The regression models in Table 1 require knowing the ID of the two user interfaces to be compared, and the difference that is most pronounced between them: this can be the user's speed, accuracy, target distances, or target widths.
A model should be selected based on this information.
The regressions models in Table 2 require knowing the ID s of the two user interfaces that are being compared.
The lower of these should match or must be matched to the closest baseID in the table.
The output of the model is an estimation of the probability that a user will be able to reliably notice the difference in user performance.
It is important to note that the regression models in Table 2 are likely to suffer from overfitting.
For general design problems we recommend using the generalized model in Equation 3.
The methodology presented in this paper is a starting point for understanding the perception of user performance more fully.
With little effort, the paradigm could be extended to continuous aimed movements , allowing the study of steering and pursuit tasks common in for example gaming and driving.
The methodology can also be extended to tasks involving timing and rhythm, such as music .
To extend it beyond aimed movements, the challenge is to find models that allow manipulating user performance with predictable effects.
This research was supported by the Scottish Informatics and Computer Science Alliance, the Max Planck Centre for Visual Computing and Communication and the Cluster of Excellence for Multimodal Computing and Interaction at Saarland University.
Max Nicosia's undergraduate internship in St Andrews was supported by a donation from Nokia.
