In this paper, we report our experience deploying the MonoTrans Widgets system in a public setting.
Our work follows a line of crowd-sourced monolingual translation systems, and it is the first attempt to deploy such a system "in the wild".
The results are promising, but we also found out that drawing from two crowds with different expertise poses unique problems in the design of such crowdsourcing systems.
The MonoTrans2 UI was designed to show all possible tasks for a collection of sentences .
While this provides users with ample context and the freedom to choose among the available tasks, understanding, selecting and then performing those tasks became so complicated that it was unrealistic to expect significant engagement with casual users.
In our previous experiments, even recruited users who were fully committed to using the MonoTrans2 UI expressed confusion over this task model.
This high entry barrier for casual users became an even more significant problem when MonoTrans2 was built as a standalone website without an existing user base.
We have addressed the task complexity and the user population problems with our new MonoTrans Widgets design .
To address the task complexity problem, we simplified the MonoTrans2 system into widgets, small, embedded web pages with a single, short task.
To further alleviate the user population problem, we chose to draw from an existing, stable user base that we have access to, the users of the International Children's Digital Library .
The MonoTrans Widgets system has a goal directly related to the ICDL users: translating children's books, so the books can be viewed in more languages on ICDL, and this goal gives the ICDL users a strong motivation to help.
However, the widget approach has a price.
Crowd-sourced monolingual translation  is a method to obtain translation without bilingual translators, but instead via the collaboration of two crowds of monolingual people coupled by machine translation systems.
Our previous experiments with crowd-sourced monolingual translation have shown that significant quality improvement over machine translation alone is possible .
However, no such system has been deployed to large crowds of users in everyday use.
Encouraged by our initial success, we take the monolingual system a step further and deploy it "in the wild".
By doing so, we hope to identify the real-world challenges to building a crowd-sourced monolingual translation system - or, more broadly, a crowd-sourcing system that draws expertise from multiple different crowds.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The ICDL user population is unevenly distributed, with a majority of English speakers, adding a further challenge.
In this paper, we present our experience deploying MonoTrans Widgets "in the wild", including quantitative results about translation quality.
We also discuss general design lessons from this process.
Because crowd-sourced monolingual translation draws from multiple crowds with different language skills, our lessons may be especially useful for designing systems that organize collaboration among crowds with varying expertise.
Within a task, minimal context is provided.
Users can optionally see the previous and the next sentences.
They cannot see background images in picture books as in MonoTrans2.
In this case, the system  chooses the task.
Doing this right turns out to be a surprisingly subtle problem.
This is because the system simultaneously organizes multiple crowds  that participate in multiple book translations , and that language distribution is very uneven among current ICDL users.
It is further complicated because the system does not require logins  and needs to be efficient since many tasks are performed .
There are two steps in the task assignment algorithm: task type selection and sentence prioritization.
When a new user  starts using MonoTrans Widgets, the initial task type is selected from a predefined random distribution.
The user is then given tasks of the same type, with a probability to be given a different type after each task.
Once the task type is selected, the system chooses a sentence for the users to perform the task on.
Sentence prioritization is independent of task type.
This is a crucial adjustment to the multiplecrowd-multiple-language issue described above.
The highest priority sentence translating from or to the user's language is assigned to every first-time user.
After that, sentences that follow within the same book are assigned in sequence until the user has seen the last sentence of the book.
Then the newly prioritized sentences are assigned in the same way.
The widget approach, or embedding a small task into users' web browsing experience, is not new.
Anyone who has an online account may have encountered reCAPTCHA , and thus contributed to the crowd-sourced OCR project.
Similarly, Google Translate offers a mechanism to let users modify or rate the translation .
Providing users with short, self-contained tasks  that encourage quick completion is one of the most adopted crowd-sourcing approaches, because fine task granularity is crucial to solicit answers from a large crowd .
Micro-tasks are capable of supporting complex tasks, as shown by various designs .
In particular, bilingual translation can be done via Mechanical Turk.
However, since MonoTrans breaks down the task of translation further between two crowds and into multiple steps, the effectiveness of micro-tasks still needs to be studied.
The concept and first prototypes of crowd-sourced monolingual translation were proposed in the Language Grid system  and MonoTrans .
The MonoTrans Widgets system described in this paper is the latest member of the MonoTrans system family , whose members all implement similar iterative protocols.
The same as its ancestor MonoTran2, the MonoTrans Widgets system implements an asynchronous iterative protocol in which the source and the target language speakers take edit or attach extra information to the translation together .
However, MonoTrans Widgets cannot provide users with nearly as rich context as MonoTrans2 does.
MonoTrans Widgets support the same types of tasks as MonoTrans2 , with each one tailored into a customized widget.
Here we use the operational definition that a sentence is finished being translated if  there have been at least two rounds of back-and-forth between the target and the source language speakers, and  the translation candidate with the most rounds has been voted for at least three times.
Notice that this definition pertains the order in which sentences are worked on, and does not affect translation quality per se.
The widget as shown on ICDL web page link on the top of the page with the text "Help Translate Books without Speaking Both Languages".
When users click on the link, a widget is displayed as an embedded frame with instructions and a task .
Users can also switch to other languages within the widget.
Deploying on ICDL brings the MonoTrans Widgets about 1,000 daily visitors.
This user population is very different from the participants in our previous experiments because they are not hired or directly recruited, and in general they do not routinely take part in translation of children's books.2 In the first 21 days after deployment, 27,858 users visited the MonoTrans Widgets, and there were 6,358 widget task submissions.
Accuracy distribution of edited sentences with two bilingual evaluators  evaluation, the evaluators were not aware of how translations were done, and the sentences were presented to them randomly.
For each output  paired with its corresponding source sentence, the evaluator's task was to rate the translation's fluency and accuracy on a 5-point scale, where fluency of 5 indicates complete fluency and accuracy of 5 indicates complete preservation of meaning .
The evaluation results are shown in figures 3 and 4.
A pairwise t-test was run between scores given by the evaluators to corresponding translations by the two systems.
All the evaluators rated the MonoTrans Widgets translation statistically significantly higher quality than the Google Translate translation.
Among the 10 children's book translations being translated through the MonoTrans Widgets, we selected one English book  and one Spanish book  to conduct an evaluation on translation quality.
The English book contains 30 sentences, and the Spanish book contains 24 sentences.
These books are intended for 6-9 year olds.
We chose Spanish and English for this study for rapid experimental turnaround, based on ICDL's user population.
Both books were translated from the language in which they were originally published.
The initial machine translation  was done using the Google Translate Research API .
The books were deployed in the MonoTrans Widgets system for 14 days , during which there were 3,678 submissions  from 739 IP addresses.
On average, each sentence completed 1.1 round-trips between the English speakers and the Spanish speakers.
For each submission, the average time spent was 126 seconds.
Independent to the MonoTrans Widgets system, two native bilingual evaluators were recruited to assess translation quality for fully automatic output of Google Translate  and for output of MonoTrans Widgets .
For this paper's purpose, we designed the widgets to only show tasks in one language.
This design guaranteed users to be effectively monolingual.
Nevertheless, deploying to a specific user population did help the MonoTrans Widgets avoid some quality control issues.
For example, there was very little spam or irrelevant user input.
This will need to be taken into account when deploying to other user populations.
During the deployment of MonoTrans Widgets, we learned some important design lessons, which we believe can be helpful to designers of other crowd-sourcing systems.
Favor the smallest crowd: In a crowd-sourcing system that involves multiple crowds, task assignment should favor the smallest crowd, because it is often the bottleneck of throughput.
Early in the deployment, we observed a disproportionately low throughput for German-Spanish tasks.
The reason turned out not to be the German or the Spanish speakers, but the English speakers: On ICDL, English speakers are the majority, followed by the Spanish speakers, and the German-speaking population is very small .
Initially, our system did not prioritize tasks by speaker population, and since Spanish speakers were overwhelmed by English-Spanish tasks that the English speakers were performing, no Spanish speaker was available for any Spanish-German task.
The lesson here is that since there are always "more than enough" English speakers and not enough German speakers, some Spanish speakers should be allocated to collaborate with the German speakers first.
Prepare for scanning: In a system where users quickly browse some tasks before committing to finishing one, task viewing should have low overhead.
We observed that there is a roughly 2:1 skipping/submitting ratio with the MonoTrans Widgets.3 For every task viewed, the system needs to perform task assignment .
We optimized this process by pre-calculating and caching sentence priority scores, and this allowed quicker scanning performance.
Context versus complexity: More context can usually help users understand the task, but it also requires more screen space, and more reading on the users' side.
In our case, MonoTrans widgets' ability to obtain significant improvement over machine translation implies that it is possible to deploy with little task context4.
In this paper, we presented our study of deploying MonoTrans Widgets "in the wild".
By introducing microtasks, MonoTrans Widgets were able to be deployed to the ICDL web site, and to be used by its many daily visitors.
A comparison to machine translation showed that the MonoTrans Widgets can obtain significantly improved quality with little context provided to the users.
We also discussed design lessons that may be valuable to other crowd-sourcing system designers in general.
1. von Ahn, L., Maurer, B., McMillen, C., Abraham, D., and Blum, M. reCAPTCHA: Human-Based Character Recognition via Web Security Measures.
A hands-on study of the reliability and coherence of evaluation metrics.
Hu, C., Bederson, B.B., and Resnik, P. Translation by iterative collaboration between monolingual users.
Law, E. and von Ahn, L. Human Computation.
Exploring iterative and parallel human computation processes.
Morita, D. and Ishida, T. Designing Protocols for Collaborative Translation.
In Principles of Practice in Multi-Agent Systems.
Zaidan, O. and Callison-Burch, C. Crowdsourcing Translation: Professional Quality from NonProfessionals.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics , 1220-1229.
University Research Program for Google Translate Google Research.
