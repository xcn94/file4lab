Numerous robots have been developed, and some of them are already being used in homes, institutions, and workplaces.
Despite the development of useful robot functions, the focus so far has not been on user interfaces of robots.
General users of robots find it hard to understand what the robots are doing and what kind of work they can do.
This paper presents an interface for the commanding home robots by using stroke gestures on a computer screen.
This interface allows the user to control robots and design their behaviors by sketching the robot's behaviors and actions on a top-down view from ceiling cameras.
To convey a feeling of directly controlling the robots, our interface employs the live camera view.
In this study, we focused on a house-cleaning task that is typical of home robots, and developed a sketch interface for designing behaviors of vacuuming robots.
However, the old-time dream of having a housekeeping robot at home has not really become a reality.
The main reasons may be cost and hardware capabilities, but we believe that the lack of vision on how to "control" advanced robots is also a significant limiting factor.
One may think that simple vacuuming does not require much control, but in reality, individual users have their own needs and usually want to specify when, where, and how to clean, which is not really well supported by the current systems.
Furthermore, if we have an advanced humanoid robot at home, the problem of controlling it is much more serious.
Typical demonstrations of these robots assume almost autonomous behaviors, but perfect autonomy is difficult because users have their own preferences and needs, which are hard to estimate beforehand.
The above observation led to our efforts to explore better user interfaces to control robots.
Significant progress has been made in robotics technology in recent years.
In research environments, it is already common to see humanoid robots walking around with biped legs , and some of them are capable of using tools designed for humans such as knives and brooms .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Natural languages are a popular option.
However, in many cases, a graphical representation is much more direct and intuitive for communicating spatial information.
Therefore, we examine the possibility of using a graphical representation to control robots in this work.
This paper reports on our initial experiment to build a specialized graphical interface for giving spatial information and controlling home robots.
The main idea is to use sketching on a hand-held computer screen showing a top-down camera view as the user interface .
We chose this configuration for several reasons.
First, sketching on a top-down view can directly provide positional information .
Second, sketching naturally supports asynchronous control because sketching implies an instruction staying in the environment and executed late.
Third, this configuration frees the user from tedious calibration or setup because the user-interaction space  is identical to the robot-controlling space.
Currently, these controllers are expensive.
However, this type of controller is getting cheaper.
If the remote controller for home electronics goods had more power of expression and higher functionality, we could add an interface to it to control home robots.
We believe that this is a natural evolution of remote controllers for home electronics.
We believe that an intuitive user interface for robot control would extend the potential of home robots and the knowledge of graphical user interfaces  is naturally applicable to control robots.
There are a couple of attempts to design intuitive interfaces for robots in the real world were conducted.
Kemp et al proposed an interaction technique to designate objects with a green laser pointer .
However, we consider that the GUI technique is more familiar to general users.
It is easy to give feedback to users.
Among existing GUI techniques, we are especially interested in pen-stroke gestures.
To analyze input strokes, there is a well-known algorithm  that estimates specific paths based on statistics and learning.
Arvo and Novins proposed a smoothing algorithm for ambiguous hand-drawn shapes .
In a recent study, Wobbrock et al.
We adopt this algorithm for the detection of halt and resume gestures.
As mentioned above, stroke-based interfaces are relatively common, and their potential has been demonstrated in a variety of applications.
However, their work focused on the reasoning of handwritten sketches.
We focus on the design of a user interface using a ceiling-mounted live camera view.
We consider that this approach is also applicable to robot controlling to enhance controllability.
Various types of sketching interfaces have been proposed in the field of human-computer interaction.
In computer graphics, Igarashi et al.
On the other hand, in the field of human-robot interaction, some research has been done on the teleoperation of various robots, for example, army robots , rescue robots , and humanoid robots .
The interface of the teleoperation systems for these robots were proposed and evaluated in these studies.
However, these robots are not intended for general users, and it would be hard to use these interfaces in a home environment because they require high computational power.
We consider that the user interface is the most important element for home robots.
At home, the main users are who are not necessary technology oriented people such as children, elderly people and housewives.
The usability of the interface must be high.
Recently, some remote-control devices for home electronics products have been developed and marketed with a touch panel display.
The system receives gesture commands from the user and controls the robot in the real world.
Users draw freeform strokes on the computer screen to control the robot.
Then the system conducts a detection and recognition process of the sketch.
If a detected sketch is a command, this system executes the task by driving the robots in the real world.
In the prototype system, we focused on a floor-cleaning task in a room with a robot system because this cleaning task is a realistic one for current home robots.
We describe the details of this interface below and show the system overview in Figure 2.
The Roomba by iRobot1 is a commercially available product .
We can control the robot by using an accessory remote control.
The Roomba is 92 mm high and 340 mm in diameter.
It has two degrees of freedom in the base for driving.
The robot can drive 500 mm per second at maximum.
A user program can be developed through an open serial control called Roomba Open Interface .
In our system, we control the Roomba from a computer that runs the sketching interface via the Bluetooth Serial Port Profile .
We believe that a top-down view, which is an actual view of a room from a ceiling camera, can give users a more realistic feeling of controlling the robot than a robotcentered view because the user can observe the robot's state in the actual room from a top-down view.
For this reason, we propose a behavior direction interface by using a topdown view of ceiling cameras.
This makes it possible to give users the feeling of sketching on the actual room.
A top-down view is also useful for detecting the location of robots and objects because it is not necessary to apply coordinate transformation.
We can reduce the error by using a vertical image.
A single ceiling camera can capture a 2.25 x 3  area.
However, this is too small to cover the working area.
Therefore, we combine images from four cameras into a single global image.
This system uses ARToolkit 2 to detect the locations of robots and objects .
ARToolkit detects objects by using a specified marker in a given image.
The marker is a squared image like a 2D barcode.
Note that ARToolkit is only used as an initial test bed.
It lacks speed and accuracy necessary for more advanced control.
With users able to command home robots to perform various tasks, it will also be necessary for the robots to have "stop" and "pause" functions.
We have developed four auxiliary commands for controlling robots.
These commands will appear as menu items on the screen when a lasso stroke is used to select, or encircle, the robot.
Furthermore, we adopt stroke recognition algorithm  for the detection of auxiliary commands gestures .
Resume - The robot will resume its movements.
Stop - This command will stop/cancel the robot's movements.
Go home - The robot will return to its base and charge its battery.
Once we draw a path on the interface, no other actions are required to get the robot to perform the task.
This means that when the robot starts following the given path, the user can work on something else.
This asynchronism is an important function of this interface.
Most robots that are already on the market have asynchronism.
Users want to reduce the amount of time spent on housework and increase their personal time.
Our interface makes it possible for the user to save their own time.
The speed of the robot was not very fast, at 100 mm per second on average.
This is because the ARToolkit that detects the location and orientation of the robot sometime made a mistake.
It detected the robot four or five times per second.
Furthermore, the top-down view was a combined image from four cameras.
When the robot is at image bounds, the ARToolkit was sometimes unable to detect the location of the robot reliably.
For these reasons, we reduced the robot's speed to be detected by ARToolkit stably.
We plan to use a 3D motion capturing system to allow the robot to move faster and with sufficient stability in the next system.
We proposed an interface for controlling robots using stroke gestures.
This interface allows users to control robots intuitively.
For example, the user can move the robot by drawing a path, initiate vacuuming by drawing a circle, and select the robot to execute specified commands.
The test users successfully controlled  robots using this interface.
The stroke gesture interface provides the user with a novel experience to control a robot in home environment.
We used a vacuuming robot in this paper.
However, we are not focusing only on vacuuming robots.
We believe that this interface can be useful for other home robots.
We want to use this interface with robots for delivering objects, picking objects up, and throwing objects into trash bins at home.
In these cases, we need to develop other robots that execute the specified tasks.
