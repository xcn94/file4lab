Biological research in the field is constrained by the speed and difficulty of species determination, as well as by access to relevant information about the species encountered.
However, recent work on vision-based algorithms raises the promise of rapid botanical species identification.
The potential for mobile vision-based identification provides opportunities for new user interface techniques.
To explore these issues, we present LeafView, a Tablet-PC-based user interface for an electronic field guide that supports automated identification of botanical species in the field.
We describe a user interface design based on an ethnographic study of botanists, field tests of working prototypes by botanists at the Smithsonian Institution on Plummers Island, Maryland, and observations at an internal exhibition at the Smithsonian at which other staff members tried the prototypes.
We present functionality specific to mobile identification and collection in the electronic field guide and use this to motivate discussion of mobile identification in general.
Our global environment faces increasing pressure as the human population grows and more resources are consumed.
When species and habitats disappear in response, we lose opportunities to understand biological complexity.
New technologies and methods are required that enable rapid identification of species, access to biodiversity information, and construction of ecoinformatic knowledge.
Of particular importance is the need to identify specimens in the field and associate them with an existing species or potential new species.
In this process, data must be collected for review, comparison, and later use.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Columbia University, University of Maryland, and the Smithsonian Institution are working together to develop electronic field guides for botanists that support visionbased automated species identification .
The goal of this overarching project is to develop new tools and radically improve contemporary practice.
Currently, identification starts with paper field guides and botanists' own specialized personal knowledge.
This involves inspection of multiple characteristics, such as plant structure and leaf venation , and comparison of those characteristics with field guide content.
To confirm the identification, botanists must compare the sample with a canonical specimen of the species, called a voucher.
Vouchers are stored in academic or institutional herbaria and inspecting them requires travel to herbaria or shipment of the vouchers to the remote locale for verification.
For each identification task, this process is time consuming and requires the movement of unique and fragile voucher specimens, which may be lost or destroyed during transfer.
Thus, botanical research is constrained by the identification task in the field and availability and access to botanical data.
In previous work , we described an ethnographic study of the collection and identification process as part of the design of prototype augmented reality user interfaces to species identification.
We developed a task model that includes collection of sample species, finding a set of possible matches, inspection and comparison, and selection of the best match as part of the collection process.
This data includes additional imagery of the whole plant and root systems, location and date of acquisition, name of the collector and of the identifier, regional information, articles about the specimen, and links to related specimens.
The term more generally describes a holistic virtual representation of an object in the physical world.
In addition to these augmented reality prototypes using seethrough head-worn displays, we have been developing user interfaces based on the same conceptual model that are designed for stand-alone Tablet PCs, web browsers, and cellular phones.
Our contribution focuses on interaction and use specific to automated identification in the field.
Issues we address include inspection and comparison, immediate and batch processing, feedback from the identification process  as first steps to interacting with a vision algorithm, and the incorporation of identification in the collection process in contrast with post hoc identification.
Interfaces involving mobile recognition and identification have been explored in the context of augmented reality and mobile phones.
For example, the OSGAR augmented reality toolkit  takes into account error and uncertainty in tracking using vision and other approaches, and represents this to the user.
The DoubleShot interactive image segmentation tool  allows users to assist a computervision-based object recognition system by taking multiple photographs of a scene before and after moving an object, and guides the user in setting up the shots.
Prior research has used mobile devices for recording data in field work, such as the PDA-based FieldNote  system, which emphasized context-aware data collection and minimal attention user interfaces.
A similar system, CyberTracker  is a PDA-based system used in a variety of field projects for tracking animals.
In both cases, these systems are used to record observations made by the user.
Cyberguide provided mobile context-aware information for use in a tour guide .
More recently, ButterflyNet  has relied on a paper notebook with an Anoto digital pen, in conjunction with a small computer and camera to capture time-stamped and barcoded images of specimens, thereby associating field notes with collected biological specimens.
While we are inspired by this work, we focus, in contrast, on a mobile user interface to an electronic field guide that supports automated species identification combined with collection and information access.
A thumbnail of the sample is also placed in the history tab and all contextual data about the sample, such as GPS location, collector, and time/date, are stored in a database.
The search component uses the inner distance shape context  algorithm  to match plant species, and we provide hooks for integrating other algorithms.
Human interaction is required because the vision algorithms are not perfect.
The user can pan and zoom to inspect individual virtual vouchers and compare them with the plant sample.
Semantic zooming is accomplished by either tapping on a virtual voucher to zoom in a level and reveal sets of voucher images, identification information, and textual descriptions, or by dragging up or down for continuous zooming.
Once the identification has been verified by the botanist, a button press associates the identified species with the sample.
A zoomable history of samples can be browsed to recall prior samples and search results.
The software was developed using C#, MatLab, and Piccolo  for the zoomable user interface.
To gain insight into the use of the system in the field, four of our six botanist collaborators have been using the LeafView prototype on Plummers Island, a small island in the Potomac that has been extensively studied by naturalists since 1908.
We have joined them on trips to observe them at work, and received reports after independent usage.
Complementing the field tests on Plummers Island, two LeafView prototypes were available for attendees to try in an exhibit at the 2006 Smithsonian Institution staff picnic.
Buckets containing a large variety of plant samples collected on Plummers Island were positioned around the exhibit.
Attendees could pick a leaf, take a photo of it that was automatically submitted for identification, and explore the user interface.
While this was quite different from actual field experience, it gave us an opportunity to gather feedback from a wide variety of potential users, both professional and non-professional.
This provides immediate feedback regarding the quality of the segmentation.
For example, if a shadow causes poor matching, the botanist can observe this and retake the photograph to fix the error.
In observing botanists using the system on Plummers Island and at the exhibition, we found that providing feedback by displaying the segmented images enabled them to retake better pictures than those that originally produced bad matches.
As discussed by Ling and Jacobs , the correct species match is found in the top 10 results 98.5% of the time by the algorithms we are using.
However, some inspection and comparison is still required.
From our ethnographic study , we found that the inspection and comparison tasks often start at a high level, with general shape, and then focus in on distinguishing details, such as venation or edge serration.
Aspects not represented in the voucher images may also be examined, such as plant height.
As part of our design, we support comparing the original leaf with high-resolution species voucher images, which can be accessed through semantic zooming on any virtual voucher.
Additional information about the plant species and context are also maintained in the virtual voucher, but not shown until requested by the user, also through semantic zooming.
If uncertainty remains, we support the ability to associate a new sample with multiple matches and save the entire matching results.
We have also found it useful to provide access to the full set of species in the database.
When a botanist believes a plant species is present in the data set, but the plant is not matched, we make possible visual and textual browsing of the entire data set used for matching to give closure to questions regarding inclusion in the data set.
Our botanist colleagues verified that the prototype was effective, in place of a physical voucher, for examining detailed characteristics such as venation.
During our field studies of the LeafView prototype, we observed that botanists performed identification in two very different ways.
In the first approach, an image was taken and the system was immediately checked to see the results of the search.
The retrieved virtual vouchers were inspected and a match was chosen.
The botanist then went on to find another leaf to collect.
In the second approach, the botanist took a series of pictures and then used LeafView to review and match the images, in some cases also comparing multiple samples to see if they were the same species.
In the first approach, the prototype successfully fit into the botanists' preexisting collection patterns, as they had originally described and demonstrated to us during initial system design, and matched a large variety of plant specimens.
In contrast, the second approach was not predicted by either the botanists or us, and our earliest prototypes did not support it.
As one botanist put it, "the system gets very confused if you send too many ".
We considered this a design opportunity and changed the conceptual model and user interface based on observed use.
A queue and pipeline model was added to the interface and the history was changed to represent and support this.
Vision algorithms are often treated as black boxes that provide no feedback on success or failure modes.
Although some aspects of the species recognition algorithm currently used in LeafView do not directly correlate to visual representations, we can provide feedback on the segmentation of the leaf image.
The history display acts as both a queue and as an indication of the stage of progress for each leaf sample, supporting both individual and batch identification, as described above.
When a leaf is photographed, it immediately appears in the sample tab and is inserted into the history.
Once the IDSC algorithm and final matching has completed, the results are shown in the results tab and reflected in the history.
At each stage--from photo, to segmentation, to matching--the botanist can observe distinctions across images, so that poor quality results can be improved.
Images can also be deleted from the history if they are immediately observed to be problematic.
This also addresses an earlier comment by one of the botanists regarding their desire to see relationships across matches for a collection.
Over time, they wanted to "...display the name of the plant selected for a match.
That way the user would know what name was selected for something they saw earlier in the day..."
As a qualitative litmus test of our success thus far, we received numerous protests when we took the prototype back to upgrade the hardware and software.
In addition, one of our colleagues told us, "We have received very favorable comments and lots of questions as to when the software will be available to buy" after the exhibition from botanists and other scientists unaffiliated with the project.
As a next step, we will be taking the prototype to Barro Colorado Island, an ecological preserve in Panama, for further field tests and to explore integration with ecological data.
We plan to investigate other ways to interact with the vision algorithms as part of that research.
We are also developing web-based and cell-phone-based user interfaces for identification so that non-specialists have access to the same tools.
We hope that wider access will increase curiosity, understanding, and appreciation for the natural world.
Our initial design was primarily focused on identification.
While this was supported by the six botanists who are directly collaborating with us, some other botanists have reacted with some apprehension to the idea.
We discovered these reactions at the exhibition.
Of the many botanists who used the system with our guidance at the exhibition, three  initially had hesitant reactions on hearing that this was a "plant identification system."
However, they responded positively when they understood that it was a collection tool intended to assist rather than replace them in identification.
This difference in reaction appears to be due to the perception that a pure identification system is somehow replacing the botanist, while an intelligent collection system or electronic field guide maintains the locus of control with the botanist.
In a subsequent conversation with biomedical informaticist Ted Shortliffe , we learned that he had experienced similar responses from physicians with regard to automated diagnosis systems.
While this may not be an issue with non-experts, it is worth remembering when designing for and presenting to groups with sensitivity towards their own knowledge.
A relatively minor  change in system emphasis made a significant difference in perception of the system by potential users.
The LeafView UI is a small part of a large joint research project currently involving the efforts of Peter Belhumeur, David Jacobs, John Kress, Ravi Ramamoorthi, Gaurav Agarwal, Norm Bourg, Nandan Dixit, Charles Macanka, Ellen Farr, Mark Korthals, Haibin Ling, Ida Lopez, Rusty Russell, and Sameer Shirdhonkar.
We have presented LeafView and interaction techniques relevant to botanical species identification in the field, including recognition of the importance of interacting with vision algorithms through visualizing segmentation results ; identification of the need to support both individual and batch identification, and addressing it through queuing and pipelining; discussion of recognizing relationships of images across matches using history; and discussion of the importance of a potential user's perception of the system as complementing their expertise, not replacing it.
We believe these techniques and issues generalize to other domains in which automated identification may be used.
