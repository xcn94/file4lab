We show that large consensus exists among users in the way they articulate stroke gestures at various scales , and formulate a simple rule that estimates the user-intended scale of input gestures with 87% accuracy.
Our estimator can enhance current gestural interfaces by leveraging scale as a natural parameter for gesture input, reflective of user perception .
Gesture scale can simplify gesture set design, improve gestureto-function mappings, and reduce the need for users to learn and for recognizers to discriminate unnecessary symbols.
We investigate the use of gesture scale to enter discrete parameters for a command.
Our goal is to enrich the expressibility of gesture input, without increasing the physical or cognitive load of gesture articulation or the complexity of the recognizer.
We are motivated by the ability of people to write at different sizes  without affecting the form of the produced symbols .
In addition, gestures drawn at different scales are perceived by users as dissimilar , and, therefore, can have different meanings in an application.
In turn, exploiting gesture scale opens many opportunities to enhance today's gestural interfaces.
For example, a small "S" saves a file, while a large "S" could invoke "Save as".
For e-books, the scale of forward and backward flicks can specify the number of pages to go back or forward.
However, despite such simple and intuitive ways to map scale with command parameters, gesture scale has not been exploited yet in user interfaces.
This can be explained by today's lack of understanding on the relationship between user-intended scale  and the actual size of produced strokes , which can vary across users and input surfaces.
We show that large consensus exists among users articulating stroke gestures at various scales, which legitimates the search for an estimator of user-intended gesture scale.
Using Bayes' classification technique, we derive a simple, training-free estimation rule that can be directly implemented in today's gestural interfaces with just three lines of code.
We report 87% accuracy for user-independent and device-independent tests.
Stroke gestures are commonly employed as shortcuts to invoke commands on touch-sensitive devices for which keyboard shortcuts are either not available, or demand substantially more cognitive effort .
Once the gesture has been recognized , the command is fired away.
However, some commands need parameters to be specified .
One option is to enter such parameters after the gesture has been articulated , or to employ mode specifiers that set the meaning of the command in a particular context .
Another option is to map geometric or kinematic features of the articulation profile to parameter values, such as pen pressure for selecting menu items , or descriptors of oscillatory movements to control zoom and pan levels .
We designed an experiment to determine whether a relationship exists between the intended scale at which users produce gestures  and actual gesture size .
We are interested in whether the size of gestures produced for a given scale label is consistent across users, and whether user perception of gesture scale is influenced by the size of the available input area.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We make distinction between the scale of a gesture and its size.
Whereas we understand by scale a linguistic label denoting amplitude at a subjective level , size is the result of an objective measurement on the produced gesture shape .
Following previous work , we measure gesture size as the area of the gesture bounding box .
Participants were divided into five equal groups corresponding to each value of I NPUT-A REA -S IZE to prevent learning effects from one input size to another.
Each combination of G ESTURE and S IZE was repeated for five times and randomly presented to participants.
The experiment took around five minutes per participant.
In summary, the experimental design was: 5 I NPUT-A REA -S IZE x 8 participants x 4 G ESTURE x 3 S CALE x 5 repetitions = 2,400 total trials.
The display was positioned horizontally to approximate a physical pen and paper context.
Forty unpaid participants with ages between 19 to 24 years old, all right-handed, were recruited for the experiment.
None of them had previous experience with pen gesture input.
The task consisted in entering gestures at various scales, as guided by a software application that controlled the experiment.
Gestures were entered inside a square input area centered on screen.
The application displayed a text message before each trial indicating the gesture to articulate and the requested scale , without displaying any visual feedback of gesture shape.
A trial ended once the pen was lifted.
Participants were instructed to enter gestures at their regular speed and at the size they best felt it matched the instructions .
No additional indication was provided for gesture scale other than the one in the text message.
A practice session preceded the experiment in order for participants to become accustomed with the stylus and the interactive display.
The session consisted in gesture drawings in Microsoft R Paint under the supervision of the experimenter.
No scale indication was provided during this phase.
The first repetition was therefore removed from subsequent analysis.
It is important to note that no interaction was found between G ESTURE and the other factors.
Post-hoc analysis revealed significant differences  between the three S CALE levels for each I NPUT-A REA -S IZE.
This shows that participants' executions and their perception of scale were indeed influenced by the size of the available input area.
On average, medium gestures were 4.3 times larger than small ones, and large gestures were 3.0 times larger than medium ones.
Figure 2 shows gesture sizes for each I NPUTA REA -S IZE and S CALE, with a monotonic trend clearly identifiable.
A mixed-subjects design was used.
The independent variables were the size of the input area in which participants entered gestures , the scale at which participants were instructed to produce gestures , and the gesture type .
I NPUT-A REA -S IZE was administrated as a between-subjects factor while S CALE and G ES TURE were administrated as within-subjects factors.
These values were chosen to cover the input areas available on current mobile phones , and get close to the maximum size of the largest tablets .
S CALE was evaluated with three levels: small, medium, and large .
G ESTURE was evaluated with four levels: rectangle, diamond, circle, and triangle.
We preferred gestures with clear meaning to participants, considering they were indicated as text during the experiment .
In the following, we are interested in devising an automatic procedure that would reliably estimate the user-intended scale of input gestures by analyzing gesture measured size.
As gesture size was found to be affected by the size of the input area, we normalize S IZE by I NPUT-A REA -S IZE, and refer to the outcome as the S IZE -R ATIO of gesture g : S IZE -R ATIO = S IZE area of I NPUT-A REA -S IZE 
Figure 3 shows the distribution of S IZE -R ATIO for the 2,400 samples from our data set, together with normal distributions overimposed for each gesture S CALE.
When class distributions can be easily approximated with known functions , Bayes' rule is the preferred classifier choice as it minimizes classification error .
Bayes' rule uses class-conditional density probabilities , together with the a priori probability of each class , in order to compute the a posteriori probabilities for g to belong to each class.
In the end, Bayes' rule assigns gesture g to class S CALE for which the a posteriori probability p is maximized: arg max {p} = arg max {p*p} 
We also evaluated the generalization power of the classifier across different screen sizes .
This time, data from a single I NPUT-A REA -S IZE was used for testing and the classifier was trained with the remaining input areas.
The testing procedure was repeated for each value of I NPUT-A REA -S IZE and recognition rates were averaged.
Input area-independent estimation accuracy was 86.9%.
We evaluated the accuracy of the Bayes' classifier using the standard leave-one-out cross-validation technique that has the desirable property of being almost unbiased  .
Data from a single participant was used for testing while the classifier was trained employing data from the remaining participants.
Training consisted in computing the means  and standard deviations  for the normal distribution of each class.
We repeated this procedure for each participant and averaged recognition rates.
In total, we report results from 40  x 60  = 2,400 classification tests.
User-independent accuracy was 87.3% which confirms the high level of consensus between participants' articulations of gesture scale.
Bayes' rule reports the class corresponding to the maximum probability for a given value of S IZE -R ATIO, which points to the normal curve being "on top" in Figure 3.
This observation can be used to derive a much simple classification rule in which thresholds serve as class separators, while preserving the principle of maximum a posteriori probability.
The intersections of the three normal distributions are approximatively given by values 0.1 and 0.4.
We therefore introduce the following rule to estimate the user-intended scale of gesture g :  if S IZE -R ATIO  0.1 small S CALE = medium if S IZE -R ATIO   The simplified rule achieved 87.2% accuracy on the same data set and same leave-one-out user-independent testing methodology.
A Wilcoxon signed-rank test showed no significant difference between the accuracy of Bayes' classifier and that of the simplified rule  =-0.269, n.s..
Input area-independent testing also revealed 87.2% accuracy, not significantly different from that reported by Bayes' classifier.
The accuracy of this simple, easy-to-implement, and trainingfree rule is comparable to that delivered by today's common gesture recognizers, $1 and Protractor6 .
Although both recognizers report 99% accuracy with user-dependent training, this value drops to 91% for user-independent tests  .
Our rule achieves a comparable 87% accuracy, is training-free, user- and device-independent, and can be used in conjunction with any gesture recognizer.
We illustrate interactive scenarios for scale gestures and note interesting directions for future work.
Gesture scale can be exploited in many applications with semantically-close commands.
For example, a small flick can turn one page at a time, a medium flick five pages, while a large flick goes to the next chapter.
The same principle can be used to browse images, fastforward a movie, or browse contacts in the address list.
In a video game, a small gesture can fire a single shot and a large gesture several.
The gesture set can be reduced by reusing the same symbol with different meanings at different scales: small "S" saves the current document, medium "S" the project, and large "S" the entire solution; small "R" replies to the first contact, large "R" replies to everyone for an email client.
Text input can reuse the same symbols for lower and upper case letters: small "A" and medium "A" can output different cases, without the need for users and recognizers to memorize two different symbols .
This idea is similar to , which used pressure to enter mixed-case text , but extends it to devices with no pressure-sensing features.
Upper bounds for gesture size.
We note the interesting observation that the size of gestures entered for the 12 x 12 cm2 area was smaller than for 10 x 10 cm2 .
Although significant, this difference could suggest that the notions of small, medium, and large could remain roughly the same starting from some area size .
This result was expected due to findings from motor control theory showing dependency between writing speed and path length .
In turn, this advocates large gestures as practical shortcuts, as people compensate the extra path length with increased articulation speed.
Simultaneous use of different devices.
Our scale estimation rule is device-independent, but frequent transitions between devices with different form factors could affect the size of articulated gestures, as an effect of short-term muscle memory.
However, we hypothesize the effect is negligible due to handwriting adaptation with input area size .
Future work will investigate the existence of such cross-device effects.
Our rule is user-independent and trainingfree, which inevitably affects estimation accuracy .
User training will likely increase accuracy and we suggest this option for practitioners, should they absolutely need the extra accuracy for their specific gesture sets.
We considered three scales only as they are reflective of the perception of psychological space  and were used by gesture works before .
A partial, 2-level rule could discriminate between small and large gestures only.
However, the labels of a 4-level scale may not be that easy to find.
Such investigations are left as future work.
We align our work in the tradition of simple techniques with high potential impact, such as the 2-level pressure text input of Brewster and Hughes  or the 4-level pressure marks of Ramos and Balakrishnan .
We presented a simple rule that estimates the user-intended scale of stroke gestures with 87% accuracy.
Our estimator works with any gesture recognizer, is training-free, user- and device-independent, and can be immediately integrated into existing gestural interfaces with just three lines of code.
We hope this simple rule will be useful for practitioners prototyping gestural interfaces to leverage scale as a naturallyarticulated gesture parameter, reduce the size of application gesture sets, and reduce learning requirements for users.
