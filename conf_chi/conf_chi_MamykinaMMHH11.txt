This paper analyzes a Question & Answer site for programmers, Stack Overflow, that dramatically improves on the utility and performance of Q&A systems for technical domains.
Over 92% of Stack Overflow questions about expert topics are answered -- in a median time of 11 minutes.
Using a mixed methods approach that combines statistical data analysis with user interviews, we seek to understand this success.
We argue that it is not primarily due to an a priori superior technical design, but also to the high visibility and daily involvement of the design team within the community they serve.
This model of continued community leadership presents challenges to both CSCW systems research as well as to attempts to apply the Stack Overflow model to other specialized knowledge domains.
Within two years, SO has become one of the most visible venues for expert knowledge sharing around software development.
With approximately 300,000 registered users and >7 million monthly visits , SO has an answer rate above 90% and a median answer time of only 11 minutes.
The site has captured significant mindshare among software developers: anecdotally, users report that the site has replaced web search and forums as their primary resource for programming problems; others now consider their portfolio of SO answers a valuable component of their professional resumes.
This community "buzz" about SO's success prompted our investigation.
How might we understand the factors behind this success?
We first conducted a statistical data analysis of the entire SO corpus to understand usage patterns.
We investigated answer time, user types, suitability for different question types, and possible extensions of the SO model to other domains.
To ground this aggregate view in concrete user experiences, we also conducted a qualitative interview study with users and the design team.
The authors are not affiliated with the site.
This mixed method approach is shared with prior work ; interviews with site designers are, to our knowledge, novel in studies of Q&A sites.
Consistent with prior work, we found that certain features of the SO design were critical to its effective functioning as a Q&A service: fast answer times and high answer quality arise from a carefully crafted reputation system and a strict set of community guidelines that favor factual, informational answers.
However, our analysis also demonstrated that these features were a consequence of a particular design philosophy and organization espoused by its founders.
In short, the design team is strongly and publicly involved in both control of and debate within the community.
This involvement is made possible by the site's focus on a single domain in which the design team had prior standing as community leaders.
In contrast, many large-scale self-organizing Q&A sites are broad in reach-- the site operators supply a general platform for question answering but are not directly involved in either content creation or moderation .
This tight engagement with the community led to three factors that we believe were critical to the success of SO.
Individuals increasingly rely on their distributed peer communities for information, advice, and expertise.
In aggregate, these studies suggest that general-purpose Q&A sites have answer rates between 66% and 90%; often attract non-factual, conversational exchanges of limited archival value; and may be poorly suited to provide high quality technical answers.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Tight focus on technical answers enabled by the Q&A format and a voting system created a strong alternative to the more conversational software forums.
Adding game mechanics through a reputation system harvested the competitive energy of the community and led to intense short participation for some users, and long sustained participation for others.
It also helped them to get their community on board with the proposed design and editorial practices.
While many in the community lament the lack of possibilities for discussion and debate, they continue to uphold the founders' vision because they acknowledge the resulting benefits.
A forum for discussion about the site, but external to it  helped the founders understand challenges and concerns of their users, and prioritize feature requests.
These requests were addressed through rapid design iterations with new releases of the site introduced almost daily.
Other similarly successful knowledge sharing communities, such as Slash  or TuDiabetes , also had founders who not only provided the tools, but also actively shaped their communities.
This pattern raises a challenge for HCI researchers.
The SO approach is predicated on ongoing, deep community involvement and, simultaneously, continuous technical adaptation of their software platform.
This model conflicts with the canonical process of humancentered design, in which it is more typical for technology developers to have intense, short periods of interaction with perspective users early on but then step down once the tools are introduced.
The questions for HCI researchers are twofold: Is it possible for outsiders of a community to foster knowledge sharing with the right set of tools?
And what impact can CSCW systems research make without longterm community involvement?
In the remainder of this paper, we position our contribution with respect to related work; we then support our assertion of SO's success through a data analysis and comparison to other Q&A sites.
Next, we distill themes that emerged from our interviews with site designers and users; and conclude with a discussion of implications for research in social computing systems.
The blueprint for Q&A sites was established by Ackerman's Answer Garden , which focused on expert answers in a single domain.
Recent Q&A platforms operate at Internet scale and often strive for generality.
Their size has led to the creation of new analysis methods -- some driven by data, others by qualitative studies.
One class of research relies predominantly on analyses of Q&A data sets.
For example, network analysis algorithms e.g., HITS , have been used to characterize user activity and identify users with high expertise for the Java Forum  and Yahoo!
Network analysis has also been used to discriminate sub-communities on Yahoo!
Prior work that uses the Stack Overflow data set conducted quasi-experiments about impacts of design decisions through post-hoc database analyses ; and considered Stack Overflow as an example of a two-sided market .
This paper also applies data analysis to describe the performance of Stack Overflow; this analysis then guides our qualitative study of SO's design process.
To better understand individual users' experiences, Nam et al.
Dearman surveyed users to find out why they don't answer certain questions .
Torrey exclusively used interviews to find patterns of seekers of craft knowledge online .
We also rely on user interviews but additionally report on interviews with founders.
We next summarize trends highlighted by Q&A studies.
Several distinct types of questions on Q&A sites can be distinguished: factual ; advice, ; opinion, , and non-questions  .
Significant parts of general Q&A sites are conversational; the sites perform poorly on focused technical questions .
Algorithms to distinguish between informational and conversational threads have been proposed .
We do not investigate such distinctions as SO explicitly  discourages conversational contributions.
Motivations of individuals who contribute answers to Q&A sites can be categorized as either intrinsic  or extrinsic  .
Point systems and other game mechanics are frequently used extrinsic motivators.
Adding monetary rewards can transform the user's sense of the system from a social interaction space to a more formal transaction space .
Researchers have studied patterns of community moderation on knowledge sharing sites.
Lampe and Resnik  looked at distributed moderation practices on Slash.
Active members of this site earn privileges that include the ability to cast votes that either increase or decrease a post's prominence and visibility.
Stack Overflow relies heavily on community moderation.
Active participants can vote questions and answers of others up and down and recommend closing inappropriate questions.
Questions about programming are also posed and answered in other online media such as mailing lists, newsgroups, and internet relay chat.
When contributions are ordered temporally in active forums, it may become difficult to locate relevant posts.
Research has proposed ways to find relevant subsets of posts, e.g., through collaborative filtering ; and to build improved models of discussion structure, e.g., by tracking quotations .
In social Q&A, questions are directed to the asker's online social network; this choice trades off size of the answer pool for social proximity.
Answers are normally not aggregated in a common knowledge base.
Social Q&A may leverage existing platforms--for example, people ask questions through Facebook or Twitter status messages .
Aardvark, a social Q&A service  routes questions to the most relevant users who are online at the moment a question is asked; neither questions nor answers are shared with the community.
Researchers have hypothesized that services like Aardvark are especially suited for eliciting opinions and subjective answers from trusted sources.
Most Q&A and social search systems  rely on plain text messages.
For questions about programming, researchers have also integrated techniques for finding help, examples, and debugging strategies directly into programming environments .
Such systems have not yet seen widespread adoption, making it premature to compare their benefits to text-based Q&A.
To enable conversations about the site itself without interfering with the main Q&A function, Stack Overflow has a "meta" site, comparable to discussion pages on Wikis.
Moderation is achieved through voting  user edits , and through actions of official moderators.
Moderators can modify, close and delete posts and user profiles.
Moderators are elected by SO users in a formal, annual vote.
Stack Overflow follows a common model of Q&A site design: users post questions, answer questions, comment, and vote on posts.
Users can edit their own prior submissions.
Figure 1 shows the main page of questions, with the most recently asked questions at the top; Figure 2 shows a question page with answers and comments underneath.
Users may register on the site to gain reputation points and badges, as determined by user activity and votes by others on a user's posts.
With higher reputation scores users are awarded editing rights on the site, including editing other users' answers.
As of early August 2010, Stack Overflow has a total of 300k registered users who asked 833k questions, provided 2,2M answers, and posted 2,9M comments.
In August 2010 the site served 7.8 million monthly visitors.
This makes Stack Overflow smaller than general Q&A sites, but larger than social Q&A or programming forums .
Expanding our analysis to all questions, including those that are never answered, yields that 50% of all questions receive a first answer within ~12 minutes; an upvoted answer within 25 minutes, and the accepted answer within approximately 6 hours.
This is an astonishing result-- one interviewee remarked: "If you complained about a lack of response on a newsgroup after 24 hours you were labeled impatient; now you can realistically expect an answer within 20 minutes."
Social Q&A site Aardvark is faster , due to the fact that Aardvark routes questions to users known to be online.
Most questions are answered: 92.6% of questions receive at least one answer.
This rate exceeds rates reported for Yahoo!
More importantly, the answers are predominantly technical.
63.4% of questions receive strictly more than one answer.
Since SO focuses on informational questions, we expect fewer answers than in sites that permit conversational questions.
This intuition is supported: Harper reported 5.71 answers per question for Yahoo!
However, each post can also have a list of comments associated with it.
Comments are used for follow-up and statements of agreement or disagreement.
We analyze the thread length of a question by summing answers and all comments.
The complete distribution of answers and thread lengths is shown in Figure 3.
Accounting for comments, the average thread length is similar to Yahoo!
While first answers are fast in the median, relatively few additional questions are answered after the first hours .
There is also a long tail of questions that remain unanswered: the mean time for first answers, which is heavily skewed by these long-latency responses, is 2 days and 10 hours.
How long do users have to wait until they receive answers?
Fewer questions have accepted answers as accepting is not required.
The median time for a first answer is only 11 minutes : half of all questions that eventually receive answers are answered within 11 minutes.
This trend suggests that motivating users to frequently return and participate is more important than the total number of users.
It also suggests that Stack Overflow has been operating at a response time minimum and that further improvements in response time are unlikely.
10 minutes appears to be the minimum time for a knowledgeable programmer to find a question, read and think about it, formulate a reply, and publish that reply.
Of these, the largest group of roughly one quarter have not yet asked, answered, or voted on any questions.
The second most frequent group consists of users who only ask, but do not answer or vote on questions , followed by users who only answer, but never ask or vote .
Overall, nearly half  of registered users have answered questions.
The overlap of users who ask and answer is significantly larger  than in Nam's analysis of KiN , and approximates Gyongyi's analysis of Yahoo!
Shooting Stars: Registered users who have a single, short period of high activity followed by low activity.
Low-Profile Users: Registered users who have intermittent activity, but who never become highly active.
Lurkers and Visitors: Users who have not been asking or answering questions; visitors without user accounts.
We calculate the percentage of users in each class, and the percentage of answers that are supplied by those users, using regular expression matching on activity signature strings.
94.4% of users are never highly active; they supply 34.4% of answers; shooting stars make up 4.2% of the user base and supply 21.9% of answers; community activists make up 1% of users but supply 27.8% of answers .
The remaining 15.9% of answers are provided by nonregistered users or users that do not fit these profiles.
We hypothesize that the game mechanics of the site draw in both community activists and shooting stars, but convert only the first group into highly active contributors.
The second group moves on after a short infatuation period.
Figure 7 shows that user activity follows a power law: most users have very little activity, and the number of users with higher activity falls off exponentially .
Infrequent users  post more questions than answers.
For these users, the median ratio of answers to all posts is at or below 0.5.
In contrast, frequent users overwhelmingly tend to have high answer ratios, i.e., they answer more questions than they ask, with the exception of a few outliers.
The unique success of Stack Overflow can be understood in terms of the ecology of different user behaviors it enables.
We identified four distinct groups of users, based on the frequency with which they provide answers in the system.
We distinguish between low-activity users  and high-activity users .
Each user has an activity signature that describes their activity month-to-month .
We found four types of signatures: Community Activists: Registered users who are highly active on the site for multiple months.
Users who visit SO without ever creating an account are largely invisible to our analysis.
Anecdotally, many of these individuals find answers to previously asked questions through search engines.
Future work could quantify the size of this user group through web server log analysis; we note that the site receives 7 million monthly visitors, but has only 300,534 registered users.
Figure 10 shows the distribution of views for questions, which includes both logged-in users and visitors.
Most questions receive dozens to hundreds of views ; few questions receive thousands to tens-of-thousands of views.
The design choices made by Stack Overflow bring tradeoffs with them: certain types of questions are better suited to be asked and answered on SO than others.
Our interviewees hypothesized about several classes of questions that remain unanswered or are answered slowly: 1.
Questions about relatively obscure technologies for which there are few users.
Questions that are tedious to answer.
Problems that cannot be easily reproduced with a small, self-contained code fragment.
Questions that do not have a clear best answer and thus invite discussion, even if that discussion is technical.
Thus far, our data analyses have only been able to confirm the first hypothesized reason.
Figure 11 shows answer times for a selection of 30 tags that occur frequently for fast and slow questions, respectively.
Fast tags on the left tend to cover widely used technologies ; while slow tags on the right are more obscure .
Attempts to characterize questions as slow or fast by analyzing question topic, question type, or term frequencies have been inconclusive.
In addition to the main SO site, there are two sibling sites that utilize the same platform, Server Fault  and Super User .
These sites receive approximately 1/10th of the traffic of SO, but have similar answer times and ratios.
In recent months, an additional crop of sites has been created.
Detailed data about these offshoots was not available; we report summary data in Figure 12.
Even the most active of these sites are a factor of 100 less active than Stack Overflow.
Interestingly, a site dedicated to subjective questions about programming is now among the most active offshoots.
This suggests that it is more important to draw community boundaries narrowly, with precise definitions what is "in bounds" and "out of bounds" for a given site.
The meaning of those boundaries may matter less.
In the previous section we discussed the patterns of questions and answers that emerged on SO over time.
This analysis showed SO to be largely successful in accomplishing its primary goal: giving software developers fast, informative answers to their questions.
To better understand the driving factors behind these patterns we conducted a qualitative study of the community.
Participants of the study included SO founders , members of the site design team  and users .
Participants among the design team were recruited using leads from site founders.
Users were recruited based on their reputation level on SO, creating a mix of top users and moderately active users.
The study included interviews conducted over the phone, on Skype, and in person, and lasted for about 1 hour.
We used a semi-structured interview format following general themes, but exploring emergent topics in conversations.
These questions elicited both strengths and limitations.
The interviews were recorded and transcribed verbatim.
We used inductive iterative coding  to allow common themes to emerge from the data.
Then you're skipping interim conversation."
This choice had important consequences for the design of the system of external incentives and its impact on user engagement.
Our analysis yielded a number of design choices our interviewees perceived as critical to the success of SO, and several design strategies adopted by the site's founders that led to this success.
In the next section we first discuss positive findings that we believe contributed to the success of the site; we then turn to challenges and barriers the site continues to experience.
Consistent with previous research, our interviewees demonstrated a combination of intrinsic motivational factors, including a desire to help their community and learn, and extrinsic ones, for example a wish to enrich their professional portfolios or simply collect reputation points.
Many of the early users of SO had extensive track records of educating their community through blogs, technical books, and active participation in software forums.
However, all individuals we interviewed that actively participated, even the most established educators, described SO's system of external incentives as one of the main factors that "got them hooked" and kept them coming back: "I am very competitive and you give me indication that a high number is good and I will try to get a high number.
I don't think it's to do with reputation so much as, `This is a game.'"
In SO this strategy was highly effective.
Many users set their goals on reaching the reputation cap-- the maximum number of points one could earn in one day, and developed multiple tools and strategies to maximize their gains.
Several interviewees compared their experience explicitly to games, where cleverly designed reward systems also produce dramatic effects: "Stack Overflow -- it's like World of Warcraft, only more productive."
Providing faster brief answers gave users more reputation points than providing more detailed answers that took longer to write.
As a consequence, the community's focus drifted somewhat away from optimizing the quality of information.
One aspect of the reputation system remains a contention point among site experts we interviewed.
After reaching 10,000 points, an individual has all the moderation and editing privileges the site offers and can no longer benefit from increase in reputation.
Stack Overflow is the result of a collaboration between two individuals well known within the software development community for their heavily-read blogs, Joel Spolsky and Jeff Atwood.
Their main goal was to create a sustainable resource where anybody with a question pertaining to software development could quickly find "the right answer".
Their design approach specifically prioritized information over conversation through a Q&A format and a voting system, and encouraged participation through a system of game incentives.
Both founders were active participants in software development discussion forums.
They witnessed many of the forum threads devolve into conversational spaces ridden with rants, spam and anti-social behavior, thus complicating search for valuable information .
To address this ongoing challenge, SO was conceived as a Q&A site rather than a discussion forum.
To help valuable pieces of concrete technical knowledge to become more visible, SO designers introduced a voting system, in which users earn rights to vote on posts of others through active participation.
These community moderation mechanisms were previously explored in discussion forums, however they produced sub-optimal results:
For users who relied primarily on external motivators, reaching the plateau led to a subsequent reduction in participation, creating the shooting star pattern.
Users who additionally had strong internal motivation continued to actively participate and contribute, for the sake of the community.
Votes 1109 908 840 802 797 Question Title What is the single most influential book every programmer should read?
What should a developer know before building a public website?
What is your favorite programmer cartoon?
In addition to being active participants on discussion forums, both founders were active and prolific bloggers.
In the summer of 2008, when SO was introduced, their respective blogs, Joel on Software  and Coding Horror  had a combined readership of approximately 140,000 people.
This prominence gave the founders two unique advantages: the ability to gather a critical mass of dedicated users, and a high initial level of trust for their vision.
While experienced in traditional software engineering processes, the Stack Overflow team took a different approach to design that is becoming more popular among software startups: "We pretty much had to forget all the software engineering processes we learned."
Even before SO was designed and deployed, its prospective users became a significant guiding force, providing comments and often challenging the designers' vision.
After introduction of the first version of the site, the feedback loop was formalized in a user forum, User Voice, later replaced by the SO Meta site.
Meta used the same Q&A engine as Stack Overflow, but was meant to engage users in the discussion about the site, its features, editorial policies, and community values.
Introduction of Meta gave the designers an opportunity to keep an ongoing discussion with their users.
In addition, it moved conversational topics away from the main site, to preserving the high "signal-tonoise" ratio for technical information.
The five most popular questions from Meta  are indicative of the variety of topics this site covers:
Votes 771 626 401 365 356 Post Title The official FAQ for Stack Overflow, Server Fault, and Super User Could we please be a bit nicer to the noobs Using what I've learned from stackoverflow.
All social software systems have to address the problem of building an initial critical mass of users .
The SO design team addressed this challenge before launching by discussing SO on their blogs and holding a series of weekly podcasts describing their vision, inviting readers to share their thoughts and provide feedback.
As a result, when Stack Overflow was introduced, thousands of people were asking and answering questions within the first day: "So on the first day, the first question I could come up with had already been asked and answered, and there were three or four answers, and some voting had happened.
The best answer had already been voted to the top.
So on the first day when I saw everything working, I knew that we were in really good shape.
While discussion forums and news groups were popular with software developers, a strictly informational Q&A site was novel.
Many early users of SO were skeptical of this approach.
So I think that was my involvement, was sort of evangelizing sort of that new paradigm."
Many current users lament the lack of possibilities to engage in a debate over more controversial issues related to software development.
The second factor that contributed to the success of the site was a particular design approach adopted by the founders and designers of the site.
Specifically, they adopted a practice of constantly adjusting the design of the site and immediately releasing the new modifications to the community: "We pretty much release new versions every day.
Sometimes they are really small changes; the bigger ones often get announced on Meta."
These rapid iterations allow for experimentation; the team can tweak the design and observe resulting changes in community behavior.
For example, the "Fastest Gun in the West" problem we discussed earlier, often led to longer, more informative answers being buried under shorter but quicker ones.
Changing the design so that answers with equal numbers of votes were presented in random order partially alleviated this challenge.
This radically affected how the community worked because it meant that the secondary questions that were really good had a much higher probability of getting voted up."
Point systems are equally popular in social software.
But careful tuning of the reputation allocation mechanisms within the point system reinforced the general design philosophy.
For example, designating "community" questions that do not gain reputation points reduced incentives to ask overly broad questions.
At the same time, our study suggests that deep engagement and prominent membership in the community gave the SO founders important advantages.
The nuanced insider's perspective on needs and driving motivations of the community; the ability to attract a critical mass of users ; and the ability to continue this engagement throughout the system's deployment have all been recognized as important contributors to the success of groupware applications .
While user-centered design methodology aims to increase the chance of attaining these goals when designing for others, SO demonstrates the power of designing from within a community.
However, SO is also not a naive case of designing for oneself, as the continuous refinement based on user feedback attests to.
Despite site's success, there remain many challenges hotly debated by the site's designers and users.
Some of them relate to the nature of the community the site generated.
Our interviewees with high reputation levels were aware of the challenges experienced by the new members of the community who find it difficult to gain visibility and status.
Disparities between novice and expert users are exemplified by the thread "Could we please be a bit nicer to the noobs", which continues to be highly active on Meta .
Other concerns relate to the potential longevity and future success of the site.
An ongoing discussion about "Stack Overflow Fatigue" among members reflects these threads.
Yet another set of concerns is related to the practices that the design of the site enabled and encouraged.
Earlier we mentioned "The Fastest Gun in the West" problem; similar challenge is related to the difference in point allocation between providing a new answer and correcting an earlier answer.
Because of this difference, users feel less compelled to correct previous answers; as a consequence, suboptimal answers persist on the site if they were accepted as correct ones.
These and other challenges continue to demand further refinement of the site and improvements to its design.
The overarching theme that emerged from our work is that tight engagement with the community was critical to the success of knowledge sharing within the SO community.
To what extent then can the success of SO be replicated -- even by the same design team -- in other domains of specialized knowledge?
An early experiment to license the SO Q&A engine for profit to third parties largely failed.
Many of the communities started "with a credit card" but without appropriate leadership languished; the program was ultimately abandoned.
Currently, a second effort to broaden the reach of SO beyond programming is underway that attempts to improve on these prior missteps.
New sites attempt to leverage the existing user base by largely focusing on leisure activities the current users already engage in .
The new approach includes introduction of a formal community proposal system in which communities must define clear topic boundaries; demonstrate commitment to the project; and show viability through sufficient user activity in a test phase.
To our knowledge, this formalization of online community formation is novel; the outcome is still uncertain.
The study of Stack Overflow demonstrated that careful consideration of design decisions significantly contributed to the success of SO over prior and competing platforms.
While few of SO's features are truly novel, the details of their design effectively promoted desirable behaviors and discouraged undesirable ones.
For example, voting systems are common in many online forums.
In this paper we discussed lessons learned from a popular Q&A site for software developers, Stack Overflow.
Analysis of the patterns of user interactions with the site helped to highlight some of the more common user groups and behaviors.
A qualitative study helped to explain these behaviors and shed some light on the reasons for the site's success.
These findings present a number of questions to the HCI and CSCW communities in regards to the importance of socio-cultural context for the adoption of social computing platforms.
While these questions are not new, new technology platforms for sites like SO bring unique challenges.
If the success of social computing applications depends on both deep community engagement and, simultaneously, on continuous modification of those applications in response to shifting community behaviors, this presents a significant barrier to systems research in social computing.
Some researchers already argue that the most innovative work in this area is not produced in academia .
Instead, startup companies, dedicated to their cause, and willing to maintain engagement with their user communities for extended periods of time appear to have a stronger position to iterate towards novel, successful models.
Given these concerns, what role should  systems research play in social computing?
Academic research should not be relegated to studying exciting new communities created by others.
But what is the right vector for introducing new tools in this space from research?
Answer Garden: a tool for growing organizational memory.
Proceedings of the ACM SIGOIS and IEEE CS TC-OA conference on Office information systems, ACM , 31-39.
Knowledge sharing and yahoo answers: everyone knows something.
Barcellini, F., Detienne, F., Burkhardt, J., and Sack, W. Thematic coherence and quotation practices in OSS designoriented online discussions.
Bouguessa, M., Dumoulin, B., and Wang, S. Identifying authoritative actors in question-answering forums: the case of Yahoo!
Example-centric programming: integrating web search into the development environment.
The Design of Design: Essays from a Computer Scientist.
Basics of Qualitative Research: Techniques and Procedures for Developing Grounded Theory.
Answers do not answer questions.
Grudin, J. Groupware and social dynamics: eight challenges for developers.
