This project represents a first step towards bridging the gap between HCI and cognition research.
Using functional near-infrared spectroscopy , we introduce techniques to non-invasively measure a range of cognitive workload states that have implications to HCI research, most directly usability testing.
We present a set of usability experiments that illustrates how fNIRS brain measurement provides information about the cognitive demands placed on computer users by different interface designs.
Using brain measurement to quantify the level of WL experienced by computer users is a difficult task because "workload" is somewhat of an umbrella term.
The brain is a complex structure, and there are many cognitive resources that work in serial and in parallel to process information.
Indeed, when we compute arithmetic, compose a poem, or chat with a friend, we are experiencing some form of WL.
Usability researchers attempt to formalize and quantify the process whereby an interface is evaluated, and to measure precisely the degree to which an interface meets the goals of its intended audience.
Although one can measure the accuracy with which users complete tasks and the time it takes to complete a task with a user interface , measuring subjective factors such as workload, frustration, and enjoyment is more difficult.
These factors are often "mea sured" by qualitative observation of subjects or by administering subjective surveys to subjects.
Such surveys are inherently subjective and they can elicit participant biases, as participants often attempt to please experiment investigators in their responses.
Additionally, surveys are often administered after a task has been completed, lacking insight into the users changing experience as they work with a UI.
Our research addresses these evaluation challenges with respect to mental workload .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
However, for each task that we take part in, we may use different  cognitive resources.
While there are many definitions in the literature describing WL , for the purposes of this paper we define it in functional terms as: The load placed on various cognitive resources in the brain in order to complete a task that involves the processing of information.
Since the processing of information is a key element in users interactions with computers, the field of humancomputer interaction  is derived from and heavily influenced by cognitive psychology.
We seek a thorough understanding of the effects that a new UI will have on users mental resources.
Ideally, a UI will be easy to use, allowing users to focus their mental resources on the task at hand.
However, there remains a large gap between the high-level references made to these mental resources in HCI research and the low-level ability to pin point and measure these resources in human users.
For this reason, Czerwinski and Larson  discuss the need to tie together cognition research and HCI design principles.
They note that this is not an easy task, as most cognitive research focuses on task manipulations that are on a low-level, with a cognitive load that is small, and the jump from these specif-
We introduce techniques to measure via noninvasive means a range of cognitive WL states that have implications to HCI research.
We describe several cognitive resources at a low-level, as they pertain to the brain, and at a high-level, as they relate to the field of HCI.
This additional information can yield a more thorough, comprehensive understanding about a UI design than can be achieved with standard usability testing.
We begin this paper by describing related work on the fNIRS device and the human brain and go on to describe an experiment protocol designed to measure the level of load placed on users low-level cognitive resources while working with a UI.
We then present a series of usability experiments that apply this protocol to the evaluation of design choices of UIs.
Finally, we describe our results and analysis, as well as avenues for future work in this area.
Working Memory WM refers to an information processing system that temporarily stores information in order to serve higher-order cognitive functions such as planning, problem solving, and understanding language .
One of the most common tasks used in cognitive psychology to elicit load on participants WM resources is the n-back task .
The n-back task is depicted in Figure 3.
A series of letters is presented to participants, one letter at a time, on the computer screen.
For each letter, the participant indicates whether or not that letter matches the letter that she saw n letters previously.
During the 3-back condition, participants must store and manipulate, three items at a time in WM, and for the 1-back task, they only manipulate and store one item at a time.
EEG has been used in the HCI and Human Factors domains to measure various aspects of mental workload .
While a promising tool for non-invasive brain measurements, EEG has several drawbacks such as low spatial resolution, susceptibility to noise, and long set-up time which can make EEG challenging to use in realistic human-computer interactions.
The tool, still a research modality, uses light sources in the near infrared wavelength range  and optical detectors to probe brain activity, as depicted in Figure 2.
Deoxygenated  and oxygenated hemoglobin  are the main absorbers of near infrared light in tissues during hemodynamic and metabolic changes associated with neural activity in the brain .
These changes can be detected measuring the reflected light that has probed the brain cortex .
Parasuraman  found that WM and executive functioning tasks activate areas in the prefrontal cortex, and that the amount of activation increases as a function of the number of items held in WM.
The presence of WL activation and the relative load  can be quantified using fNIRS .
WM is at the foundation of all interactions between humans and computers.
If we can determine that a UI design elicits high WM load on users, we can modify the UI to alleviate this load.
For example, a poorly constructed menu may require users to keep several menu items in their WM while searching for the correct selection.
Perhaps this menu can be redesigned to alleviate this load on the users WM.
Visual Perception and Search Another WL related set of tasks involves visually searching for items within a set of distracter items.
There are two kinds of visual search; efficient and inefficient.
If a target item is saliently different as compared to the distracter items surrounding the target item, it can be found immediately, regardless of the number of distracter items .
Inefficient search occurs when the search item is not highly salient as compared to the distracter items.
In this case, a serial visual search must be performed in order to locate the item of interest.
To date, most experimental psychology studies explore brain functioning while people conduct simple, highly controlled tasks that have been designed to target a specific cognitive resource, such as visual scanning and perception, working memory , and similar higher-order cognitive functions.
Many of these resources relate directly to current research in HCI.
One task that has been used to induce inefficient visual search is called the "finding As" task.
This task can be found, among a variety of other experimental tasks, in the Kit of Factor Referenced Cognitive Tests .
In this task, users are instructed to scan through a list of words and to cross off every word that contains the letter a. fNIRS has been used to measure the resources responsible for visual search .
If we can determine that users are conducting inefficient visual searches while looking for items in a UI, we can modify the UI to alleviate these demands.
For example, a poorly structured web page may not direct users visual attention to the relevant content that they are likely to search for.
The page can be restructured to provide salient visual cues directing people to the most relevant semantic content on the web page.
Executive Processes and Response Conflict Pinpointing functional brain regions is difficult to do with our executive processes, as they are involved in high order processing that involves the recruitment of a number of overlapping cognitive resources .
While we still have much to learn about executive processing, functions related to response conflict have been empirically validated.
Response conflict deals with the central executives suppre ssion of automatic, but incorrect responses.
We use response conflict throughout our daily lives.
For example, while driving a car we may see a squirrel run across the road.
Our initial, automatic reaction may be to swerve the car away from the squirrel.
However, a quick look around may show oncoming traffic on one side of us and a cliff on the other side.
Our central executive helps us to inhibit the automatic response of swerving in order to choose the response of staying in our lane .
A common test used in experimental psychology research to induce response inhibition in the brain is the Stroop test .
In this task, a color name, which is written in a font of a particular color, is presented to participants.
Participants must say the color that each word is written in out loud.
In the congruent condition , the name of the word and the color that the word is written in are one and the same.
In the incongruent condition , the name of the word and the color that the word is written in are different.
Peoples ability to name a color is slightly slower than their semantic ability to read a word.
Thus, the incongruent condition of the Stroop test requires people to use their response conflict resources, suppressing the automatic response of saying the name of the word and answering correctly, with the color of the word.
If we determine that a user has high response conflict while working with a UI, this may indicate that something about the UI is unintuitive.
In order to enhance usability testing with fNIRS, we aim to measure which low-level resource are being taxed and the level of load  that is placed on each of these resource while users work with UIs.
Although there is some disagreement as to the effects of multitasking in the brain , most agree that combining several lowlevel, simplified tasks has an additive effect in the brain.
This suggests that we can use fNIRs to measure different patterns of activation associated with the various cognitive resources targeted in this experiment.
Thus, we expect to be able to measure different levels of load placed on users WM, visual search, and response inhibition resources.
We refer to these exercises as cognitive benchmark tasks.
We refer to these as UI tasks.
Users also work with the UI tasks.
Brain activity is measured throughout the experiment.
Participants completed the following benchmark tasks: Finding A's In the Finding As task, participants were instructed to look at a matrix of words and click on those that contained the letter a.
In one version of the task, the as in t he words were highlighted.
In the other version, the as were not hi ghlighted.
There were 14 words with as in them on each screen presented to participants.
If a participant clicked on all of the words containing as, a new screen was shown, though there was a maximum of two screens per 50 second task.
These two tasks induced benchmark levels of high and low load on users visual search resources.
Stroop The Stroop task had two variations, an incongruent Stroop and a congruent Stroop, which are depicted in Figure 5.
In both variations, adapted from previous fNIRS research , one of the words BLUE, RED, YELLOW, or GREEN appeared on the screen for .5 seconds before another of those four words was added to the screen beneath the first.
Participants determined whether the bottom word correctly described the color of the top word.
In the congruent version, the top word was always colored to correspond with its meaning.
In the incongruent version, the top word did not have to match its own meaning.
These tasks induced benchmark levels of high and low load on users r esponse inhibition resources.
The task took place in a parking lot where there were three lines of cones.
Participants were instructed to drive around the cones in a slalom pattern using the arrow keys on the keyboard to navigate.
There were two variations of the task.
In the first, the commands associated with the arrow keys performed as expected.
That is, pressing the  and  arrow keys caused the car to turn to the left and the right, respectively.
In the second variation, the functions of the  and  arrow keys were reversed.
Pressing the  key turned the car to the right, and pressing the  key turned the car to the left.
In the 0-back condition, participants were first shown a single letter.
As the task progressed, participants were presented with new letters on the screen.
They were asked to identify whether the letters they were seeing were the same or different than the first letter.
In the 3-back task, they were asked to judge whether the letter they were looking at was the same as the letter they saw three letters before.
These two tasks induced benchmark levels of high and low load on users WM resources.
Figure 7: The web search task with highlighting.
Participants were instructed to highlight their answer using the mouse and then to proceed to the next question .
For each instance of the web search, there was a maximum of two questions.
There were two versions of the web search task.
In one version, the users search terms were highlighted in the search results and the articles.
In the other version there was no highlighting of search terms.
There was only one speed enabled when subjects moved forward during the driving tasks, placing a limit on the number of cones users could access in each task.
During the finding as and web search tasks, subjects only had a set number of task-items to complete in each 50-second period of time.
Subjects were instructed  to work at a rate that would result in the completion of all tasks by the end of the task period.
By controlling for time in this way, we were able to focus more on the difference in brain activity during different tasks rather than on differences caused by working at different paces.
In the rest of this paper, we use the following terminology: Cognitive benchmark tasks: Experiment tasks that have been pulled from experimental psychology research.
In this experiment, these tasks target the following resources: low and high WM, low and high visual search, and low and high response inhibition tasks.
UI tasks: Experiment tasks that represent the UIs to be evaluated.
These include: driving with correct mapping, driving with incorrect mapping, web search with highlighting, and web search with no highlighting tasks.
Therefore, we had a recording of HbO and another recording of Hb at four depths on the left side  and four depths on the right side of the brain .
Both HbO and Hb contribute to what is known as the blood oxygen level dependent  signal, which is correlated to brain activity.
For a review on the BOLD signal see  and for a review on the measurement patterns seen in HbO and Hb in fNIRS studies see .
We cut off the first 4 seconds of each task, as blood takes several seconds to move to areas of activation.
Next, we extracted the following features from our preprocessed fNIRS data: largest value, smallest value, average, slope, time to peak, and full width at half maximum; for the first and second half of each task.
We used a cross validation scheme that takes into account the block design of the experiment, as traditional cross validation produces higher classification accuracy that is not representative of real world HCI-relevant applications .
Once we had partitioned our data into training and testing data, we used CfsSubsetEval, a feature subset selection algorithm from the Weka open source toolkit  on our training data.
The function selects feature subsets that are highly correlated with the class and have a low correlation to one another.
We used these features to classify our test data.
Ten participants completed the experiment.
After providing informed consent, participants were instructed to complete a tutorial.
Participants were instructed to keep body movement to a minimum, and to only move when using the keyboard and mouse while working with the experiment tasks.
Previous research has shown that these minimal hand movements do not add detrimental amounts of noise to the fNIRS data .
Experiment tasks were presented to participants in a randomized order, with a 23 second rest period between tasks.
There were six trials and each trial consisted of 10 tasks .. Each task lasted 50 seconds.
As the experiment progressed, the answers provided by the participants were recorded.
After the experiment was complete, participants completed a post-experiment survey where they rated the tasks on a 1-7 Likert scale, with 1 representing the lowest and 7 representing the highest level of difficulty.
The fNIRs device is an ISS OxyplexTS frequency-domain tissue spectrometer with two probes.
Each probe has a detector and four light sources.
Each light source produces near infrared light at two wavelengths  which were sampled at 6.25Hz.
As brain activity differs widely on a person-by- person basis, we ran all preprocessing of data separately for each participant.
We normalized the fNIRS light intensity raw data in each channel by their own baseline values.
We can use this classifier to determine the users workload while working with a driving UI and a web search UI.
In the rest of this section we describe our analysis techniques and we relate our results to each hypothesis.
Then we make recommendations to the UIs based on the results.
As the figure shows, we were able to distinguish between WM and response inhibition , WM and visual search , and response inhibition and visual search with over 80% average accuracy across subjects.
Also, we were able to distinguish between the three classes of WM, response inhibition, and visual search with over 70% accuracy.
These classification accuracies support our first hypothesis.
To address our second hypothesis, we used our Naive Bayes classifer to distinguish between the load  placed on each cognitive resource.
Mean classification accuracies for all subjects are in Fig.
As the figure shows, for each resource, we were able to distinguish between low and high-levels of load on that resource with average accuracies ranging from 76-94%.
These promising classification accuracies support our second hypothesis.
We can use this classifier to determine the user's workload while working with a driving and a web search UI.
We used the process described in our usability experiment protocol to predict the WM, response inhibition, and visual search load of the UI variations.
Next we will illustrate, for each participant, how we built three distinct machine learning classifiers.
Construction of Classifiers For each participant, we built three distinct classifiers.
Figure 10 depicts our process for creating a machine learning classifier to predict the WM load  associated with our driving UI tasks.
Next, for each participant, the low and high response inhibition benchmark tasks were used as training data to build another Naive Bayes classifier.
The same process as that depicted in Figure 10 was followed, however, this time the training data input into the classifier were the response inhibition benchmark tasks rather than the WM benchmark tasks.
This resulted in a response inhibition classifier that predicted the load of response inhibition  placed on users while working with the UI variations.
For each subject, we refer to this second classifier as the response inhibition classifier.
Lastly, a third Naive Bayes classifier was built using just the visual search benchmark tasks as training data.
This resulted in a visual search classifier that predicted the visual search load  placed on users while working with the UI variations.
For each subject, we refer to this third classifier as the visual search classifier.
Composition of Classifiers For each of our 10 subjects we built three Naive Bayes classifiers; a WM classifier, a response inhibition classifier, and a visual search load classifier.
Thus, we built 30 distinct classifiers using 30 completely separable sets of training data.
Thus, we had 16 timeseries, and we generated 12 features from each of these timeseries .We used the CfsSubsetEval feature selection algorithm to prune our features .
The feature selection algorithm pruned the features greatly, and the number of features selected by each classifier ranged from two to 15 features.
On average, 7.6 features were selected for classification when building the WM classifiers while an average of 5.2 and 9.1 features were selected while building the response inhibition and visual search classifiers, respectively.
Figure 11 displays the composition of the three classifier types across subjects.
Classifier Predictions Across Subjects We ran machine learning predictions for 10 participants x 3 classifiers x 4 UI variations x 6 trials, which resulted in 720 distinct machine learning predictions.
We describe our classifier predictions for all participants next.
Weve br oken down our results into four graphs .
Each graph shows a tally of the predictions made by each of the 10 subjects three classifiers while each subject worked on one of the UI variations.
Thus, each graph contains a tally of 10 subjects x 6 trials x 3 classifiers = 180 classifier predictions.
These results are depicted in Figure 12.
We created a WM classifier, a response inhibition classifier, and a visual search classifier for each of our 10 participants.
Thus, we created 10 distinct WM classifiers which were suited to the individual.
We tallied the features selected by each of these 10 classifiers and grouped the features based on the sensor location of that feature, and on whether or not the feature represented HbO or Hb data.
We did the same for the response inhibition and for the visual search classifiers.
As shown in the figure, each classifier type differed in structure from the other two classifiers.
Participants HbO on the right side of the head and Hb on the left side of the head were the most predictive for the WM tasks.
Participants HbO and Hb on the right side of the head were the most predictive for response inhibition tasks.
The HbO and Hb on the left side of the head were the most predictive for visual search tasks.
This supports the hypothesis that the benchmark WM, response inhibition, and visual search tasks used different  cognitive resources, and that these differences were measurable with fNIRS.
The results show that across all subjects most of the driving with correct mapping instances were classified as having a low WM, high response inhibition, and high visual search load.
The majority of the driving with incorrect mapping instances were classified as having a high WM, high response inhibition, and high visual search load.
It is not surprising that driving with the correct mapping caused users to exert lower levels of WM than driving with the incorrect mapping.
It is likely that when using the controls in the incorrect mapping condition, users had to keep the unusual mapping stored in their WM throughout the driving task.
It is also not surprising that both driving conditions caused a high visual search load, as users were continually scanning the cones ahead of them while steering.
Interestingly, both keyboard mappings  caused a high level of response inhibition.
Postexperiment interviews support this speculation.
13, the results showed that across all subjects the majority of the web search with highlighting instances were classified as having a high WM, high response inhibition, and low visual search load.
The majority of the web search with no highlighting instances were classified as having a high WM, low response inhibition, and high visual search load.
It is not surprising that both web search UIs caused users to have high-levels of WM.
After all, users had to remember the content of the question that was asked of them while they searched for the solution in the articles in front of them.
It is also not surprising that the level of visual search load was higher when highlighting of search terms was not available than when the highlighting was available for users.
We expected that users would have to conduct an inefficient visual search of the articles for the keywords when highlighting was not available.
This resulted in users having a higher level of response inhibition that reflected the added load that they were able to place on semantic, task related, processing.
We hypothesize that the added effort that users were able to place on semantic  processing rather than on syntactic  processing resulted in the higher accuracy that users achieved in the web search with highlighting condition.
Our classification results support our third hypothesis; the cognitive load classifications depicted in Fig.
11 and 12 are in line with what we would expect users cogn itive resource load to be while working with each UI.
The accuracy and survey results will support the results from the fNIRS data analysis, and the fNIRS results will provide information above and beyond the information provided by the more traditional usability metrics of accuracy and survey data.
We recorded the number of correct and incorrect responses made by participants during the experiment.
A repeated measures ANOVA was used on this accuracy data to make comparisons between conditions.
A Kruskal-Wallis test was used to analyze the Likert scale survey data for each condition.
Table 3 provides a summary of these results.
Highlighted cells indicate significance with 95% confidence.Not surprisingly, all accuracy data  and all survey data indicated that the low benchmark workload  tasks were more difficult than the high benchmark WL tasks.
Additionally, the accuracy data supported the fNIRS findings that driving with the correct mapping was easier than driving with the incorrect mapping, and that web searches with highlighting were easier than searches with no highlighting.
Interestingly, users did not report a difference in difficulty between the UI tasks in the Likert survey.
This is in contrast to the accuracy data and to the data from the fNIRS results.
Some common, and well known, issues with selfreport surveys are that users may not be aware of subtle differences in their own user experiences, and post-surveys may lack insight into the users real time exp eriences while working with a given task.
One of the primary benefits of non-invasive brain measurement during usability studies is to overcome the short coming of self report surveys.
In general, the results from the accuracy and survey data supported the fNIRS findings.
Furthermore, the fNIRS findings provided information about the load placed on users cognitive resources that was above and beyond that which could be acquired with the behavioral results alone.
This supports our fourth hypothesis.
We did not expect, however, the web search with highlighting to be associated with a higher load of response inhibition than the web search with no highlighting.
Since the use of highlighting enabled our users to avoid an inefficient visual search of search items, they were able to spend more time processing the verbal information presented to them in the research articles than in the no highlighting condition.
A thorough review of recent experimental psychology literature shows that people use response conflict resources while processing sentences and semantic information .
Therefore, based on the information acquired in our study, we discuss the implications of our findings on the design of the web search and driving UIs next: Driving UIs As reflected in Table 3, the driving with correct keymapping appears to be a preferable UI design than the driving with incorrect keymapping as it is associated with fewer errors.
Furthermorre, the results in Figure 11 show that the driving with correct keymapping was associated with a lower level of WM load for our subjects than the driving with incorrect keymapping.
However, the level of response inhibition and visual search were high for both UI variations.
We would suggest using an actual driving wheel or joystick to alleviate the demands placed on users response inhibition resources while driving.
We are currently conducting a follow-on experiment to compare the level of response inhibition exerted by users driving with the keyboard and the response inhibition exerted by users driving with a steering wheel.
We expect to see a lower level of response inhibition when users work with the steering wheel than when they use the keyboard.
Our experiment results showed that we were able to use brain measurement to measure high and low-levels of load experienced by users various cognitive resources while working with our driving and web search UI variations.
The usability metrics provided by our survey and accuracy data yielded results that are in line with our fNIRS results.
The protocol presented in this paper, combined with the fNIRS data acquired during the usability experiment, provided us with information above and beyond the knowledge gained by the more traditional survey and accuracy usability metrics.
With the fNIRS data, we were able not only to determine which tasks were more difficult for partic ipants, but to shed light on the low-level cognitive resources in the brain that were more heavily taxed by a given UI design choice.
While our fNIRs device only provided measurements on the left and right side of the participant's forehead, there are devices that can acquire more measurements across participant's cortex.
In order to find regions of the brain that are activated while load is placed on various cognitive resources, we are using a new 52-channel fNIRS device to explore the use of more sensor locations across the forehead, enabling the measurement of small spatiotemporal changes that occur when different cognitive resources are taxed.
In the future, we foresee brain data as an additional metric gathered in usability tests.
This cognitive state information, combined with more traditional usability metrics such as speed, accuracy, and survey results, can provide indepth evaluations of a range of UIs.
Functional neuroanatomy of executive processes involved in dual-task performance.
Proceedings of the National Academy of Sciences of the United States of America, 97 .
Anderson, E.J., Mannan, S.K., Husain, M., Rees, G., Summer, P., Mort, D.J., McRobbie, D. and Kennard, C. Involvement of prefrontal cortex in visual search.
Baddeley, A. and Della Sala, S. Working memory and executive control.
Philosophical Transactions of the Royal Society of London, 351.
Berka, C., Levendowski, D., Cvetinovic, M., Petrovic, M., Davis, G., Lumicao, L., Zivkovic, V., Popovic, M. and Olmstead, R. Real-Time Analysis of EEG Indexes of Alertness, Cognition, and Memory Acquired With a Wireless EEG Headset.
International Journal of Human Computer Interaction, 17 .
Blackmon, M.H., Kitajima, M. and Polson, P.G., Tool for accurately predicting website navigation problems, nonproblems, problem severity, and effectiveness of repairs.
Web Search UIs As reflected in Table 3, the web search with highlighting UI seems to be a preferable UI design than the web search with no highlighting UI as it is associated with fewer errors.
Furthermore, the results in Figure 12 show that the web search with highlighting still causes high levels of WM and response inhibition .
One possible UI enhancement could show the users search terms in a semi-transparent window that follows the users mouse across the computer screen.
This could alleviate users WM demands by enab l-
In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, .
Buxton, R. Introduction to functional magnetic resonance imaging.
Cambridge University Press, Cambridge, United Kingdom, 2002.
Chance, B., Anday, E., Nioka, S., Zhou, S., Hong, L., Worden, K., Li, C., Murray, T., Ovetsky, Y. and Thomas, R. A novel method for fast imaging of brain function, noninvasively, with light.
Czerwinski, M. and Larson, K. Cognition and the Web: Moving from Theory to Web Design.
Eckstrom, R., French, J., Harman, H. and Derman, D. Kit of factor-referenced cognitive tests.
Gevins, A. and Smith, M. Neurophysiological Measures of Working memory and Individual Differences in Cognitive Ability and Cogntive Style.
Grimes, D., Tan, D., Hudson, S., Shenoy, P. and Rao, R., Feasibility and Pragmatics of Classifying Working Memory Load with an Electroencephalograph.
Development of NASA-TLX : Results of empirical and theorical research.
Enhancing Usabiltiy Testing with Functional Near Infrared Spectroscopy Computer Science, Tufts University, Medford, MA, 2009.
Hirshfield, L.M., Solovey, E.T., Girouard, A., Kebinger, J., Jacob, R.J.K., Sassaroli, A. and Fantini, S., Brain Measurement for Usability Testing and Adaptive Interfaces: An Example of Uncovering Syntactic Workload in the Brain Using Functional Near Infrared Spectroscopy.
Hoshi, Y. and Tamuraa, M. Near-Infrared Optical Detection of Sequential Brain Activation in the Prefrontal Cortex during Mental Tasks.
Hoshi, Y., Tsoub, B., Billockc, V., Tanosakia, M., Iguchia, Y., Shimadaa, M., Shinbaa, T., Yamadad, Y. and Odae, I. Spatiotemporal characteristics of hemodynamic changes in the human lateral prefrontal cortex during working memory tasks.
Izzetoglu, K., Bunce, S., Onaral, B., Pourrezaei, K. and Chance, B. Functional Optical Brain Imaging Using NearInfrared During Cognitive Tasks.
International Journal of Human-Computer Interaction, 17 .
Jaeggi, S.M., Seewer, R., Nirkko, A.C., Eckstein, D., Schroth, G., Groner, R. and Gutbrod, K. Does excessive memory load attenuate activation in the prefrontal cortex?
Load-dependent processing in single and dual tasks: functional magnetic resonance imaging study.
Jancke, L., Brunner, B. and Esslen, M. Brain activation during fast driving in a driving simulator: the role of the lateral prefrontal cortex.
Jasdzewski, G., Strangman, G., Wagner, J., Kwong, K., Poldrack, R. and Boas, D. Differences in the hemodynamic response to event-related motor and visual paradigms as measured by near-infrared spectroscopy.
Joanette, Y., Ansaldo, A., de Mattos Pimenta Parente, M., Fonseca, R., Kristensen, C. and Scherer, L. Neuroimaging investigation of executive functions: evidence from fNIRS.
Lee, J.C. and Tan, D.S., Using a Low-Cost Electroencephalograph for Task Classification in HCI Research.
Meek, J., Elwell, C., Khan, M., Romaya, J., Wyatt, J., Delpy, D. and Zeki, S. Regional Changes in Cerebral Haemodynamics as a Result of a Visual Stimulus Measured by Near Infrared Spectroscopy.
Muller-Plath, G. Localizing subprocesses of visual search by correlating local brain activation in fMRI with response time model parameters.
Scerbo, M., Frederick, G., Freeman, F. and Mikulka, P. A brain-based system for adaptive automation.
Theoretical Issues in Ergonomics Science, 4 .
Schroeter, M.L., Zysset, S., Kupka, T., Kruggel, F. and Yves von Cramon, D. Near-Infrared Spectroscopy Can Detect Brain Activity During a Color-Word Matching Stroop Task in an Event-Related Design.
Smith, E. and Jonides, J.
Storage and Executive Processes n the Frontal Lobes.
Solovey, E., Girouard, A., Chauncey, K., Hirshfield, L., Sassaroli, A., Zheng, F., Fantini, S. and Jacob, R., Using fNIRS Brain Sensing in Realistic HCI Settings: Experiments and Guidelines.
Tamborello, F. and Byrne, M.D., Information search: The intersection of visual and semantic space.
Wickens, C., Lee, J., Liu, Y. and Becker, S. An Introduction to Human Factors Engineering.
Wilson, G.F. and Fisher, F. Cognitive task classification based upon topographic EEG data.
Conflict control during sentence comprehension: fMRI evidence.
