Digital pen systems, originally designed to digitize annotations made on physical paper, are evolving to permit a wider variety of applications.
Although the type and quality of pen feedback  have a huge impact on advancing the digital pen technology, dynamic visual feedback has yet to be fully investigated.
In parallel, miniature projectors are an emerging technology with the potential to enhance visual feedback for small mobile computing devices.
In this paper we present the PenLight system, which is a testbed to explore the interaction design space and its accompanying interaction techniques in a digital pen embedded with a spatially-aware miniature projector.
Using our prototype, that simulates a miniature projection , we visually augment paper documents, giving the user immediate access to additional information and computational tools.
We also show how virtual ink can be managed in single and multi-user environments to aid collaboration and data management.
User evaluation with professional architects indicated promise of our proposed techniques and their potential utility in the paper-intensive domain of architecture.
Such enhancements are not limited to the capture and recording of annotations, but can also be extended to support paper-based command systems.
For example, Anoto  based applications allow users to interact with images of icons printed on the paper to provide computational results.
Alternatively, the PapierCraft system  supports stroke-based commands to allow for active reading.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
A challenge with such systems is that while the pen provides the user with rich and dynamic input capabilities through the creation of ink and command strokes, current digital pen devices have very limited output capabilities.
In its basic form, the user would receive no feedback at all.
To address this issue, most digital pens have been enhanced with various forms of feedback, such as audio , haptic and visual feedback .
However, the visual feedback explored so far is limited to what can be displayed on the pen barrel itself, such as colored LEDs , or small OLED displays .
One potential solution to supporting rich applications on a digital pen is to mount a projector on the digital pen.
To project a visual overlay in the context of a paper document , the projector needs to be aware of its spatial location relative to the paper.
Capturing the 3D location of the pen tip on or above the paper surface would allow the system to display virtual information which is relevant to the existing physical content on the paper, which may have been either printed or hand-written.
A projection pen would thus increase the user's ability to work with functionality that requires visual feedback, such as viewing the results of computations, and overlaying contextual information.
A spatially-aware digital pen projection has yet been explored and will introduce new types of interactions and challenges.
Currently, state-of-the-art miniature projectors are getting smaller, and soon will become small enough to be unobtrusively mounted on a pen.
This also applies to the 3D optical tracking technology.
To explore our vision of the projection pen, before these technologies are readily available, we implement a prototype of this configuration.
In this paper, we present PenLight , our proof-ofconcept system for a digital pen embedded with a spatiallyaware miniature projector.
We present the interaction design space that the PenLight configuration introduces followed by a description of our high fidelity prototype implementation.
Our system is implemented within an architectural application domain, chosen due to the significant use of paper throughout the current practices of the design, construction, and review phases.
PenLight was used to conduct an informal user study with professional architects.
Among several interaction techniques, overlaying building information on top of the blueprint and sharing annotations between remote users was most appreciated.
Lastly, we present an analysis of possible challenges in building the actual setup of the projector pen.
PenLight differs from previous systems in that query results are flexible in size and are projected in context of the paper.
Another group of paper-based research focuses on managing the link between paper and electronic content .
However, these systems explore an indirect link where input and output exist on separate devices.
In contrast, PenLight examines a direct link between input and output.
The type of feedback provided by the digital pen  plays a major role in diversifying the possible applications.
Haptic vibration  and audio feedback  was provided by the first generation of digital pens.
Recently, an 8 by 20 millimeter OLED display was embedded into the barrel of a digital pen  to display the result of the pen function.
This has enabled commercial applications such as the display of a translated word.
However, richer forms of visual feedback, which PenLight provides, have not been previously explored.
PenLight utilizes a peephole display metaphor that has been used in earlier systems .
While some of these previous systems support navigation of virtual content by a fixed size viewing window, PenLight's viewing window dynamically changes, based on the location of the pen relative to the paper.
Yee  and Cao's work  explore pen input combined with a display in a bimanual setting to define input and output areas in the environment or to support a travelling input area.
The PenLight system demonstrates different interaction techniques when the pen input and the display are integrated and used simultaneously.
The main goal of the PenLight system is similar to the goal of some previous systems : to visually augment physical paper to enable virtual functionality.
The DigitalDesk  extends the computer workstation to include the affordances of a real desk such as tactile manipulation.
PenLight takes the opposite approach to extend a physical pen to include the affordances of a workstation to be suitable for lightweight mobile system.
Paper-based systems, systems such as PapierCraft , ButterflyNet , PaperPoint  explored the use of a digital pen to directly interact with physical printouts.
Digital operations presented in these systems capture and manage the annotations made on the document.
With the recent advancement in mini-projector technology, projectors are being embedded into a variety of handheld mobile devices such as cell phones and PDAs .
To our knowledge, no one has previously explored the potential of augmenting digital pen applications with an onboard projector display.
Cao's work  looks into specifying projectable areas in the environment to create interactive information spaces.
Similarly, PenLight explores different implicit information spaces, defined by the contents of the paper, both on and above its surface.
Cao's multi-user scenario  also investigates how a hand-held projector can be used in a collocated multi-user scenario.
PenLight manages pen input between remote users that share the same printout.
In handheld projector systems, the size and the resolution of the display also changes based on the proximity of the projector to the surface.
Cao explores the granularity of visual content  at different distances.
The PenLight system interacts with multiple input layers above the paper surface.
Multi-layer input interaction has previously been explored in devices such as tabletops , tablets  or pure virtual environments .
PenLight also explores the concept of multivalent documents  that consists of multiple abstract layers of distinct but closely coupled content.
This concept is especially prevalent in the application domain that we are exploring.
In architecture, building information modeling  comprises managing multiple data sets  all intimately related to each other as part of a single virtual 3D model.
The surface layer is where the pen tip is in physical contact with the surface .
We highlight two properties of the content of this layer: visibility and context.
Visibility: The visibility of the surface input layer indicates whether or not input within the layer will produce a visible trail of ink.
With a standard physical pen, this input is visible.
However, it may be desirable to provide input on the surface layer, without leaving a trail of ink.
For example, when providing command input, an ink trail which was used for selection is of no use after the menu item is selected .
Also, it may be useful to support invisible ink annotations created on top of the original of a physical image, to avoid undesirable clutter, and to preserve the original.
Context: An important property of digital pens is that they are aware of the content that has been created, and of the pen location on the physical surface .
Thus, input created on the surface layer can be either high level global system commands, or contextual, acting on the data which is in proximity to the input.
The idea behind PenLight is to provide richer visual feedback while interacting with paper.
Our vision of the PenLight system consists of two components that will be available in the immediate future:  a pen sized projector and  a digital pen with 3D optical tracking.
This configuration opens up a unique interaction design space.
We have partitioned the interaction design space into input layers and display layers .
The physical display layer is the layer which physically exists on the paper.
This can consist of a number of different elements.
There may be  printed content, such as a diagram or a 2D building layout,  ink, created by the user, and  user interface elements, such as menus and icons, preprinted onto the paper .
For pen input, we use a Destiny IO2 Bluetooth digital pen .
The digital pen allows the creation of physical ink and high resolution 2D tracking so that the system can store the created pen strokes.
The 2D tracking is accomplished with a camera inside the pen that recognizes its location on the page and the page number, by reading a small high-resolution Anoto pattern which is physically printed on the page.
A pressure-sensitive tip switch on the pen senses when the pen is in contact with the paper.
A wireless Bluetooth connection links the pen with the CPU, so that the pen strokes can be stored virtually, and if desired, displayed, in real time.
Above the physical layer are virtual display layers that can be conveniently described in terms of display elements, and display metaphor.
Display Elements: We consider three categories of display elements which can be projected onto the virtual layer.
Two traditional forms of display elements are the user interface elements, and the user generated data, or in the case of PenLight, ink.
A third form of display element is auxiliary data relevant to the printout stored in other databases, which is not explicitly created by the user with a pen.
Often, only a subset of associated virtual content is transferred to the physical printout during the printing process.
This form of content could be useful for displaying aspects of the data which are not already shown on the physical display layer.
Display Metaphor: There are two metaphors we use for displaying virtual data: content locked on-surface  and content locked in-hand .
In the content locked on-surface metaphor, the peephole reveals a virtual world that is stationary relative to the physical world .
As PenLight is aware of the contents and location of the physical display layer, virtual data is directly overlaid in the context of the physical printout, locked on-surface.
For example, ink annotations made by a remote collaborator could be positioned on top of the content which they are referring to, or virtual content which augments the physical content can be registered with the printed content and be overlaid.
In the content locked in-hand metaphor, imagery is projected without any calibration or transformation.
As this metaphor uses the default projection style, it does not rely on the projector to be spatially aware.
It can be used as an alternative display metaphor when tracking is likely to fail.
Additionally, this metaphor is useful if the user wants to change the position or scale of the content as it moves.
Such content could be displaced, or displayed indirectly.
Currently, miniature projectors are small enough to be embedded into small gadgets such as cell phones  and off-the-shelf digital pens  have internal cameras for 2D tracking.
With today's camera technology, it would actually be possible to acquire the 3D location of the pen using the integrated pen camera, by analyzing the Anoto pattern on the paper, even when it is above the surface.
However, combining these technologies in their current state would not produce a prototype suitable for use.
Our current implementation of PenLight  simulates the unique configuration of a pen sized projector mounted on a digital pen before such hardware and technology is available.
We chose to simulate the virtual surface input layer using a physical transparency, positioned under the pen with the non-dominant hand.
The user can directly stroke on this surface without leaving an ink trail on the physical paper.
For any of the interaction techniques which involve input on the actual surface, the user can choose to input them on this virtual surface input layer.
To project imagery which overlays and augments the physical paper, the 3D location of the pen, relative to the paper, must be known.
We acquire this 3D information by fixing a Polhemus FastTrak 3D magnetic tracker  onto the digital pen.
The tracker is in the shape of a small pen, and senses full 6 degree-of-freedom information.
Fixing a pen shaped tracker to the pen also gives us an initial understanding of how a pen shaped projector fixed to the pen would look and feel.
The above mentioned problems have a close relationship to the management of multiple layers of data and input which we discussed in the design space section above.
We thus chose to implement an architectural application which allows users to query and augment physical architectural sketches, addressing the limitations of the current practices.
Instead of actually projecting from the pen, we use a top mounted projector , which projects downwards onto the paper .
It is mounted 120 cm above the table projecting a maximum area of 90 cm by 66 cm.
The 3D tracker is used to calculate the frustum for the simulated projector, as if it were mounted on the pen.
The simulated location of the miniature projector is 1 cm above and 5 cm away from the pen tip on its front side.
The simulated angle between the pen and the projector is 7, and the field of view angle is 30 with an aspect ratio of 4/3.
This configuration creates a 3.5 cm x 2.5 cm projected image when the pen tip is 5 cm above the display surface .
The actual topmounted projection image projects only into this simulated display region .
In improving the visual feedback provided by digital pens, we believe that PenLight will have several interesting usage scenarios for paper-based interaction.
However, for the purpose of our explorations, we focus our implementation on a single application domain, allowing us to develop a working application supporting specific tasks.
Many of the core concepts will easily generalize to other domains.
The architecture profession has one of the most paper intensive workflows as paper is the common medium to distribute designs among different parties and it represents the actual contract commitment .
We consulted a practicing architect to discuss how paper is frequently used in the architecture domain and the potential practices in architecture for which the PenLight system could be useful.
While paper drawings are ubiquitous in each stage of architecture practice, they have limited capabilities.
In particular:  it is difficult to access additional information related to the printout.
During a discussion between architects and their clients in a meeting room, it is often the case that customers want to see a 3D rendering of the design.
This normally requires a computer nearby and realtime applications to simulate the walkthrough.
The multiple input and display layers which PenLight introduces bring forth new interaction techniques that, in combination, have not been explored in previous digital pen interfaces.
Our system allows users to navigate among different virtual ink and content layers, perform operations on physical and virtual content, extract and display different representations of the printed content, and access functionality through a menu system.
We designed a hierarchical radial menu system which can be used to access the various functionality of the system.
The radial distribution of menu items simplifies its use, since users only need to remember what direction to move.
Users can access the menu system by clicking the barrel button on the digital pen.
This causes the top level of the menu to be projected .
Displaying the menu on the virtual layer addresses one problem with current digital pen menu systems - they cannot be displayed to the user, unless they are preprinted on every page.
Another problem which we want to address is that physical ink marks created from command activations result in undesirable clutter.
Since PenLight has a spatial input layer, we explored using the height information to control the semantic scale of the menu.
When the user lifts the pen above the hover layer, two levels of menu items are shown around the ring, allowing the user to see more items at once .
Although the menu items are bigger, the motor space is smaller , making them difficult to select.
This technique is similar to previously developed multi-scale widgets .
The most basic functionality of digital pens is creating and managing ink.
In PenLight, creating physical ink is not different from sketching in the physical realm with pen and paper.
In addition to the physical ink, PenLight allows users to create and manage virtual ink that users can make use in different functions: tracing, and virtual guides.
An ideal hardware implementation of enabling virtual ink would be to use a mechanical button that would change to a pen tip with no physical ink.
We use a transparency instead, so the user has to select a menu item to enable the virtual ink input when using the transparency.
When enabled, all strokes are added to the virtual ink layers, in the location of the paper which they are created.
By creating the strokes in the virtual surface input layer, the annotations can be added to only the virtual layer.
This allows a user to annotate a blueprint without altering the original document.
Users can trace over both physical and virtual content and then apply the trace data to different spatial locations.
Users can also load existing virtual templates to trace out with physical ink input.
Tracing is different from previous guided sketching projects , as PenLight requires users to rely on limited field of view that changes its resolution and size depending on the location of the input device.
The second "dragging" technique utilizes both display metaphors  in the hover input layer.
Once the menu is activated, both menu and virtual cursor is locked in-hand and menu items cannot be selected because the virtual cursor remains in the center of the menu .
To lock the menu to the surface, the user holds the button down, makes a mark in the appropriate direction, and then releases the button.
If the menu is hierarchical, the next level of the menu would then be displayed and the process is repeated.
This technique would be appropriate if the user only wants to find their desired menu item while the pen is stationary, but then make their selection without having the menu displayed.
The third "pendown" technique is similar to the "dragging" technique, but the marks are made on the surface input layer .
Unlike the previous two techniques this could leave an ink trail from the menu use.
If the user did not want this to occur, the mark could be made on the virtual surface input layer, by using the physical transparency under the input location.
Instead of tracing, virtual guides can be created to aid a physical sketch.
Such grids and guides are widely used in image editing applications, but unavailable when working on physical paper.
To create a geometric guide, the user can select one of the menu items; line, circle, rectangle, or grid.
Instead of entering points that define the geometry, the user can draw a similar shape and the system approximates the selected shape, similar to Arvo's approach .
For example, the user can draw a circle and the system figures out the center point and the radius.
In grid mode, users can draw a rectangle that serves as the unit rectangle shape of the grid.
Once the pen is lifted, the entire virtual layer is covered with a self replicating grid layout.
One of the main benefits of PenLight is being able to present overlaid content.
This can, for example, be an important operation when working with physical blueprints.
In the architecture domain, managing the various aspects of building data in a single 3D model is a recent trend called Building Information Modeling .
The dimension tool is another tool which overlays the result of a computation on the physical workspace.
Using the menu the user can choose to measure a distance, path length, area, or volume.
The user then makes an appropriate stroke on the desired location of the image, and the computation result is overlaid on top of the paper as part of the virtual display layer .
In PenLight, the printed content on the paper is actually only one abstract view of a larger electronic file that is stored in the system.
For example, when a 2D floor plan is printed out on paper, the digital pen could directly store the highly detailed 3D model.
This type of imagery could be displayed, possibly in an empty area of the page, or on a nearby blank surface.
The overlaid content or the original physical content can be copied to another location to be overlaid.
The user enters a copying mode from the menu, and circles an area using the pen to specify a contextual parameter of the image on the surface.
The user then enters a pasting mode from the menu, and the copied content is displayed using the lockedin hand metaphor, and copied when the user clicks the button.
When in 2D section view mode, the user draws a line on the paper to define a cutting surface to extract a 2D section of the current 3D building based on the position and orientation of the line .
The temporary view is locked in-hand and can be dropped on-surface when the pen button is clicked.
The search command allows users to search for an item that exists on the physical display layer.
The user can perform the query in two ways.
They can choose from a list of query objects in the search menu using a virtual cursor, such as sprinklers, or they can directly circle an instance of an object on the printout, such as a power outlet using the pen tip.
Once the query is performed, all instances of the objects are highlighted in the virtual display layer.
Instead of having users performing a linear search , we use the Halo technique to guide the user to instances of the object  .
Users can raise the pen to see a larger portion of the display to navigate to the object, so that the items of interest can be found faster.
By using the spatial input layer of PenLight, users can extract a 3D snap shot of the model.
When choosing this operation, the user can use the location and direction of the pen in reference to the paper to specify the camera location and the viewing vector into the digital model.
Varying the pen height determines what building view is to be captured: the interior view  or exterior view .
As with the section view, the 3D snapshot can then be displaced and locked on-surface nearby .
The most positive response received during these sessions was for overlaying additional building information on top of the blueprint.
The architects felt that this tool would be extremely useful and easy to use.
Furthermore, all three architects also felt that the ability to capture and subsequently display a user's annotations to a second user could be useful, as miscommunications due to the absence of such abilities in current practices end up increasing the time and cost to complete a project.
One of the architects also suggested the support for "consistency checks".
Such functions inform the user of potential problems when a layout is modified, such as inserting a new pipeline.
It would be useful if these consistency checks could be performed at an earlier stage of the design process, taking place on the paper while it is being marked up.
A similar operation can be used to create a 2D walkthrough of the building.
When using this operation, the user can draw a path through and along the floor plan .
When the pen-up event is detected, the system locks the video under the pen , and clicking the barrel button triggers the system to play the video and lock its location on-surface.
As the video is being played, a red marker dynamically moves along the path which indicates the current location of the video .
Overall, participants liked the various interaction techniques that were implemented.
Searching for an item of interest in a large-sized blueprint was mentioned as being "priceless".
However, the participants did not see an immediate benefit of the dynamic measurement computations for their work activity.
Participants also commented on the configuration of the hardware.
One issue discussed was the location of the simulated projector.
Users were satisfied with the location and size of the projected image, and liked the ability to raise the pen to view a larger area of the virtual image.
They especially liked this type of interaction when performing a search to get context.
The system supports illustrative communication between remote collaborators, such as a designer and a fabricator.
We briefly explored this scenario by introducing a second Anoto pen to our system, and printing out a second copy of a floor plan.
When annotations are made by one user with the standard Anoto pen, they can be displayed in real time as virtual ink by the user of the PenLight system .
Annotations from the remote user are displayed in a different color.
We implemented this locally only, but in practice it could be implemented for remote scenarios by connecting the pens over a network.
PenLight was demonstrated to three professional architects to assess the usefulness and the potential of each interaction technique.
The first 15 minutes was used as a demonstration session to provide the participants with an overall understanding of the PenLight system and its functionality.
The next 45 minutes was used as a semi-structured interview.
During this interview, participants were asked to comment on the features of our system, including the applicability of each feature to their own everyday practices.
Our assumption is that pen size projectors will emerge in the near future and high accuracy 3D tracking will be made possible.
However, the stability that we were able to achieve using an overhead projector may not be immediately replicable with a pen-mounted projector.
Here we discuss how the tracking can be improved in hardware components, and in software techniques.
Today's Anoto technology only provides 2D location information when within 0.5 cm of the paper.
However, there are other tracking solutions to improve long range, 3D optical tracking.
Traceable patterns can be added to retrieve camera calibration parameters , similar to ARTags to detect 3D location and orientation.
There is a significant technical barrier to keeping the virtual image stable if it is meant to be displayed with a peephole metaphor.
With a pen, the problem would only be exasperated, due to its high frequency of movement.
There are hardware solutions that can alleviate this issue.
One such technique is image stabilization, which is a feature of many commercial cameras.
In the interaction design space section, "Locked-in-hand" projection  is a solution that we already make use of in our interaction techniques.
Another alternative interaction paradigm is a "discrete display mode" which only projects imagery at discrete intervals, when the pen is in a relatively stable location.
Once the pen begins moving faster than a threshold value, the imagery would fade out.
This introduces a unique interaction style, where the user may be able to see the virtual imagery when viewing it, but have to rely on their persistence of vision  to interact with it.
PenLight simulates a miniature integrated projector, instead of having a separate pen and projector.
This decision was made with mobile usage scenarios in mind, where fewer and more lightweight hardware components are preferred.
Furthermore, there are interactions that are not possible with a separate projector and a pen.
For example, a pen mounted projector introduces a dynamic display area, which is useful in selecting and moving virtually overlaid content.
This large dynamic display area with varying resolution can be used to display different focus+context information .
However, a separate projector configuration, such as a paper-mounted projector or even a removable "pen cap projector", would be interesting to explore and compare to the current configuration.
In this paper, we have initiated the exploration of augmenting digital pens with miniature spatially-aware projectors, and defined and explored the main aspects of a design space that this introduces.
Novel aspects of this design space can be narrowed down to three items.
First, ink no longer has to be represented in a physical form.
Virtual ink benefits users in many ways.
For instance, users can get visual feedback without permanently modifying the physical surface, and virtual strokes can be used to communicate with a remote user.
Second, we showed that the interaction space is not merely locked to the surface input layer but extends to the space above the paper.
Third, a spatially-aware pen and projector allows a user to visibly correlate information that is stored inside the pen or on any connected resource with the document.
As a result, paper is no longer just a static source of data, but it can be used as a dynamic workspace.
In essence, PenLight illuminates information that was hidden due to the static nature of physical paper, just as a traditional penlight lights up unseen parts of a document in the dark.
An obvious line of future work is the development of a working prototype with the projector mounted on the digital pen.
The smallest miniature projectors developed to date are almost adequate for such a prototype.
Significant issues remain to be researched including: providing mobile 3D location sensing; providing projector power; continued miniaturizing of pen computation and mass storage; ergonomic considerations of the pen shape; and, technical issues covered in the discussion section.
The location of the miniature projector must be carefully considered, as it has a number of implications.
The location of the projector on the pen determines the size of the projected image and the pen's center of mass.
Furthermore, the angle of the projector will determine where the tip of the pen is in reference to the projected image.
This is an important consideration for any technique which requires the user to rely on visual persistence to interact with virtual imagery, such as tracing.
The angle of the projector could also determine if any "finger shadows" will exist on the projected image.
One of the participants in our interviews commented that the task may have been easier if the display size was bigger.
Mounting the projector with a wider angle lens or a redirection mirror may assist this issue.
Hand-held projectors provide a dynamic resolution and brightness.
In terms of dynamic resolution, focus will be an issue for a lens based projector.
For this problem, a laser based projector will keep the image in constant focus at different distances.
The dynamic brightness could also be accommodated, using a projector that modulates the brightness based on its distance and rendering software that takes the dynamic dpi into account.
Anoto, Development Guide for Service Enabled by Anoto Functionality.
Apitz, G. and F. Guimbretiere.
CrossY: A CrossingBased Drawing Application.
PaperLink: a technique for hyperlinking from real paper to electronic content.
Fluid sketches: continuous recognition and morphing of simple hand-drawn shapes.
Baudisch, P. and R. Rosenholtz, Halo: a technique for visualizing off-screen objects.
Toolglass and magic lenses: the see-through interface.
Cao, X. and R. Balakrishnan.
Interacting with dynamically defined information spaces using a handheld projector and a pen.
Cao, X., C. Forlines, and R. Balakrishnan, Multi-user interaction using handheld projectors.
Eastman, C., P. Teicholz, R. Sacks, and K. Liston, BIM Handbook: A Guide to Building Information Modeling for Owners, Managers, Designers, Engineers and Contractors.
Erwin, D.E., Further Evidence for Two Components in Visual Persistence.
Fitzmaurice, G.W., Situated information spaces and spatially aware palmtop computers.
Zoom-and-pick: facilitating visual zooming and precision pointing with interactive handheld projectors.
Grossman, T., K. Hinckley, P. Baudisch, M. Agrawala, and R. Balakrishnan.
Hover widgets: using the tracking state to extend the capabilities of pen-operated devices.
Guimbretiere, F. Paper Augmented Digital Documents.
Heikkila, J. and O. Silven.
A Four-step Camera Calibration Procedure with Implicit Image Correction.
Linking and messaging from real paper in the Paper PDA.
A survey of design issues in spatial input.
