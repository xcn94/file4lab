Urban designers and urban planners often conduct site visits prior to a design activity to search for patterns or better understand existing conditions.
We introduce SiteLens, an experimental system and set of techniques for supporting site visits by visualizing relevant virtual data directly in the context of the physical site, which we call situated visualization.
We address alternative visualization representations and techniques for data collection, curation, discovery, comparison, manipulation, and provenance.
A real use scenario is presented and two iterations of evaluation with faculty and students from the Columbia University Graduate School of Architecture, Planning and Preservation provide directions and insight for further investigation.
Urban planners, urban designers, and architects often visit a site prior to a design activity related to the site.
These site visits are used for different purposes by different professionals, but the general goals are to get a sense for the physical site, find patterns, and discover and record new insights about the physical location and its characteristics.
Site visits are similar to ethnographic study in human-computer interaction research, but focus on the physical place, as well as the people in that place.
For example, an urban planner might first create a series of maps about a site that represent its demographics and use.
She may then visit the physical site to view and photograph it, and look for patterns such as congregations of people,
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
On returning to her office, she might record patterns she found onto the maps.
Existing tools for this process include geographic information systems such as ArcGIS, and still and video cameras.
Several issues arise in the current process.
First, there may be aspects of the site that are not visually apparent while visiting the site; for example, air quality and CO levels can be important when considering development, health and environmental justice issues, but cannot be seen with the naked eye.
Second, the map data and the physical site are separate, imposing additional cognitive load on the user to place data in the scene or recall the scene when looking at a map offsite.
Finally, still photos and video may not represent the dynamics of the physical site and environment when trying to understand correlations or associations between the data and the site.
Visualizations are typically shown on a stand-alone display, whether desktop, hand-held, or head-worn.
In the figureground relationship, the physical environment that serves as the ground in which the visualization is presented need have no meaningful relationship to it.
Situated visualizations gain meaning through the combination of the visualization and the relationship it has to its environment.
In the context of site visits, the visualizations become a virtual part of the site.
Note that visualization in augmented reality  is not necessarily situated visualization by our definition.
However, there are several excellent examples that we discuss next.
He freezes the scene, queries both sets for provenance and notices that someone from the community collected the first dataset and the US Environmental Protection Agency  collected the second.
He makes sure that both datasets are visualized differently, and sees that there is a large difference between the EPA data and local data.
He freezes the image again, captures it for later use, and walks to the next street to investigate further.
We draw inspiration from several projects.
The Vidente project  has been investigating visualization of subsurface features such as pipelines and power cables for utility field workers.
Their approach takes geographic data models of these subsurface features and transcodes them for visualization and filtering.
In contrast, we focus on invisible aspects of a site, beyond the built environment, that may not have a natural visual or spatial representation, and on comparing multiple related datasets.
We note that sensed data has become an important topic in the HCI community as new ways of collecting data such as participatory sensing  and mobile sensors  evolve.
Our work complements these systems by exploring alternative ways to visualize and interact with sensed data.
Although we focus on visualization and interaction, data curation is a necessary and integral component of situated visualizations.
As part of this project, we have been collecting and curating a variety of datasets to better understand the tools for collection, aggregation, and distribution.
The red dataset in Figs.
1-2 encodes CO levels we collected with a Lascar EL-USB-CO data logger and a Honeywell GyroDRM, which combines GPS with a gyro-stabilized dead reckoning module, for geocoding.
Custom software combines data logs and converts the output to KML , an XML-based language schema and file format for representing geographic data, maintained by the Open Geospatial Consortium.
KML is used in Google Earth and Google Maps, making it easy to import datasets from these applications into SiteLens.
CO data was also obtained from EPA sites, and additional datasets have been curated from geocoded US census data and single-location environmental sensing stations.
The following use scenario provides a description of the types of interaction and tasks we support, and is followed by explanations of specific elements of our prototype.
John is an urban planner.
He typically looks for patterns in a physical location when he visits a site and today is interested in environmental issues.
He arrives at the corner of 133rd Street and Broadway, an area of interest for future design activities, and takes out his SiteLens.
The SiteLens shows him there are several different datasets in the location, so he filters for environmental data.
He sees two sets of CO data.
He opens the next set and notices that it is displayed fixed to the screen, indicating that it was not collected nearby.
SiteLens has three primary loci of presentation: a screenfixed display in the upper left corner, a screen-fixed, worldoriented map display, and a world-fixed augmented reality display.
Our system considers the nature of the data itself and defaults to displaying it in a locus that is appropriate to the spatial nature of the data.
For instance, our system defaults to display data as screen-fixed if it is beyond the current view or is not inherently spatial.
2a, census data is relevant to the site, but is recorded on a block or superblock scale.
Therefore, we present the data screen-fixed in the upper-left corner.
In contrast, the locally recorded CO data is presented world-fixed because it is displayed in the locations in which it was recorded.
Later, we discuss breaking these boundaries when comparing data.
When mapping a non-physical characteristic such as CO level to properties of a visual mark such as the size or altitude of a sphere, we consider the representation both by itself and in the context of the physical scene.
To explore different representations, we use three different visual types: spheres, cylinders, and smoke.
These three representations were developed in collaboration with our colleagues in urban design and urban planning.
We chose these generic representations as a first cut at virtual representation of physical data because dots  are familiar carto-
For spheres, the parts per million  value is mapped to both continuous altitude and bi-level color.
Higher, red spheres have higher values, while lower, grey spheres have lower values.
For cylinders, ppm is mapped to both length of the cylinder and color.
Taller cylinders have higher values and color mapping is the same as spheres.
For smoke, ppm is mapped to density.
Denser smoke represents higher ppm values.
Data comparison facilities provide a way to validate existing datasets.
If two datasets contain spatial data relevant to a given physical location, they can be compared directly.
However, sometimes data intended to represent a physical location is actually collected remotely.
In this case, we provide a means to spatialize data to match a related dataset.
1d, the red CO dataset was collected in the locations in which it is represented.
However, the green CO dataset, which was collected several miles away, is the closest EPA dataset.
Instead of comparing the red data to the single value representing the green data by default, we let the user spatialize the green data.
We do this in SiteLens by touching the data panel for the EPA data and dragging it to the spatialized red data.
We hypothesize that this makes visual comparison simpler without losing the relevance of the physical context.
As with any medium, additional information about the data being visualized can help the viewer better understand potential issues such as bias or reliability.
For example, a visualization of CO data may be perceived differently depending on whether it was created by a community member or a known industrial polluter.
Our prototype runs on a 1.2lb Sony VAIO VGN-UX390N Ultra Mobile PC with a built-in camera, GlobalSat BT-338 GPS, and InterSense InertiaCube3  inertial orientation tracker.
SiteLens is built on top of Goblin XNA , which supplements Microsoft's XNA infrastructure with AR functionality, including 6DOF optical marker tracking using ARTag .
Each visual representation is a subclass of a data node in our architecture, so we can easily create new visual representations and data mappings.
Spatial data is stored in an octree to provide quick access to location and distance information from a given data node to the current location.
Current location  is gathered through a combination of ARTag fiducials, GPS, and IC3.
Stability of distant objects is increased by combining IC3 orientation and ARTag or GPS location.
Plaisant  argues that evaluations of visualization techniques should incorporate real tasks and field studies.
North  suggests insight as an indicator for validating visualization techniques.
As a first step in evaluating our prototype, we obtained feedback from urban designers and planners in the Columbia University Graduate School of Architecture, Planning and Preservation  through two iterative field studies.
The studies were conducted in the Manhattanville neighborhood of New York City.
In the first study, two colleagues from GSAPP explored the site using the scenario described earlier in this paper.
In the second study, four participants from GSAPP used a revised prototype at the same site and were given a brief post hoc questionnaire, eliciting opinions about visual representations and system use.
In both cases, researchers were present and observed subjects as they used the system.
Additional unstructured discussions with subjects followed both studies.
Moments of insight were observed.
For example, map data alone could not explain why the locally recorded CO levels were higher towards the end of one street; however, visual inspection of that street during the field study revealed that, near where the higher CO levels were recorded, cars were idling as they prepared to enter the highway.
One frustration with the system was that the data was considered stale.
This brought up two issues.
First, while there was a closer spatial association between the site and sensed data, the temporal association was unclear.
Second, there was a desire to have live or dynamic sensing coupled with existing data to "further explore an area or fill in the gaps."
Selection of specific nodes in dense areas of data was still difficult because of overlapping nodes.
Our users felt that capturing combined images of the physical and virtual scene to create a single "real" image was useful for documenting the site visit.
Using the SiteLens prototype was not felt to be significantly harder than using a video or still camera and could be imagined as a common tool.
It was even suggested that SiteLens could be used for an iterative process of data curation, where visualization and sensing are combined with organizational tools to help create new datasets that create a portrait of the site.
Reactions to the different representations were mixed.
Spheres were considered better than cylinders for localizing the data.
In terms of specific data values, participants were initially confused about whether the CO ppm value was mapped to sphere size or height.
Surprisingly, we found that the psychological impact of the smoke was more important than the more accurate localization and value of the other representations.
One participant said "I like the smoke...It's hard to see quantity of things, but... psychologically it helps to represent the idea better."
Another suggested that perhaps "you just need to know bigger or smaller, but not the actual value."
In further discussion, smoke with the option of visualizing spheres was suggested because the initial representation of smoke provided a stronger psychological effect, provoking stronger reactions.
In general, we see the need to provide the user with more control over visual form  and data mapping in the spirit of Chuah et al.
For example, while shadows were considered useful for enhancing the sense of realness and provided distance cues, our design choice for mapping CO concentration to height was not considered obvious.
Participants wanted to try alternate visual representations to explore changing data perception.
In terms of presentation, a difficulty with the screen-fixed display was that, while the data was representative of the site , participants felt that it was insufficiently dynamic.
One stated that the data "doesn't change when I move around, so it feels less important."
Our contributions include a new application space that benefits from AR and visualization techniques, a prototype system incorporating techniques for presenting and interacting with situated visualizations, novel visualizations of CO sensor data, and discussion of early feedback from colleagues in urban design and urban planning.
Based on initial usage, we plan to pursue two areas further.
First, we are interested in increasing the dynamics and symmetry of sensing and visualization by extending the system to live sensor data.
Second, we plan to further explore alternative visual representations for different data types.
In our first iteration of the system, subjects were distracted by the instability of data.
Our combination of data position and sensor fusion in the second iteration significantly stabilized the visualization.
While the actual placement of data was slightly less accurate, the location of data was sufficient for associating with local features of the environment.
Freezing the camera image, when desired, while keeping the overlaid graphics live, supported manipulating the interface and visualization without having to keep SiteLens pointed at the scene being overlaid.
As an extension of this, we found that the on-screen user interface controls were best positioned in the lower left and lower right of the screen and along the edges where the user's thumbs could easily access them.
However, direct manipulation of the visualizations, such as touching them to show metadata,
