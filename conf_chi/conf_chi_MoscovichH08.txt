Touchpad and touchscreen interaction using multiple fingers is emerging as a valuable form of high-degree-of-freedom input.
While bimanual interaction has been extensively studied, touchpad interaction using multiple fingers of the same hand is not yet well understood.
We describe two experiments on user perception and control of multi-touch interaction using one and two hands.
The first experiment addresses how to maintain perceptual-motor compatibility in multi-touch interaction, while the second measures the separability of control of degrees-of- freedom in the hands and fingers.
Results indicate that two-touch interaction using two hands is compatible with control of two points, while twotouch interaction using one hand is compatible with control of a position, orientation, and hand-span.
A slight advantage is found for two hands in separating the control of two positions.
Continuous, coordinated control of multiple degrees-offreedom is common in real-world manipulation tasks.
An artist drawing a brush-stroke on canvas, a chef slicing vegetables, and a surgeon placing sutures all rely on this type of control.
Yet coordinated manipulation of more than two degrees-of-freedom is rare in today's user interfaces, which mostly depend only on the two dimensions of continuous input provided by a mouse or similar device.
However, due to the physical constraints of the human hand, direct-touch interaction on a touchscreen suffers from limited precision, occlusion issues, and limitations on the size and proximity of the display.
Indirect multi-touch input mappings offer a rich design space that can overcome many of these limitations , yet little is known about the human factors that determine their success.
Our research aims at improving the understanding of continuous high degree-of-freedom input using multi-point touchpads.
We discuss concepts and design guidelines for creating effective mappings between fingers and software, and place these ideas in the context of bimanual interaction.
Our experiments uncover how the structure of the degrees of freedom of the hands and fingers and their relationship to the visual nature of the task influence the effectiveness of a mapping between hand measurements and software parameters.
In particular, we show that interaction using one finger on each hand is structured as control of two positions, while interaction using the index finger and thumb of one hand is structured as control of a position, an orientation, and a scale or distance related to the span of the user's hand.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
However, the design space is large, and inappropriate mappings can lead to poor user performance and confusion.
Designers of multi-touch interaction techniques currently rely only on guesswork and intuition in their work.
Most existing multi-touch interaction techniques use the interaction surface as a touchscreen.
The display and motor spaces are aligned, and users interact with interface components by touching their image with their fingers.
This leaves the assignment of fingers to parameters up to the user.
Users are well prepared to make this choice since the interaction is based on the analogy of touching and manipulating physical objects.
Examples of such methods are found in Wu and Balakrishnan's RoomPlanner , the deformation and animation system of Igararashi et al.
A related approach taken by Krueger  and by Malik and Laszlo  places an image of the user's hand on the display.
The scale and position of the hand image is based on a homography between the motor and visual space, so interaction is accomplished using the same physical analogy used in touchscreen systems.
The advantage of these touchscreen techniques is that they are easy to learn and understand.
The interface designer can meet users' expectations by maintaining an analogy to the physical world.
However, this analogy imposes rigid constraints and limitations on possible interactions, which reflect the physical constraints of the hand .
Fingers are wider than many UI widgets common today, making precise selection difficult.
The hands and fingers often obscure the very object the user is manipulating.
On large displays many objects are simply out of reach.
These limitations can be overcome by creating more complex, indirect, mappings between the control and display space .
Perhaps the most successful example of such a mapping is the one between the mouse and the cursor.
This mapping increases both the range and precision of the cursor by allows for clutching and cursor acceleration.
Similar types of indirection have been used to enhance pointing precision on single-point touchscreens , and to increase the user's reach on large displays .
These indirect methods represent powerful tools for highdegree-of-freedom control, but selecting a mapping between the user's fingers and parameters of the software is now up to the interface designer, who has many more options available than in the case of touchscreen interaction.
An appropriate choice is not always obvious, since there is no clear physical analogy.
Even if a clear mapping exists, its effectiveness is difficult to predict, as it is governed by a large number of physiological and cognitive factors.
Some of these factors have been explored in the area of bimanual interaction.
For example, several researchers have noted that in two-handed manipulation using two cursors, users become disoriented when the right-hand cursor crosses to the left of the left-hand cursor .
Balakrishnan and Hinckley have shown that transformation of the hands' relative frames of reference can reduce performance.
It has also been demonstrated that bimanual coupling is affected by visuomotor scale transformations .
On what basis, then, can a designer choose an effective mapping?
We suggest that appropriate mappings can be selected by examining two types of relationships between the degrees-of-freedom of the hands and the control task.
The first relationship is the degree to which the user's physical actions are similar to the visual feedback provided by the system.
Stimulus-response compatibility is a well-studied principle which states that matching properties of a control to properties of a visual stimulus leads to superior performance over a mismatched control.
Worringham and Beringer demonstrated that matching the direction of motion of an input device to the direction of cursor motion in the user's visual field yields shorter reaction times and fewer errors than mismatched input .
This leads to the first question of our study: How can an interface designer ensure the perceptuomotor compatibility of a multi-touch interaction task?
The second relationship we consider is discussed by Jacob et al., who suggest that to select an appropriate input device for a task it is important to match the control structure of the device to the perceptual structure of the task .
In particular, their work discusses the integrality and separability of control dimensions.
A group of dimensions is integral if they vary together, like the coordinates of a pencil-point on a sheet of paper, while separable dimensions vary independently, like the knobs on an Etch-A-Sketch.
Later work by Wang et al.
The structure cannot be inferred; it must be determined empirically.
We argue that interface designers can analyze interaction tasks to determine what parameters would benefit from coordinated, parallel control, and what parameters are better controlled separately .
Such an analysis would allow them to assign param-
However, a question remains: What is the control structure of multi-touch input?
This work addresses these questions regarding the control structure and perceptuomotor compatibility of multi-touch input, and demonstrates how an understanding of these relationships can be applied to the design of an interaction task.
In this investigation we limit our focus to interaction using two fingers on a touchpad.
We believe that two point interaction makes a good starting point for studying the more general problem of multi-touch input.
Furthermore, since using one finger on each hand is essentially two-handed interaction, this choice relates multi-touch interaction to bimanual interaction, which has been extensively studied.
Another reason to focus on two-finger interaction is that it is likely that the major degrees of freedom of the hand can be represented by only the thumb and finger.
A study by Santello et al.
Both components describe the opening and closing of the grasp via flexion of the finger joints and rotation of the thumb.
The motion of fingers is linked both mechanically and neurologically , yielding highly correlated movements.
Thus we expect that use of more than two fingers on the same hand would produce only a slight increase in the number of usable degrees-offreedom.
We leave the study of the affordances of multiple fingers on a touchpad, or different pairs of fingers, to future work.
As finger opposition in grasping behavior requires the application of symmetric forces, we limit this investigation to methods which assign symmetric roles to the two points .
While many bimanual interaction techniques assign asymmetric roles to the two hands based on Guiard's kinematic chain model , the model's applicability to finger control is uncertain.
The work of Malik  proposes that asymmetric roles may be assigned to one hand by assigning one task to the position of the hand, and a dependent task to the relative position of a finger.
We expect that the difference between the kinematics of opposing fingers on one hand and the kinematics of two separate hands would cause their actions to be controlled and perceived differently.
For example, fingers of the same hand inherit that hand's motion, so their movement may be perceived as being relative to the hand's frame of reference.
The motion of two hands is more likely to be controlled relative to a global reference frame, or relative to each other.
Similarly, we would expect that the motion of fingers on separate hands may be more easily uncoupled than that of fingers on the same hand, while the motion of one hand's fingers may be more easily coordinated than that of fingers on opposing hands.
An abstract concept such as coordination is difficult to measure.
However, a number of metrics have been proposed in the literature as easy-to-interpret correlates of coordination.
We employ two such metrics to assess the degree to which users can coordinate multiple degrees of freedom.
The first is parallelism, which was proposed by Balakrishnan and Hinckley .
The metric measures how well two hands  work together to simultaneously reduce tracking error.
A mean parallelism of 0 results from sequential use of the hands, while a value of 1 results from both hands simultaneously reducing their fractional error by the same amount.
The second measure, proposed by Zhai and Milgram , is efficiency.
It relates the actual distance d users traverse through parameter space to the length s of the shortest path.
It assumes that any extra work users perform is due to imperfect coordination.
This extra work, or inefficiency is defined as a fraction of the minimum necessary work: /s.
Perfect coordination yields zero inefficiency, while less coordinated action has a greater inefficiency.
The goal of this experiment is to establish mappings that ensure compatibility between the user's finger movements and the visual feedback presented by the system.
In particular we examine mappings for an object transportation and orientation task.
We use a two-point object manipulation technique known as two handed "stretchies" that has appeared frequently in the literature .
A one-handed equivalent has also been described .
The technique allows a user to simultaneously translate, rotate, and scale an object.
In the case of two fingers on a touchpad, each contact point is mapped to a fixed position in the object's coordinate frame.
The transformation of the line segment connecting the user's fingers is applied to the manipulated object.
Change in its length scales the object, change in its angle rotates the object, and change in its position moves the object.
We present participants with a segment tracking task similar to one previously used to study bimanual parallelism  .
Participants are asked to pursue a short line segment as it randomly moves and rotates on the screen by controlling a "match-segment" so that its position and orientation match the target-segment as closely as possible.
This continuous pursuit task forces participants to coordinate their control of parameters as much as possible, allowing us to measure their coordination ability.
Participants manipulate the control-segment  as though it were a stripe drawn on a transparent sheet of rubber.
The instantaneous parallelism for two points is then min / max  if both fractional reductions are positive, and 0 otherwise.
It is important to note that these points are never displayed in order to ensure that the task is seen as manipulation of a single object.
Showing these points could disrupt the task's visual integration, an important factor in bimanual coordination .
Participants can manipulate the sheet using either one finger on each hand , or the thumb and index finger of their right hand .
As discussed above, the movements of the fingers on one hand are highly correlated.
Therefore we hypothesize the following: H1 The unimanual manipulation condition will exhibit greater parallelism than the bimanual condition.
The manipulation is performed under two visual conditions: aligned and rotated.
In the aligned condition, the controlsegment is drawn so that its endpoints are aligned with the positions controlled by the user's fingers .
For the rotated condition the segment is drawn rotated 90 about the center of the aligned segment .
In both visual conditions the motor control task is identical.
Any finger motion would result in the same visual transformation under both conditions.
However, we predict that alignment or lack of alignment with the user's fingers will have different effects in the one and two-handed conditions.
If the task is compatible with control of position, orientation, and scale, then the alignment of the segment should have no effect on performance.
We predict that this is the case in unimanual multi-touch interaction: Motor rotation is compatible with visual rotation, and motor extension of the fingers is compatible visual expansion.
However, if the task is compatible with control of two points, then only the aligned condition will maintain perceptuomotor compatibility.
In the rotated case, moving the left finger up will result in the leftmost endpoint moving to the right.
Attempting to control points instead of orientation and scale makes the task more difficult.
The match-segment was maintained at a length of 3 cm in touchpad space.
The center of the segment was constrained to a 8.6 x 12.6 cm region of the touchpad, the angle of the aligned segment was constrained to lie between 0 and 86 from the horizontal.
This range is accessible within the joint limits of both the bimanual and unimanual condition, and ensures that the left and right endpoints never cross.
The path of the center of the segment was interpolated using a cubic interpolant through random points in the constrained region of the touchpad at a rate of 5 seconds per point.
The angle was interpolated through random angles in the constrained range at a rate of 6.25 seconds per angle.
The match-segment was drawn with a gray seven pixel wide stroke with 14 pixel long tick marks at its endpoints.
The control-segment was drawn with a two pixel wide black stroke.
If more or less than two fingers were detected on the touchpad, tracking temporarily stopped, and the controlsegment turned red to alert the subject.
In the aligned condition, the control segment was drawn so that its endpoints corresponded to the mapped positions of the contact points on the touchpad.
In the rotated condition, both match- and control-segments were drawn rotated 90 about the center of the corresponding aligned segment.
Twelve right handed university students  participated in Experiment One.
All were regular computer users, but had no previous experience using a multi-point touchpad.
They spent approximately 25 minutes performing the task and filling out a short questionnaire.
Participants were paid U.S. $5.
A within-subject full factorial design was used.
The independent variables were the hand condition , and the visual presentation .
Participants completed four 30 second tracking trials under each of the four condition for a total 8 minutes of tracking.
The first trial in each condition was for practice.
For the later three data-collection trials participants were asked to track the match-segment as closely as possible.
The order of presentation of the four conditions was balanced according to a Latin square.
The series of transformations used to generate the animation path for the match-segment was identical under all conditions.
Dependent variables were mean error and mean parallelism in each 30 second trial.
Mean error was calculated as the mean sum of the distances between the endpoints of the control-segment and the endpoints of the match-segment.
Note that this error is preserved under rotation, so we can use the segment endpoints in the rotated condition as well.
Parallelism was calculated as a ratio of error-reduction as described above.
As the touchpad sampling rate is somewhat variable, the data was resampled at 50 Hz.
Segments where the user had too few or too many fingers in contact with the touchpad for more than 0.5 seconds were removed from the analysis, while shorter segments were linearly interpolated.
Participants interacted with the system using a FingerWorks iGesture Pad .
The touchpad measures 15.5 x 11.5 cm, and tracks finger contacts at approximately 100 Hz.
The system made an absolute mapping between points on the touchpad and a 1024 x 746 pixel region on the screen at a control/display ratio of 0.55.
The display was placed approximately 45 cm from the subject.
For the unimanual condition the touchpad was placed in front of or in front and slightly to the right of the subject's right shoulder, while in the bimanual condition it was placed directly in front of the subject and screen.
The display was updated at over 100 frames per second.
Another said that this condition was the most difficult, and that she "found it hard for the two sides of my body to work together," and difficult to "fix my sight on the two invisible spots on the screen where my fingers `were'."
No such comments were made about the unimanual rotated condition.
We hypothesize that the small increase in error in the unimanual condition may be due to the fact that changing the span of the hand is an oriented expansion, rather than a uniform one.
The interaction between hand span and orientation is important in grasping behavior.
This relation to grasping was visible in one variation of our pilot study.
When the system ignored the inter-finger distance and kept the segment length constant, we observed that participants brought their fingers much closer together in the rotated condition than in the aligned condition, as if they were attempting to hold the segment between their fingers.
In the aligned conditions this represented a 25% increase in error.
This appears to suggest that unimanual manipulation may be better suited for manipulation tasks that requires a high degree of coordination than bimanual manipulation.
However, the fitness of one or two handed multi-touch techniques for a given task may have more to do with the structure and nature of the manipulation task and the particular degrees of freedom that require coordination.
Our next experiment explores this issue further.
Results for parallelism can be seen in Figure 2 .
An analysis of variance revealed a significant main effect for hand condition  but no effect for, or interaction with, visual presentation.
This supports hypothesis H1, that one hand exhibits more parallel control than two.
However, the difference is small, and the overall parallelism observed is low.
The low parallelism value may indicate that an equal-rate reduction in percent error is not a strategy employed by our motor control system.
We will explore this issue further in Experiment Two.
Results for tracking error are shown in Figure 2 .
While this does not meet our H2 prediction that rotating the visual presentation will have no effect on the unimanual condition, the relative magnitudes of the changes in error do provide support for our hypothesis.
In the unimanual condition, rotating the segment increased error by 28%, while in the bimanual condition it increased error by 75%.
Thus, it is reasonable to surmise that, to a first approximation, control of a position, orientation, and span is perceptually compatible with unimanual manipulation, but is not compatible with bimanual manipulation in the absence of a clear finger-to-point correspondence.
When such a correspondence exists, bimanual manipulation is compatible with the control of two positions.
Participant feedback appears to corroborate this view.
Participants were asked if they found any aspect of the task particularly difficult.
The goal of this experiment is to assess the structure of one and two handed multi-touch interaction.
In particular, we propose that in an object manipulation task, two hands are better able to isolate the control of two individual points than one hand.
Furthermore, in the light of Experiment One, we expect that one hand would be better able to coordinate control of an object's position, orientation, and size.
Participants are presented with an object alignment task.
Using the same two-point "stretchies" technique used in Experiment One, participants used two fingers on one or two hands to move, orient, and scale a control-shape so that it is aligned with a target-shape .
The experiment uses two types of shapes.
The first is a thin, pointed shape with two prominent, sharp "features" at opposite ends .
We believe that a clear alignment strategy for this shape is to align each prominent feature on the controlshape with the corresponding feature on the target-shape.
We align the mapped position of the user's fingers on the touchpad so that they lie directly on the two feature points.
This ensures that moving a single finger will only move its corresponding point, while leaving the opposite feature point fixed.
Since we expect that separate control of two points is easier with two hands than one, we predict the following: H3 Bimanual alignment of the pointed shape will be quicker than unimanual alignment.
The center of the line segment connecting the target positions of the subject's fingers was randomly placed within a 3 x 2.5 cm rectangle in the center of the touchpad.
The segment was oriented at a random angle between 0 and 80 from the horizontal, and was assigned a random length between 2.5 and 4 cm.
The end position of each trial constituted the start position for the next trial.
Twelve right handed university students  participated in Experiment Two.
All were regular computer users, but had no previous experience using a multi-point touchpad.
They spent approximately 30 minutes performing the task and filling out a short questionnaire.
Participants were paid U.S. $5.
Visual display and mappings for the alignment task.
Subjects were asked to align a control-shape to a congruent target-shape using two fingers on one and two hands.
For each hand condition users manipulated both a round, featureless shape  and a thin, pointed shape, whose key features were aligned with the subjects' fingers .
H4 Bimanual alignment of the pointed shape will be more efficient than unimanual alignment .
The second shape is a smooth, round shape with no obvious features .
Lacking such features a reasonable alignment strategy is to attempt to align the entire boundary of the shape.
We expect that this strategy would benefit from a high degree of coordination between the adjusted dimensions, since separately adjusting the scale, position, or orientation, would throw-off previous adjustments.
Thus, we make the following hypotheses: H5 Unimanual alignment of the round shape will be quicker than bimanual alignment.
H6 Unimanual alignment of the round shape will be more efficient than bimanual alignment.
A within-subject full factorial design was used.
The independent variables were the hand condition , and the shape .
Participants completed three sets of 20 alignment trials under each of the four conditions.
The first set of trials in each condition was considered practice, as was the initial trial in each set.
In the later two data collection trials participants were asked to work as fast as possible.
The order of presentation of the four conditions was balanced according to a Latin square.
The ordered series of transformations used to generate the target shapes was identical under all conditions.
Dependent variables were trial completion time and inefficiency .
Inefficiency was measured with respect to the path traveled by the two control points.
Due to tracking discontinuities  3% of trials were discounted.
A sharp drop in the trial-timing distribution occurred at about 10 seconds.
Trials longer than 10 seconds  were removed as outliers.
The hardware and display setup were identical to those in Experiment One.
The control-shape was drawn in a translucent violet, keeping the target-shape  always visible.
If more or fewer than two fingers were in contact with the touchpad, tracking was temporarily stopped, and a red border was drawn about the control-shape to alert the subject.
When every point on the boundary of the control-shape was within 1 mm  of a point on the boundary of the target-shape, the control shape was considered aligned and was drawn in green.
Maintaining alignment for 0.5 seconds ended the trial.
To avoid a speed/accuracy trade-off, participants had to complete all trials successfully.
The 1 mm upper bound on error was selected, via a pilot study, as the lowest error participants could consistently achieve.
In the bimanual condition users aligned the pointed shape significantly faster than the round shape .
No such difference was found in the unimanual condition.
Furthermore, users aligned the pointed shape significantly faster using two hands than using one .
We interpret this to mean that users are better able to separate the control of two points when using fingers on opposing hands than when using fingers of the same hand.
Notably, no significant difference between hand conditions was found for the round shape.
This contradicts hypothesis H5 that one hand would perform faster for this shape.
This could be interpreted in two ways.
First, it is possible that the strategy participants used for aligning the round shape did not entail the high degree of coordination we expected.
Alternatively, it is possible that two hands can coordinate the necessary degrees of freedom just as well as one.
We look at the efficiency data to help resolve this issue.
Inefficiency for experiment two is shown in Figure 4.
When manipulating the pointed shape, two hands were significantly more efficient than one .
Two hands were also more efficient when manipulating the pointed shape than when manipulating the round shape .
No difference was found between one and two hands on the round shape.
This confirms H4, but contradicts H6.
That is, two hands were more efficient than one for the task requiring separation, but one hand was not more efficient for the task requiring coordination.
Due to the significant positive correlation between inefficiency and completion times  we conclude that shorter completions times are due to greater efficiency.
While it is not surprising that two hands show a greater amount of coordination for the separable task, the results for the integral task appear to contradict Experiment One.
In the first experiment, one hand displayed slightly more parallelism than two for a task that required a high degree of coordination.
In the second experiment, no such difference was found.
This may be attributed to several differences between the two experiments.
First, Experiment One involved moving the center of the control-shape greater distances than in Experiment Two.
This would result in greater parallelism for fingers with a close mechanical link.
Furthermore, while in the first experiment, both fingers had to reduce absolute error at an approximately equal rate, the setup of the second experiment yielded a different start-to-goal distance for each finger.
This may favor greater parallelism in a separable control structure.
It should also be noted that while both parallelism and efficiency are intended as measures of coordination, they do not measure precisely the same thing.
However, analysis of the parallelism in Experiment Two revealed no difference in parallelism for the round shape, and more parallelism in the bimanual condition for the pointed shape .
One- and two-handed multi-touch input mappings are not interchangeable.
Each has advantages over the other, and can be more effective at particular tasks.
Our experimental results indicate that while a kinematic analysis of the hands and fingers can help predict the control and perception of manual action, it cannot fully explain observed manipulation behavior.
The expected behaviour is modified by cognitive aspects of motor control that can overcome structural constraints.
Gaining a sound understanding of these aspects would require further empirical research.
Nevertheless, our experiments produced a number of clear conclusions that will allow interaction designers to select appropriate multitouch input mappings.
Our studies show that unimanual multi-touch manipulation is compatible with a visual rotation task, even when lacking a clear point correspondence between fingers and object.
Specifically, transporting, rotating, and stretching an object is compatible with positioning and orienting the hand, and adjusting the span of the fingers.
By contrast, two handed multi-touch manipulation is only compatible with an object manipulation task when there is a clear correspondence between the fingers and the manipulated control points.
The absence of such correspondence results in confusion and reduced performance.
This has a number of design implications.
It indicates that control of orientations may be performed with one hand with less reliance on visual feedback.
Such control may be useful for the design of dials and other rotational widgets.
This result also suggests that while applying a gain function to object rotation  could be beneficial for a one-handed interaction technique, it may degrade two-handed performance by breaking the compatibility of the finger-to-object mapping.
Another clear result is that two hands perform better than one at tasks that require separate control of two points.
This is the case even when the controlled points are within the range of motion of one hand's fingers.
Examples of such tasks include window manipulation, marquee selection, image cropping, or control of separate objects.
Since these task show a clear correspondence between fingers and control points, they are also perceptually compatible with bimanual control.
A number of open questions still remain.
The cause of the small increase in error in the rotated unimanual condition is not yet clear.
While we hypothesize that it is caused by an interaction between the orientation and span components of the manipulation, our experiment did not separate these two components.
Further investigation of this issue may provide designers with a better model of user perception of one-handed multi-touch interaction.
It is also not yet clear under what conditions one hand can coordinate the degreesof-freedom of an object better than two hands can.
Our experiments hint that the answer may depend on the scale or symmetry of the action.
From the perspective of an interface designer, however, the question may not be of much practical value.
Balakrishnan, R. and Hinckley, K. Symmetric bimanual interaction.
Benko, H., Wilson, A. D., and Baudisch, P. Precise selection techniques for multi-touch screens.
Buxton, W., Hill, R., and Rowley, P. Issues and techniques in touch-sensitive tablet input.
Cutler, L. D., Frolich, B., and Hanrahan, P. Two-handed direct manipulation on the responsive workbench.
Dietz, P. and Leigh, D. Diamondtouch: a multi-user touch technology.
Forlines, C. and Shen, C. Dtlens: multi-user tabletop spatial data exploration.
Forlines, C., Vogel, D., and Balakrishnan, R. Hybridpointing: Fluid switching between absolute and relative pointing with a direct input device.
Gingold, Y., Davidson, P., Han, J., and Zorin, D. A direct texture placement and editing interface.
Guiard, Y. Asymmetric division of labor in human skilled bimanual action: The kinematic chain as a model.
Hager-Ross, C. and Schieber, M. Quantifying the independence of human finger movements: Comparisons of digits, hands, and movement frequencies.
Weigelt, C. and de Oliveira, S. C. Visuomotor transformations affect bimanual coupling.
Wilson, A. D. Touchlight: An imaging touch screen and display for gesture-based interaction.
International Conference on Multimodal Interfaces, 2004.
Wilson, A. D. Flowmouse: A computer vision-based pointing and gesture input device.
Worringham, C. J. and Beringer, D. B. Directional stimulus-response compatibility: A test of three alternative principles.
Wu, M. and Balakrishnan, R. Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays.
Wu, M., Shen, C., Ryall, K., Forlines, C., and Balakrishnan, R. Gesture registration, relaxation, and reuse for multi-point direct-touch surfaces.
In First IEEE International Workshop on Horizontal Interactive Human-Computer Systems, TableTop 2006, 2006.
Zhai, S. and Milgram, P. Quantifying coordination in multiple DOF movement and its application to evaluating 6 DOF input devices.
