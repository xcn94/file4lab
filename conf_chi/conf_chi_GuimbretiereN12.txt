We describe a mouseless, near-surface version of the Bimanual Marking Menu system.
To activate the menu system, users create a pinch gesture with either their index or middle finger to initiate a left click or right click.
Then they mark in the 3D space near the interactive area.
We demonstrate how the system can be implemented using a commodity range camera such as the Microsoft Kinect, and report on several designs of the 3D marking system.
Like the multi-touch marking menu, our system offers a large number of accessible commands.
Since it does not rely on contact points to operate, our system leaves the nondominant hand available for other multi-touch interactions.
With the wide availability of multi-touch surfaces, we have seen a revival of gesture-based interfaces based on this technology .
The two prevalent tracking technologies  are focusing on contact interactions.
Although optical tracking technology can be modified to allow for interactions above the surface , interactions in free space may cause fatigue during long periods of use.
Following the example of hover interfaces in pen-computing , we are exploring nearsurface interactions, which take place a couple of inches above the surface.
Because these interactions can be performed with the users' arms resting on the surface, they are better adapted to long periods of use.
To explore this design space, we built an indirect tracking pad using a Microsoft Kinect, a commodity depth of field camera scanning a hand through a piece of acrylic .
Using this system, we implemented a mouseless, near surface version of the Bimanual Marking Menu technique .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
As an invocation mechanism, we extended Wilson's pinching interaction  to take into account depth data and detect which finger is creating the pinch.
This lets us simulate multi-button mouse interactions.
We then explored the possibility of significantly increasing the number of markings by allowing users to mark in the 3D space just above the interaction surface.
In the layered setting, users are marking on three different levels that are each parallel to the resting surface.
In the spherical setting, an approach similar to C3  and Grossman et al.
In the hemispherical setting, users are also marking in 3D but markings are flat or directed upward.
The 3 designs have a similar number of available markings  but different pros and cons we report on.
Together, these extensions offer access to a very large number commands without introducing any devices or contact points for the non-dominant hand.
This leaves this hand free to participate in other gesture-based activities.
Our work draws on prior extensions of the original Marking Menu system .
Several systems have demonstrated the use of marking menu in 3D to interact in virtual environments , to control devices from a distance , and interact with Volumetric displays .
While these approaches are considering free space interaction, we are studying near surface interaction for which both arms are resting on the interactive surface.
Further with the exception of Lenman et al.
Odel's bimanual marking menu   is an extension of the marking menu  in which the marking is performed by the non-dominant hand.
This setting makes it a very efficient bimanual technique as shown by Chen et al.
The original implementation relies on a simple mouse for marking.
This makes BMM somewhat unpractical for table-top surfaces because it forces users to switch between the mouse and bare handed interactions.
One solution might be to use the SDMouse proposed by Matejka et al.
Another alternative might be to use Lepinski et al.
Our approach is an alternative solution to provide a large number of marks.
It does not conflict with other multi-touch gestures, can be extended to parameter entry in 3D space and relies on a more relaxed posture for the marking hand.
To activate the menu system, the user performs a pinch gesture with his or her non-dominant hand.
The user then moves the hand towards the direction of the desired menu command, and finally releases the pinch to finish marking.
Our system includes a dead-zone of about 30mm, and feedback is provided after a 500ms timeout.
A typical gesture is 45 mm long.
We initially expected that implementing the pinching interaction with the Kinect would be fairly simple since the hand would appear much closer than any other object visible through the window.
Unfortunately, the complex shape of a pinching hand creates a set of shadows in the tracking pattern .
This in turn creates "holes" in the tracking data returned by the camera which makes simple 3D thresholding unreliable.
Upon receiving the depth data from the Kinect, our system uses PrimeSense NITE library to detect a hand entering the frame.
It then limits further processing to a 200 pixel wide bounding box around the hand.
Next, we apply Felzenszwalb et al.
If a hole is detected, we proceed to compute the hole's plane by using a least square plane-fitting on the points at the border of the hole.
To normalize the data, we then re-project the point cloud so that the plane's normal becomes the vertical axis.
Our setting, shown in Figure 1, is based on a Kinect placed under a table that tracks the non-dominant hand above the table through a transparent window.
To the right of the window is a Wacom tablet used to track pen interactions performed by the dominant hand.
While the system described here could be used to mark with the dominant hand, BMM is very well adapted to the capabilities of our Kinect-based prototype.
The Kinect is a somewhat lowresolution sensor .
As such, it seems best to focus on marking interactions performed by the non-dominant hand, while leaving touch or pen interactions for the dominant hand.
We replace the multi-button mouse by combining Wilson's pinching system  with depth data provided by the Kinect.
To simulate a multi-button mouse, we use the 3D information provided by the Kinect to detect which finger is forming a pinch with the thumb .
This is possible because, while the thumb is able to move in front of each finger, each finger has relatively little leeway to move up or down.
These parameters are used as features to infer which finger is forming the pinch using a Support Vector Machine multiclass classifier .
We observed 93% recognition rate for the index pinch and 89% for the middle finger.
We also considered pinching with the ring and pinky, but the recognition rate was too low and pinching with the pinky proved uncomfortable.
Since we receive 3D information from the camera, we can track 3D gestures.
We focused on near surface interactions for which the forearms can comfortably rest on the table during marking.
Importantly, the hand can still perform small vertical movements in this context.
We considered several different approaches for marking in near space.
The layered design was inspired by hover interactions .
In that case, a different set of menus is called depending on the height at which the marking is performed .
Three levels  offered a good compromise between the size of hand movements and robust detection.
Assuming a detection system differentiating between two pinch types , this allowed us to access 48 commands with a single mark .
Another approach is to consider marking in 3D instead of 2D .
In this spherical approach, one can mark in one of 26 directions accessible by combining compass directions with up and down gestures .
When differentiating between two pinch types, users can access up to 52 commands in a single marking.
Noting that marking downward might be awkward, we considered a hemi-spherical scheme, in which marking is performed either flat, at a shallow upward angle , or at a steeper upward angle .
Assuming two pinch types, this yields 48 commands in a single mark.
For all layouts, the number of accessible commands can be significantly increased by considering multi-marks hierarchical menus .
While the Kinect served well as an inexpensive prototyping tool, its main limitation is its resolution.
This curtails reliability in detecting the pinch gesture.
Based on an analysis of video footage, we discovered that the system detects a pinch earlier than perceived by users.
This leads to longer stroke marking time, but may not influence total performance time since the extra marking time occurs during the preparation phase.
Another important problem is the default latency introduced by the processing time of the Kinect, which we estimate at about 150ms.
At present, this limits the overall response time of our interface, although a more powerful image processing system would address this concern.
The placement of the camera is very important.
In our implementation we had to place the Kinect below and to the left of the tracking area to allow for a clear view of the pinch gesture when the hand was in a natural resting position .
This setting also limited glare problems observed when the Kinect was placed below the mid-point of the window in the table.
We believe that a custom designed optical system would allow us to address most of the occlusions problem we observed.
Among the 3 marking solutions described above, the layered approach  was found to be the most reliable in informal testing.
As expected, the spherical approach was awkward with respect to downward markings, since one has to remember to raise the hand above the surface before marking down.
The hemispherical design was unreliable since it is difficult to gauge the proper angle of a mark.
Instead one relies on the height of the hand at the end of the mark.
This approach negates the scale invariance which is key to Marking Menu 
As a next step, we are planning to conduct an empirical evaluation of our system to compare its overall performance with systems such as the Multi-Touch Marking Menu .
We are also planning to explore how it can be extended to implement versions of FlowMenu  and control menu  allowing the non-dominant hand to control one parameter with 6 degrees of freedom after completing command selection.
We presented an extension of the Bimanual Marking Menu in which users mark in 3D using a pinch gesture of their non-dominant hand.
We implemented our system using an off-the-shelf Kinect camera and demonstrated that we can reliably detect two different pinch types and up to 52 different markings.
Our preliminary findings indicate that assuming a faster camera, the system has the potential to be a viable option for selecting a large number of commands on a multi-touch surface.
Given these characteristics, our approach can aid in the implementation of complex applications on multi-touch surfaces.
Comparing free hand menu techniques for distant displays using linear, marking and finger-count menus.
Benko, H. and A.D. Wilson, DepthTouch: Using DepthSensing Camera to Enable Freehand Interactions On and Above the Interactive Surface, Microsoft Research, MSR-TR-2009-23, 2009.
Chen, N., F. Guimbretiere, and C. Loeckenhoff, Relative Role of Merging and Two Handed Operation on Command Selection Speed.
International Journal of Human-Computer Studies, 2008.
Hand distinction for multi-touch tabletop interaction.
International Journal of Computer Vision, 2004.
Evaluation of the Command and Control Cube.
Grossman, T., K. Hinckley, P. Baudisch, M. Agrawala, and R. Balakrishnan.
Hover widgets: using the tracking state to extend the capabilities of pen-operated devices.
Grossman, T. and R. Balakrishnan.
Collaborative interaction with volumetric displays.
Guimbretiere, F. and T. Winograd.
FlowMenu: combining command, text, and data entry.
Kurtenbach, G., The Design and Evaluation of Marking Menus.
PhD thesis, University of Toronto, 1993.
Lenman, S., L. Bretzner, and B. Thuresson.
Using marking menus to develop command sets for computer vision based hand gesture interfaces.
Lepinski, G.J., T. Grossman, and G. Fitzmaurice.
The design and evaluation of multitouch marking menus.
Loclair, C., S. Gustafson, and P. Baudisch.
PinchWatch: A Wearable Device for One-Handed Microinteractions.
Proceedings of MobileHCI'10, Workshop on Ensembles of On-Body Devices, pp.
The design and evaluation of multi-finger mouse emulation techniques.
Toolglasses, Marking Menus, and Hotkeys: A Comparison of One and Two-Handed Command Selection Techniques.
Pook, S., E. Lecolinet, G. Vaysseix, and E. Barillot.
Control menus: excecution and control in a single interactor.
Proceedings of CHI'00 Extended Abstracts, pp.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y. Altun.
Support vector machine learning for interdependent and structured output spaces.
Vijayanandh, R. and G. Balakrishnan.
Human face detection using color spaces and region property measures.
Detecting and leveraging finger orientation for interaction with directtouch surfaces.
Empirical evaluation for finger input properties in multi-touch interaction.
Robust computer vision-based detection of pinching for one and two-handed gesture input.
Userdefined gestures for surface computing.
Zhao, S. and R. Balakrishnan.
Simple vs. compound mark hierarchical marking menus.
