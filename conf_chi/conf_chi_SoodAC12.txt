Social contracts and normative behaviors, however, are unique to specific socio-technical systems.
What is considered inappropriate in a given context is both site and community specific.
On many sites, community managers are primarily responsible for the task of removing inappropriate content.
However, the flood of user-generated content on many sites quickly overwhelms community managers' ability to effectively manage it.
The detection of negative content of malicious intent  in forums and comment streams is a challenging and nuanced problem .
Recent work in machine learning and natural language processing has approached this task with varying degrees of success: with maximal f-measures of 0.298 for detection of harassment on Slashdot and 0.313 on MySpace from one study  and a maximal f-measure of 0.5038 for detection of personal insults on another .1 Given the recent attention to the complex and sometimes grave consequences of cyberbullying , the ability to recognize and potentially mitigate profanity and other forms of harmful negativity in user-generated content is more important than ever .
Compared to the challenges of detecting malicious content or spam, detection and removal of profanity is often thought to be an easier task.
Most current approaches to profanity detection check new content against large lists of profane terms.
However, these systems are flawed in at least two major ways.
First, static term-lists quickly lose currency and are relatively easy to circumvent.
Users often disguise or partially censor profanity by replacing one or more letters with punctuation marks .
Thus, these systems face issues of recall; they are unable to catch most cases of profanity.
As user-generated Web content increases, the amount of inappropriate and/or objectionable content also grows.
Several scholarly communities are addressing how to detect and manage such content: research in computer vision focuses on detection of inappropriate images, natural language processing technology has advanced to recognize insults.
However, profanity detection systems remain flawed.
Current list-based profanity detection systems have two limitations.
First, they are easy to circumvent and easily become stale-that is, they cannot adapt to misspellings, abbreviations, and the fast pace of profane slang evolution.
Secondly, they offer a one-size fits all solution; they typically do not accommodate domain, community and context specific needs.
However, social settings have their own normative behaviors-what is deemed acceptable in one community may not be in another.
In this paper, through analysis of comments from a social news site, we provide evidence that current systems are performing poorly and evaluate the cases on which they fail.
We then address community differences regarding creation/tolerance of profanity and suggest a shift to more contextually nuanced profanity detection systems.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
After all, what constitutes profanity differs greatly based on the specific community and topic at hand.
For example, in a forum about dog breeding, "bitch" is a term of art that refers to a female dog, while in many other contexts it is a profane term.
Furthermore, sites for children have a drastically different tolerance of profanity than those for adults.
In this paper we make three primary contributions to research on profanity detection.
First, we address the state of current list-based profanity detection systems.
In what cases do they fail?
Secondly, under the assumption that a major oversight in these systems is a lack of tailoring for specific communities, we examine how profanity use differs between communities.
Is profanity used more or less in some communities?
Do certain communities use profanity in different ways?
And finally, we explore the social context of profanity use in different topical communities.
How might specific communities receive profanity differently?
In the sections that follow, we examine these questions through analysis of a data set of comments from a social news site.
Because of these misunderstandings, perhaps, comparatively little research has focused on detecting inappropriate text in user-generated content systems.
As mentioned above, two groups have built systems to detect insults and harassment in online forums  and another has focused on cyberbullying of teens , but even fewer have addressed the identification of profanity.
Due to the target audience being children, some have analyzed the content of video game web sites and video games themselves to verify that presented content meets ratings standards .
However, work in this area does not generally strive for automated analysis.
Advancing our ability to detect and remove profanity could have several significant, positive social consequences.
The growth of collaborative information products such as Wikipedia, Yahoo!
Answers, and Stack Overflow rely on the provision of interaction environments that are supportive, productive, and meet the specific needs of their user communities.
Open-source software projects also rely on email lists and forums to support the necessary community building, coordination, and decision-making processes.
No automated system, by itself, can appropriately filter and manage ongoing discourse and interaction so that it meets the needs of a particular topic, domain, or user community.
Indeed, research has illustrated the important role of established community members for implicitly and explicitly communicating language norms to new members .
The enforcement of these norms is often ad hoc, however.
In large systems, the sheer volume of content means ad hoc strategies often leave a large amount of profane or inappropriate user-generated content undetected.
The existence of such content can actually fight against the positive influence of community managers and long-time participants by setting a bad precedent that communicates to new users that profanity and other negative content is acceptable .
Automated systems that help community managers, moderators, and administrators to manage the flood of user-generated content in these environments could help to promote more productive large-scale collaboration and thus more valuable information products.
As more and more of the web has grown to include usergenerated content, the detection and management of inappropriate or objectionable content has become an important task for web sites.
One common technique is social moderation, in which users themselves undertake the task of identifying and flagging of profane or inappropriate responses.
However these systems have been only moderately successful, and suffer from potential collusion flagging can be used to indicate disagreement or dislike of a post that is not otherwise inappropriate or profane .
Instead of relying on social moderation, recent proposals have been made to automate the detection of inappropriate or abusive content.
Research in computer vision has given much attention to the related issue of detecting inappropriate videos and images.
Advances in this space have largely included systems that detect "too much skin" in images and videos .
Other systems utilize textual metadata , while some combine the two; one such system, WebGuard has reached 97.4% accuracy in detecting pornographic web sites .
While many would argue that textual analysis is more tractable than visual content analysis, this may be in part because of a general misunderstanding about how difficult the problem of profanity detection is in real-world contexts.
Furthermore, text has a visual element that is socially understood.
Expressive forms such as emoticons and "ASCII art" use visual properties of text, punctuation marks and symbols to mimic lexical units and thus convey meaning, denote profanity and circumvent automatic filters.
Such visual-for-textual substitution is best illustrated through examples such as the use of "@" in "@ss"..
Social news sites  typically allow users to post links to stories of interest, vote on contributed stories, and most important to the present study, comment on stories and respond to others' comments.
Our data set is the complete set of user-contributed comments over a three-month period  to Yahoo!
Buzz, a social news commenting site that is no longer active.
We also have information about each news story including its country of origin, language, and category .
More information about this data set, including the distributions of comment lengths, comments per user and comments per thread can be found in .
All three authors independently judged the `correct' labels for gold comments.
For most gold comments the authors agreed on an answer which was likely to be self-evident.
For example, the following comment was judged to contain profanity but no insult:
In order to generate a data set describing the presence of profanity, insults, and the objects of the insults, we employed Amazon Mechanical Turk .
MTurk is an online labor market in which requesters post jobs that can be easily decomposed into a large number of small tasks.
MTurk workers  are presented with a short description of available tasks, and then choose which tasks to complete.
Individual tasks typically take between 5 and 20 seconds to complete and workers are generally paid about 5 cents for each task.
Recent studies have suggested that using MTurk for similar content analysis tasks can be both faster and more economical than using dedicated trained raters .
We selected a random sample of 6500 comments spanning all categories.
Comments that were likely to be too short to meaningfully interpret or too long to quickly process were not sampled: we restricted sampling to the 2nd and 3rd quartiles for overall comment length .
Each worker was shown one comment at a time.
For each comment, they were asked to answer the following questions:
Over the course of approximately 5 days, 221 MTurk workers provided 25,965 judgments on 6500 comments.
Following the model suggested by Sheng and colleagues , we employed multiple coders for each item.
As a result, each item was rated by a minimum of three raters.
We adopted a simple consensus model on the labels.
To ensure labeling accuracy, our final profanity labeled data set only includes those comments for which at least 66% of labelers agreed on the profanity label.
Similarly, our final insult and insult object labeled data sets only include those comments for which at least 66% of labelers agreed on the insult or insult object label.
This method resulted in a different N depending on the focal phenomena .
For example, one hundred and forty-six comments  were dropped from the final profanity labeled data set because raters did not reach consensus on the profanity label.
Finally, beyond the requirement of a consensus threshold from multiple coders, we also employed a `gold data' model to improve label quality.
Gold data were a set of comments for which the `correct' labels  were designated by the researchers prior to the labeling task.
If a Turker mislabeled one of the gold comments, he was shown a short explanation for the correct answer.
In this way the gold data functioned as a lightweight training program.
The standard approach to profanity detection in online communities is to censor or filter text based on lists of profane terms.
When user-generated text contains listed words, those words or the entire contribution may be flagged for review or automatically removed.
Some profanity lists are shared between multiple sites, and administrators contribute additional terms as they become prevalent or problematic.
As noted above, list-based systems often suffer  as profane language evolves over time with slang and Internet abbreviations.
As such, we downloaded a second list of profane terms from noswearing.com.
This site hosts a list of community contributed profane terms.
This list evolves over time with user contributions and is larger than the phorum.org list.3 While both lists contain traditional profane terms, they also contain inappropriate terms such as racial slurs and vulgarities.
In another attempt to improve recall, we employ a stemmer.
Beyond simply looking for the presence of a word on a profanity list, the stemmer allows the system to see if any words in a comment have a shared stem with any word on a profanity list.
To evaluate the efficacy of list-based methods we built several systems that employed the two lists and stemming in various combinations.
For each system, we average its performance over 5 trials of 10-fold cross validation on our 6500 profanity-labeled comments from Yahoo!
While the data set as a whole contains 6500 comments, 6354 meet the 66% labeling consensus across the MTurk labelers for the profanity label.
Of those 6354, 595  are positive cases, meaning that they contain profanity.
All systems are evaluated based on their precision , recall , f-measure  and accuracy-this is the standard array of evaluation metrics for systems of this type .
The performances of all systems are summarized in Table 1, sorted in descending order of f-measure.
Similarly, the weighted random system labels comments randomly, weighted by the distribution of profane/non-profane comments in the training set.
The performances of these systems are included for comparison purposes, though they of course approach the theoretical random baselines.
The remaining systems are list-based approaches, based on the lists gathered from phorum.org and from noswearing.com.
In an attempt to reach higher recall, we created additional systems that marked a term as profane if it appeared in either one of the two lists.
Finally, in some systems we combined word lists with stemming.
While a peak accuracy of 0.913 seems promising, recall that 9.4% of the comments in our corpus contain profane terms.
For this testing data, if one built a system that, given a comment, always returned a negative classification , it would have an accuracy of 0.906 as 90.6% of the testing corpus is comments that do not contain profanity.
Therefore, f1, precision, and recall are much more descriptive evaluation metrics.
As seen in table 1, peak performance of the list-based approaches is reached using the profane terms list from no swearing.com combined with a stemming system.
This system detected 40.2% of the profanity cases at 52.8% precision.
Based on our results, we must conclude that even the best of these list and stemmer based systems would not perform well at detecting and removing profanity in user-generated comments.
As we have already discussed, list-based approaches perform poorly because of three primary factors: misspellings , the context-specific nature of profanity, and quickly shifting systems of discourse that make it hard to maintain thorough and accurate lists.
The latter values are included to adjust for differences in the profane and non-profane corpora sizes.
Of the thirty-three words in this table, notice that only eight are standard spellings that would be found in listings of profane terms .
Thus, a list-based profanity detection system, such as the ones evaluated in the previous section, would fail to catch twenty-five of the top thirtythree profane terms  used in our data set.
While these words could, of course, be added to a profanity list for future detection via a list-based system, there are countless further ways to disguise or censor words.
This makes representing them in a list a significant challenge.
As further evidence of how widespread the particular problem of disguised or partially censored profanity is, we analyze use of one specific character, the @ symbol.
The popularity of Twitter and other social media have resulted in adaptations and specializations of language for online communication .
Just as text messaging has an established shared dictionary of acronyms, social media share some community established abbreviations that allow users to pack more content into short messages.
One such abbreviation is the `@' symbol.
When a user writes `@rick', they are directing their message to `rick', but in a public medium.
The `@' symbol provides a short and easy mechanism for directing public comments towards specific individuals, but also helps to bridge the gap between directed and undirected interaction in computer-mediated communication.
For example: "@xeeliz Check this out!
Two more example tweets are shown in Figure 1.
The top tweet from edchi, shows a use of the #.
The bottom tweet from kevinmarks includes a use of the @ symbol, indicating that this tweet is directly addressing the user feliciaday.
We found that usage of the @ symbol is somewhat common, however, as you might imagine, not all uses of the `@' symbol were in the conversational manner presented above.
To explore the multiple uses of the `@' symbol we built a rule based system using regular expressions.
Classifying `@' usage as within email or web addresses is easily accomplished with regular expressions, however, automatically determining that `@ss' is profanity while `@john' is conversational is a more difficult task.
We employ a corpus of profane terms , along with a tool to calculate the Levenshtein edit distance between two terms .
This calculation adds the number of letter insertions, deletions and changes to transform one word into another.
When a term contains the `@' symbol, in order to determine if it is profanity, we check to see if the Levenshtein edit distance between the term and any known profane term is equal to the number of punctuation marks present in the term.
For example `@ss' has one punctuation mark  and has an edit distance of one from the profane term `ass.'
Using this approach, we have a very high precision tool that takes a term containing the `@' symbol and determines if it is a profane term .
The recall of this tool is only as good as our list of profane terms.
Table 4: The distribution of comments containing profanity within topical story domains.
Reported  values are the results of the comparison of profanity, insult, and directed insult frequency within a given category to the frequency across all other categories.
Throughout this paper, reported significance values are Bonferroni adjusted where there are multiple comparisons.
Using this tool, we labeled all uses of the `@' symbol in our corpus with `email address,' `web address,' `profanity,' `conversational,' or `other' .
The results can be seen in Table 3.
Within this set, 39.9% of `@' usage was within the context of a censored or disguised profane term, while only 24.9% of `@' usages appear in a conversational context.
Nearly 40% of all occurrences of the @ symbol came in the form of disguised or author-censored profanity.
The @ symbol is just one of many punctuation marks that could be used to disguise profanity.
Moreover, the @ symbol is one that is thought to be commonly used in social media in a conversational manner, yet an astonishing 40% of its uses within our data come in the form of disguised profanity.
This is likely to be a conservative estimate, as it is known that list-based measures suffer in recall, as shown in the previous section.
Table 4 shows the distribution of profanity, insults and directed insults in comments within the different domains.
To avoid the possibility of Type I error we applied the conservative Bonferroni adjustment to all significance values reported.
For clarity, the first value in Table 4 can be read as 10.7% of political comments contain profanity.
As previously discussed, the N differs between profanity, insults, and directed insults because, for each we use only items for which coders reached consensus.
Each comment in the table 4 analyses was labeled with one of the 10 categories shown.
Profanity usage in political comments is significantly more common than in other comments.
Political comments contain significantly more insults and directed insults than in other domains.
On the other end of the spectrum, the lifestyle comments contain significantly fewer insults and directed insults than other domains.
The business domain also held significantly fewer insults, while the sports domain had significantly fewer directed insults.
As expected, from these data we can conclude that different domains of news story incite varying amounts of profane language and use of insults .
In addition to facing issues of recall, list-based approaches are a one-size fits all approach that do not take into account how profanity is used within different domains, contexts, and communities.
Through our MTurk labeled data set, we explore the use of profanity in comments on news stories in order to understand more about the frequency and context of profanity use and how it is received.
First, we examine the prevalence of profanity in different topical domains.
Given that our data set contains insult and insult object labels in addition to profanity labels, these labels will be utilized as a measure of context.
To further understand how profanity is used within our data set, we investigate the cooccurrence of the `comment contains profanity' label with `comment contains insult' and `comment contains directed insult' labels.
This Country is as good as gone.
The Chosen ones and the Zionists won.
Check out `Rules for Radicals' and `The Protocals of the elders of Zion' to see exctly whats going on.
Was nice while it lasted USA.
You mean the 7.25 an hour job offered my daughter who has been managing a DQ for 3 years now?
Or the temp clerical position that MAY go perm if the employer can make some money that they offered my wife.... by the way for $10 an hour.
How about the President getting paid $40k a year and pay the bills on the white house and feed his family with that.
Any excuse to raise friggen gas so some CEO can make a big salary is bull s***" "Hey, Happy St. Patricks!
Time to suck in new generations to drinking.
Show them how fun and cultural getting sh*tfaced on St Patricks Day is.
Let them see the drunk tanks, impound lots, women shelters, ER's, and morgues."
First, we analyze the differences between comments that contain profanity and those that do not.
Table 5 summarizes this breakdown.
From Table 5, we see that, of all profane comments 58.66% contain an insult.
Of all non-profane comments 16.19% contain an insult.
That is, if a comment contains profanity, it is significantly more likely to also contain an insult.
Similarly, 39.49% of all profane comments contain a directed insult while 8.15% of all nonprofane comments contain a directed insult.
We also found significance for the inverse questions.
While these correlations do indicate that insults  and profanity are closely tied, it is still interesting to note that nearly 42% of all profane comments do not contain an insult at all.
This indicates that there are uses of profanity within the corpus in a non-insulting context.
The next logical question is - in what context do these profane words occur if not in an insult?
A manual investigation of this set of comments showed that nearly all occurred in negative `rants' on the topic at hand.
For example, the comments in Table 7 were labeled as profane comments that do not contain insults.
Future work includes a more detailed analysis of comments that contain profane terms, yet no insult.
Next, we analyze differences in the context of profanity use between domains.
Our method involves the profanity/insult co-occurrence measures used above to characterize the data set as a whole.
Table 8 shows the distributions of insults and directed insults among profane and among non-profane comments.
Profanity use in the politics domain is tied more to insults and directed insults than in comments in other domains.
While there was a good variety of content in this data set, some domains had far fewer comments than others.
As such, analysis beyond that accomplished in this paper will be done on a data set where the number of comments in each domain is balanced.
Secondly, we have examined the nature of profanity use on just one user-generated content site.
It would be inappropriate to generalize our findings beyond that site, as specific sites often attract distinct types of users who set up different norms about appropriate behavior.
Little is known about how those norms are established and how they evolved.
However, this study is a first step in establishing such an understanding.
One might assume that profanity, like flames or personal insults, would discourage active user participation and engagement.
To understand more about how profanity is received/tolerated, we looked to measures of the popularity of a comment within our data set.
Most social news sites allow users to vote on comments in addition to stories, using features such as `digg,' `like,' `thumbs up,' `buzz up,' `thumbs down,' and `buzz down.'
These features give us some additional popularity information about each comment.
The social news site we studied allows users to both `rate up' and `rate down' each comment, and the number of `rate up's and `rate down's per comment are represented in our data set.
We made the assumption that `rate up's and `rate down's could be interpreted as a measure of popularity or how much attention each comment received.
We divided our data set into comments labeled by MTurk workers as containing profanity, and those labeled as not containing profanity, and then looked at the difference in number of `rating up's per profane comment and `rating up's per non-profane comment .
Table 9 shows the percent of profane comments with 0 and 1 or more `rating down's, .
For example, the upper left-most data point can be read as 36.64% of all profane comments received 0 rating down's.
Thus, profane comments are more popular or more widely read than non-profane comments.
This confirms our intuition that passion  towards a topic typically engenders either passionate agreement  or strong disagreement .
In this paper, we made three primary contributions.
The first concerned the state of current list-based profanity detection systems.
Through an evaluation of the current state of the art in profanity detection, we argued that current systems do not suffice.
This performance is quite poor for what is often underestimated as a simple task.
Through the use of a data set of user-generated comments from a social news site, labeled by Amazon Mechanical Turk workers, we analyzed the salient differences between comments labeled as profane and not profane.
This analysis exposed and emphasized our argument that current systems do not suffice because they fail to adapt to evolving profane language, misspellings , and profane terms disguised or partially censored by their author.
The latter proved to be very prevalent in our finding of the most common features that distinguish profane from non-profane comments in our MTurk labeled data set .
Our second contribution is with regard to a major oversight of profanity detection systems - a lack of tailoring for specific communities.
To establish the importance of this oversight, we provide evidence that communities not only use profanity with different frequencies, but also in different ways or contexts.
In Table 4, we showed that comments in the politics community of Yahoo!
Buzz were significantly more likely to contain profanity, insults, and directed insults , than other communities.
Similarly, we found that comments in the lifestyle community of Yahoo!
Buzz were significantly less likely to include insults and directed insults, comments in the sports community of Yahoo!
Buzz were significantly less likely to include directed insults, and comments in the business community of Yahoo!
Buzz were significantly less likely to include insults than other communities.
From this evidence, we conclude that different communities incite and permit differing amounts of profane language as these comments remained on the site and were not removed by a community manager or social moderation.
Next, addressing the context in which profanity is used, we find that overall, comments with profanity are significantly more likely to include an insult and a directed insult .
While this is an intuitive conclusion, it also provided us with a method by which to analyze the differences between the contexts of profanity use in different domains.
We analyzed how the propensity for a profane comment to include an insult differs by domain.
Table 8 shows that profane comments in the politics domain are significantly more likely to contain insults and directed insults than in other domains.
Combined with evidence that profanity is used at different frequencies in other domains, this drew us to conclude that profanity is used differently between communities.
Finally, we provided an analysis of how profanity is received.
Using the standard community feedback mechanism of `rate up' and `rate down' we judged the popularity of comments with and without profanity.
Surprisingly, we found that overall comments with profanity were both significantly more likely to receive `rate up's and to receive `rate down's.
Following the conclusions drawn in this article, there are a few clear next steps with regard to moving beyond listbased profanity detection systems, and tailoring systems for specific communities.
First, since list-based profanity detection systems don't suffice, future work involves building profanity detection systems from a machine learning point of view that take into account the context in which profane language is used.
Learning the context, in addition to the actual profane words, has a greater potential for robustness, enabling the system to stand up to misspellings, disguised or partially censored words and evolving profane language.
Similarly relevance feedback can be used to adapt and improve the model over time.
Secondly, since we established that profanity use and tolerance is very specific to a community, it is very clear that these systems will have to be developed or trained by each community.
Future work involves building toolkits that allow this sort of tailoring.
The use of Amazon Mechanical Turk and other low cost crowdsourcing mechanisms will prove crucial in labeling profanity in data sets from each community in order to train these machine learning systems.
Finally, we believe our results are most valuable as part of a larger investigation into the social nature of profanity and negative content within specific domains and user communities.
In future studies we intend to extend our explorations of the social meanings of profanity and its context-specific use through qualitative interviews and survey studies.
Boyd, D. and Marwick, A.
Why Cyberbullying Rhetoric Misses the Mark.
Callison-Burch, C. Fast, cheap, and creative: evaluating translation quality using Amazon's Mechanical Turk.
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, .
Dinakar, K., Reichart, R., and Lieberman, H. Modeling the Detection of Textual Cyberbullying.
Proceedings of International AAAI Conference on Weblogs and Social Media, Workshop "Social Mobile Web," .
Fleck, M.M., Forsyth, D.A., and Bregler, C. Finding Naked People.
Hammami, M., Chahir, Y., and Chen, L. WebGuard: a Web filtering engine combining textual, structural, and visual content-based analysis.
Haninger, K. and Thompson, K.M.
Content and Ratings of Teen-Rated Video Games.
A content analysis of profanity in video games and its prevalence across game systems and ratings.
Cyberpsychology & Behavior: The Impact of the Internet, Multimedia and Virtual Reality on Behavior and Society 12, 4 , 457-460.
Jacob, V., Krishnan, R., Ryu, Y., Chandrasekaran, R., and Hong, S. Filtering objectionable internet content.
Proceeding ICIS '99 Proceedings of the 20th international conference on Information Systems, .
Internet content filtering using isotonic separation on content category ratings.
Joachims, T. Text categorization with Support Vector Machines: Learning with many relevant features.
In C. Nedellec and C. Rouveirol, eds., Machine Learning: ECML-98.
Statistical color models with application to skin detection.
Laboreiro, G., Sarmento, L., Teixeira, J., and Oliveira, E. Tokenizing micro-blogging messages using a text classification approach.
Binary codes capable of correcting deletions, insertions, and reversals.
Li, Q. Cyberbullying in Schools.
A collusionresistant automation scheme for social moderation systems.
Nguyen, D. and Rose, Carolyn.
Language use as a reflection of socialization in online communities.
Proceedings of the Workshop on Language in Social Media., .
Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '08, , 614.
How useful are your comments?
Automatic identification of personal insults on social news sites.
Journal of the American Society for Information Science and Technology, .
