Personal fabrication machines, such as 3D printers and laser cutters, are becoming increasingly ubiquitous.
However, designing objects for fabrication still requires 3D modeling skills, thereby rendering such technologies inaccessible to a wide user-group.
In this paper, we introduce MixFab, a mixed-reality environment for personal fabrication that lowers the barrier for users to engage in personal fabrication.
Users design objects in an immersive augmented reality environment, interact with virtual objects in a direct gestural manner and can introduce existing physical objects effortlessly into their designs.
We describe the design and implementation of MixFab, a user-defined gesture study that informed this design, show artifacts designed with the system and describe a user study evaluating the system's prototype.
Personal fabrication of 3D objects has been spurred by rapid advances in printing tools and techniques.
The quality, speed, capability and ease of use of 3D printers is rapidly increasing while cost is going down, enabling wider audiences as "makers" of physical objects.
The outcome of the process is distinguished by the immediate presence it has in the real world - physical, tangible, and usable in a real world sense.
The front-end of the process is in stark contrast; the tools that help users design things they would like to make still reside on flat computer screens.
In this paper, we introduce a new system, MixFab, to support design for personal fabrication with a mixed reality approach.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The system is aimed to lower the barrier for casual design of 3D content, and to enable a design experience where the digital model of an artifact-to-be-made is created in a physical work space that affords direct manipulation and interaction with artifacts that are real.
To this end, the system integrates three core concepts:  use of immersive augmented reality to provide a 3D visualization of the artifact under construction projected in the real world;  support for users to shape artifacts directly with their hands, replacing the need for advanced modeling skills with intuitive gestures;  enabling use of real artifacts in the design process such that new artifacts can be shaped to fit existing ones.
MixFab's mixed-reality environment is by virtue of a Holodesk-like structure  where the user sees virtual content merged with the real world.
Users can introduce physical artifacts as size-reference or to capture their shape - Figure 1a shows a user placing a glue-stick inside a virtual object to create the glue-stick's virtual replica in place.
Hands and other physical artifacts properly occlude other objects and face-tracking provides a parallaxcorrected image, creating important depth-cues.
We make four contributions in this paper.
First we propose and implement an immersive mixed-reality environment by combining an augmented reality setup, gesture recognition and 3D scanning capabilities.
Our second contribution is a set of user-defined gestures for 3D modeling obtained through a study in which we observe how users would perform basic tasks unconstrained by any system or augmentation.
We then present MixFab's design environment, which is based on these gestures.
It is centered around direct and natural interaction with virtual artifacts, effortless integration of physical objects into the design process and a self-explanatory interface.
Fourth, we perform a user study evaluating the design decisions of MixFab and providing evidence that especially the effortless integration of existing objects is of value.
MirageTable is a general-purpose mixed-reality environment targeting collaborative scenarios, that supports physics based interaction with existing physical objects .
Designing fabricable objects in an AR setting has been explored.
Modeling-in-Context uses a single photo as reference for size, perspective and ratio .
Situated Modeling takes this a step further by stamping primitives into an augmented 3D space, designing artifacts in their designated place .
MixFab provides a mixed-reality environment, in which users can interact naturally without the need to wear props  or being constrained by physicality .
Shape creation has been transported into virtual space in a variety of ways.
Some let users deform models using both hands ; others use the motion of the hand  or its curvature  to define the shape.
Gestures for 3D manipulation and navigation can be distinguished between symbolic and natural gestures.
FingARtips  introduces finger-based augmented reality interaction with haptic feedback.
The Perceptive Workbench uses pointing in combination with 3D objects .
Data Miming follows a natural gesture approach, in that they do not prescribe a set of gestures but record users unconstrained hand movement .
Whereas related work often has author-defined gestures, we inform our gesture-set by a user defined gesture study .
Several semi-commercial products seek to enable nonengineers to design objects for personal fabrication machines, typically by providing a feature-reduced version of traditional CAD systems .
Others reduce the complexity of the modeling task by specializing on a specific purpose.
For example, SketchChair  lets users draw chairs and produces the patterns required to fabricate them, while Plushie  follows a similar idea for stuffed toys.
Using existing objects in the design process has been explored in a variety of ways.
KidCAD  lets children combine the 2.5D shape of their toys, while CopyCAD  enables the reuse of 2D shapes in a CNC milling setting.
Constructable  extends the 2D contour reuse by copying textures to workpieces.
A camera takes a photo of the texture and transfers it on the part using a laser-cutter.
Enclosed  uses electronic components as handles and size references during enclosure design.
It automatically adds cutouts to the enclosure patterns so that the components can be mounted.
MixFab lets users remix existing objects, use them to construct new shapes and as a physical size reference, combining the features above in one system.
Hand pose recovery often relies on user augmentation.
Surface Drawing utilizes gloves , others use reflective markers .
For an overview of hand pose recognition techniques, see .
We implemented an appearance based approach using a single depth-camera, requiring no user-worn equipment or prior calibration.
At MixFab's core is an immersive mixed-reality system creating a high permeability between the virtual and physical world.
It enables new and exciting interactions that were not possible with each component taken by itself.
We implemented MixFab's physical configuration by building upon the Holodesk frame , although other hardware implementations may also be used.
The setup superimposes virtual content with the real world by using a beam-splitter and a display mounted at a 45 degree angle.
It provides an interaction volume roughly the size of modern 3D printers.
A depth camera placed at the top of the frame provides data for interaction within the system.
We further add a motorized turntable, to the bottom of the frame, for 3D scanning.
Our processing pipeline is designed to specifically support seamless interaction between virtual and physical objects, blurring the border between the two.
Tangible building blocks that can sense their spatial configuration are used to create the models; a 3D scanner for scanning clay figures is also proposed.
The clay based modeling is extended by Sheng et al.
Gesture Recognition which is solely based on the depthdata provided by the Kinect serves as input modality for the interaction with virtual objects.
It does not require any user-augmentation or prior calibration.
3D Shape Acquisition is supported at a trade-off between time and precision.
One can capture the rough shape of an object in real-time or acquire a more precise scan in about a minute.
Physical objects can be captured anywhere in the frame, allowing their placement relative to virtual objects.
Sketch Recognition enables users to describe objects they want to create without having to be very precise.
Mesh data manipulation serves as back-end for object creation, acquisition and manipulation.
In previous work, hand gestures are typically defined by the respective authors, rather than users .
We are interested in what gestures users would intuitively perform to create and manipulate objects, also to inform our subsequent system design.
To this end we conduct a user-defined gesture study with a methodology similar to Wobbrock et al.
Each participant was subsequently given a set of tasks .
For each task, they were shown one or two images of the desired outcome and asked to perform a gesture to create that desired outcome.
Participants were instructed to imagine the objects depicted on the images as being displayed in front of them.
Once all ten tasks were completed, all users completed a survey querying their age and gender.
We further asked for prior CAD experience and how much that experience influenced the proposed gestures .
Users were seated at a table with a camera placed a meter above the surface, resulting in an interaction area of about 60 x 50 cm.
Upon completion of the study, the recorded video material was transcribed and coded to extract the suggested gestures.
Quantitative data collected through the questionnaires, as well as user agreement  is used to judge the quality and confidence of users in the proposed gestures.
We invited twelve participants from various departments at our university.
List of tasks and corresponding gestures  during the study.
A the agreement among the users as defined by Wobbrock.
Gestures recommended for each task group are written in italic.
We believe that the "gap-trace" gesture is an artifact of the picture that demonstrated the task: the gap to close was of regular and linear nature.
Suggestions would most likely be different when manipulating more organic shapes.
Rotating and moving objects are everyday tasks performed using two variants of the same gestures: one-handed vs. twohanded.
The pictures shown to the participants contained a keyboard and a mouse next to the virtual object, as a size reference.
It seems that the size of virtual object was interpreted differently by each participant, leading to the use of one hand if the object is perceived to be small or both hands if the object is perceived to be large.
Transforming physical objects to virtual ones proved to be the most challenging task; to some extent because it is difficult to convey the need of the operation without a system being present.
Participants proposed a variety of actions , but only one was mentioned multiple times: dwell time.
Users place the object in the desired position, move their hands away and wait for a certain amount of time.
Removing objects is a daily task.
We often throw things away, place them elsewhere or deform them prior to disposal.
The gestures suggested for removing objects tend to resemble such actions.
Wipe and move out - the two most prominent gestures - have the same intent: move the object out of the workspace.
Wiping objects  was suggested more than move out and rated easier/more suitable.
Users without CAD experience proposed gestures resembling such actions.
Four users moved their flat hand or thumb where they wanted to cut, miming a knife.
Five users indicated how they wanted to cut by performing a "Shuto"  motion from Karate.
Those experienced in CAD, suggested that one might select three points on the object to define a plane used for cutting.
We observe a similar pattern as Wobbrock et al.
More complex tasks  have low user agreement scores, whereas more simple ones  yield higher agreement amongst users.
Despite low agreement rates, suitability and easiness remain at high levels, suggesting confidence in the proposed gestures.
To choose an appropriate gestures for each task group, we use the count of how often a gesture was suggested as main metric.
In cases where the suggestion count is not distinctive, we decide based on suitability and easiness rating.
The gestures recommended for each task are marked in italic in table 1.
For most task groups suggestion count, easiness and suitability are sufficient criteria, except for the creation of primitives.
When looking at box and cylinder creation separately, we'd be required to choose different gestures for each of them which is undesirable as it would be likely to cause confusion with users.
Adding the suggestion counts within the task group however, yields a slight preference for the draw outline gesture .
MixFab's user interface is centered around the gestural creation, modification and assembly of objects.
The user mainly interacts with "gestural icons" and the virtual objects being created and assembled .
Gestural icons depict a certain hand pose, showing the user what gesture to perform to trigger a certain action or change a certain property.
The icons are context-sensitive, hence inform the user about the possible operations of the system.
Virtual objects displayed in MixFab can have three different states.
Inert objects that and cannot be modified without selecting a gestural icon, is colored in a slightly transparent gray .
Once an object becomes modifiable using a gesture, it turns yellow .
Objects that are currently being modified, are colored green .
This colorcoding provides feedback about the current system state, especially the grasping of objects.
It allows users to determine whether the system recognizes them as engaged in a gesture and what influence their movement will have on the scene.
Translation and rotation is performed using a one-handed grabbing gesture, much like one would grab a cup.
If the object is grabbed so that the hand intersects the object, it attaches to the user's hand so that the object can be moved freely within the interaction volume.
Grabbing any point away from the object lets users change the object's orientation.
A lever is formed between the base of the object and the hand-tip, which is then used to rotate the object.
Objects can be uniformly scaled using a two-handed compression gesture.
Users place their hands on either side and the scaling factor is a function of their distance.
We implemented relative, yet direct scaling using a fixed controldisplay gain.
When users first assume the compression posture, their hand distance is identified as 100% scale.
Changing the distance between both hands then scales the object.
Users can draw the outline of primitive shapes on the floor of MixFab, using only their index finger.
The system then recognizes the sketch as either a circle or rectangle and extrudes it to 3D space.
The height of the object is set by the height of the hand above the systems ground.
Once the height is as desired, the other hand taps the floor to fix the height.
Cutting objects removes material, rather than splitting objects.
To perform a cut, the user indicates the desired position of the cut using their flat hand  along the X-axis.
Tapping on the ground with the other hand confirms the cut.
If the user indicates the cutting position with the right hand, the right side of the cutting plane is discarded; indicating with the left hand removes the left side.
Capturing the 2D outline and extruding it to 3D space is a simple but fast method to capture an object's shape.
Users can place existing objects anywhere in the frame and after a fixed dwell time, the system captures the outline and automatically extrudes it to the existing object's height.
Users can manipulate the object's height, by indicating the height with one hand and confirming it with the other.
Object assembly combines two objects, either by adding them together or by subtracting one from the other.
Fusing two objects can be used to add material or refine the shape of an object.
Subtracting one object from the other is commonly used to create holes or cavities to hold other objects.
There is no specific gesture for assembly.
Object assembly is simply a matter of selecting the way the two objects are to be combined.
Union or difference of meshes are symbolized with a plus or a minus sign respectively .
The MixFab frame has a turntable built in which serves to rotate objects so that objects of more complex shapes can be scanned.
To scan an object, the user selects the "scan object" icon, and places the physical object together with the scanning rig on the turntable .
The system then waits until all hands are out of the frame before it starts rotating the object to capture it from all sides.
Once scanning is complete, the virtual object appears where the existing one was placed.
We illustrate the systems use by constructing a desk organizer  that will hold a pen and a glue-stick.
We start with creating the base shape by drawing a circular outline .
The system recognizes the drawing as a circle, beautifies it and offers the outline for extrusion.
We set the height using one hand; the height snaps to 5mm increments and is displayed just above the object.
To confirm the height, we tap with the other hand .
MixFab's user interface:  a user drawing an object's outline,  setting the height of the cylinder,  plane cuts of the cylinder,  capturing the shape of a physical object  positioned in the virtual one,  moving an object  upwards by grabbing it with one hands,  object assembly ,  rotating an object ,  the desktop organizer 
To create the semi-rectangular shape of the organizer, we cut off both sides.
First we cut off the right side of the object by indicating the cut position with the right hand and confirming with the left.
To cut the left side, we repeat the procedure, this time holding the left hand where we want to cut and confirming with the right one .
Next, we create the first hole which will hold the glue-stick.
We position the real glue-stick where we want the hole to be within the virtual object.
Once in position, we select "capture outline" and move our hands out of the frame .
The system then captures the outline of the glue-stick and extrudes its height.
Confirming that initial height with the left hand, turns it into a virtual glue-stick replica.
As the glue-stick was standing on the ground of the frame, the virtual glue-stick replica is on the ground as well.
If we were to assemble the object as it is, we would create a hole through the whole shape.
To have some material at the bottom of the hole, we grab the virtual glue-stick, move it a few millimeters up and release it to fix it in that position .
Eventually we assemble the virtual glue-stick and the previously created base to create the hole for the stick.
After selecting assembly, we are asked to choose the method of assembly .
Choosing subtract removes material where the glue-stick was, leaving a hole of correct size and position.
Lastly we repeat the steps above for the pen, placing it in its desired position, capturing its outline, extruding it and moving it up a few millimeters.
To make the pen easier to access, we tilt it forward by grabbing at a point in space, forming a lever with which the object is re-oriented .
We built a prototype to implement the MixFab system by using Holodesk's hardware frame .
Holodesk provides an immersive environment with an interaction volume roughly the size of modern 3D printers.
Our hardware differs in that we use a Kinect for Windows rather than a Kinect360 and mirror setup, and have a turntable built into the frame for 3D scanning.
Most importantly, on the software side, we employ a different processing pipeline and provide a gesture-based interface rather than a physics-based one.
The hardware consists of a display mounted at a 45 degree angle, being reflected through a 50/50 half-mirror into the interaction space.
A Microsoft Kinect depth sensor mounted at the top of the frame is used for capturing the interaction with the system, while a second camera placed between the display and half-mirror is used to implement perspective correction through face tracking.
A motorized turntable for 3D scanning is built into the floor of the frame .
Two cameras need to be calibrated once : the facetracker and the Kinect.
The facetracking camera is calibrated to a plane perpendicular to the half-mirror .
As the dimensions of the frame are known, the exact position and orientation of the facetracking camera  can be mapped to real-world coordinates.
MixFab follows an appearance based approach to hand posture and gesture recognition that requires no prior calibration or user augmentation.
Depth data from the Kinect is processed to extract a set of features which is later used by specialized gesture recognizers.
We rely solely on the depth image, as the half-mirror occludes the hands in the color image.
The general processing pipeline is as follows: first we acquire a depth frame from the Kinect, filter it using a 5x5 kernel, remove points using previously defined clipping planes and tessellate the remaining points to provide an occlusion mesh; all of which is implemented in OpenCL.
For each such contour, we compute its center, orientation via the Hu moments and finger-tip which is the convexity defect farthest along the principal axis.
A not-grabbing hand forms several convexity defects .
If a defect with an angle greater than min and depth greater than dopen is found, the hand is considered to be in an open state.
If no such defect is found, the hand is considered to be in a grabbing state.
While maintaining a grabbing pose, the finger-tip detection heuristic does not work reliably.
Part of the grabbing pose is to bend the hand compared to the arm, causing the finger-tip to move away too far from the orientation axis previously computed.
The real tip of the hand and origin of the hand contour form another convexity defect, which is approximately stable when the correct posture is maintained.
Kalmann filtering yields a usable hand-tip estimation.
When performing the wiping gesture, users move their flat hand from one side of the frame to other in a speedy fashion.
To detect that gesture, we continuously sample the contour centers X component with a fixed window size w .
If all points in that window are equidistant, their distances monotonously inc-/decreasing and the start/end points are at least dwidth apart, a wipe gesture was performed.
A circle has it's center N at the center of the polygon and its radius is the average distance of each point to that center.
Rectangles are the bounding rectangle of the polygon.
MixFab's processing pipeline distinguishes between hand and object contours, if they are not connected .
All contours are subject to perspective distortion, which we correct using the previously acquired calibration.
To capture the outline of an object, we build the convex hull of all object contours in the frame.
Thus, objects can be grouped and produce a smooth shape from the noisy Kinect data, but we also slightly reduce precision.
We further find the hightest point within the hull, making that the initial extrusion height.
3D scanning in MixFab uses Kinect Fusion and a custom built turntable/scanning rig.
Kinect Fusion estimates the camera to world coordinates using the iterative closest point  algorithm.
ICP implicitly requires geometric features to converge, resulting in a poor scanning performance on "uninteresting" scenes.
In MixFab however, we scan single objects only, resulting in severe alignment errors without our scanning rig.
After filtering the depth image using clipping planes, there is still a some degree of noise left .
Due to the sparse nature of the images produced by the clipping, noise has a drastic impact on the camera tracking and thus scanning performance.
To alleviate those issues, we designed a scanning table with an extreme width to height ratio, that aids Kinect Fusion in producing its camera alignment.
Once an object has been scanned, the resulting mesh data has to be cleaned.
The noise integrated during scanning has to be removed, the scanning table has to be removed, holes have to be closed and the mesh has to be made watertight.
The scanning process begins with integrating plane-clipped point clouds into the truncated signed distance function  representation maintained by Kinect Fusion.
Once the object has been captured from all sides, the TSDF is transformed to a tri-mesh.
All unconnected components with less than cnoise faces are removed.
The scanning table surface is found by computing the largest connected component using a threshold of the discrete RMS curvature as connectivity condition.
We then fit a plane to the scanning table vertices and rotate the mesh, so that the scanning table is in the XZ plane.
All vertices and faces closer than dcut to the scanning table plane are removed.
We remove unconnected components with less that 1 2 cnoise faces, as well as non-manifold vertices/faces and fill gaps less than chole units in arc length.
The resulting mesh is likely to contain holes - at least one from cutting away the scanning table.
We smooth the mesh  using Laplacian smoothing.
For each remaining hole, we fit a plane to the boundary vertices, project those vertices to that plane and compute a Constrained Delaunay triangulation , hence closing the hole.
In a last step, we remove non-manifold vertices/edges created by the CDT and fill the resulting gaps .
The mesh processing pipeline is implemented using the Visualization and Computer Graphics Library, sgCore and qHull.
It takes 30 seconds to complete one revolution of the turntable and less than 10 seconds to perform the mesh processing.
Once the example was completed, we asked users to design a phone dock and provided our phone dock example .
Participants could use their own phone or an LG Nexus4 we provided.
Users were encouraged to design the object on their own, but were assisted when necessary.
Upon completion of all design tasks, we presented users with a set of statements and asked them to rate how much they agreed with each of them, on a 5-point Likert scale.
We then went into a semi-structured interview asking about their experience, trying to gain insight into the usability of the system.
We invited 10 participants  from various departments on our campus.
All except one participant had no experience with CAD systems or an engineering background .
MixFab's construction mechanism was quickly understood by all users.
Knowing when to create a new object, modifying it and assembling two objects seemed to pose no problem for the participants.
User 2 reported that "when I was told to create the phone-dock I had a strategy in my head, thus knowing when to use a physical object."
The method of assembling objects to create new ones, was particularly well received - some users said that they " very much liked this way of putting things together, to compose objects" .
Participants were first asked to sign a consent form and given an introduction to the system.
We started by introducing the idea of designing objects for 3D printing by showing example objects created with MixFab .
They were then shown the "desktop organizer" walk-through  and given five minutes to familiarize themselves with the system.
Using existing objects during the design process was deemed useful by all users .
Not having to measure objects and being able to place them in their desired position was highlighted by users "I very much liked  the thing that you can bring real physical objects in there."
Being able to use an existing object as starting point or base for designing new ones was mentioned as one of the benefits of the system: "I like the idea of being able to put my phone in there and design something around it."
The effortless integration of existing objects was even considered fun: " it's fun because you know there is no sort of effort required to replicate existing objects."
For interaction to be natural users to feel immersed and have a sense of object size and location.
90% of the participants agreed that they were immersed into the system.
A majority of users  agreed that they had a sense of size and location of objects as well as their hands .
Users had no issues with selecting the gestural icons, further indicating that they had a sense of where things were in the frame.
Manipulating objects was reported to be easy  and interacting with the system felt natural .
Users have a sense of size and location of the object they're designing.
In MixFab existing objects first have to be digitized before they can be used which is beneficial in that it allows us to e.g.
Using the physical object as tangible proxy however, would likely increase immersion.
We could introduce recursion by designing an object, fabricating it and introducing its physical manifestation back into the design process, making it semi-interactive fabrication.
Capturing existing objects comes at a detail vs cost trade-off.
Our prototype can capture a crude form of objects in realtime, a more detailed one can be had at a small time cost.
This trade-off is likely to shift towards an increased level of detail at decreasing costs.
Other material properties, such as color and texture will likely be capturable in the near future.
With recent advances in appearance fabrication and 3D printing such features could also be physically reproduced.
The mixed-reality environment of MixFab helps users to get a sense of size of the objects they're designing, bringing both closer together.
In the MixFabs prototype implementation objects look artificial however.
Immersion could likely be increased by providing a more realistic object representation taking environmental lighting, proper material appearance and texture into account.
Stereoscopy, in combination with the head-tracking, would further improve realism.
Not having to wear special equipment increases immersion thus naturalness of the interaction; not having to go through a calibration procedure prior to using the system increases the users readiness to engage with the system.
Being free of user-augmentation and calibration comes at a cost, however: precision and accuracy.
To some extend this is caused by the coarse spatial resolution of consumer depth cameras - something that is likely to change in the near future.
A model-based hand tracking approach or specialized sensors hand-tracking sensors  are bound improve precision.
Gestural modelling is less precise than traditional CAD environments.
First, the RGBd sensor limits attainable precision, compared to i.e., a mouse; something that will get better as such sensors improve.
Second, gestures themselves can limit precision.
It is hard, for example, to accurately place an object in mid-air without haptic feedback.
MixFab currently implements snapping to the ground when moving objects, or snapping to 45 degree increments when rotating.
Extending this approach to tool-specific constraints  will improve gestural modeling precision, and enable users to design symmetrical, reflected and parallel features - something that is not yet possible in MixFab.
Several usability aspects are subsumed under "ease of use": navigating within the system, ergonomic aspects and implementation specific artifacts.
While interacting with the system, users at times asked what to do next but shortly afterwards selected the appropriate icon and continued on their own; all users reported that the gestural icons were useful .
Mid-air gestural interaction runs the risk of inducing armfatigue when used for an extended period of time.
During our study, 9 of 10 users reported no arm fatigue.
When using MixFab , many of the gestures are performed on the floor of the system and are often interleaved with short pauses of rest.
As users sit close to the system, they do not have to extend their arms very far, further reducing the risk of arm fatigue.
The accuracy of the gesture recognition had the biggest impact on usability.
Some users found it hard to execute precise movements 
Most of the issues revolved around moving objects .
Others however, found the precision to be sufficient.
When asked if precision was a problem, user 7 answered: "no, that was easy".
Overall, users agreed that the system was easy to use .
Our study participants had no experience with CAD and modeling tools.
When asked if they would be able to design the items they designed during the study with other systems, 40% answered that they would be capable of doing so, despite no prior experience.
Some users expected our system to be the way items are commonly designed: "I have never used any of the CAD tools, but I think it's kind of like this one" .
In this paper we have presented MixFab - a mixed-reality environment for personal fabrication.
We described the system rationale, user-defined gesture study that informed its gestures and its implementation.
The system was evaluated in a user study showing that users were successfully able to use the system - even considered it fun.
