Many tasks in graphical user interfaces require users to interact with elements at various levels of precision.
We present FingerGlass, a bimanual technique designed to improve the precision of graphical tasks on multitouch screens.
It enables users to quickly navigate to different locations and across multiple scales of a scene using a single hand.
The other hand can simultaneously interact with objects in the scene.
Unlike traditional pan-zoom interfaces, FingerGlass retains contextual information during the interaction.
We evaluated our technique in the context of precise object selection and translation and found that FingerGlass significantly outperforms three state-of-the-art baseline techniques in both objective and subjective measurements: users acquired and translated targets more than 50% faster than with the secondbest technique in our experiment.
However, when a user performs a zoom, there is a loss of contextual information.
In addition, the input of such tools has usually been limited to single-point input devices such as the mouse, and repeatedly switching back and forth between navigation and interaction with the same input device is time-consuming.
Multitouch workstations provide more degrees of freedom than single-point input devices.
They also reduce the mental effort required for interacting with virtual objects by removing the indirection of an external pointing device.
These advantages come at the cost of screen occlusion and reduced precision.
The precise selection of small on-screen targets has been well-studied.
However, with the recent advent of multitouchbased content creation applications such as Eden , we require tools for more complex interactions than just selection.
In this work, we focus on developing a more general technique enabling users to quickly navigate through the space of potential viewports while selecting and translating targets.
We propose FingerGlass, a technique that lets the user define a viewport using one hand.
The other hand can simultaneously interact with objects in the scene.
We interact with our environment in many different scales.
For example, creating a painting requires us to work on its global composition as well as its finest details.
The physical world provides a natural way of transitioning between different scales by allowing us to move our viewpoint towards or away from our objects of interest.
Some computer applications operate on virtual scenes or artboards.
Examples are graphical content creation systems such as Adobe Illustrator or map browsing tools like Google Maps.
In contrast to the physical world, their user interfaces are limited in size and resolution and therefore encompass a small range of scales.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Precision and occlusion issues associated with touch-based input devices are an active topic of research.
Researchers have evaluated their solutions on the task of target selection.
Offset Cursor  was the first technique addressing occlusion by remapping the physical touch location to a nonoccluded screen location.
ThumbSpace  and Dual Finger Offset  extend this concept.
Instead of remapping the touch location, Shift  displays an offset copy of the occluded area.
While these techniques address occlusion, they do not increase precision.
To address precision, PrecisionHandle  enhances the control-display ratio by giving the user a lever-like widget at hand.
Correctly placing this widget requires additional time and taps.
A simpler technique doubling the control-display ratio is Dual Finger Midpoint .
Researchers have proposed a number of content-sensitive techniques to solve input precision problems.
Bubble cursor , MagStick  and Semantic pointing  increase the target size in motor space.
Escape  assigns different directional vectors to targets lying next to each other.
To select a target, the user swipes over it in a target-specific direction.
Sliding Widgets  provide a generalization of this technique to a wider scope of widgets.
Enhanced Area Cursors  let the user specify a coarse area in the first phase.
Then, in a second phase, they invoke an angular menu containing all available targets in this area.
All these techniques are designed for selection tasks: after the initial touch, the user performs corrective dragging movements or disambiguation gestures.
Selection is then usually triggered by releasing the finger.
This makes extensions to translations or complex multi-finger gestures nontrivial.
A few techniques remedy this problem for singlepoint dragging operations by offering both a tracking and a dragging state: DTMouse  works similarly to Dual Finger Midpoint , and Pointing Lenses  are pen-based precision enhancement techniques.
In addition to being limited to single-point, they still require corrective dragging movements which are relatively slow to perform and can be uncomfortable.
Furthermore, none of the techniques above serves for visual exploration.
Visual exploration is addressed by Zoom Pointing , TapTap  and Rubbing and Tapping  which let the user redefine their viewport by zooming.
Dual Finger Stretch by Benko et al.
The technique by Mankoff et al.
All these techniques suffer from a loss of context after zooming in since the original area of interest is occluded.
Researchers evaluated the above techniques only on target selection, but not on translation tasks which comprise a large part of our interaction with GUIs.
Context-sensitive techniques to facilitate dragging operations over long distances are Drag-and-Pop and Drag-and-Pick .
These techniques distort geometrical relationships and assume that there is a small discrete set of possible destinations for a scene object.
We call the latter the magnified view.
Any interaction with objects in the scene takes place in the magnified view.
This way, fingertips do not occlude the area of interest in the zoomed-out view.
Figures 1 and 2 show sample applications of FingerGlass.
We evaluated FingerGlass and its variant, FingerMap, in the context of precise object selection and translation.
Our formal user study shows that FingerGlass significantly outperforms three state-of-the-art techniques: with FingerGlass, users acquired targets more than 50% faster than with PrecisionGlass, the second-fastest technique in our experiment.
Translation times were between 40% and 100% faster than the second-fastest technique for distances between 2.7mm and 350mm.
Users also subjectively preferred using FingerGlass than the other techniques.
Participants also responded positively to the ergonomics of our tool: FingerGlass requires only individual taps to perform precise selection.
For translations, the amount of physical dragging is limited to local corrections.
For the exploration of two-dimensional scenes, researchers developed zoomable user interfaces  such as Pad++  or Speed-Dependent Zooming .
However, ZUIs inherently suffer from context loss and are hard to navigate once zoomed in.
Researchers have addressed this in a variety of ways: Pook et al.
Other focus+context approaches include Fisheye Views , Perspective Wall , Document Lens , Melange  or High-Precision Magnification Lenses .
DTLens  is a similar technique for multitouch screens.
Furnas and Bederson introduced the Space-Scale Diagram , an analytical framework for analyzing multiscale navigation interfaces such as the ones discussed here.
None of these systems consider manipulations of the scene but are limited to navigation.
Magic Lenses  allow users to place virtual lenses using one hand and interact with the scene using the other hand.
However, they are not designed to be fast and there are no time measurements provided.
Figure 3: Interaction with Google Maps using FingerGlass:  Task description.
Figure 3 shows a walkthrough of FingerGlass in a trip planning application: the user would like to move the marked waypoint  to a different street intersection in the same neighborhood.
At the initial scale the waypoint is too small to be selected by a finger, and street names are not visible.
To get a close-up view of the waypoint and its surroundings, the user touches the screen with two fingers.
Their tips span a circle which we call the area of interest.
FingerGlass immediately displays an enlarged copy of this area which we call the magnified view .
We call the fingers spanning the area of interest defining fingers, their hand coarse hand and the other hand fine hand.
Based on Guiard's studies on human bimanual action , we suggest that users use their non-dominant hand as the coarse hand.
The magnification ratio of the magnified view is prescribed by the application.
Developers are advised to use a ratio that enlarges the smallest pickable targets to about the size of a fingertip.
If the magnification ratio is too large, small translations require too much physical movement by the fine hand.
If it is too small, selection of small targets can be difficult.
For our street map application, ratios between 4x and 6x have worked well.
The magnified view is always tangent to the area of interest.
A placement algorithm determines its position such that the prescribed zoom ratio can be achieved as closely as possible and that the fine hand can comfortably interact with its contents.
As the user moves his coarse hand, the magnified view follows.
Once the user releases his coarse hand, the magnified view vanishes.
To translate the waypoint, the user touches it inside the magnified view  with a finger on his fine hand.
He then drags it to the desired destination .
Glass translates the waypoint in the zoomed-out view accordingly in real-time.
This behavior allows users to focus on the original viewport during dragging operations in order to judge their effect in a global context.
While the fine hand is translating objects in the magnified view, the area of interest is locked and any movement of the coarse hand has no effect.
The destination of the waypoint might lie outside the current area of interest.
The user can release his defining fingers while retaining the finger of the fine hand on the screen.
The magnified view then centers around the selected waypoint and shrinks down to a size encompassing just the object's immediate surroundings.
The area of interest shrinks accordingly to maintain the zoom ratio .
A finger of the coarse hand then taps on the desired destination in the original viewport.
The selected waypoint immediately moves to the location of the tap .
The fine hand can then refine its position: FingerGlass applies any movement of the fine hand to the waypoint, scaled down by the magnification ratio.
The magnified view follows the finger and its content continues to display the current neighborhood of the waypoint .
If the desired destination in the original viewport is occluded by the magnified view, the user can first move the magnified view by dragging his fine hand, and then tap at the desired destination.
The user might want to explore the neighborhood of the desired destination before finishing the translation.
To do so, he defines a new area of interest by pressing and holding two fingers of his coarse hand.
The magnified view then grows again to accomodate the size of the new area of interest while maintaining the zoom ratio.
Figure 4: - Placement of the magnified view, assuming a righthanded user:  Optimal placement.
In some cases, the user may not be satisfied with the placement of the magnified view.
For example, the placement algorithm is unaware of the current position of the fine hand as it hovers over the screen.
Hence, there may be cases in which the magnified view opens up in a place that would require the fine hand to travel a large distance or to make uncomfortable movements.
In other cases, the user may want to employ a different magnification ratio than the prescribed one.
For example, the scene may contain objects with a wide variety of sizes, and small objects need a higher magnification ratio than large ones.
FingerGlass allows users to redefine the size and location of the magnified view: once the user touches the screen outside of the current magnified view, FingerGlass will reposition the magnified view such that it lies exactly between the area of interest and the touching finger .
To determine the exact location of the waypoint, we use its relative position at the time before the magnified view shrunk .
The interaction then continues as in Figure 3d, except that the area of interest and the magnified view are detached.
The translation operation ends once both hands are released.
In this section, we assume a right-handed user.
We noted in the previous section that FingerGlass computes a suitable size and location for the magnified view.
We developed an optimization algorithm with the following three goals.
Thus, FingerGlass only considers configurations in which the magnified view is adjacent to the area of interest.
The right hand can then comfortably interact with objects in the magnified view while the left hand specifies the area of interest.
FingerGlass will try to use this ratio if possible, but resort to smaller ratios if necessary.
Figures 4a-d show areas of interest in different scenarios, and the magnified view as determined for a right-handed user by our algorithm.
Without any boundary restrictions, FingerGlass places the magnified view to the right of the area of interest .
If the magnified view would thereby extend beyond the screen boundaries, we call this location incompatible.
FingerGlass then searches for compatible locations along the boundary of the area of interest .
Our system also tries to shrink the magnified view to obtain more compatible locations.
There can be a tradeoff between reducing the magnification ratio and shifting the magnified view further to the left .
We control this tradeoff with a parameter in our code.
If there are multiple locations with very similar qualities, a term for temporal coherence prevents the magnified view from jumping back and forth .
FingerGlass occludes a significant portion of the screen and requires users to shift their attention to the magnified view to interact with scene objects.
Although we minimized the required eye movement with careful placement of the magnified view, performing many interactions could still lead to fatigue.
FingerMap is an alternate design without magnified views which follows the interaction model underlying FingerGlass as closely as possible.
FingerMap is optimized for situations in which the user wants to maintain his focus on the area of interest at all times.
Figure 5 shows an abridged walkthrough of FingerMap in the same trip planning task we used in Figure 3.
As with FingerGlass, the user specifies the area of interest with his coarse hand.
Then he touches the screen with a finger of his fine hand anywhere outside of the area of interest.
We call this finger tracking finger.
FingerMap then displays a selection cursor at the center of the area of interest .
As an extension, we made the magnification ratio dependent on the distance from the initial touch of the tracking finger to the area of interest: a larger distance leads to a bigger magnification ratio.
Figure 5: Interaction with FingerMap: The coarse hand specifies an area of interest.
Once the cursor overlaps with the desired waypoint, the user selects it by releasing the coarse hand.
As long as the tracking finger remains pressed, it operates as an indirect control for the selection cursor: FingerMap applies any movement of the tracking finger to the selection cursor, scaled down by the magnification ratio .
Once the cursor overlaps with the desired waypoint, the user selects it by releasing the coarse hand.
To translate the waypoint, the user keeps the tracking finger pressed.
This finger then indirectly controls the position of the selected waypoint, scaled down by the magnification ratio.
The remaining interaction works analogously to FingerGlass in Figures 3f-h.
The initial world space position of the tracking finger is always at the center of the area of interest.
To minimize corrective dragging movements, the user should choose the area of interest such that its center is as close as possible to the desired target.
This makes target selection using FingerMap somewhat similar to Dual Finger Midpoint , but adds small-scale corrective movements.
Pilot testers of our system stated that acquiring targets using FingerMap feels similar to capturing them with a lasso.
The choice of this mapping is crucial for translation tasks: if the range does not contain both target and destination, a translation can not be completed without altering the mapping.
Such a change is slow and the user needs to reorient himself.
Yet, if target and destination are far apart, this change may be necessary to keep the zoom ratio large and should be well supported by the tool.
For example, FingerGlass lets the user specify the range and determines a wellsuited domain that does not intersect with the range.
Translation operations with objects in graphial user interfaces are a two-stage process.
In the first stage, the user specifies the object of interest.
In the second stage, he continuously translates this object.
The transition into this second stage can be seamless: some mouse-based interfaces allow users to hit an object and to immediately transform it by translating the mouse.
Other interfaces may require the user to first release the input controller in order to complete the selection process before the transformation can begin.
For an efficient interaction technique, we suggest a seamless transition from the selection to the transformation phase.
Based on the techniques presented above, we establish design principles for bimanual interaction techniques on touch screens that enable users to efficiently navigate to objects and manipulate them across different scales.
Our principles should be general enough for various types of multiscale interaction scenarios.
Most touch devices report both the time stamp and the two-dimensional location of touch events.
These events comprise, at a minimum, press, move and release.
In order to design for a wide range of multitouch systems, we will not make use of any other input information.
In particular, we do not use pressure  or touch area  information to implement a tracking state.
No target should be acquired in ambiguous cases.
Moscovich  pointed out that direct screen touches by a fingertip should not be interpreted as if only one single pixel was touched.
Doing so would ignore ambiguities and inappropriately resolve them in an unpredictable way.
Rather than selecting one single point in a somewhat arbitrary fashion when multiple points are touched at once, the system should perform no selection and indicate all points under the finger to the user.
Thus, the user can navigate to a smaller scale and retry.
This strategy addresses our goal of designing a tool with minimal error rates.
Another advantage of contact area interaction is that the effective width of a target is increased, making selection easier in scenes with a sparse distribution of small objects.
In his study on the division of labor in human bimanual action, Guiard  noted that the two hands assume very different roles and strongly depend on each other in the vast majority of everyday tasks.
The nondominant hand defines a frame of reference and performs coarse granularity actions.
Subsequently, the dominant hand performs fine grain interactions within this frame.
More recently, Schmidt et al.
Many target selection tools in the literature provide an increased zoom ratio to enhance precision and to facilitate selection of small targets.
However, this approach is not sufficient for tasks that go beyond target selection.
Our design process aimed for an efficient tool to navigate through virtual scenes and to select and translate objects.
Navigation is a task which is difficult to quantify and to formally evaluate.
However, the feedback from our pilot testers using FingerGlass in a map browsing application showed that our technique is a very promising alternative to the existing baseline tools for navigation.
For the selection and translation of targets, we conducted a lab experiment in which we measured the performance of participants for every technique.
Participants also answered a questionnaire in which they subjectively ranked the techniques according to different criteria such as personal preference or ease of learning.
In addition, they were asked to compare a multitouch screen employing their favorite technique to a pen- and a mouse-based interface.
The questionnaire also contained some space for written comments and suggestions.
We compared FingerGlass and FingerMap to three existing techniques from the literature.
However, while they all facilitate precise target selection, none of them support subsequent translation: they let users first approach the desired target by corrective dragging movements and then complete the selection task by releasing their finger.
This behavior is not extensible to subsequent translation operations without violating principle .
However, since our device could not sense pressure or touch area to introduce a tracking state, we had to implement the baseline techniques in a way that employs an additional discrete event to start the translation.
To this end, we created a technique we call ReleaseTapping  that works as follows: once the user releases his finger, the system displays a semi-transparent solid circle around the selected target.
This circle remains on-screen for a given time before it disappears.
In order to translate the target, the user can touch this circle, keep his finger pressed and translate the target by dragging.
For the radius of the circle, we used the radius of the target object plus 20mm.
Regardless of whether or not the user hits the circle, tapping on the screen makes the current circle vanish immediately.
We measured the time users spent for performing RT.
Any translation performed by the finger at the end of the handle will be applied on a smaller scale at the tip, thus increasing precision.
Since the other two techniques offer visual magnification, we altered the technique to display a magnifying glass instead of a handle.
After deploying the magnifying glass on the screen, it will remain on the screen for one second.
During this second, the user can press and hold a target to start the translation.
Our pilot studies showed that PrecisionGlass performed better than the original PrecisionHandle.
As with our version of Shift, the user can change the zoom and CD ratio using a secondary finger.
We asked participants to complete a series of target translation tasks with all 5 techniques - FingerGlass, FingerMap, Dual Finger Stretch, Shift, and PrecisionGlass.
Depending on the current technique, each task consisted of two or three phases.
Initially, the system presented two circular targets of width 7 pixels  on the screen, separated by a given distance.
The first touch event then started the acquisition phase, during which participants had to acquire the yellow source target as quickly and accurately as possible.
For Dual Finger Stretch and Shift, this was followed by the ReleaseTapping phase.
Finally, during the translation phase, participants had to translate the selected yellow target onto the blue destination target.
We considered the acquisition phase successful if the user acquired the correct target.
In the case of failure, the system did not proceed to subsequent phases.
Instead, it presented a new pair of targets and the acquisition phase was repeated with the same parameters until successfully completed.
The translation phase was considered successful if the source target and the destination target overlapped after releasing.
In the case of failure, the entire task was repeated with the same parameters until successful completion.
At the beginning of each phase, a shrinking circle visually highlighted the corresponding target.
The system displayed targets in front of a high-resolution street map in order to facilitate orientation in magnified views.
In addition to the two targets in the task definition, 1500 red distractor targets were distributed uniformly at random across the screen.
These distractors made it impossible to reliably acquire targets without assistive tools.
In this section, we discuss the three techniques to which we compared our tools in the study.
For fair comparison, we extended these techniques as follows: * Dual Finger Stretch : The user specifies an initial anchor location by pressing and holding his primary finger.
Then he uses a secondary finger to scale a screen portion around the anchor location.
The zoom ratio is proportional to the distance between the two fingers.
We added Release-Tapping: performing RT with the primary finger starts the translation.
Releasing the secondary finger makes the scaled portion vanish, and the user can enlarge new screen portions.
The translation ends once the user releases the primary finger.
The user can then refine this position by dragging.
We added ReleaseTapping: performing RT will start translation.
The authors of Shift discuss CD gain as an extension, hence we added this functionality in the spirit of Dual Finger Slider : by touching the screen with a second finger and dragging towards or away from the primary finger, Shift will magnify or de-magnify the offset view and modify the CD ratio accordingly.
The experimental apparatus was M2256PW, a prototype 22" LCD touch screen manufactured by 3M.
Our experiment used an area of 1371 x 914 pixels  for the scene interaction, the remaining space was reserved for feedback about the completed tasks .
The refresh rate of the screen was set to 59 Hz.
Participants were allowed to choose a tilt angle and desk height that was comfortable for them.
10 volunteers  with a mean age of 22.9 years participated in the experiment.
All of them had some experience with touch screens from automated teller machines.
All participants have used multitouch based phone or PDAs before, 5 participants use them on a daily basis.
Only one participant has ever operated a multitouch workstation before.
We gave a $10 gift card to every participant as a reward.
We implemented the techniques FingerGlass, FingerMap, Dual Finger Stretch, Shift and PrecisionGlass.
We assumed a fingertip diameter of 10mm for contact area interaction.
For the techniques FingerGlass and PrecisionGlass, the prescribed zoom ratio was 6x, thus leveraging the effective target size from 2mm to 12mm.
Our implementation was written in C++ using the Qt and OpenGL APIs.
We chose to make use of graphics hardware acceleration in order to ensure maximal framerates in our test scene with thousands of targets displayed on top of a 4096 x 4096 texture.
Running on a single-core CPU with an ATI Radeon HD 2600 card, we obtained frame rates consistently above 30fps.
Unlike Dual Finger Stretch, Shift and PrecisionGlass, FingerGlass supports high-precision selection without the cost of any dragging operations.
Both FingerGlass and FingerMap support subsequent and fast multiscale translation operations according to our principles  and .
Therefore, we hypothesize: *  Acquisition Time - FingerGlass has significantly shorter task completion times than the three baseline techniques when acquiring small targets .
In accordance to our design goals, the basic dependent measures for all phases of the task were completion time and error rate.
The completion times of the individual phases are denoted acquisition time, Release-Tapping time and translation time.
Their sum is denoted total time.
For timing measurements, we took only successful attempts into account.
In a similar fashion, we define the error rate for each subtask: the acquisition error rate is defined as the number of failed acquisitions divided by the number of total acquisitions.
The translation error rate is obtained by dividing the number of failed translations by the number of total translations.
We performed repeated measures analysis of variance on both trial completion time and error rate for the tasks of acquisition and translation.
We classified timing results outside of 3 standard deviations as outliers.
In this section, we summarize our data analysis.
To verify if we could aggregate across the independent variable Block, we investigated the effect of this variable on task completion time.
Concerned that the learning effect could influence the results of our study, we removed the first two trial blocks after visually inspecting the data.
Although Block had no significant main effect on task acquisition time anymore , it still had some on translation time .
However, there was no interaction between Block and Technique, neither for acquisition  nor for translation .
We are mainly interested in a quantitative comparison of the different techniques, rather than in their absolute measurement.
Therefore, it is sufficient to know that no tool is at an unfair advantage due to learning effects.
We used a repeated measures within-subject factorial design for the study.
The independent variables were Technique and Distance.
We chose 8 values for Distance on a logarithmic scale.
The longest chosen distance was 350 mm.
To obtain the other distances, we successively divided by a factor of two.
For the translation subtask, the combination of our target size with the chosen distance results in a range of index of difficulty  values in Fitts' law terms , from 1.2 to 7.5 bits.
Techniques were presented to each participant in random order.
For every technique, 12 blocks had to be completed.
Each block contained a random permutation of the 8 distances.
We collected a total of 5  x 12  x 8  = 480 successful trials from each participant.
Paired samples t-tests show that FingerGlass was significantly faster than any other technique  for the selection of 2mm sized targets.
This observation confirms our hypothesis H1.
The second-fastest technique for this task was PrecisionGlass.
The differences in acquisition time between FingerMap and all three baseline tools were insignificant .
Table 1 lists the acquisition times for all techniques and comparison to FingerGlass.
The mean completion time for the subtask ReleaseTapping was 211ms .
This time is not included in the acquisition time.
With FingerGlass, some participants started redefining the area of interest during translation for distances 21.9mm and 43.8mm.
For distances equal to or greater than 87.5mm, this was almost impossible to avoid: with an area of interest encompassing both the source and destination targets, the zoom ratio was often limited to 2x or less.
Note that the threshold distance is about twice as large for FingerGlass than for PrecisionGlass: FingerGlass allows users to define the area of interest in a way that both the source and the destination of the dragging operation just barely fit in the magnified view, making full use of its space.
For translation, we performed a 5 x 8  within subjects ANOVA aggregated across Block.
For the verification of our hypothesis H2, we were interested in post hoc multiple means comparisons.
Paired samples t-tests showed that FingerGlass was significantly faster than the three baseline techniques for all distances equal to or greater than 10.9mm .
For the smallest two distances, FingerGlass was significantly faster than Dual Finger Stretch and Shift , but not significantly different from PrecisionGlass.
These results for translation time confirm our hypothesis H2 for FingerGlass for all distances greater or equal to 10.9mm, but not for the two shortest ones.
We reject the hypothesis for FingerMap: even at a distance of 350mm, the difference to Shift in translation time is insignificant .
Paired samples t-tests on Total Time show that FingerGlass outperforms any other tool at any distance .
Extending PrecisionGlass and Shift with a virtual slider for changing the CD ratio proved to be inefficient.
With Shift, where users did not need to use the slider, this feature was hardly ever employed.
With PrecisionGlass, changing the CD ratio was the only possible way to accomplish longdistance translation tasks.
Thus, although PrecisionGlass performed very well for short distance translations, the timings were poor for medium- and long-distance ones.
A closer analysis of the recorded performances showed that users often overshot or undershot the destination target after changing the CD ratio.
The reason for this is that users chose a different CD ratio in every trial and thus could not predict the required distance in motor space.
The performance of FingerMap did not meet our expectations: acquisition times were about twice as long as those of FingerGlass, and translation times were worse for all distances.
For selecting targets, FingerMap sacrificed direct touch selection in order to minimize eye movement.
Our result indicates that this does not lead to better task completion times.
Whether or not it reduces fatigue would be subject to further research.
For translation, the users might confuse the role of their hands without visual feedback, resulting in worse performance.
Our performance logs show that participants often tried to move their fine hand, which only applies relative movements, towards the absolute position of the target.
In addition, the design of FingerMap suffered from the same problem as our extension of PrecisionGlass: participants hardly made any strategic use of the controllable CD ratio.
More often, they were overshooting or undershooting targets during both acquisition and translation.
Subsequently releasing the first finger dropped the target in the wrong location.
We noticed that our touch device sometimes reported erroneous touch release events.
This resulted in targets getting released early in translation operations and yielded false translation errors.
To discard these cases, we used the event log to compute the velocity of a touch point immediately before its release event.
We then removed trials with a higher release velocity than a threshold we determined by visual inspection of the data.
To compute the velocity of a touch point, we averaged the pairwise distances of the last 5 touch move events.
We noticed that targets in the right half of the screen were somewhat harder to interact with than those in the left half.
In some cases, the magnified view must be placed on the left side of the interest circle.
Because all our subjects were right-handed, they had to either cross their arms or perform interactions in the magnified view with their non-dominant hand.
To investigate this effect, we created a new grouping variable XPos indicating whether the source target was placed in the left, middle or right third of the screen.
A 3 x 5 analysis of variance  on acquisition time and translation time revealed that there is a significant effect of the horizontal target position on translation time , but not on acquisition time .
FingerGlass had an average total time of 3214ms in the left, 2957ms in the middle, and 3561ms in the right third.
As all other tools have a total time of over 5000ms in all thirds, this effect changes little about the relative performance of the tools.
In the post-study questionnaire, participants were asked to rank the five techniques according to their preference for performing everyday tasks on their hypothetical personal multitouch workstations.
Of the ten subjects, eight preferred FingerGlass.
When asked to compare their preferred technique to a pen-based interface, users preferred the multitouch technique .
Comparison to a mouse yielded similar results .
We also asked users which tool they find easiest to learn.
The results show that FingerGlass was considered almost as easy to learn as Shift: 5 subjects would recommend Shift as the easiest technique to learn, 4 persons would recommend FingerGlass.
Finally, the participants ranked the tools by their subjective impression of the performance in object acquisition, shortdistance dragging  and long-distance dragging .
The results confirmed our timing measurements.
We assigned scores between 1 points  and 5 points  to the votes and calculated the average scores as shown in Table 2.
To investigate selection and translation errors, we created variables AcquisitionFail and TranslationFail which measured the error rates for every condition , aggregated across subjects.
Note that there were more than 5 x 8 x 12 trials for this analysis since subjects had to repeat erroneous attempts.
The error rates are plotted in Figure 8.
Distance also had a significant effect on TranslationFail, but there was no interaction between Technique and Distance.
Paired samples t-tests showed that Dual Finger Stretch had higher error rates than other tools with borderline significance.
It had significantly higher error rates than both FingerGlass and Shift for distance 87.5mm, and than Shift for 43.8mm .
By replaying the performances, we determined that many of the translation errors in Dual Finger Stretch happened in cases where users released the secondary finger before the first finger to end the translation.
We constructed two techniques enabling users to quickly navigate to different locations in different scales of a virtual scene, and to efficiently select and translate objects therein.
Our experimental results show that one of these techniques, FingerGlass, significantly outperforms the current state-ofthe-art techniques on touch screens for both precise selection and object translation.
FingerGlass does not require any information of the underlying scene and thus can be implemented independently on top of any existing application.
In order to retain the gesture vocabulary of the underlying system, we suggest providing a modifier button in the fashion of the "Shift" or "Caps Lock" keys on computer keyboards to activate the tool temporarily or permanently.
In terms of limitations, our method requires at least three fingers to operate, and is designed for large multitouch workstations.
We did not vary the screen size in our experiment.
However, since three fingers occlude a significant area on small displays, it is likely that the advantage of FingerGlass decreases as the screens get smaller, compared to singlefinger techniques such as Shift.
We believe that some of our findings are general enough to be applied to a wider range of applications.
Therefore, we extracted a set of interaction principles for the efficient bimanual interaction with more general multiscale datasets.
An example for such applications would be the modification of surfaces in 3D space.
In such a system, the user could use his coarse hand to specify a small section of a surface.
The view from a camera pointing along the surface's normal onto the surface would then be displayed in the magnified view.
This technique would allow a user to temporarily look at scenes from a different point of view and perform operations like surface painting with his fine hand or moving small objects which are invisible from the original perspective.
The range of scales that can be explored using FingerGlass could be leveraged by allowing the user to recursively define areas of interest.
By specifying a new area of interest in an existing magnified view, a new magnified view could appear, visualizing the scene at an even smaller scale.
