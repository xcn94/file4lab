As various home robots come into homes, the need for efficient robot task management tools is arising.
Current tools are designed for controlling individual robots independently, so they are not ideally suitable for assigning coordinated action among multiple robots.
To address this problem, we developed a management tool for home robots with a graphical editing interface.
The user assigns instructions by selecting a tool from a toolbox and sketching on a bird's-eye view of the environment.
Layering supports the management of multiple tasks in the same room.
Layered graphical representation gives a quick overview of and access to rich information tied to the physical environment.
This paper describes the prototype system and reports on our evaluation of the system.
This dream has come partially true with the emergence of "electronic personal assistants", e.g., handheld devices.
People are getting used to instructing these assistants to deal with their personal affairs, such as document and image editing, instant messaging, and web browsing, in information space.
However, help with activities in the physical world, e.g., house cleaning, object delivery, and dishwashing, by nonhuman assistants are still restricted and people tend to do those tasks themselves.
One obvious reason is the shortage of advanced commercial home robots.
Fortunately, significant efforts have been made by the electronics industry and we can now expect capable home robots to be on the market in the near future.
However, if people use an independent robot control interface for each robot to assign tasks, then the users' separate manipulation efforts will still be unsatisfactory.
A tool for managing coordinated action among multiple robots and appliances is needed.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Users might like to have the whole house cleaned, with the windows opened to let in fresh air, and a vase of fresh flowers delivered to the livingroom coffee table by a robot system.
It might also be nice to have a robot system prepare for a dinner party, e.g., setting up the lighting, music, and gaming devices first and then getting cold beer and drinks out of refrigerator five minutes before the party starts.
These scenarios require coordination among multiple robots: independent robot setting would introduce task organization problems.
To achieve such a goal, in this paper we propose a task management tool based on a graphical editing interface .
Since the interface has a similar layout to Adobe Photoshop, we call it Roboshop.
The user selects a tool from the toolbox and gives instructions to the robot system by sketching on a bird's-eye view of the environment.
For example, the user might select the "vacuum" tool and specify the area to vacuum by coloring the area to be cleaned.
We designed "Rainbow Sketch" to support multicolored freehand drawings with predesigned optional annotations.
Rainbow Sketch aims to convey clear instructions to robots and also meaningful task details to users.
The system also provides a layer mechanism to manage complex task composition as well as a grouping method to extract tasks of interest.
Given that the system supports asynchronous heterogeneous robot controls, layers and grouping help users to review and manage the task sets efficiently.
The overall advantage of using graphical representation is that it gives a quick overview of and access to information tied to the physical environment.
While those systems allow users to invoke robot commands through various methods, they do not handle the reuse and editing of previous commands well: no direct strategy for managing command reuse through simple sketching, laser pointing, the use of remote controllers or paper cards.
Roboshop's layers are intended to provide easy access to previously set commands and hence to enable them to be reused conveniently.
Along with the pervasiveness of home electronics, a universal remote control  is becoming a popular research topic, e.g., CRISTAL , Pronto  and inVoca .
A personal universal controller  is a slightly different interface since it supports self-programming.
However, those studies did not focus on coordination among multiple robots.
But those projects emphasize sensor technology or focus on the interface for controlling and debugging sets of appliances rather than on a tool for organizing multiple physical tasks to be executed by home robots.
Various sketch-based interfaces have been intensively developed since tablets and PDAs have become our good friends.
Chronis and Skubic  have developed a PDA interface through which the user sketches a route map as a means of directing a single robot along a designated path.
Multi-modal robot interfaces that include the ability to sketch waypoints on top of a robot-sensed image have also been proposed .
Since these interfaces are designed for one-time robot control, the pen-stroke gestures are merely for dispatching simple commands to the robots, but not very suitable for describing and organizing multiple commands.
Roboshop supports more structured sketching, called Rainbow Sketch, for the assignment and management of multiple tasks.
In contrast to other sketch-based interfaces with plain tracepoint drawing, Rainbow Sketch conveys rich information and task details with the predesigned supplementary annotations to users and dispatches concrete robot commands.
A realm of innovative robot instruction interfaces in the home environment, encompassing many interaction media, has been proposed since the emergence of home robots.
Laser pointer-based user interfaces for giving robots instructions have been presented .
Our work builds on various efforts to develop versatile robot platforms.
The iRobot home robot series can perform a variety of house-cleaning tasks, such as vacuuming and mopping.
HERB  from a joint effort by Intel and CMU  is an autonomous mobile manipulator that performs useful manipulation tasks at home.
Cody  from Georgia Tech Healthcare Robotics Lab is a humanoid mobile-manipulating robot.
One distinctive feature of those two robots is that they can manipulate doors, cabinets, and other constrained objects using caging grasps  or equilibrium point controllers .
The assistive robot EL-E  can fetch objects from flat surfaces.
These robots, along with other education and entertainment home robots, will make robots into valuable members of many households.
Our interface is thus designed to support a wide variety of home robots.
Several user interfaces that aim to control multiple robots have been implemented recently.
Most of them instruct the robots directly.
This interface focused on driving multiple robots, rather instructing them to accomplish any tasks.
Another existing system has been presented by Skubic et al.
The system works by having a user sketch a rough environment map and trajectory on a PDA.
This interface is suitable when the environment is uncomplicated or the path-points need not be very precise.
These robots provide guidance in public places.
Both of these interfaces attempt to save limited resources, e.g., human attention, among multiple robots in order to make the synchronous control effective.
Our system lets users focus mostly on the tasks by supporting asynchronous workflow instructions to various robots in the home environment.
Scenario B: A busy businessman, Bob, works from Monday through Friday until late.
To keep his house in better condition, he bought three types of robots: a mobile camera robot, an armed robot, and a laundry robot.
He usually does his grocery shopping weekly on Friday evening on his way home at around 7:00pm.
Before he leaves work, he sketches on his tablet to instruct the mobile camera robot to photograph his kitchen, so he can check that the inventory of supplies is sufficient for the next week and also to take a photograph of the toilet so he can check the stock of toilet rolls and other daily needs.
He sketches to instruct the armed robot and the laundry machine to collaborate in doing the laundry before he arrives home, so he can relax in peace and quiet without the noise of the washing machine.
This scenario shows that the system has to support asynchronous control.
Besides, setting multiple tasks in a constrained location  requires spatial and temporal coordination.
Assigning each task individually would create unintentional time overlaps or gaps.
The interface needs a time scheduling mechanism to address possible task conflicts and inefficiencies.
Scenario A: An old lady, Lily, lives alone.
Her son intended to hire a housekeeper for her but she refused since she cannot fully trust an outsider.
So instead he bought some home robots for his mother: an armed robot, which can pick up and deliver objects, and a vacuum-cleaning robot.
Lily makes a simple sketch on her tablet screen to schedule the armed robot to get her pills for diabetes and hypertension twice a day at 9:00am and 3:00pm along with bottled water from the kitchen and bring them to the sofa so that she will never forget to take the pills even if she is preoccupied watching TV.
After she has taken the day's pills, the robot should return the remaining pills and take the water back.
Lily also sketches instructions for the vacuum robot to vacuum under the dining table at 1:30pm after lunch and at 7:00pm after dinner to remove crumbs on the floor.
She also has it vacuum the whole house once a week.
This scenario shows that the interface needs to be easy to use, especially for elderly people.
Users should be protected from the diversity and complexity of robot control panels.
It is also preferable to be able to reuse the housework settings in the same environment.
Cleaning is a daily household chore and existing cleaning robots are very helpful, but have limitations.
Cleaning dining areas after dinnertime as an everyday routine is trivial for a human but requires a considerable amount of setting up for robots, e.g., area specifications and time settings.
When people encounter spots with stubborn dirt, they spend extra time cleaning them, but robots do not yet have such intelligence, so the users must explicitly specify an extended time.
Moreover, if both vacuum and mop robots are present in the same room, coordinated action among them is necessary.
Object delivery by robots saves a fair amount of labor.
People might use a robot to tidy up the room after a party, e.g., gathering up gaming devices and storing them in a cupboard and throwing bottles into the recycle bin.
The robot may also be capable of delivering clothes, newspaper/magazines, food and drinks from the fridge, and tools from the garage.
These delivery tasks may require special instructions such as to move slowly when carrying fragile objects like wine glasses, avoid collisions with furniture, and collaborate with other robots to move a heavy object.
Another benefit of robot delivery is the reliable automation of regular actions that a user might forget, as illustrated by the pill delivery in Lily's scenario.
Remote or automated monitoring of the home environment helps users check the house interior.
A mobile webcam robot can not only monitor but also take photographs of specified spots.
As indicated in scenario B, you can get an overview photograph of the kitchen worktop and the toilet in advance so you know what is needed at home, since it is irritating to discover after shopping that you bought something that you already have or missed some necessity, e.g., toilet paper.
Although humans can adjust their angle of view to check objects at different heights, camera robots currently rely on preset camera angles.
The appearance of our system is similar to typical graphical editing systems, especially image editing tools such as Photoshop and GIMP.
A task consists of tool , place, and time.
Tools are in Housework Toolbox where users select a housework tool.
The place is specified on the SketchPanel.
Time will be selected on the Scheduler.
Layer Palette manages the tasks, which is represented as a Layer.
Note that not all actions listed below are implemented in our current robot system.
Some of the actions are included in our prototype user interface, but the robot system cannot execute the specified action.
We include them for explanatory purposes only to clarify our vision for the future.
We state clearly when we describe features that are not implemented in a working robot system.
All other features are implemented and working.
Each stroke in Rainbow Sketch is associated with a set of properties.
We provide a property panel for users to set these properties .
Available properties differ according to the housework tool, and only buttons correspond to available properties appear in the panel.
When the user clicks on a button, a corresponding annotation is attached to the stroke.
Multiple annotations can be set for one stroke.
Our current Property Panel supports several types of annotations.
This applies only to the Grab&Deliver and Push&Deliver tool.
It increases or decreases the size of the brush making the stroke.
The meaning of the weight differs depending on the tool.
For Push&Deliver jobs, more weight implies a larger number of pushing robots are required.
In room-cleaning tasks, more weight means a longer cleaning time.
Housework Toolbox lists all available robot operations.
Only one tool can be selected at a time: to select the tool, you click on the icon in the Toolbox.
The Toolbox acts as a link between the icon and the actual robot.
After selecting a tool, you draw a stroke on the sketch panel to command the robot to perform an action.
Our current software implementation supports five tools .
The Mop and Vacuum tools instruct the robot system to mop and vacuum the specified area, respectively.
The Grab&deliver tool tells the robot to collect a specified object and take it to a preset destination.
The Push&deliver tool makes the robot move an object on the floor a short distance by pushing it.
The Video&pics tool drives the robot to specified locations to take videos and still photographs.
Rainbow Sketch is designed to provide a quick overview of the given tasks.
We use a set of rainbow-colored sketch pens to distinguish the chores, e.g., the yellow pen represents vacuuming.
Since the inherent characteristics of housework differ, we use distinctive types of strokes for different operations.
A freehand lasso circles the area to be mopped or vacuumed, drop and drag of the object icon for delivery, and a short arrow specifies the proper location and orientation for the camera robot.
SketchPanel show the bird's-eye view of the room in which users assign location-specific instructions by sketching .
We aim to support a set of housework in multiple rooms.
One concern is that some furniture, e.g., cabinets contains other objects.
The interface should have the capability to specify the objects inside.
Therefore, furniture such as cabinets and wardrobes in the image should be designed to be clickable.
When the users click on the furniture, the ObjectCollection window pops up to list the objects inside.
Then users can select the target objects.
This feature is not implemented in our current robot system.
We envision that objects in these containers will all be tagged and tracked by means of some kind of tracking system.
Scheduler lets you set the time and date for an action.
It has two parts: time-picker and date-picker.
After you sketch the route map for the home robot, Scheduler calculates the time span for that task.
You only need to assign a start time and date.
In our current implementation, vacuum tasks in a room are always executed before mop tasks.
If you set a time for a mop task, Scheduler will display the time span of already set vacuum tasks for that room, so that you can set the mop task start time for after them.
Scheduler also supports simple automatic multi-robot collaboration by getting all available suitable robots to engage in completing one task.
If you set the task of vacuuming the whole house and more than one vacuum robot is available, the system commands all the available vacuum robots to work cooperatively.
We run Roboshop on an Apple MacBook Pro and used the Wacom Bamboo Fun Tablet as our input device.
In Roboshop, a bird's-eye view of the environment is shown to the user.
This is not a live streaming video of the room, but a pre-shot still image.
Webcams  Pro for Notebooks are mounted on the ceiling of every room in the home to take pictures at appropriate intervals.
To detect objects and robots in the real world, we use MotionAnalysis, one of the major motion capture systems.
It detects and tracks objects in the environment with eight cameras  mounted on the ceiling very accurately with an error of 1  at 100 .
All the objects that need to be detected by the system should be defined by the user, i.e., by attaching marker balls to them, and registered with the Motion Capture system in advance.
Although the system initialization currently requires this procedure, we hope that future systems can avoid this need through the use of better technologies.
Once the initial setup has been done, the Motion Capturing System detects the object in the environment.
The system will send detected object with the name and markers' three-dimensional position to any applications via TCP/IP.
A Robot System manages home robots, which includes a server function to receive a command from the Roboshop application.
The robot system first connects to the Motion Capturing System, and starts receiving the position of robots and objects.
Then the robot system controls robots to perform housework in the environment.
Layer Palette is provided to manage multiple tasks .
It has three columns: Layer Picker, Task Info, and Task Routine.
The Layer Picker shows the task settings for that layer, Task Info displays the task element information , and Task Routine is used to set the task frequency.
Only one type of task can be set in one layer, only one layer can be active at a time, and you can make changes to the task setting only for the active layer.
Layers can be saved and retrieved for future use.
The advantage of layers is that you can edit the task elements in one layer without disturbing the others.
Layers of housework settings are presented as transparent sheets stacked on top of each other to perform a set of housework chores in the living space.
You can use the Layer Palette to hide, display, duplicate, and delete layers.
The Room Grouping tool displays overlaid task contents in a particular room by collecting all the task settings scattered among separate layers.
It provides you with a quick overview of the entire task set in one room.
Roomba can navigate through a living space containing obstacles while vacuuming the floor with a maximum speed of 500 mm/s.
We attached a Bluetooth adaptor to each robot to provide connections to the computer running the main application, which controls Roomba and Create through Roomba Open Interface application programming interfaces .
Users can control an iRobot's speed, direction, etc.
The system supports three household tasks: vacuuming, object delivery, and house monitoring.
The motion capture system provides accurate location information about the robots and objects, and the robot system uses this information to control the robots.
An actual home environment has all kinds of furniture, such as chairs, tables, and bookshelves.
These are not detected by the system automatically; you must specify them via the sketching interface.
Then these objects will be detected by the system, and the robot will avoid interacting with them.
We use Roomba for vacuuming.
In Roboshop, you circle the area to be vacuumed, set the vacuuming characteristics, e.g., the operation's weight , and specify areas to avoid.
The robot system then navigates Roomba to the specified vacuuming area to vacuum it.
Roomba avoids the furniture and areas to avoid that you set.
When Roomba has finished its task, it returns to its home base automatically.
Rovio is responsible for house monitoring .
You specify Rovio's destination and the vertical and horizontal shooting angles, and Rovio moves to the specified position, sets the specified angle, and takes photographs and sends them to you.
The photographs appear on the screen of your computer or PDA.
Rovio then returns to its dock.
The object delivery task is more complicated since the iRobot Roomba we use to deliver objects does not have any pincers to hold objects.
To support object push and delivery, the system includes a dipole-field object transportation mechanism .
This enables Create to push an object to the destination over a flat surface.
We extend this idea to further support an object avoidance mechanism .
We set up a simulated one-room home in our lab .
The size was 4  x 3 , which is a standard size for a Japanese living space.
Cameras for the motion capturing system were mounted around the area.
The room contained a couch.
Along the wall opposite the couch was a bookshelf.
On the other side was a table for the user.
We recruited seven participants, aged 21-24 years old : 1.05 years, five male and two female, from the local university to participate in our study.
Recruited participants had no prior associations or experience with our lab, group, or project.
Students' majors included engineering, business, art and design.
All participants were single: three live with their families and four live alone.
Each participant was paid $20 per hour for taking part in the study.
For the user study, the interface supported three rooms: one for demonstrating the interface, one for user trial operations, and one for the actual evaluation, which was actually the working system with robots.
Before we started the user study, we asked participants to answer a demographic questionnaire.
The aim was to understand their current life styles , daily household chores, and so on.
We started by showing our system demonstration and describing the motivation of our research.
The participants just listened and watched how to use the interface and how it works.
Next, the participants tried out the interface with an experimenter.
Finally, the participants gave actual instructions to the robot system.
They were shown the actual movements of the robot system that resulted from their instructions.
At the end of the study, all participants were asked to answer a post-experimental questionnaire.
We also interviewed them and recorded their feedback.
We asked participants to make three instructions to achieve three tasks: 1.
Set a vacuuming area avoiding the couch in the environment and set the task start time for 10:00am.
Deliver to the vicinity of the couch a box containing glasses, requiring careful delivery, at 4:00pm.
Monitor the bookshelf at 2:00pm.
All participants successfully made three instructions.
Most of them created instructions in Roboshop without any help and said, "This is basically a very easy-to-use interface."
The user study confirmed that the robots are able to successfully perform all the user-created instructions.
The users watched and confirmed that the robot system actually worked with the Roboshop interface.
However, we also got various suggestions for the future improvement: details of these and an analysis of the experiment are given below.
Users reported through comments and feedback that the interface was visually understandable and easy-to-instruct the household works.
The Drag & Drop instruction for the object delivery was easy and useful.
Despite that users reported the sketching gestures were simple, they claimed that it was difficult to create dedicated instructions, saying, "I needed to concentrate to use."
Some users were confused with sketching gesture for the monitoring operation; a user wrote an opposite angle to take a picture, and the other user tried to use arrow gesture to do that.
Likewise, one user argued that the instruction of the vacuuming area does not need to use lasso gesture; it might be enough to specify several vertices around the area.
We asked six ease-of-use questions via questionnaire after the user study.
The results are shown in Table 1, which shows the mean, SD, and percentage of positive responses  to each question.
Five users reported that the interface was easy-to-use , they felt confident while using the interface , and thought most people would be able to use the interface .
Furthermore, six users reported that they appreciated the interface for giving housework instructions .
On the other hand, three users reported that they needed technical support and concentration to use it .
From our analysis of post-questionnaire , even though all participants thought that having household performed by robots is very practical and helpful , they did not feel completely comfortable having robots execute some kinds of housework .
They were concerned that fully automatic robots might break furniture, get stuck, or drop glasses and spill water while delivering them, and so forth.
Some places in the house are private or need to be quiet, so robots are not welcome there.
At this point, they preferred to add a comment using property, to the instruction like "careful" to the task of delivering a fragile object or to increase the weight for the vacuuming task.
Since Roboshop supports asynchronous control, which means the users may not be present to watch the robot when it performs the task, setting users' minds at ease is especially important.
A few of users said, "The robot should detect the situation automatically," but the property will be necessary for Roboshop and future home robot interfaces.
The interface automatically estimates and displays the estimated time of the robot's housework.
One user reported that the estimation function "was useful to understand the situation, and to plan multiple household work instructions" On the other hand, the current working prototype only supports rough time setting; it does not support specific time instruction, e.g., start vacuuming at 9:38am.
The rough instruction "was easy-to-use," but some users "wanted to specify the exact time."
Regarding the layers, one user said "the thumbnail images in Layers enhance the speed of reviewing and retrieving the tasks," " the Save and Reuse will be useful for the future home."
Rainbow Sketch represented the tool with its color, and it seems to help the user to grasp the instruction of the Layer.
However, the interface only supports listing of Layers and one user was concerned "when the Layer number is increased, it will be difficult to search and select the Layer."
Interface designers would keep the interaction styles simple and intuitive so that not much technique support would be needed for the users.
An ambiguous or complicated interface would hesitate the users to touch it, particularly for elders and housewives.
The results of our interviews reveal that if the delivery task setting takes too much time or thoughts, the users would rather do it by themselves.
At the same time, most of the users reported that if the instructions were merely simple drawing of the robot routes, then the tasks are so abstract that they feel difficult to trust the robots.
All these concerns remind us at every stage of the designs that the interface needs a good balance between simplicity, intuitiveness and concreteness, accuracy.
Some users said that the interface should support multiangle views and three-dimensional sketching.
The bird'seye view does not allow the user to specify the area under furniture, such as tables and chairs, or other tall furniture, such as lamps.
Likewise, it does not allow the user to specify tasks that need to consider the "height", such as placing a book on the 3rd level of the bookshelf.
The 3D image of the room and 3D sketching for the tasks will "have capability of supporting much more household works."
Further, one user said, "hopefully the interface supports a live camera view of the room.
It will be helpful to know the situation of the room."
In this paper, we focused on the robot control with a common HCI technique, but it is not limited to robots.
One user said, "I hope it will actuate the other things, like curtains, lights, air conditioners, etc."
We consider that Roboshop will be able to support these home electronics and home automation systems by applying HCI technique.
The CRISTAL system  was developed as a universalremote for the home, but it did not support the asynchronous control, and the save and reuse function of the control.
Roboshop can have the functions, which the CRISTAL system has, and manages them as Layers.
Also, one user reported that the system "will be used as a household work TODO management system."
If a user noticed household works to do, s/he then just uses Roboshop to create instructions of the household work on the interface.
After that, home robots will do the actual work no matter the user is in the home or not.
At this point, the user does not need to care about the actual household works and home robots.
They only manage and arrange the household work on the interface.
We consider this as one of main future usage of the Roboshop at real home environment.
Selecting sketching gesture to use in the interface is a significant challenge.
Some users were confused with sketching gesture for the monitoring operation.
One user argued that the instruction of the vacuuming area does not need to use sketching gesture.
We consider that the design of the gesture for instruction of household work is still an open question.
Especially, not many researches about the gesture control of the housework had been conducted in the around the area of robotics and HCI.
Feedbacks from the users are very useful and valuable for improving the system.
The design of Layers should be improved as well.
One Layer only has one task, and layers are only listed in the Layer palette.
Even through the user can watch multiple tasks at the same time with the grouping tool, other functions will be necessary for the future implementation.
A folder, which has multiple tasks, might be one solution.
We designed the Roboshop user interface carefully to balance robot autonomy and user-control.
If the system is mostly automatic, it is difficult for the users to customize control; if everything is manual, then interaction becomes too tedious.
We use sketching and layers to allow the users to easily control the system with some autonomy while freeing them from low-level details.
For example, the user specifies the region to vacuum, but does not specify detailed robot movement in the region.
Low-level issues such as collision avoidance among robots are handled implicitly: it is taken care of by the system.
A Heterogeneous group of robots is able to perform possibly different tasks simultaneously according to the layers; the Roboshop user interface  can support various forms of robot collaborations as a framework.
For a vacuum task that may require engaging multiple robots, the system can divide one vacuum area into multiple disjoint areas so that the robots clean those areas in parallel, applying specializations and avoiding collision etc.
For an object delivery task, the system can divide it into several parts: picking an object out from a shelf, and delivering it to the user.
The system performs this complex task by carefully utilizing single-function robots while avoiding space conflictand resource collisions.
We confirmed that the property of the instruction helped end-users to specify the robot operations concretely, however, the current implemented property was limited.
For example, our current implementation does not allow the users manually increase the number of robots for pushing and delivering heavy objects.
Current working prototype did not support multi-robot collaboration work.
With the development of the robot technology, we will improve our system for supporting much more properties of instructions.
For now, the motion capturing system is expensive equipment.
Despite the high accuracy, it is unrealistic to install into the home.
Recent years, with the development of the ubiquitous computing technology, RFID-based realtime location system had been developed .
Currently, these systems do not have enough speed and accuracy to control small home robots yet, however, they have possibility of being installed into the home because of the easiness and smallness.
This will be a future work of the system.
Restricted by the current supply of the home robots, the system only supports limited robot actions.
When the home robot interface is explored in near future, the design needs to be careful at several aspects.
The fact that some users in our study misunderstood the gestures, which led to the misuse of them reveals that the unambiguity of the stroke gestures is one of the most important factors concerning the gesture design.
The capability of setting tasks across the rooms will provide the users with much more flexibility.
For the current interface, we support tasks in multiple rooms, but one task must start and end at the same room.
The further explorations of the home robot interface should take this into consideration.
A reminder function is "one of wanted function, because machines  usually perform their work without asking."
A housework recording and playing function will comfort the users.
They will then be able to check the history of the housework by home robots.
We presented the Roboshop, a graphical user interface for managing and arranging the household work.
Once the user sets up the instruction of housework, it will be performed by home robots whether the user is in the home or not.
Roboshop supports various types of housework, composes detailed housework instructions, sets up the tasks in a coordinated time span and space, and lets users review and reuse tasks that are tied to the physical environment.
A user study had conducted to evaluate the ease-of-use the interface and to collect users' feedbacks.
The analysis of results showed that several important findings about users' attitudes and concerns towards future types of robot housework and instruction styles.
We hope to enhance the capabilities of Roboshop as a physical assistant in room environments.
The current system is a proof-of-concept working prototype with limitations in terms of both users' and designers' prospects.
The results of our study indicate that users would love to delegate as many types of housework to home robots as possible.
However, the sketching mechanism itself sometimes delivers very limited instructions to the robots, e.g., laundry and folding the garment is impossible with the interface.
Also, cooking instruction will be hard to design.
These types of tasks are not very suitable for current Roboshop interface.
