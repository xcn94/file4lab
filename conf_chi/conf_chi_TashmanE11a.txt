Active reading, involving acts such as highlighting, writing notes, etc., is an important part of knowledge workers' activities.
Most computer-based active reading support seeks to replicate the affordances of paper, but paper has limitations, being in many ways inflexible.
In this paper we introduce LiquidText, a computer-based active reading system that takes a fundamentally different approach, offering a flexible, fluid document representation built on multitouch input, with a range of interaction techniques designed to facilitate the activities of active reading.
We report here on our design for LiquidText, its interactions and gesture vocabulary, and our design process, including formative user evaluations which helped shape the final system.
LiquidText running on tablet PC.
Many of these activities are well supported by paper, but historically, computers have proven inadequate .
Recently, however, advances in hardware and software have enabled computers to achieve many of the qualities of paper, such as support for freeform annotation and navigation using the non-dominant hand.
Thus, studies suggest that recent penbased tablet PCs match or surpass paper as an AR medium .
In effect, computers are getting better at supporting AR by building upon an increasingly paper-like experience.
However, while current systems may have succeeded in mimicking key affordances of paper, paper itself is not the non plus ultra of active reading.
O'Hara's description of the actions that occur in the AR process highlights the many weaknesses of paper; while paper supports some things well--such as freeform annotation and bimanual interaction--it lacks the flexibility for others .
For example, viewing disparate parts of a document in parallel, such as for comparison or synthesis, can be difficult in a paper document such as a book.
Even annotation--traditionally seen as a strong point for paper--can be constraining, complicating the creation of large annotations, or marginalia that refer to disparate or large portions of text, or to multiple texts.
And though the tangibility of paper does support some rapid forms of navigation, the linear nature of most paper texts gives the reader little flexibility for creating their own navigational structures.
All this is not to say paper is bad; however, purely mimicking the affordances of paper in a computer-based system may not address all of the opportunities for improvements that digital technology can potentially provide.
Particularly, a more flexible fundamental approach to document representation may provide opportunities to better support AR.
From magazines to novels, reading forms a critical part of our lives.
And beyond the relatively passive reading of a blog or newspaper, many reading tasks involve a richer interaction with the text.
This interaction, known as active reading  includes processes such as highlighting and annotating, outlining and note-taking, comparing pages and searching .
AR is a regularly occurring component of knowledge work.
For example, prior studies have shown that two specific ARrelated activities, reading to answer questions and reading to integrate information , each constituted about 25% of the time knowledge workers spend reading .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
This paper presents LiquidText, an AR system we developed, which explores a substantially different approach to representing and interacting with documents.
In contrast to the model embodied by paper, which offers a stable but rigid representation, we focus on giving the reader a highly flexible, malleable kind of document.
Specifically, LiquidText provides rich, high degree-of-freedom ways to manipulate the presentation of content, control what content is displayed and where, create annotations and other structures on top of content, and navigate through content.
To do this, LiquidText provides a multitouch gesture-based user interface running on modern multitouch displays.
We also present the design processes, varying ideas, and tradeoffs involved in developing LiquidText.
In the following sections, we consider areas of related work, our design process, and the LiquidText system, followed by discussion and reflections on the project.
This requires high efficiency, even for traversing large amounts of text, and benefits from metadata cues or landmarks to help the reader maintain orientation.
It should also support reader-created bookmarks, and allow the reader to perform other tasks in parallel with navigating .
Of the systems designed to support AR as a whole, two of the most complete are XLibris  and PapierCraft .
XLibris is a stylus-based system that augments an explicitly, extremely paper-like metaphor with digital functionality.
PapierCraft goes further in offering a paper-like experience, actually letting users control digital documents through marks on physical paper.
Building atop a paper-like experience has benefits, including familiarity , but can bring some of the limitations associated with paper as well.
As examples, PapierCraft was still subject to a lack of margin space, and challenges creating annotations that spanned multiple documents; XLibris users noted an absence of flexibility that arose from the explicitly paper-like metaphor followed by the system, including the inability to alter the linear presentation of text, or to spatially alter pages to construct different, parallel representations of content.
So while the system was comfortable to use, its advantages over paper were sometimes unclear, as one user stated explicitly .
By contrast, our goal with LiquidText was to remove some of the rigid constraints imposed by a too-literal adoption of a paper-like metaphor; in contrast with these earlier systems, we sought to explore the possibilities afforded by more flexible, decidedly non-paper-like representations.
Broadly, earlier work has shown that AR involves four core processes: annotation, content extraction, navigation, and layout .
Here, we briefly discuss each process and note some of the requirements for supporting it.
As this research informed our design goals for LiquidText, we structure our later presentation of the system in terms of these categories.
To maintain efficiency, this must be closely integrated with the reading process, as there is a significant potential for interference between the two.
Users also must easily be able to organize, review, and trace their extracted content back to its source .
In contrast to the above, other systems have addressed certain AR-related tasks without seeking to support the entire active reading process.
Such work ranges from helping authors provide new types of navigational affordances for their readers  to dynamic layouts that provide detail on demand .
Still other systems have explored novel approaches to text visualization both for documents  and for source code .
While these systems are not specifically targeting AR, they do suggest various interactions and manipulations that may be relevant to AR nonetheless.
Our goal with LiquidText was to create a design that overcomes some of the innate difficulties inherent in paper-like models.
To do this, we undertook an iterative design process, through which we created the final version of the system described here.
But first, we describe our design process in more detail.
This evaluation consisted of a multi-part, 18-person study.
The study began with a weeklong journaling task, in which participants recorded each time they performed an AR task, answering questions about their goals, difficulties, etc.
This helped us obtain a baseline view of our participants' current AR practices.
Participants were then interviewed individually about their general AR behavior, followed by fifteen minutes of training on using the prototype system, which ran on a 12.1" Dell XT2 multitouch tablet.
Participants also received a help sheet showing how to perform seven of the main functions in the prototype.
Participants then performed a 25-minute AR task, where each read a 5-page popular science article using the prototype and wrote a detailed critique.
Afterward, we performed a semi-structured debriefing interview, asking participants about various aspects of their reactions to the system.
Finally, we concluded this phase of the design process with two separate, three hour participatory design workshops, where 14 participants  brainstormed and mocked-up future AR environments.
We recruited broadly for the study, seeking regular active readers in a large design and manufacturing firm.
The 18 participants  included managers, designers, lawyers, students and strategists.
Of these, 16 returned their journals and were found to perform an average of 6.6 AR tasks over the roughly weeklong journaling period.
While the complete results of this study are beyond our present scope , we describe below how our results informed the final design and functionality of the system.
We also began exploring alternative metaphors for text representation--ones that would promote a user experience that felt fluid and flexible; not only allowing, but inviting users to step outside the bounds of predefined structure that computers often impose.
From a design perspective, to construct these metaphors, we found it helpful to explore forms and substances that exemplify malleability, such as putty or water.
And while considering these ways of manipulating materials, we improvised interacting with imaginary systems inspired by such substances.
Throughout this design process, we sought to brainstorm in the context of plausible scenarios, in order to lead our thinking toward designing a complete, integrated system, rather than just a collection of standalone interactions.
Likewise, to maintain a manageable scope, we focused on supporting the core processes of AR: the interaction with source texts and the creation of notes, as opposed to a larger AR workflow including creating wholly new documents, sharing documents, and so on.
This led to several semi-formal critiques where other HCI professionals offered feedback on our designs and prototype.
This provided us with feature suggestions and design criticism, and helped to inform the required feature set of the project.
In developing this initial prototype, we also chose to focus on single-document scenarios, as the e-reading appliances with which LiquidText would most likely be used are relatively small, tablet-sized units, and these simpler scenarios seemed a more tractable design goal.
We focused on areas that were especially important to users, but also relevant to our larger design objectives of exploring more flexible representations of text documents.
The section below describes the final LiquidText system and its features.
Finally, users may also create custom navigation links among all the various forms of content in the system, such as from the main document to comments or excerpts in the workspace.
Some of the basic interactions in LiquidText reuse a number of common gestures that appear in other touch applications .
For example, the user can position objects, such as comments, excerpts, and documents, just by dragging with a fingertip.
The user can rescale an object by using two fingers to horizontally pinch or stretch it, or she may rotate it by twisting with three fingers.
The preview pane next to the document  provides a scaled-down view of the document, and the user may simply touch it to navigate the document to the corresponding point.
In the following sections we discuss the details and rationale for the central aspects of the design, organized according to the major AR processes: content layout and navigation, content extraction, and annotation.
Second, past literature reveals that readers rely on bimanual control while reading with paper, such as when arranging content, navigating, etc.
The interaction gestures that resulted from our design phase similarly reflected this style of rich, parallel input, and so lent themselves well to multitouch interaction.
Naturally, other configurations of devices could potentially satisfy these criteria--such as multiple mice.
In comparison, multitouch has advantages and disadvantages.
For example, multitouch is faster than multiple mice for many bimanual, and even some unimanual, tasks, such as selection .
But by contrast, multitouch also tends to incur far higher error rates than mice , although improved touch sensing and modeling may remediate this significantly in the future .
Additionally, a mouse reduces each hand's input to a roughly single point, reducing the richness of the interactions each hand can perform.
A potential compromise then could come from multitouch-plus-pen interaction , allowing richer, multi-finger input from one hand and simpler but more precise input from the other .
But while we wanted both of the user's hands free to engage in multi-point interaction, we ultimately opted against pen-plus-touch because we were unaware of an adequate hardware platform that could process pen and multitouch input simultaneously--which would be needed for bimanual interaction.
Therefore, we chose pure multitouch input for LiquidText.
LiquidText provides a number of novel features intended to support the AR processes of content layout and navigation.
These features allow users to access an existing text linearly in its original form , yet leverage a range of interactions to selectively view multiple regions of text, and create custom navigational structures through the text.
The major LiquidText features for content layout and navigation are Collapsing, Dog-Earing, and Fisheye Views in the workspace area.
But as our intuition suggested, and our study participants told us, viewing disparate areas of a paper document in parallel is often difficult, requiring frequent flipping back and forth.
To better support this in LiquidText, we were motivated by elastic substances which can be selectively compressed or expanded--suggesting a document that lets one compress or shrink some areas to bring text from disparate areas of a document together--resulting in a sort of 1-dimensional fisheye.
The visualization is thus similar to , and the interaction similar to , but in a very different context.
The visual representation used in collapsing reduces selected rows of text to horizontal lines, thereby indicating the amount of text hidden.
Additionally, multiple regions of text may be collapsed at once, letting the user choose precisely which portions of the document are visible in parallel.
Interactions involving this effect are used throughout LiquidText.
Since this "collapsing" process is applied to a vertical, linear document , LiquidText uses a vertical pinch gesture to provide a natural mapping for this interaction .
This section provides a high-level overview of the basic functionality of LiquidText; subsequent sections provide detailed explanations of the system and its features.
LiquidText starts by presenting the user with two panes : The left contains the main document the user has loaded for reading ; a range of interactions  allow the document to be panned, scaled, highlighted, partially elided, and so on.
The large workspace area  provides space for any comments or document excerpts the user may have created; here, they can be grouped and arranged, and the workspace itself can be panned or zoomed.
Three stages of an increasing amount of text being collapsed together.
This interaction, however, raises the design question of what should happen when the user scrolls a partially collapsed document.
Under the first option, the collapse is a part of the document itself, and can be used to conceal irrelevant material.
Under the second option, the reader effectively has two windows into the document, each of which can be scrolled independently.
Both choices have advantages, but we selected the first option as it better fit our metaphor of pinching/stretching the document.
By contrast, the latter option is similar to systems like Adobe Reader, which show disparate regions of a text by allowing the user to create multiple, independent views into the document via a split pane.
But that approach to multiple, independent views comes at a cost, offering little clue as to the relative order and distance between the document regions; this disruption to the document's underlying linearity may also interfere with a user's sense of orientation .
Collapsing, through either approach though, guarantees that visible regions are always in the order of the original document, and provides a visual cue as to how much text has been collapsed between two visible regions as well.
While this basic collapsing interaction provides a useful way to hide irrelevant text, and bring disparate document regions into proximity, manually pinching a long document together to view, say, the first and last lines of a book, is tedious.
LiquidText thus provides two indirect ways to collapse a text: first, touching the preview pane with two or more fingers causes the document to automatically collapse to show all the indicated areas at once.
Alternately, holding down a desired part of the document with a first finger--effectively holding it in place--while using a second to touch the preview pane,
Holding one finger on document while scrolling another finger on preview pane collapses document to show both locations.
And in contrast to traditional document viewing software, in which users must create separate panes and scroll them individually, this functionality lets a user view two or more document areas with just one action, parallelizing an otherwise serial task.
We created the collapse interaction before our formative study, so we evaluated it in the study and found user feedback to generally be positive.
Notably though, two participants used collapsing as a way to conceal irrelevant material, even though we only presented it as a way to bring disparate material together--suggesting our decision to make collapses part of the documents themselves was of value to users.
Such "ephemeral" ways of book-marking are often used when rapidly flipping between regions of a text.
In addition to offering several options for persistent book-marking , LiquidText includes a type of transient bookmark we call dog-earing.
This feature was focused on scenarios where the user would create and refer to a bookmark over a short period of time, and is intended to support near effortless creation and deletion of the bookmarks.
In LiquidText, this interaction is modeled on its paper counterpart: users associate a finger with a given document location, much like putting a thumb on a book page allows a user to rapidly flip between places in the text.
To create one of these bookmarks, the user simply puts down a finger in the dog-ear region of the LiquidText application , which creates a small orb under her finger corresponding to the current state of the document .
The user can then navigate as she wishes using the other hand, while keeping her finger on the orb to "save her place."
To return to the captured state, the user simply lifts the finger off the orb and LiquidText returns to that state while the orb fades out.
To discard an orb without using it, the user drags it away from the dog-ear region before releasing it.
The choice of gesture here enforces transience, in effect: the bookmark will be kept for only as long as the user's finger is in place .
Bookmarks held in this way vanish as soon as the finger is lifted, meaning that the user is freed from having to explicitly delete the bookmark.
Though we developed dog-earing before the formative study, we found our tablets could not reliably detect the gesture, and so we disabled it in the build given to users.
To avoid these downsides, we created a novel type of fisheye effect; users access this functionality through one or more fisheye "lenses" they create in their workspace .
But in contrast to typical fisheye effects, all text in a given object is scaled to the same level in order to promote readability .
In the final version of our system, the workspace is panned or zoomed using typical drag/pinch gestures.
Fisheye lenses are created by tapping with three fingers; once created, they can be moved by dragging and resized by pinching or stretching the lens border.
The magnification level is controlled by rotating the lens--akin to the zoom lens of a camera.
Thus, the user can control all the essential properties of the distortion in tandem and, using multiple hands, for two lenses at once.
Thus, while fisheye distortions have been deeply explored, our approach combines two unique features: consistent scaling within each object, and the ability to create and manipulate the distortion's attributes in parallel through multitouch.
These can be rescaled to see them more closely, or to make more space as needed.
During the study, however, users commented on the dearth of workspace available to them on the 12.1" tablet which ran the prototype version of our system.
Likewise, our participatory design workshops indicated that users required a large space in which to see all their documents at once--and to get an overview of any cross-document relationships.
In light of this, we explored a set of new features for the final version of our system, designed to overcome the limitations of simply scaling individual objects.
We considered several alternatives for letting users work with larger numbers of objects effectively.
Obviously, a physically larger display would be helpful, but this trades off against portability .
We also considered providing some form of organizational structures, such as allowing users to make hierarchical outlines of comments and excerpts in which tiers can be shown or hidden as needed.
Outlines have certain downsides, however: they impose a strict organizational structure that may be inflexible, and they also privilege hierarchical over spatial relationships.
Ultimately, for the final system, we settled on the notion of a quasi-infinite, continuous workspace for comments and excerpts, extending beyond the display.
This workspace can be panned and zoomed, thus supporting spatial overviews of comments and excerpts, and maintaining consistent spatial relationships among the objects within the space.
Since simultaneous viewing of multiple pieces of content is important in AR, we considered a number of approaches to supporting this functionality in the workspace region.
For example, one might allow regions of the workspace to be collapsed, to bring distant objects into proximity even when zoomed in.
Although this idea is appealing--especially since the collapsing concept is already present in LiquidText--the technique was most suited to one-dimensional spaces.
As one of the central processes of AR, extracting textual excerpts from documents serves to support aggregation and integration of content from across one's documents.
It lets the reader maintain peripheral awareness of key material, and explore alternate organizations of an author's content .
But, in most present approaches to AR, content extraction has been quite heavyweight: paper-based active reading may require copying, scanning, or rewriting portions of content in a notebook; even computer-based AR may require one to open a new document, copy and paste text into it, and save it.
We sought to create a fast, tightly-integrated set of mechanisms for content extraction in LiquidText that would not distract from or interfere with the AR process itself .
To devise an intuitively appealing, flexible interaction to support content extraction we again sought to draw on physical metaphors.
Imagining a document as puttylike, we conceived of extracting content as analogous to pulling it off of the document.
The two major parts of this interaction are, first, selecting the text to be excerpted, and, second, actually transforming that selected text into an excerpt.
Creating an excerpt begins with selecting the desired text.
This way, the user did not have to shift her gaze away from the document to invoke a selection mode.
Our formative study revealed disadvantages to both approaches.
Several users disliked the bimanual requirement of the modal approach, and the non-modal approach was difficult to perform reliably.
By contrast, users indicated that they wanted to select text by simply dragging a finger--the same gesture they preferred to use to move objects.
Thus, for the final system, we sought a gesture distinguishable from, but very similar to, simple dragging.
The result, replacing our initial non-modal interaction, was a simple tap-and-drag--also used in some iOS apps.
That is, the user puts down a finger at the start of the intended selection, lifts her finger and then lowers it again, then drags her finger to the desired endpoint.
To give the user prompt feedback, a selection cursor is displayed as soon as the finger touches the screen.
Attaching two excerpts to form a group.
In designing the ultimate appearance and behavior of these groups of excerpts, though, we faced a tension between the advantages of structure versus flexibility.
In one approach we considered, grouped excerpts move into a container-object, where they are aligned and sequentially listed.
This approach is visually simple and organized, especially for a small screen.
Alternately, we considered letting grouped objects be positioned arbitrarily and freely within a group, and have visual links rendered between them to show they are grouped.
This option gives users more means of expression, letting them indicate differences in group constituents by position/size/angle, but would likely be messier.
We felt however, that the fisheye workspace would adequately accommodate any potential disorganization, and so chose this latter option, informing the interaction's behavior with the putty metaphor used in excerpt creation.
Excerpts are grouped simply by dragging them together-- which creates a fluid-like border surrounding all objects in the group .
Pulling them apart stretches the border until it eventually snaps, separating the group.
Following the putty metaphor, the user creates an excerpt simply by anchoring the document in place with one finger, while using another to drag the selection into the workspace until it snaps off of the document .
The original content remains in the document, although it is tinted slightly to indicate that an excerpt has been made of it.
While excerpts alone can be helpful, being able to trace them back to their source or context is known to be vital for AR .
We explored two alternatives to providing such context: the first was in-place expansion, where the original context of an excerpt was made to appear around the excerpt, showing the context without disturbing the original document.
The second was linking, where users can interact with an excerpt to cause the source document to position itself so the excerpt's context can be seen.
Each has advantages: in-place expansion lets the user keep her gaze in the vicinity of the excerpt and avoids disturbing the position of the original document.
But if the user needs to do more than just glance at the source context, she will likely need to navigate the original document to that point anyway--which linking does immediately.
Likewise, an in-place expansion on a small screen may well cover much of the user's workspace, raising visibility problems, especially if one wanted to see the contexts of two excerpts at once.
For this and other reasons we ultimately chose linking, which we implemented bi-directionally, so excerpts could link to sources and vice versa.
To support this and our general design requirement of flexible content arrangement, excerpts can be freely laid out in the workspace area; they can also be attached to one another  to form groups.
This allows users to create whatever structure they may desire on their excerpts, rather than the system imposing explicit relationships that may not be necessary or may not fit with users' models of the structure.
By touching the arrow button near an excerpt, the source document immediately moves to a new position in which it can show the excerpted text in its original context; likewise, touching the arrow button near the source document will move the excerpt object in the workspace into view.
This mechanism effectively provides a way for users' own excerpts to be repurposed as a way to quickly navigate through the original source document.
Further, we again take advantage of multitouch to enable users to view multiple contexts at once: multiple arrow-buttons can be held down to scroll and collapse the document to show multiple source areas simultaneously.
As the excerpt interactions were designed before the study, we investigated how they were used by participants.
We found that the idea of extracting content via touch was in-line with user expectations, as even before seeing our prototype system, some users expressed an interest in using touch to pull content out of a document, and several described the existing copy/paste process as being laborious.
Perhaps as a result, of the various functions in the prototype, the ability to create excerpts received the most strongly positive feedback, with eight users noting it as a feature they liked.
In the AR study task, participants typically used excerpts as a way to aggregate content or to provide peripheral awareness.
They noted that they pulled out quotes and even whole paragraphs so they could refer back to them later.
And partly due to the support for grouping, several users discussed its value in allowing them to reorganize the content of the document.
One user described this as, "What  was really helpful for was because I had kind of written my own version of the story on the side."
Excerpting, therefore, seems to have been used in much the way it was intended, but its frequency of use allowed it to easily consume available display space.
So the findings of the study did not lead to significant changes to the excerpts themselves, but did provide part of our motivation for adding the fisheye workspace .
Comment's arrow-button collapses document to show both pieces of associated text.
Like excerpts, they can be pulled off, rearranged, grouped with other items , and maintain persistent links back to the content they refer to.
This especially helps support separation, aggregation and retrieval of annotations, as suggested by .
LiquidText also breaks away from a one-to-one mapping between content and annotations.
Rather, comment objects can refer to any number of pieces of content across one or more documents .
And since they maintain twoway links to their referents, annotations can thereby act as persistent navigational affordances, letting readers freely create trails of links within and between documents.
To actually add a comment, the user simply selects the desired text and begins typing.
This creates the comment attached to the document, and next to the selected text.
The user can also create a comment by just pressing a button on the button bar , creating a comment that is not linked to other text.
This allows comments to act as very simple text editors, letting the user begin to work on the content creation tasks which often come later in the AR workflow.
As the commenting functionality was included in our prototype, we received feedback on it in our user study.
We found comments to be used frequently, employed by 10 of the 18 users.
These annotations were used for summarization and commenting, such as identifying contradictions and noting relationships to other projects.
For the particular task we used in our sessions, users did not appear to have a need to associate comments with more than one piece of content; nonetheless, one user spoke more generally of this being one of the most valuable features of the prototype, "I liked the ability to annotate and connect different chunks, so the ability to say I want to highlight this piece,  I want to highlight this  piece, and make a comment about both of them for some later use."
Comments on paper must generally be fit to the space of a small margin, and are typically only able to refer to a single page of text at a time.
While software such as Microsoft Word or Adobe Reader avoid some of these difficulties, they still largely follow paper's paradigm; their annotations are thus still limited to single referents, and control of the size or scale of annotations is very limited, so available space is easily consumed.
Following from our general design goals, we sought to provide a more flexible alternative in LiquidText.
Our user study showed this to be partly true--we found that while 9 of the 18 participants used highlighting, those users who tended to make more use of LiquidText's excerpt and commenting functions also seemed to make less use of the highlighting.
One participant even described highlighting as possibly redundant with the system's other functions--especially excerpts.
Nonetheless, some users' preference for highlighting led us to further develop the feature beyond the humble implementation in our initial prototype.
User feedback led to more color options, which we provided through a color palette .
To improve efficiency, we also added an inplace highlight gesture: after selecting text, an upward swipe on the selection highlights it, with the swipe distance controlling the highlight intensity.
Swiping downward dims and removes the highlight.
Participants also requested highlight aggregation functions.
The final version of our system includes this by building on our existing collapse notion, and provides an auto-collapse function to show only the highlighted text, plus some variable amount of context.
Since the aggregation function was built on the concept of collapsing, we chose to control it in a similar way.
While holding a soft-button accessed through the color palette, the user simply does the vertical pinch gesture to cause the document to collapse to show all the highlights.
This was necessary for scenarios where the user might wish to aggregate highlights in part of the document, while reading another part normally.
Multitouch fisheye control was added only after our study, but may be viable in many navigation and layout situations as multitouch platforms become more popular.
Underlying the specific interaction techniques, however, is the notion of flexible, high degree-offreedom representations, combined with comparably rich input devices to take advantage of them.
We sought to apply this in the context of active reading, but the approach may be advantageous in other domains as well.
Our formative study participants, for example, informed us that tasks like email, spreadsheet use, and manipulating presentations often intersect with AR, and share many of it challenges--like visualizing disparate pieces of content in parallel.
Our use of multitouch, specifically, turned out to be an essential component for realizing our design goals.
Interactions involving many degrees of freedom, such as the fisheye lenses and collapsing, depended heavily on this type of input--they otherwise would likely have required many separate controls.
Our study also supported this input model, with many users seeing touch as natural and very direct.
However, they also described an interest in more multi-modal input, combining touch with gaze, a mouse, or a pen--which we may investigate in our future work.
But while the spatial flexibility of our representation is also essential to LiquidText, one potential pitfall of this approach is that a sense of position within the document can be sacrificed, impeding the spatial cues that help the reader maintain a "sense of a text" .
While a full assessment of this issue remains for future work, we did ask the participants about their sense of orientation while using LiquidText; of the 15 who addressed the issue, 13 claimed they maintained orientation when using the system.
Throughout the design of LiquidText, we observed a tension between the flexibility we offered users in performing their task, versus the amount of predefined structure we provided to simplify their task.
Even approaches to making structure optional sometimes seemed to impose structure.
For example, we considered allowing users to name object groups, but to make the feature visible required a default title already in place; but any meaningless, default title  would be effectively begging the user to rename it, meaning that the choice of whether or not to name is no longer so optional.
Given the goals of LiquidText, we generally opted for flexibility, but we leave for future work the question of how better to balance these competing ideals.
While LiquidText was intended to be used as a unit, several of its interactions may be applicable on their own in other contexts.
Comments, for example, supporting multiple referents and two-way links allows them to act as simple navigational affordances, and might make sense in many document annotation situations.
It is difficult to disentangle the factors that contribute to users' impressions of a system, but over the course of our design process, people generally seemed to appreciate the "feel" that LiquidText provided.
For example, p2's comments about the bubbliness and personality of the system, and the positive reactions from people to the putty-like connections between objects.
Seeing that these things, at least in some cases, are not lost on users, hints to us that even in supporting mundane tasks like reading, design must encompass not just what the system does, but the message it conveys about itself and how it expects to be used.
As we explore the potential value and uses of LiquidText, it will be important to conduct additional evaluation both to provide empirical assessment of our final designs, as well as understanding the types of AR tasks where it might be of the most use.
One of these tasks, our formative study suggested, is multi-document scenarios--which LiquidText currently does not support.
We plan to explore this in the future and believe some of LiquidText's interactions, such as the fisheye zooming, will help provide the flexible workspace control that multi-document work requires .
In summary, LiquidText offers readers a substantially new way to interact with their documents.
While most systems supporting AR replicate the experience of paper, paper is in many ways inflexible, and carries significant limitations.
LiquidText takes a different approach, providing a flexible, malleable representation that gives the user fine grained control of the visual/spatial arrangement, navigational affordances, and annotations of their documents.
To control this representation, LiquidText uses a vocabulary of multitouch gestures allowing efficient, often in-place, interaction.
A diary study of work-related reading: design implications for digital reading devices CHI, ACM Press, 1998.
Askwall, S. Computer supported reading vs. reading text on paper: a comparison of two reading situations.
International Journal of Man Machine Studies, 22.
Fluid annotations through open hypermedia: using and extending emerging web standards Proc.
Holz, C. and Baudisch, P. The generalized perceived input point model and how to double touch accuracy by extracting fingerprints CHI 2010, ACM.
Holzinger, A., Holler, M., Schedlbauer, M. and Urlesberger, B.
An investigation of finger versus stylus input in medical scenarios ITI 2008, IEEE, Dubrovnik, 2008.
Hornbaek, K. and Frokjaer, E. Reading patterns and usability in visualizations of electronic documents.
Liao, C., Guimbretiere, F., Hinckley, K. and Hollan, J. Papiercraft: A gesture-based command system for interactive paper.
Introducing a digital library reading appliance into a reading group Proc.
Murray, T., Applying Text Comprehension and Active Reading Principles to Adaptive Hyperbooks.
O'Hara, K. Towards a Typology of Reading Goals RXRC Affordances of Paper Project, Rank Xerox Research Center, Cambridge, UK, 1996.
A comparison of reading paper and on-line documents CHI 1997, ACM, 1997.
Student readers' use of library documents: implications for library technologies CHI 1998, ACM Press, 1998.
Understanding the materiality of writing from multiple sources.
Int'l Journal of Human Computer Studies, 56.
Linking by inking: trailblazing in a paper-like hypertext Proc.
An Outline for a Functional Taxonomy of Annotation, Presented at Microsoft Research, Redmond, WA, 1999.
Beyond paper: supporting active reading with free form digital ink annotations CHI 1998, ACM, 1998.
Sellen, A. and Harper, R. Paper as an analytic resource for the design of new technologies CHI 1997, ACM, 1997.
Tashman, C. and Edwards, W.K., Active Reading and Its Discontents: The Situations, Problems and Ideas of Readers.
Wu, M. and Balakrishnan, R. Multi-finger and whole hand gestural interaction techniques for multi-user tabletop displays UIST 2003, ACM, 2003.
Zeleznik, R., Bragdon, A., Adeputra, F. and Ko, H.-S. Hands-on math: a page-based multi-touch and pen desktop for technical work and problem solving Proc of ACM UIST, ACM, New York, NY, USA, 2010.
