While proponents of adaptive user interfaces tout potential performance gains, critics argue that adaptation's unpredictability may disorient users, causing more harm than good.
We present a study that examines the relative effects of predictability and accuracy on the usability of adaptive UIs.
Our results show that increasing predictability and accuracy led to strongly improved satisfaction.
Increasing accuracy also resulted in improved performance and higher utilization of the adaptive interface.
Contrary to our expectations, improvement in accuracy had a stronger effect on performance, utilization and some satisfaction ratings than the improvement in predictability.
We use the term accuracy to refer to the percentage of time that the necessary UI elements are contained in the adaptive area .
We focus on these properties because they reflect a common design trade-off in adaptive UIs: whether to use a simple, easily-understood strategy to promote functionality, or whether to rely on a potentially more accurate but also more opaque machine learning approach.
We present a study showing that increased accuracy significantly improved both performance and adaptive interface utilization.
Furthermore, both predictability and accuracy significantly increased participants' satisfaction.
Contrary to our expectations, we found that in our particular design, increasing the adaptive algorithm's accuracy had more beneficial effects on the participants' satisfaction, performance and utilization of the adaptive interface than did improved predictability.
Our results suggest that machinelearning algorithms deserve further consideration in the context of adaptive UIs, because the benefits of a large improvement in accuracy may outweigh the disadvantages of decreased predictability.
Despite considerable debate, automatic adaptation of user interfaces  remains a contentious area.
Proponents of machine learning-directed adaptation  argue that it offers the potential to optimize interactions for a user's tasks and style.
Critics , on the other hand, maintain that the inherent unpredictability of adaptive interfaces may disorient the user, causing more harm than good.
Fortunately, recent studies have presented suggestions for which properties of adaptive UIs increase user confusion and which improve satisfaction and performance .
But the design space for adaptive UIs is large, with a multitude of characteristics that may determine an adaptive interface's success or failure.
The tradeoffs between many of these characteristics are still poorly understood.
In this paper we explore the relative effects of predictability and accuracy on the usability of adaptive interfaces.
Building on previous research, we hypothesized:  the higher the accuracy of the adaptive algorithm, the better the task performance, utilization and the satisfaction ratings;  the more predictable the adaptive algorithm, the better the task performance, utilization and the satisfaction ratings;  increased predictability would have a greater effect on satisfaction and utilization than increased accuracy.
We formulated this last hypothesis based on the design heuristic asserting that successful user interfaces should be easy to learn .
Twenty-three volunteers  aged 21 to 44  participated in this study.
All participants had normal vision, moderate to high experience using computers and were intermediate to expert users of Microsoft Office-style applications, as indicated through a simple screener.
Participants were given a software gratuity for their time.
Post-experimental interviews confirmed that our participants easily formed a mental model of this MRU policy.
Hence, the study was a 2   2  or Low  factorial design.
We predetermined the sequence of buttons that had to be pressed in all conditions as well as the contents of the adaptive toolbar in the random condition to ensure the desired level of accuracy.
After familiarizing themselves with the task and completing a practice set using a non-adaptive interface, participants performed four counterbalanced task sets, one for each of the four conditions.
Participants filled out a brief satisfaction survey after each task set, and an additional survey and an exit interview following the last session.
Participants took 2.5 to 5 minutes per task set, and the whole study took less than an hour.
In a split interface, functionality  that is predicted to be immediately useful is copied to an adaptive area .
This allows the user to either follow their familiar route or potentially save time by exploiting the adaptation.
We used a carefully controlled performance task that allowed us to eliminate complications associated with more complex tasks.
Our task and procedures are modeled on those used in 's second experiment.
In the study, we showed participants a picture of a target UI button, which they had to find and click within the interface.
They could use either the traditional  toolbars and menus or the adaptive toolbar if the button was displayed upon it.
We used a modified Microsoft Word interface, where several toolbars were placed toward the left and the center of the toolbar area .
Some of the toolbar buttons revealed popup menus, in which additional functions were located.
In the center of the screen was a panel that displayed the target button as well as a "Next" button, which participants used to proceed through the study.
We only asked participants to find buttons originating in popup menus, which were one level deep.
The adaptive toolbar, whose contents changed during the experiment, was located in the upper right so that it was far enough from the closest, relevant, non-adaptive button .
This ensured that it required an explicit change of gaze to discover if a helpful adaptation had taken place.
Eight buttons were always shown in the adaptive toolbar, and no more than one button was replaced per interaction.
Participants clicked on 60 target buttons in each task set.
We considered the first 10 clicks to be a "ramp-up" time and did not include performance metrics for these clicks in our analysis.
We collected overall task times as well as the median times to acquire individual buttons , distinguishing times for buttons in their original locations from those located on the adaptive toolbar.
We also measured the adaptive toolbar utilization levels, or the number of times that the participant selected the requested UI element from the adaptive toolbar divided by the number of times that the requested element was present on the adaptive toolbar.
Additionally, we collected the subjective accuracy of the adaptive algorithm, and participant satisfaction ratings .
Finally, we asked a random subset of participants to perform an extra set of tasks following the main experiment; here we used an eye-tracker to determine which strategies our participants employed.
Performance considerations prevented us from using the eye tracker during the main part of the experiment.
Our study considered two accuracy levels: 50% and 70%.
Because it is difficult to implicitly measure predictability, and measuring it explicitly might influence user performance , we considered two extreme cases.
In the unpredictable condition, updates to the adaptive toolbar were entirely random - a worst-case simulation of a complex algorithm's inscrutability.
Because we cannot justify any assumptions about the distribution underlying Likert scale subjective responses, and because three participants omitted answers to some of the questions, we used ordinal logistic regression   to analyze those data.
Subjective data from one participant were lost due to a software error.
Table 1 summarizes the results.
In the free response part of the post-task questionnaire, 11 out of 23 participants spontaneously commented, after at least one of the two random conditions, that the toolbar behavior was "random," "confusing" or otherwise unpredictable.
In contrast, after the predictable conditions only two participants commented that they did not understand the adaptive toolbar's behavior, while three specifically observed that it behaved more predictably than in earlier conditions.
Similarly, when debriefed after the study, the majority of participants correctly described the algorithm in the predictable condition as selecting the most recently used items, while a few felt that the system selected the most frequently used items.
Participants often described the algorithm in the random condition as behaving in an apparently random manner though they often assumed that the behavior was purposeful  and that the algorithm was trying to "guess" or "help".
No significant effects were observed for the algorithm's predictability.
Treating the condition with low predictability and 50% accuracy as a baseline, we investigated which change would more greatly impact the participants: raising predictability or improving the accuracy to 70%.
Thus, we compared two conditions: predictable but only 50% accurate versus random but 70% accurate.
In analyzing the eye tracking data , we identified three regions of interest : the static buttons on the top left, the adaptive toolbar at the top right, and the task presentation area in the center of the screen .
The small sample collected led to low statistical power, but the data shed some light on the approaches used by the participants.
We looked at transitions between the ROIs to see if the participants were more likely to look at the adaptive toolbar or the static toolbar after being presented with the next button to click.
We found that users moved their gaze from the task presentation area to the adaptive toolbar  much less in the low accuracy condition than the high one .
The difference between predictable and random conditions did not elicit similar difference in behavior.
We also looked at the percentage of times participants first looked at the adaptive toolbar, failed to find the desired functionality there, and then shifted their gaze to the static toolbar.
We found that participants looked but could not find the appropriate button on the adaptive toolbar much more often in the random than the predictable conditions .
Participants seemed to be performing better than the expected 40% failure rate  in the predictable condition, suggesting that the more predictable algorithm did help the participants to best direct their effort.
Specifically, if a machine-learning algorithm can more accurately predict a user's next action or parameter value, then it may outperform a more predictable method of selecting adaptive buttons or default values.
However, because predictability and accuracy affect different aspects of users' satisfaction, improvements to one of these factors cannot fully offset the losses to the other.
Our contribution is initial evidence showing the relative impact of these two dimensions on adaptive UIs.
We believe that much future work remains in moving beyond laboratory studies and into the field with users' applications and projects, as well as understanding the crossover points in the tradeoff between improved accuracy and reduced predictability.
Acknowledgments This work was funded in part by the Microsoft Graduate Research Fellowship, NSF grant IIS0307906, ONR grant N00014-06-1-0147, DARPA project CALO through SRI grant number 03-000225, and the WRF /TJ Cable Professorship.
Besides commenting on the predictability of the adaptive toolbar behavior, 10 of the 23 participants commented that they wished the adaptive toolbar were closer to the original locations of the buttons used in order to aid opportunistic discovery of adaptation.
Similar comments were also reported by  - their moving interface, although not statistically better than the non-adaptive baseline, was frequently praised by participants for placing adapted functionality right next to the original location.
We chose 's split interface for this study, because it was statistically better than the baseline, but a hybrid approach might be even better.
We have examined the influence of accuracy and predictability on adaptive toolbar user interfaces.
Results show that both predictability and accuracy affect participants' satisfaction but only accuracy had a significant effect on user performance or utilization of the adaptive interface.
Contrary to our expectations, improvement in accuracy had a stronger effect on performance, utilization and some satisfaction ratings than the improvement in predictability.
