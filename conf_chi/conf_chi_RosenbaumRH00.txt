This paper describes the organizational approaches and usability methodologies considered by HCI professionals to increase the strategic impact of usability research within companies.
We collected the data from 134 HCI professionals at three conferences: CHI 98, CHI 99, and the Usability Professionals' Association 1999 conference.
The results are the first steps towards a toolkit for the usability community that can help HCI practitioners learn from the experiences of others in similar situations.
We hoped to produce a toolkit for the usability community, to enable usability practitioners to learn from the experiences of others in similar situations.
This toolkit would consist of the organizational and methodological approaches, the organizational demographics, the usage rating , the effectiveness rating, and whether any of the correlations were statistically significant.
This work is a continuation of the research from the CHI 98 workshop the authors organized on "Unpacking Strategic Usability: Corporate Strategy and Usability Research.
This paper discusses the sequence of data gathering and proposed theories, and the state of the toolkit to date.
Usability organizations often say they would like to be more effective and influential in how corporations develop products.
Since usability organizations have grown within many companies over the last 5-10 years, we now have the opportunity to examine how influential various approaches have been at the more strategic level.
For this research, we defined "strategic usability" as embedding usability engineering in the organizational processes, culture, and product roadmaps.
In strategic usability, usability data contributes to corporate-wide decision-making, such as product priorities and make vs. buy decisions.
Since we couldn't realistically conduct a controlled study in dozens of organizations, instead we asked usability experts their perceptions of the strategic effectiveness of a variety of approaches.
These approaches included organizational approaches  and methodological activities .
To copy otherwise, to republish,to post on serversor to redistributeto lists.
While preparing for the CHI 98 workshop, we observed that the organizational backgrounds of the participants would be closely tied to their workshop contributions.
Therefore, we asked the participants to fill out a preworkshop profile, with 20 detailed questions about their companies and their HCI groups.
We distributed the answers to all participants before the workshop, along with the position papers, and this information informed everyone's participation, During the workshop, the 13 participants and 3 organizers identified and described 17 organizational approaches and 10 usability methods that they believed contribute to strategic usability.
However, after the workshop was over, the organizers recognized an opportunity to gather additional data and perform more analysis in two areas: * How did the workshop participants' opinions of the organizational approaches and usability methods compare with opinions from other members of the HCI community?
To begin answering these questions, the authors edited the pre-workshop profile questionnaire in two ways.
We simplified the questions somewhat , and we asked for effectiveness ratings of the 27 activities identified during the workshop.
We then asked the workshop participants to update their responses, and we solicited more respondents from a usability listserv.
The 23 responses received from these two groups-workshop participants and listserv respondents--became our pilot sample; they included 9 respondents from large organizations , 8 respondents from smaller firms, and 6 HCI/usability consultants.
The pilot survey questions are listed in Appendix A at the end of this volume.
The complete survey listed the activities in Question 19 and included explanations of Questions 3, 6, I0, and 20; this text is available on the first author's website  under Courses and Papers.
Survey respondents were asked to rate only those organizational approaches and usability methods that they had actually used, to avoid obtaining ratings based on hearsay.
Table 1 shows how the pilot survey participants rated the organizational approaches and usability methods as contributing to strategic usability.
Note that the number of respondents reporting use of each of the organizational approaches and usability methods varies considerably.
We then looked at the collected data describing all 23 organizations to see if there were any correlations between that information and the toolkit ratings.
When considering organizations' size , we categorized respondents as being from large companies , small companies , and consultancies .
There were no statistically significant correlations between any of the answers to the above questions and the respondents' toolkit ratings.
However, when we looked at the organizations' size categories, there were suggestions that the effectiveness of some approaches might vary by.
In particular, smaller settings  might find the following approaches better: high-level/founder support, task analysis, and contextual inquiry.
Consultants might find the following better: usability testing without a lab, lab usability testing, and communities of practice .
Activities with means less than 2.00 might be considered promising based on consistently good experiences, and those with means over 2.50 might be risky based on consistently bad experiences.
In addition, a few activities received highly variable scores: fit into current engineering processes, contextual inquiry, UI group reports to UI, focus groups, and corporate mandates.
After the pilot survey, we removed "design cafe" from the usability methods; it was suggested by one CHI 98 workshop participant and was unfamiliar to the others.
We believe some pilot test respondents interpreted it as a crossfunctional team approach, such as participatory design.
We began the new questionnaire with the definition of strategic usability given in the Introduction.
Sixty percent of the survey respondents who answered the question are funded by an annual budget.
Another 15 respondents were funded by another budget .
The second most common approach to funding is bill-back by project, with 21 respondents.
Some groups had a blend of funding, combining an annual budget or government funding with project funding.
Some survey respondents did not know their funding source.
We then asked the 10 questions listed in Appendix B at the end of this volume; the complete text of the CHI/UPA survey can be reviewed on the first author's website  under Courses and Papers.
For the questions that were retained from the pilot survey, we made a few changes.
We removed "design cafe" from the usability methods.
Since the pilot list did not include participatory design, we added it.
We also added "UI staff members co-located with engineering" to the organizational approaches.
Finally, we listed three kinds of "usability testing"  rather than the two in the pilot test.
Therefore, when considering the number of respondents citing any of these activities, note that some of them appeared only in the pilot survey and others only in the CHI and UPA surveys.
In addition to these changes, we clarified the distinction between consultants and corporate practitioners by changing the "HCI/Usability" entry under company categories to read "HCI/Usability Consulting."
We administered the survey at two conference sessions about strategic usability: the CHI 99 Panel on "What Makes Strategic Usability Fail?
Lessons Learned from the Field " and a UPA session on "What Makes Strategic Usability Succeed or Fail?
Note that the CHI surveys were administered at the end of the session, during which we gave a brief overview of the pilot data.
The UPA surveys were distributed and collected at the beginning of the session, preceded only by an oral definition of strategic usability.
The pilot survey data and the UPA survey data included most respondents' affiliations; the CHI survey data did not.
Reviewing the pilot and UPA surveys, only one company  was represented by more than three respondents; the large majority of respondents were the only ones from their organization.
After the CHI and UPA surveys, we tabulated the data from all three groups .
Tables 2 and 3 show the results for Questions 1 and 2.
Table 4 shows the results for Question 4, in which we asked respondents to select a category describing their company; note that the total of responses is 147 rather than 134 or fewer, because some respondents selected more than one category.
We also asked  for a free-form description of what their company does, but the answers were so ambiguous, disparate, and incomplete that a meaningful compilation wasn't possible.
Instead, we relied on Question 4 for our insights about this topic .
Table 5 summarizes the answers to the question about funding .
Table 6 summarizes the respondents' reports of the top two obstacles they perceive to creating greater strategic impact for usability engineering/HCI within their organizations .
See the discussion of these responses under Discussion of Results, next.
When asked to describe the top two obstacles to achieving strategic impact in their respective organizations, respondents most often cited resource constraints and resistance to changing the status quo of "we've always done things this way" .
The resource limitations most frequently listed related to the perception that "usability takes too much time" in an already tight schedule and to lack of budget to hire trained specialists, allocate facilities, or purchase equipment.
The responses comprising the "resistance to usability" category tended to be more energetically negative, describing organizational climates that included "disinterest" and "lack of any management support" to expressed resistance in the form of "engineers who feel they have HCI skills and don't need any usability" and "no one seems to see the value."
In compiling these responses, we minimized inferences about the causes of the obstacles mentioned in the openended answers.
Rather, we preserved differences in phrasing that reflect how these professionals perceived the issues in their organizations.
Table 7 shows the results for Question 10 and includes only CHI and UPA data; the question wasn't asked in the pilot survey.
The ratings and use of organizational approaches and usability methodologies  are listed in Table 8.
Table 7: How Successful is Strategic Usabilit r?
Lab Usability Testing  Usability Testing Without a Lab or Outside of Lab Facility2  !High-ProfUe Projects  Usability Test.
Equipment3  'UI Staff Members Co-located with Engineering3  IF e d Studies or Field Studies other than CI  High-Level/Founder Support  Usage Scenarios  Task Analysis  participatory Design3  Usability Advocates/Champions  Contextual Inquiry  Leveraging Related Initiatives 
Looking at the total of 134 respondents, there are no statistically significant correlations between any of the organizational approaches or usability methods and an organization's size .
We did observe, not surprisingly, that the number of people reporting use of an approach or method goes down as its perceived lack of contribution to strategic usability goes up.
This information is shown in Figure 1.
Heuristic Evaluation  Organizational Usability Planning  ICoach/Support Grass Roots Efforts  Educate/Train Other Functional Groups i
Since the large survey sample did not show correlations between the organization size and organizational approaches or usability methods, we looked for other possible relationships.
We considered whether the size of the usability group correlated with the effectiveness ratings.
In the pilot questionnaire, we asked for the size of the respondents' groups and also for the number of HCI/usability people in the company, and we offered categories .
In the CHI and UPA questionnaires, we simply asked how many HCI/usability people were in the company .
We looked for correlations between how many HCI/usability people were in the company and what toolkit items were rated more effective, and we found no statistically significant correlations.
We suggested another hypothesis, that the HCI people who are a low percentage of their company's size might rate the usability methods as being more effective than they rate organizational approaches.
Based on the possible relationships suggested by the pilot data, we looked for any correlations between organization size and respondents' ratings of the organizational approaches and usability methods.
First, we categorized the firms into the same three groups as we did for the pilot data.
However, for usability consultancies, all had less than 250 employees.
Large finns continued to be those with more than 1,000 employees.
The number of respondents in each category is shown in Table 9.
Overall we might expect both organizational approaches and usability methods to be rated less effective by the smaller HCI populations than by the larger ones, with organizational approaches rated less effective than usability methods.
A reliable ratio of HCI professionals to company size could not be constructed from the data.
So to explore this hypothesis, we looked only at the 84 people from large companies.
We divided these 84 people into two groups, those with fewer than 20 HCI people in the company and those with 20 or more.
Then, if we combine all the ratings of each type, we find the averages in Table 10.
The sample sizes in all cells are in hundreds, and the averages, according to the central limit theorem, closely follow a Gaussian, or "normal" distribution.
Using the observed means and variances and the properties of the normal distribution, three of these relationships are statistically significant: both large and small HCI groups rated usability methodologies as a whole more strategically effective than they rated organizational approaches.
And as we expected, organizational approaches were rated more effective overall by large groups than by smaller ones.
The authors identified several other questions that we hoped to answer with the survey data: * Do usability consultancies rank some or all usability methods as more effective than do in-house usability professionals?
The twelve respondents from consultancies gave 97 scores to the 12 usability methods in the survey, with a mean score of 1.74.
In contrast, 122 respondents from in-house staffs gave 621 scores to the same usability methods, with a mean score of 1.98.
For 10 of the 12 usability methods, the consultancy score was better  than that from inhouse professionals.
This is statistically significant at the 5% level.
Consultancies do rate usability methods more useful than in-house staffs.
Investigating the second question, we found that 38 respondents from small firms gave 38 scores to contextual inquiry and task analysis, with a mean score of 1.95.
In contrast, 84 respondents from larger firms gave 85 scores to the same usability methods, with a mean score of 1.98.
The difference is not statistically significant, nor are these scores significantly different from the average score given to the other usability methods.
With the ever-increasing blurring of the lines between business and consumer products, hardware and software, and products and technologies, the categorization of companies has become more complex.
Given this complexity, it was difficult to theorize on which types of companies might have significant differences in their effectiveness ratings.
To address the third question, we hypothesized that respondents in certain categories of companies--those often considered more innovative in their processes and products--would rate the organizational approaches and usability methods as more effective than respondents from other categories.
Looking at the effectiveness ratings and the usage of all the organizational approaches and usability methodologies, we compiled two groups.
Excluding the "design caf6," five items had fairly high ratings and fairly low usage, as shown in Table 11; these might be investigated for more extensive use.
Another five items had fairly low ratings and fairly high usage, as shown in Table 12; these might be investigated for less extensive use.
See also the Summary and Conclusions, next.
Based on the authors' experiences in a variety of large and small companies, the same methodologies and organizational approaches were equally effective.
What mattered most was that the usability professionals worked to involve the cross-functional teams directly in the research effort, through firsthand observation followed by participation in some form of summary activity.
In addition, consistent and visible management support at the highest levels of the organizations gave usability greater credibility and perceived importance to overall product and company success in the marketplace.
Choosing high profile projects and having high level, or company founder support were the organizational approaches that were ranked as most effective across all three survey groups.
These activities allow greater visibility across functional and organizational boundaries and lend credibility in the form of expressed management support.
The high effectiveness rating of High-Profile Projects is another argument that a more effective strategy may be to select projects carefully and staff them with sufficient HCI resources, rather than spreading limited resources too thin and providing only "Band-Aid" improvements to a larger number of projects.
Of note also is that although 47% of the respondents had utilized the organizational approach of Educate/Train Other Functional Groups , this approach was one of the lower rated in its contribution to strategic usability.
This runs counter to the belief of many HCI practitioners that building usability literacy within organizations is very helpful.
On the other hand, when we look at the obstacles respondents cited to achieving strategic usability, several of them--particularly lack of understanding of what HCI is, and lack of ways to communicate the value of results--imply a need to educate internal groups about the benefits of usability.
Usability testing--whether inside a lab facility, using portable equipment, or outside of a lab facility--was rated highest as an effective usability methodology to create greater strategic impact.
One reason for this high rating might be that the activity of product, or prototype, testing affords more team members the chance to observe firsthand how users can and cannot interact with their designs.
Even if members miss the sessions, videotapes can provide the immediate experience of product usage.
Laboratory usability testing was also a widely used methodology.
Results from usability tests tend to be immediately implementable and focused on specific changes to improve ease of use or effectiveness of the product.
In comparison, field studies often yield robust descriptive data that requires greater interpretation and is more subjective.
Applying the results, even if well categorized and tabulated, can be difficult because they often must be applied to future releases.
The mean score for usability methods was 2.01 for Group 1 and 2.00 for Group 2.
These do not differ in a statistically significant way.
Both are actually slightly greater  than the average for all other company categories, but this is also not statistically significant.
With respect to organizational approaches, the mean scores were 2.30 for Group 1 and 2.18 for Group 2, but these are still well within the limits of variation which should be expected to arise from strictly random effects .
There is no statistically significant difference in the ratings of these items either.
Our goal in the initial pilot and subsequent CHI 99 and UPA99 surveys has been to evolve a toolkit of organizational approaches and usability methodologies that contribute to making usability activities and data have strategic impact in corporate decision-making.
We began with a loosely constructed hypothesis that the demographics of an organization might affect what approaches and methodologies would work best to create strategic impact.
Organization size did not affect what organizational approaches and usability methods were rated most effective in achieving strategic usability.
Also, the results of a chisquared test showed no statistically significant differences in the rates at which obstacles were cited by respondents from large and small companies.
The size of organizations appears to have no impact on people's perception of factors that are inhibiting their ability to contribute at the strategic level in their respective environments.
The CHI 99 panel participants offered some insights and advice related to the specific obstacles cited by survey respondents: resistance, lack of understanding of HCI/usability, and lack of ability to communicate cost-benefit/ impact of usability results.
The CHI 99 panelists tended to agree with the advice offered by Don Norman in his CH199 session with the second author that we should "learn to speak the business language" of our internal functional area partners in marketing and management.
The CHI 99 panelists also said HCI practitioners should develop a business case for usability and learn enough of the technical constraints of any recommendation to communicate with engineers in their own language about the best ways to implement suggested product changes.
Using creative and innovative ways to distribute findings throughout our organizations and making them accessible on-demand from colleagues' desktops via intranet sites were still other recommendations from the CH199 panel.
Surveys are widely used , despite their lower effectiveness rating.
This indicates that surveys provide some benefit  that HCI practitioners want, while not providing data in the most effective form.
Thus, there is an opportunity for improvement in the methods used with larger sample sizes.
It's also interesting to note that the most commonly used method is heuristic evaluation, even though its effectiveness to strategic usability is ranked far below usability testing, field studies, usage scenarios, task analysis, and participatory design.
This is probably because it is relatively quick and easy to perform a heuristic evaluation, and HCI practitioners are often under pressure to provide feedback to a product that will soon be released.
There appears to be an interesting contradiction in the survey data between the usability methodologies that respondents felt were not as effective and the obstacles they cited as inhibiting their ability to have strategic impact in their organization's decision-making.
While education and training on usability were frequently mentioned, these methods were cited as less effective.
Yet several of the most often cited obstacles seem to call for more education of our internal clients and partners.
Is the content of our current educational and training efforts at the core of this seeming contradiction?
Or are our customers not demanding greater usability, and thus our organizations are not being driven to greater action?
The CHI 97 and CHI 99 panelists  agreed on the importance of building partnerships early in the product planning and design process with our internal colleagues in marketing, engineering, and corporate management.
They recommended that we apply usability methods to our internal clients and partners and learn more about their goals, priorities, customer contacts, and customer data.
The panelists believed that these activities and partnerships can help the strategic penetration of usability within organizations.
Assessing the effectiveness of strategies and tactics in the real world is not simple.
The authors could not perform a controlled study, and companies do not track all the metrics necessary to compare the various approaches more objectively.
Given these constraints, we judged that our best approach was to gather the opinions of practitioners.
Since we do see some trends and commonality, we believe that this is a valid approach.
However, it should be emphasized that all the data in this paper is based on the perceptions of the respondents, not on any direct knowledge the authors have of the respondents' activities.
Data compilation, text production, and graphics for this paper were performed by Jacie Wedberg, Josh Mills, William Cotter, Beth Almay, and Melani Cantlin.
Robert Farrell generously contributed the statistical analysis.
The authors would also like to thank the following workshop participants and pilot testers for their contributions to this research: Mark Anderson, Sarah Bloomer, Charlie Breuninger, Annette Bryce, Donald Chartier, Scott Clifford, Herb Colston, Betsy Comstock, Mary Czerwinski, Terri Ducay, Ken Dye, Chris Farnes, Alicia Flanders, Shannon Halgren, Avi Harel, Tom Holzman, Ed Israelski, Masako Itoh, Janice James, Caroline Jarrett, Irena Jaska, Michael King, O-Seong Kweon, Lori Landesman, JoAnn Lotfi, Jon Meads, Jeff McCartney, Ian McClelland, Tim McCollum, Debbie Mrazek, Michael Muller, Jakob Nielsen, Hanna Parrish, Amanda Prail, Judith Ramey, Mary Beth Raven, Ginny Redish, Mary Beth Rettger, Carol Righi, Dave Rinehart, Steve Sato, Karen Shor, Tammy te Winkel, John Thomas, Karel Vredenburg, Dennis Wixon, Cathleen Wharton, Susan Wolfe, and Ron Zeno.
Lessons from the Field," in Proceedings of the Usability Professionals' Association 8th Annual Conference , Usability Professionals' Association, Inc. 225-248.
