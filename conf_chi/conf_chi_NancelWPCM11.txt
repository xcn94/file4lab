Very-high-resolution wall-sized displays offer new opportunities for interacting with large data sets.
While pointing on this type of display has been studied extensively, higherlevel, more complex tasks such as pan-zoom navigation have received little attention.
It thus remains unclear which techniques are best suited to perform multiscale navigation in these environments.
Building upon empirical data gathered from studies of pan-and-zoom on desktop computers and studies of remote pointing, we identified three key factors for the design of mid-air pan-and-zoom techniques: uni- vs. bimanual interaction, linear vs. circular movements, and level of guidance to accomplish the gestures in mid-air.
After an extensive phase of iterative design and pilot testing, we ran a controlled experiment aimed at better understanding the influence of these factors on task performance.
Significant effects were obtained for all three factors: bimanual interaction, linear gestures and a high level of guidance resulted in significantly improved performance.
Moreover, the interaction effects among some of the dimensions suggest possible combinations for more complex, real-world tasks.
Very-high-resolution wall-sized displays can accommodate several hundred megapixels and make it possible to visualize very large, heterogeneous datasets in many domains .
Astronomers can use them to display telescope images constructed from hundreds of thousands of frames stitched together, such as Spitzer's 4.7 billion pixels images of the inner part of our galaxy .
Biologists can explore the docking of complex molecules.
Artists can create gigapixel images, such as the 26 gigapixel panorama of Paris based on 2,346 pictures stitched together.
Crisis management centers can interact with highly detailed maps of very large areas.
For example, OpenStreetMap data range from a view of the world down to street level, resulting in an image that requires 18 peta  pixels at its highest level of detail.
With resolutions up to 100-dpi, these LCD-based displays afford more physical forms of navigation  compared to conventional desktop setups or to lower-resolution projection-based large displays: Users simply step back to get an overview of the displayed data and walk forward to see details, including small but legible text.
However, as the examples above show, datasets increase in size faster than displays increase in dimensions and pixel density.
The display depicted in Figure 1 consists of thirty-two 30-inch tiled monitors and can display a "mere" 131 million pixels.
NASA's Hyperwall-2, to our knowledge the largest wall built to date, only doubles that number, and does so by adding some screens that users cannot reach.
Virtual navigation is thus still required, as datasets can be several orders of magnitude too large to fit on even wall-sized displays .
Many interaction techniques have been specifically designed to help users navigate large multiscale worlds on desktop computers, using zooming and associated interface schemes .
However, high-resolution wall-sized displays pose different sets of trade-offs.
It is critical to their success that interaction techniques account for both the physical characteristics of the environment and the context of use, in-
Input should be locationindependent and should require neither a hard surface such as a desk nor clumsy equipment: users should have the ability to move freely in front of the display and interact at a distance .
This precludes use of conventional input devices such as keyboards and mice, as well as newer interaction techniques: The powerful multi-finger gestural input techniques designed by Malik et al.
They require sitting at a desk, and are thus not optimal for displays of very high-resolution that afford more physical forms of navigation.
The recent Cyclostar approach  is very elegant, but requires the display surface to be touchenabled, a feature that wall-sized displays often lack.
Cyclostar is also not well-suited to wall-sized displays, as it requires users to be within arm's reach of the display surface.
While this is perfectly acceptable for displays up to 1.5m in diagonal such as SMART BoardsTM , users of larger displays such as the one in Figure 1  would only see a very limited portion of the display while navigating.
This lack of an overview would be a non-negligible hindrance as navigation is mostly driven by contextual information.
Our goal is to study different families of location-independent, mid-air input techniques for pan-zoom navigation on wall-sized displays.
More specifically, we seek to answer questions related to the performance and subjective preferences of users, including: Beyond their almost universal appeal, do gestures performed in free space work better than those input via devices operated in mid-air?
Is bimanual interaction more efficient in this context?
Do circular, continuous gestures perform better than those that require clutching ?
We ground our work on both theoretical and experimental work on bimanual input , the influence of limb segments on input performance , on types of gestures  and on the integral nature, in terms of perceptual structure, of the pan-zoom task .
In particular, we are interested in comparing the following dimensions: bimanual vs. unimanual input; device-based vs. free-hand techniques; degrees of freedom  and associated kinesthetic and haptic feedback; and types of movements: linear gestures vs. circular, clutch-free gestures.
Early studies investigated how users could benefit from larger displays in different settings.
Improvements to spatial task performance were also identified in several complementary studies .
Other works have focused on the size and configuration of high-resolution tiled displays.
Virtual navigation was always performed with the same device: a gyroscopic mouse.
Results from other recent studies suggest that large displays are also beneficial for information visualization and analysis tasks thanks to the larger amount of data that can be displayed .
Spatial input has been studied for years in the context of travel in immersive virtual environments and other 3D user interfaces based on virtual camera control with techniques using gloves, bimanual input and leaning, or high degrees of freedom devices .
One important issue they raise is the interdependency of all these aspects, that makes formal studies challenging, as we will see later.
Several input devices make it possible to point in mid-air on large displays: commercial devices such as gyroscopic mice, or soap , based on hardware found in a conventional optical mouse wrapped in elastic fabric.
ARC-Pad  enables seamless absolute+relative pointing on large displays through a mobile touchscreen.
The VisionWand  is a passive wand whose colored tips are tracked in 3D by two webcams.
The multiple degrees of freedom enable a richer interaction vocabulary, that includes pan-zoom navigation.
Recent advances in motion tracking and dynamic gesture recognition technologies now make it possible to investigate freehand input techniques.
Vogel and Balakrishnan  propose three pointing and clicking techniques that work with bare hands, with emphasis on important design characteristics such as accuracy, performance, but also comfort of use.
Large displays have been the focus of much research and evaluation over the last ten years.
Overall, the body of empirical work on large displays suggests that users can greatly benefit from their use.
Of particular interest to us is the work by Guiard et al.
Multiscale pointing consists of panning and zooming the view so as to bring the target in view, followed by a cursor pointing action to that target .
They performed several empirical studies, showing that multiscale pointing obeys Fitts' law, and that performance bandwidth is proportional to view size .
They introduced an experimental task adapted from Fitts' reciprocal pointing task, that we further adapt to take into account potential overshoots in the scale dimension.
An earlier paper  evaluated panzoom performance with uni- and bimanual input, suggesting that performance is enhanced with two hands, as it affords better pan-zoom coordination.
Pan-zoom navigation has however not received much attention beyond desktop interfaces, except for the recent work by Malacria et al.
A large body of literature is devoted to the design and evaluation of input devices that feature a high number of degrees of freedom .
Available degrees of freedom have a direct impact on the potential for parallelization of actions required to achieve the task.
For example, 6DOF input devices can increase the degree of parallelization of docking tasks , though studies report limits in terms of human capacity to handle all DOFs simultaneously.
Pan and zoom is a 3DOF task: the user controls the view's position  and its scale .
The possible solutions for mapping pan and zoom to three input channels are endless.
The film industry offers interesting and visually attractive scenarios with movies such as Minority Report which show users interacting via freehand gestures to navigate in a seemingly fluid and efficient way.
The technology to achieve this type of interaction is now available in research laboratories and beyond .
However, it remains unclear how freehand gestures actually fare when compared to device-based input techniques that take advantage of the human ability to use physical tools  and suffer less from problems commonly associated with spatial input , such as precision and fatigue.
Years of research in virtual reality have demonstrated that devising efficient navigation techniques for immersive virtual environments is still a challenge.
Our goal is to study families of input techniques that let users pan and zoom from any location in front of very high-resolution, wall-sized displays.
We made no a priori assumptions about relevant metaphors or technologies and considered freehand as well as device-based techniques.
An extensive design and testing phase allowed us to limit the number of candidates for the subsequent formal evaluation.
For instance, the apparently intuitive solution that consists in using two hands or two fingers to zoom with pinch and stretch gestures was considered but quickly discarded: while these gestures work well on touch-sensitive surfaces such as tabletops, they are much less natural when performed in mid-air.
Another category of techniques that was discarded are those based on first-order-of-control and operated via an elastic or isometric input device.
As reported in the literature in the case of pointing, e.g., , our pilot tests revealed that techniques based on first-order-of-control allow for fast and comfortable coarse navigation, but perform poorly during the final precise positioning phase, causing numerous overshoots.
We eventually identified a set of twelve candidate techniques.
Their design was informed by related empirical studies reported in the literature and refined through prototyping and pilot testing.
These techniques can be organized according to three key dimensions forming a design space , and introduced in the following sections.
In addition to performance , we took into account other usability issues, such as fatigue and ease of use.
In their paper on the perceptual structure of multidimensional input, Jacob and Sibert claim that panning and zooming are integrally related: the user does not think of them as separate operations, but rather as a single, integral task like "focus on that area over there" .
Buxton and Myers  and later Bourgeois and Guiard  observed high levels of parallelism for pan-zoom operations, further supporting this argument.
The level of parallelism correlates with task performance and is typically well afforded by the use of bimanual input techniques .
While we expect bimanual techniques to outperform unimanual ones, we are still interested in comparing their performance, as the latter might still be of interest in more complex, real-world tasks that require input channels for other actions.
Navigating in the scale dimension  is a task typically performed through vertical scroll gestures on, e.g., a mouse wheel or a touchpad.
The mapping from input to command is natural, but often entails clutching as the course of mouse wheels and touchpads is very limited.
An alternative consists in mapping continuous circular gestures to zooming.
Matrix of the 12 techniques organized according to key characteristics: uni- vs. bimanual, degree of guidance, linear vs. circular gestures.
1D path involves guiding gestures along a particular path in space; in 2D surface gestures are made on a touch-sensitive surface; while in 3D free gestures are totally free.
Despite the less natural mapping from input to commands, such continuous, clutch-free gestures have been successfully applied to vertical scrolling in documents , and to pan and zoom on large, touch-sensitive surfaces in CycloStar .
Circular gestures potentially benefit from an automatic Vernier effect : as zooming is mapped to angular movements, the larger the circular gesture's radius, the greater the distance that has to be covered to make a full circle, and consequently the more precise the input.
For all techniques, controlling the cursor's position is achieved naturally by ray-casting from the dominant hand to the wall display .
As mentioned earlier, first order of control was discarded for both pan and zoom operations.
Panning is achieved by dragging, as in applications such as Adobe IllustratorTM or Google MapsTM with their typical hand-shaped cursor.
As in desktop applications such as Google Maps or NASA's WorldWind, linear techniques zoom in by moving forward towards the display and zoom out by moving backwards; circular techniques zoom in by turning clockwise and zoom out by turning counter-clockwise .
Pointing plays an important role when zooming, as it specifies the focus of expansion /contraction .
Letting users specify this focus point is very important on displays of that physical size, as they will typically not be standing right in the center.
A focus of expansion implicitly located at the center of the screen would make zooming operations tedious and hard to control as every zoom operation would require multiple panning actions to compensate drifts induced by the offset focus.
All bimanual techniques  are grounded in Guiard's study of asymmetric division of labor in bimanual actions that led to the Kinematic chain model .
Two main categories of techniques have been studied for mid-air interaction on wall-sized displays: freehand techniques based on motion tracking ; and techniques that require the user to hold an input device .
Input devices provide some guidance to the user in terms of what gesture to execute, as all of them provide some sort of passive haptic feedback: A finger operating a knob or a mouse wheel follows a specific path; gestures on touchenabled devices are made on planar surfaces.
Freehand techniques, on the contrary, provide essentially no feedback to the user who can only rely on proprioception  to execute the gesture.
We call this dimension the degree of guidance.
Gestures can be guided to follow a particular path in space ; they can be guided on a touch-sensitive surface  ; or they can be totally free .
These three values correspond to decreasing amounts of passive haptic feedback for the performance of input gestures.
The main limb segments involved in the input of gestures via a device are the fingers and, to a lesser extent, the forearm .
This group of techniques is illustrated in Figure 2, columns 1D path and 2D surface.
Column 1D path illustrates techniques that provide a high degree of guidance for executing the zooming gestures.
The first row corresponds to one handed techniques: the device is operated by the dominant hand, which also controls pointing via ray-casting.
The second row corresponds to two handed techniques: the dominant hand controls pointing via raycasting, while the non-dominant hand controls zoom using the device.
Depressing a button on the device activates drag mode for panning.
The technique using linear gestures consists in pushing the dominant hand forward to zoom in, as if reaching for something, with the palm towards the target.
Turning the hand and pulling backward  zooms out.
Users point orthogonally to the palm of the same hand , with the arm slightly tilted for greater comfort.
The second row illustrates two handed techniques .
The linear zooming gestures are similar to the ones above, but are performed with the non-dominant hand, the dominant hand still being used for pointing and specifying the focus of expansion.
In the circular case, users adopt a potentially less tiring posture, pointing at the floor with their non-dominant hand and making circular movements.
All other postures and movements being ignored by the system for the non-dominant hand, the user can easily clutch.
Several options can be considered for engaging drag mode: specific hand postures such as pinching, or using a small wireless actuator .
Column 2D surface illustrates techniques that use a touchsensitive surface for input, providing a lesser degree of guidance.
The surface is divided horizontally in two areas.
Users zoom in the upper area either by moving the thumb up and down , or by drawing approximate circles .
Touching the lower area activates drag mode for panning.
Users just rely on proprioceptive information to switch between both areas and do not have to look at the device.
These techniques can be implemented with a touchsensitive handheld device such as a PDA or smartphone.
1D path techniques employing circular gestures will provide more guidance, but will not benefit from the earliermentioned Vernier effect, as input is constrained to one specific trajectory.
However, the range of amplitudes that can be covered with the thumb is limited .
This should minimize the trade-off between 1D path and 2D surface in that respect.
For 2D surface techniques, rubbing gestures  were considered to avoid clutching when performing linear gestures, but were found to be impractical when performed with the thumb on a handheld touch-sensitive surface.
As a technique designed specifically for thumb input, we were also interested in MicroRolls .
However, these were originally designed for discrete input.
Cardinal MicroRolls would have had to be mapped to first order of control, which we discarded as discussed earlier, and circular MicroRolls are not precise enough for zoom control.
We conducted an experiment using a  withinsubjects design with three primary factors: H ANDEDNESS  {OneHanded, TwoHanded}, G ESTURE  {Circular, Linear}, and G UIDANCE  {1DPath, 2DSurface, 3DFree} to evaluate the 12 unique interaction techniques described above.
We controlled for potential distance effects by introducing the D ISTANCE between two consecutive targets as a secondary within-subjects factor.
We systematically varied these factors in the context of a multiscale navigation task within a wall-sized display environment.
Measures include performance time and number of overshoots, treated as errors.
Overshoots occur when participants zooms beyond the target zoom level, and indicate situations in which the participant has less precision of control over the level of zoom.
For instance, from an overview of Canada, zooming down to street level in Google Maps when what the user actually wanted was to get an overview of Vancouver.
Based on the research literature and our own experience with the above techniques, we made the following 7 hypotheses.
Handedness: prior work  suggests that twohanded gestures will be faster than one-handed gestures  because panning and zooming are complementary actions, integrated into a single task .
Two-handed gestures should also be more accurate and easier to use .
Gesture: Linear gestures should map better to the zooming component of the task, but should eventually be slower because of clutching, the limited action space compared to zoom range requiring participants to repeatedly reposition their hand/finger .
Prior work  suggests that users will prefer clutch-free circular gestures .
Device vs. Free Space: Zhai et al.
The main limb segments involved in performing gestures in free space are the wrist, forearm and upper arm.
This group of techniques is illustrated in Figure 2, column 3D free.
The first row illustrates one handed techniques using either linear or circular gestures.
The technique using circular gestures is actually very close to the CycloStar zooming gesture, but performed in mid-air, without touching any surface.
Users perform circular gestures with the dominant hand and forearm oriented toward the display.
Nevertheless, they acknowledge that differences exist in the motor system's ability to control the different limb segments.
Based on the gestures to be performed and taking into account the physical size and mass of the segments involved, we predicted that techniques using fingers , should be faster than those requiring larger muscle groups  .
We also predicted that 1DPath gestures would be faster, with fewer overshoots than techniques with lesser haptic feedback, i.e., 2DSurface and 3DFree .
Finally, we predicted that 3DFree gestures would be more tiring .
Task :  Groups of concentric circles represent a given position and zoom level.
As mentioned earlier, we use the device's main button for 1DPath conditions, and the lower area of the touch-sensitive surface for 2DSurface conditions.
While in real-world applications we would use specific hand postures such as pinching in 3DFree conditions, for the sake of robustness we use a wireless mouse button whose activation is seamlessly integrated with the gesture.
The experiment was written in Java 1.5 running on Mac OS X and was implemented with the open source ZVTM toolkit   modified to run on clusters of computers driving display walls.
Touchstone  was used to manage the experiment.
A cluster of 16 computers, each with two high-end nVidia 8800GT graphics cards, communicate via a dedicated high-speed network through a front-end computer.
Our goal is to identify the performance characteristics of each technique from the user's perspective.
It is thus essential that each technique operates equally well from a purely technological perspective.
We use a VICON motion-capture system to track passive IR retroreflective markers and provide 3D object coordinates with sub-millimeter accuracy at 200Hz .
All 2DSurface conditions use an iPod Touch.
So as to avoid failures from gesture segmentation algorithms that would impact task performance in an uncontrolled manner, we use an explicit mode switch to unam-
The task is a variation of Guiard et al.
Participants navigate through an abstract information space made of two groups of concentric circles: the start group and the target group.
It may appear either on the left or right side of the start group.
Then they pan and zoom into the target group until they reach the correct zoom level and the target is correctly centered.
Overshoots occur when the zoom level is higher than the maximum level required to meet criteria B and C, in which case participants have to zoom out again .
The experiment presents each subject with six replications of each of the 12 techniques at three D ISTANCEs.
Each session lasts between 30 and 90 minutes, depending on techniques and participant.
Participants are required to wait at least one hour between two consecutive sessions, and to complete the whole experiment within four days or fewer, with a maximum of two sessions per day to avoid too much fatigue and boredom.
Participants stand 1.7m from the wall and are asked to find a comfortable position so they can perform gestures quickly, but in a relaxed way.
Practice Condition: Participants are given a brief introduction at the beginning of the first session.
Trials, blocks and sessions are fully counter-balanced within and across subjects, using a Latin square design.
Measures: We measure movement time MT and number of overshoots for each of 2592 trials: 2 G ESTURE x 2 H AND EDNESS x 3 G UIDANCE x 3 D ISTANCE x 12 participants x 6 replications.
Participants also answer questions, based on a 5-point Likert scale, about their perceived performance, accuracy, ease of learning, ease of use, and fatigue.
They rank the techniques with respect to the G UIDANCE factor after each session.
When they have been exposed to both conditions of H ANDEDNESS or G ESTURE, they rank those as well.
After the last session, they rank the techniques individually and by factor.
Participants are encouraged to make additional observations and comments about any of the above.
As the factors were counter-balanced, this created no adverse effects in the analysis.
Table 2 details results of the full factorial ANOVA for the model MT  H ANDS x G UIDANCE x G ESTURE x D IST x Rand.
We observe that H ANDS has a significant effect on MT .
We found a significant interaction effect of H ANDS x G UIDANCE .
The interaction does not change the significance of the posthoc test, but indicates that the magnitude of the difference is greater for 3DFree than for 2DSurface and greater for 2DSurface than for 1DPath techniques.
Unsurprisingly, performance data strongly support : all other conditions being equal, two-handed techniques are consistently faster than one-handed techniques.
An interesting observation is that using two hands is more advantageous when the degree of guidance for achieving gestures is low.
G UIDANCE has a significant effect on MT .
This time the H ANDS x G UIDANCE interaction changes the significance of the test .
The difference is that a post-hoc Tukey test shows no significant difference between 2DSurface and 3DFree for TwoHanded.
Prior to our analysis, we checked the performance for unwanted effects from secondary factors.
We checked for individual performance differences across subjects and found that, for all 12 participants, movement time and number of overshoots were perfectly correlated with the overall performance measures.
As expected, movement time data are skewed positively; replications of unique experimental conditions are thus handled by taking the median .
In all remaining analysis, we handled participant as a random variable, using the standard repeated measures REML technique.
We found no significant fatigue effect although we did find a significant learning effect across sessions.
Participants performed about 1.4 s more slowly in the first session and then became slightly faster over the next three sessions.
Both hypotheses  and  are supported: involving smaller muscle groups improves performance; providing higher guidance further contributes to this.
However, this effect is less pronounced in TwoHanded conditions.
This confirms the previous observation that a higher degree of guidance is especially useful when a single hand is involved.
A post-hoc Tukey test shows that  for Circular gestures: 1DPath guidance is faster than both 2DSurface and 3DFree with no significant difference between 2DSurface and 3DFree;  for Linear gestures, there is no significant difference between 1DPath and 2DSurface, but a significant difference between 2DSurface and 3DFree;  for 1DPath guidance there is no significant difference between Circular and Linear gestures, but there is a significant difference between Circular and Linear for 2DSurface and 3DFree guidance.
Surprisingly, Linear gestures are generally faster than Circular ones.
Performance differences between gesture types are however affected by the degree of guidance: Circular gestures with 1DPath guidance  are comparable to Linear gestures with low guidance.
We tentatively explain the lower performance of Circular gestures with 2DSurface guidance by the difficulty of performing circular gestures with the thumb , also observed here.
Another interesting observation is that our analogue of CycloStar in mid-air  performs poorly.
It seems that the lack of a surface to guide the gesture significantly degrades this technique's usability.
Another factor contributing to its poor performance in our study is likely related to overshoots, as discussed below.
As expected, distance to target  has a significant effect on MT.
A post-hoc Tukey test shows that MT increases significantly with distance.
These interactions are due to a change in the magnitude of the difference across conditions, confirming that the choice of an efficient technique is of increasing importance as the task becomes harder.
MT per D IST x G ESTURE, for each G UIDANCE 2DSurface gestures exhibit more overshoots than 1DPath and 3DFree gestures .
There is a significant difference between Linear and Circular gestures for 2DSurface and 3DFree, but not 1DPath.
Moreover, overshoots exhibit the same interaction effect for 2DSurface gestures: Circular 2DSurface result in significantly more overshoots than Linear 2DSurface .
The observed higher number of overshoots for Circular techniques helps explain the generally lower MT performance measured for this type of gestures.
The best-fitting ellipse algorithm involved in the recognition of Circular gestures has an inherently higher cost of recovery, introducing a delay when reversing course.
The poor performance of our analogue of CycloStar is at least partially due to this, knowing that there was a major difference between the zooming experiment reported in  and the present one: we included overshoots in our task design, whereas the CycloStar experiment apparently did not , thus ignoring this issue.
As detailed earlier in the description of task design, overshoots correspond to zooming beyond the target zoom level and are treated as errors.
We consider the model Overshoots  H ANDS x G UIDANCE x G ESTURE x D IST x Rand.
Circular gestures exhibit more overshoots than Linear gestures .
Qualitative data confirms our results.
Participants generally preferred TwoHanded to OneHanded techniques  and Linear to Circular gestures .
Subjective preferences about degree of guidance were mixed, with 4 participants preferring the high degree of guidance provided by 1DPath techniques, only 1 for both of 2DSurface and 3DFree techniques, and all others expressing no particular preferences.
Looking at the details of answers to our 5-point Likert scale questions about perceived speed, accuracy, ease of use and fatigue, significant results  were obtained only for degree of G UIDANCE, with 1DPath being consistently rated higher than 2DSurface and 3DFree; and for H ANDS, TwoHanded techniques being considered less tiring than OneHanded techniques .
As a side note, if we consider the model MT  G ROUP x Rand, the ANOVA shows a significant effect of G ROUP  and a post-hoc Tukey test shows a significant difference between each groups.
Optimal performance in terms of movement time implies the use of two hands and a device to guide gestural input.
Comments from participants suggest that in the OneHanded condition, zoom gestures interfere with pointing as they introduce additional hand jitter and consequently lower accuracy.
Some participants also commented that pointing and zooming were confounded in the OneHanded conditions, making the techniques difficult to use .
However, two participants strongly preferred one-handed gestures, arguing that they were less complex and less tiring.
They assumed their performance was better , probably because they experienced more overshoots in the two handed condition, which may have led to their conclusions.
One of them mentioned that for the one handed condition there was "no need for coordination"; techniques were "more relaxed" and made it "easier to pan and zoom".
All but one participants preferred linear gestures overall although one commented that he liked "the continuity of circular gestures".
Others commented that "making good circles without a guide is hard" and did not like having to turn their hands.
These findings contradict our hypothesis that users would prefer clutch-free circular gestures .
This hypothesis was based on observations made for techniques operated on a desktop, not in mid-air, and involved different limb segments.
In many of our conditions, the gestures had to be performed with the thumb, and were thus more complex to achieve than when using, e.g., the index finger in conjunction with hand or forearm movements.
Several participants commented on this interaction effect: " too hard to do circle gestures without a guide", "Linear movements are easier on the iPod" and " impossible to do circular movements on a surface, maybe with some oil?
Techniques in this group are of interest as they exhibit a relatively good level of performance while broadening possible choices for interaction designers.
For instance, the unimanual techniques in this group make one hand available to perform other actions.
The 3DFree technique is also of interest as it does not require the user to hold any equipment and is generally appealing to users.
Gr3 contains techniques that again have very close MT but about 2.3 s slower than the techniques of Gr2.
This group consists of OneHanded Circular 1DPath, TwoHanded Circular 2DSurface and 3DFree, and OneHanded Linear 3DFree.
Techniques in this group are of lesser interest, except maybe for the OneHanded Linear 3DFree technique, which is the fastest unimanual technique using gestures performed in free space.
We studied different families of location-independent, midair input techniques for pan-zoom navigation on wall-sized displays.
After an extensive exploratory design phase, we identified the following key factors for the design of such techniques: handedness , gesture type , and level of guidance .
We systematically evaluated each combination of these factors through a controlled experiment in which participants performed pan-and-zoom navigation in an abstract, very large multiscale environment, with distances up to 12 million pixels.
Experimental results identify several successful mid-air input techniques that can be used to navigate efficiently in very large datasets on wall-sized displays.
In addition to identifying groups of alternative techniques based on performance, but each with specific characteristics, the experiment also suggests clear results with respect to the factors that constitute our design space.
The analysis of variance for the model MT  H ANDS x G UID ANCE x G ESTURE x D IST x Rand does not show a significant triple interaction between the three main factors .
Formally, we cannot say more than the above about the ranking of the twelve techniques.
Adding guidance to input gestures increases, rather than decreases, accuracy.
In accordance with the research literature, bimanual input techniques perform very well.
Unimanual techniques perform honorably, and may still be considered in contexts of use where, for example, tools must be held in one hand to perform a domain/task specific action.
A more surprising result is the generally higher efficiency of linear gestures when compared to circular, clutch-free gestures.
As future work, we plan to investigate how these pan-zoom techniques combine with other interaction techniques.
Indeed, in real-world applications, users must also handle text entry, menu selection, copy and paste, drag and drop, and other activities.
This implies trade-offs among techniques: a technique with optimal performance in this experiment may prove less easy to integrate with other techniques because of its requirements in terms of handedness or type of device.
We have started to explore these questions in the context of real-world activities involving scientists visualizing and manipulating extremely large sets of multi-scale data.
We wish to thank Cl ement Pillias and Romain Primet, who helped implement the experiment on the wall-sized display, as well as Caroline Appert, St ephane Huot and the anonymous reviewers for their feedback on drafts of this paper.
We also wish to thank all the volunteers who participated in our experiments.
This research was supported by a R egion  Ile-de-France/Digiteo grant.
