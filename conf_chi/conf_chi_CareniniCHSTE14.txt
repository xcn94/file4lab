There is increasing evidence that the effectiveness of information visualization techniques can be impacted by the particular needs and abilities of each user.
This suggests that it is important to investigate information visualization systems that can dynamically adapt to each user.
In this paper, we address the question of how to adapt.
In particular, we present a study to evaluate a variety of visual prompts, called `interventions', that can be performed on a visualization to help users process it.
Our results show that some of the tested interventions perform better than a condition in which no intervention is provided, both in terms of task performance as well as subjective user ratings.
We also discuss findings on how intervention effectiveness is influenced by individual differences and task complexity.
1 In this paper, we focus primarily on this latter question in the context of designing user-adaptive visualizations.
In information visualization, the only research we are aware of targeting the question of how to adapt is on recommending alternative visualizations based on specific user, data, or task features .
By contrast, in this paper we focus on adaptive interventions aimed at improving the effectiveness of the visualization a user is currently working with.
In particular, we evaluate a set of four alternative highlighting interventions aimed at supporting analytical interaction by directing the user's attention to a specific subset of data within a visualization, while still retaining the context of the data as a whole .
Highlighting can be extremely useful in any scenario in which an agent  needs to communicate to a user several points about a possibly large and complex dataset.
For instance, in a dataset of car sales, two key points could be that "more cars were sold this year in China than in India" and that "Europe sales have been decreasing in the last 3 years".
In these scenarios, the ability to highlight subsets of the data would naturally support a more effective communication.
While the whole dataset can be compactly conveyed with an appropriate visualization, the information relevant to each point can be synchronously highlighted as the key points are sequentially expressed in language .
For instance, in our example, sales for China and India would be highlighted first, followed by sales for European countries in the last 3 years.
The ability to generate highlighting interventions would be especially useful in computer-human communication, for instance, when a system has automatically analyzed and derived insights from a complex dataset , and needs to communicate this to a user.
Information interfaces and presentation : Miscellaneous.
Recent advances in visualization research have shown that individual user needs, abilities and preferences can have a significant impact on user performance and satisfaction during visualization usage .
It is therefore important to investigate the potential of useradaptive visualizations, i.e., visualization techniques and systems that support the provision of visual information personalized to each user's needs and differences.
The benefits of user-adaptive interaction have been shown in a variety of human-computer interaction tasks and applications such as operation of menu based interfaces, web search, desktop assistance, and human learning .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
If a system could track what part of the text the reader is currently reading, and infer the corresponding point made , such a system could use one  of the interventions evaluated in this paper to highlight the relevant visualization elements .
The interventions that we evaluated in our study were inspired by the analytical interaction techniques presented in Few  and by a taxonomy of post-hoc strategies for visual prompts presented by Mittal .
While both  and  provide valuable descriptions and taxonomies of different techniques, to the best of our knowledge there is no formal evaluation of which interventions may be most useful, both in general and under particular task/user contexts.
The user study presented in this paper aims to answer the following research questions on the effectiveness of the four interventions that we target: 1.
Can highlighting interventions improve information visualization processing?
Is there an intervention that is the most effective?
Are questions 1 & 2 above affected by individual user characteristics, by task complexity, and by when the interventions are delivered?
Generally speaking, if we find an intervention that is the most effective, it should be used whenever a system needs to draw the user's attention to a subset of the data.
However, if intervention effectiveness is found to depend on the task and/or the user, the results of our study could inform adaptive highlighting for visualization support.
Three key decisions are involved in supporting useradaptive interaction: what to adapt to, when to adapt and how to adapt.
Deciding what to adapt to involves identifying which individual user features influence interaction performance enough to justify adaptation.
In visualization, there are already results on the impact of a number of user characteristics on user performance and satisfaction.
For example, user performance across different visualizations and task types has been linked to the cognitive measures of perceptual speed and spatial visualization , as well as to the personality trait of locus of control .
Also, the cognitive abilities of visual/verbal working memory, as well as visualization expertise, have been shown to impact user satisfaction .
Addressing the decision of when to adapt involves formalizing adaptation strategies that identify those situations in which the benefits of providing adaptive interventions outweigh their cost .
When to adapt has been extensively investigated in fields such as Intelligent Tutoring  or Availability Management Systems .
Addressing the question of how to adapt, which is the focus of this paper, has been studied outside information visualization to support, for example, display notifications , or hints provision .
In information visualization, researchers have so far focused on adaptivity that relates to suggesting alternative visualizations based on specific user or task features .
By contrast, in this paper we focus on interventions that relate to the current visualization.
Highlighting interventions are the most relevant techniques to our goal of devising dynamic interventions, because by definition they can be added to an existing visualization as needed to emphasize a specific aspect.
Our sources of inspiration for highlighting interventions were Few  and Mittal .
Mittal  was especially useful, as it presents a taxonomy of post-hoc strategies for visual prompts, which is based on a detailed analysis of previous InfoVis literature , and on the analysis of several thousand charts in newspapers, magazines and business/governmental reports.
We conducted a user study to investigate the effectiveness of four different highlighting interventions that can be used to emphasize specific aspects of a visualization.
We also look at how this effectiveness may be impacted by task complexity, user differences, and delivery time.
To keep the number of conditions manageable, we only studied one visualization: bar graphs .
We focused on bar graphs for three reasons.
First, bar graphs are one of the most ubiquitous and effective information visualization techniques.
Second, there is already research showing that performance with and preferences for this basic form of visualization is influenced by individual differences such as perceptual speed, visual working memory, and verbal working memory .
Thus, it can be beneficial to investigate how to provide visual interventions for different users who may be working suboptimally with bar graphs.
Finally, as we argue at the end of the paper, results on bar graphs are likely to generalize to other information visualizations.
Tasks were performed via dedicated software.
Each task consisted of presenting the participant with a bar graph along with a textual question relating to the displayed data.
Participants would select their answer from a horizontal list of radio buttons and click 'Submit' to advance to the next task .
The study questions related to comparing individuals against a group average  on a set of dimensions .
In contrast, the color of the other bars was varied from task to task, and selected at random from a set of four color schemes optimized using ColorBrewer .
All tasks involved the same number of data points  and series dimensions .
Task complexity was varied by making subjects perform two different types of task, chosen from a set of primitive data analysis tasks that Amar et al.
The first task type was Retrieve Value , described by Amar et al.
This is one of the simplest task types in the Amar hierarchy, and thus it was selected to exemplify tasks of lower complexity.
In our study, RV tasks required to retrieve a specific individual in the target domain and compare it against the group average .
The second task type we chose was Compute Derived Value , defined in  as "Given a set of data cases, compute an aggregate numeric representation of those data cases".
Bolding  draws a thickened border around the relevant bars 2.
De-Emphasis  fades all non-relevant bars.
Average Reference Lines  draws a horizontal line going from the top of the left-most bar  to the last relevant bar, to facilitate comparison.
Connected Arrows  involves a series of connected arrows pointing downwards to the relevant bars.
Notice that Bolding highlights the average only by thickening the bar, because of its black color.
This is arguably not a serious confound, because the average is always relevant in the study tasks, and it was already made to stand out through its constant and distinctive color as well as a consistent leftmost position.
Participants performed each of the two task types described earlier with each of the four highlighting interventions in Figure 2, as well as with no intervention, as a baseline.
If highlighting interventions were to be used to provide real-time adaptive support, they would be superimposed on a visualization while the user is looking at it.
This could possibly be disruptive, even if the adaptive system had a reliable mechanism to decide when the interventions should appear based on user needs.
In this study, we wanted to evaluate the relative effectiveness of the selected interventions without this confound, as well as gain initial insights on whether this relative effectiveness changes when the interventions are provided dynamically.
Thus, we added an experimental factor that varied when the interventions would be shown, consisting of two conditions, Time zero  and Time x .
In the T0 condition, the interventions are included in the bar graph from the beginning of the task, to evaluate them without the possible confound of the disruption that can be caused by a dynamic superimposition.
In contrast, the TX condition aims to gauge interventions' effectiveness when they are added dynamically.
At the time of the study, however, we had no criterion implemented to decide when an intervention should appear based on a user's needs.
Thus, we adopted a procedure designed to minimize the potential intrusiveness of an unjustified superimposition of visual prompts.
Essentially, the idea is to add the visual prompts to the target bar graph as soon as the user has had a chance to look at both the bar graph and the related task question.
This constraint is enforced in the TX condition by the following steps, which leverage the real-time gaze information provided by a Tobii T120 Eyetracker installed on the experimental machine: 1.
The bar graph appears, without the task question .
It stays visible until a user has had a total of 5 eye fixations on the graph or more than 5 seconds have passed.
The graph disappears and the question text appears.
The question stays visible until the user has had at least 6 fixations on it , or more than 5 seconds have passed.
At this point, the graph and question text are both visible.
After 500ms, the selected intervention is added.
Participants saw each intervention on each task type with both the T0 and the TX delivery strategy, thus generating 20 experimental conditions: 2 task types , times 2 delivery times , times 5 interventions .
It should be noted that participants are expected to be slower in the TX delivery condition because of the delay before both graph and text are visible on the screen .
What we aim to understand with these two conditions is whether delivery time affects the relative effectiveness of the interventions.
The user characteristics investigated in this study include three cognitive abilities , two measures of user visualization expertise with using bar graphs, as well as one personality trait .
Perceptual speed , visual working memory , and verbal working memory  were selected because they were repeatedly shown to influence visualization performance or user satisfaction in studies involving bar graphs .
Besides cognitive abilities, the study in  also looked at the impact of visualization expertise, but results were inconclusive, possibly because they measured expertise via self-report questions asked after the experimental tasks.
In this study, we aim to provide a more reliable investigation of the impact of user visualization expertise, not only on bar graph processing but also on the effectiveness of our visual interventions.
We use two separate measures for expertise, captured in a pre-questionnaire: one that gauges user familiarity with simple bar graphs  and one with complex ones , elicited as described in the next section.
Locus of control  has been shown to impact user performance with visualizations other than bar graphs , e.g., listlike visualizations and visualizations with a strong containment metaphor.
With this study we wanted to ascertain whether locus of control may also have an impact while interacting with simpler visualizations such as bar graphs and on the effectiveness of our visual interventions.
Participants were mostly recruited via dedicated systems at our university.
This resulted in a variety of student participants from diverse backgrounds .
We also recruited 7 non-student participants such as a non-profit community connector, 3D artist, and air combat systems officer.
Table 1 presents summary statistics on the user characteristics data collected from the study.
A correlation analysis over our 6 user characteristics shows no significant correlations, except for a strong positive correlation  between expertisesimple and expertise-complex, and a weak negative correlation  between perceptual speed and locus of control.
Because the expertise measures are highly correlated, we retain only expertise-complex as our measure of expertise for further analysis, given its higher variance.
The experiment was a within-subjects study, fitting in a single session lasting at most 90 minutes.
There were 20 experimental conditions: 2 task types , times 2 delivery times , times 5 interventions .
Participants were instructed to complete the tasks as quickly and accurately as possible.
To account for within-subject variance, each participant repeated each condition 4 times, which is a well-established procedure in perceptual psychology experiments measuring performance in terms of time and accuracy .
Thus, there were a total of 80 trials per participant.
To avoid participants getting bored, each of the four domains described earlier were randomly assigned to each task.
Participants began by filling out a pre-study questionnaire asking for demographic information as well as self-reported expertise with simple and complex bar graphs.
Expertisesimple was elicited with the question 'How often do you look at simple Bar Graphs', followed by a basic bar graph with 8 bars ; Expertise-complex was elicited with the question 'How often do you look at complex Bar Graphs', followed by a graph with 48 bars , as used for the experimental tasks.
Next, participants underwent a training phase to expose them to bar graphs, the study tasks, and the highlighting interventions.
Then participants underwent a calibration phase for the eye-tracker, before starting the study trials.
Participants then performed 40 of the 80 study trials, followed by a 5 minute break.
After the break, the eye-tracker was re-calibrated and the participant performed the remaining 40 trials.
The 80 trials were fully randomized in terms of experimental conditions .
The experimental software was fully automated and ran in a web-browser, with the visualizations and interventions programmed using the D3 visualization framework .
Lastly, participants took a post-questionnaire asking for their evaluations of each intervention's usefulness, as well as their relative preferences.
The questionnaire included: * 10 rating statements in the form of "I found the X intervention useful for performing Y tasks", for each intervention and task type .
The statements were rated on a Likert scale from 1 to 5.
We look at both task completion time and task accuracy as performance measures.
Completion time was normally distributed , whereas task accuracy indicated a ceiling effect with 91.4% correct answers, possibly due to the tasks being generally easy to solve, or due to participants focusing on generating the correct answer, while sacrificing their time on task.
The ceiling effect on accuracy arguably makes a separate analysis of this performance measure not very informative.
We nevertheless did not want to discard accuracy altogether, because trials that were answered incorrectly should be penalized accordingly.
We opted to use a combined score for task performance, known as Inverse Efficiency Score .
Given that participants repeated each experimental condition 4 times, task performance is calculated by averaging completion time for the trial repetitions that were performed correctly, and then dividing this score by the percentage of correct repetitions 3.
Task performance values thus calculated can be essentially interpreted as completion times penalized for incorrect trials .
Thus, performance is reported in seconds and a higher score represents a lower performance.
We use a General Linear Model  repeated measures to analyze our performance data.
We first run a 2  by 2  by 5  General Linear Model  repeated measures to investigate the effects of our experimental factors alone.
Next, we analyze the effects of each of our five co-variates separately , by running a GLM with the experimental factors and only that co-variate.
Due to the high number of covariates in our study, this approach ensures that we do not overfit our models by including all co-variates at once.
Each co-variate was discretized into three levels via a three-way split.
In the next sections, effect sizes  are reported as small for .01, medium for .09, and large as .25 .
All reported pairwise comparisons are corrected with the Bonferroni adjustment.
There was also a main effect of intervention type.
As shown in Figure 3, performance was best for De-Emphasis, and worst for None, .
Pairwise comparisons show that interventions are significantly different from one another except for Bolding and Connected Arrows, and for None and Avg.
This result indicates that all interventions, except for Avg.
Lines, were helping users solve the selected tasks more efficiently than when they received no intervention.
These results will be further qualified by interactions with task type and delivery time described in the next section.
The main effect and related pairwise comparisons for Perceptual Speed indicate that performance was similar for users with low Perceptual Speed  and average perceptual speed , whereas users with high perceptual speed were significantly better at completing tasks , hence confirming previous work .
The results for Verbal WM show similar directionality, except that the performance of users with low Verbal WM  was significantly worse than the scores of users in both the average group  and the high group .
While  previously uncovered a link between Verbal WM and user preferences for different visualizations, and  has showed that low Verbal WM increases a user's gaze fixations on textual elements , our current result on Verbal WM is, to the best of our knowledge, the first to directly link this cognitive ability to task performance with information visualizations.
The results for Perceptual Speed and Verbal Working memory will be further qualified by interactions with task type.
The main effect of task type confirms the difference in complexity between the two task types in the study, with Compute Derived Value having longer task performance values  than the simpler Retrieve Value tasks .
Lines is no longer significantly better than None.
A possible explanation is that Avg.
Lines helps the comparisons with the average bar, but it does not highlight the elements to be compared as well as the other interventions, except in the case when they are contiguous to the average bar and to each other.
This may become a greater disadvantage with the more complex comparisons involved in our CDV tasks.
Additionally, there are no-longer significant differences between Bolding and De-Emphasis, nor among Bolding, Connected Arrows, and Avg.
Lines, indicating that for more complex tasks, the relative performance between the interventions is less pronounced.
For instance, feedback we gathered from participants indicates that De-Emphasis can make it hard to see bar groupings.
Even though Bolding and De-Emphasis can be considered conceptually similar , it is possible that for complex tasks, the fading of 'irrelevant bars' removes some contextual cues for sample grouping, which would help solve the tasks .
Thus, for CDV tasks, higher values of the cognitive measures may be having a stronger impact compared to RV tasks.
The result for perceptual speed aligns with results in previous work , where it was also found that users with lower perceptual speed require more time to complete a complex task relative to their high perceptual speed counterparts.
For visual and verbal working memory, this study is the first to connect these two cognitive traits to task performance  with a visualization, possibly because previous studies relied on tasks that were not complex enough to detect these effects.
Perceptual Speed*TaskType, VerbalWM*TaskType, VisualWM*TaskType: There was no significant difference in performance with RV tasks for users with different values of Perceptual Speed, VerbalWM, and VisualWM.
For CDV tasks, in contrast, users with higher values of these cognitive measures perform better.
Figure 5 shows the interactions for Perceptual Speed and Verbal Working Memory.
Delivery Time*Intervention: This interaction effect is shown in Figure 6 and indicates that for T0, None and DeEmphasis are, respectively, significantly worse and better than all other interventions .
For TX, the difference between interventions are much smaller, with Avg.
Lines no longer being significantly better than None, and Bolding and Connected Arrows no longer being worse than De-Emphasis.
This result is important, because it suggests that when interventions are delivered dynamically, they may lose some of their value due to possible intrusiveness, and thus it is crucial to evaluate them in the right context of usage.
On the other hand, even in the potentially intrusive TX condition, some interventions are still better than none, indicating that it is possible to provide dynamic adaptive interventions that can help improve effectiveness.
As we did for performance measures, we first ran a 2  x 5  General Linear Model repeated measures on the usefulness ratings in order to investigate the effects of our experimental factors alone, followed by additional analyses on each of our five co-variates with the experimental factors.
These ratings were corrected using the Aligned Rank Transformation -Tool  to make them suitable for parametric analysis.
Results from this analysis are shown in Table 4.
A similar set of analyses on the preference rankings yielded no significant results.
There was a significant main effect of intervention on usefulness ratings, shown in Figure 7.
The goal of our study was to investigate the relative effectiveness of four visual prompts designed to support users in visualization processing by highlighting visualization elements relevant to performing target tasks.
As we discussed in the introduction, this functionality can be extremely useful for scenarios in which users need to make a variety of inferences on a visualized dataset, and may benefit from having the most relevant subsets of graph elements emphasized in turn.
Although in our study, to keep the number of conditions manageable, we only considered one type of information visualization, i.e., bar graphs, there are at least three different arguments that support the potential generality of our results to other visualizations .
First, bar graphs are one of the most popular visualizations because they rely on length and 2-D position, the only two pre-attentive attributes that can be perceived quantitatively with a high degree of precision .
Thus, results on bar graphs can arguably generalize to other popular visualizations that rely on the same preattentive attributes, like line-graphs and scatter-plots.
Second, since bar graphs are so effective and popular they have been used as building blocks of more complex visualizations.
For instance,  recently presented LineUp, an interactive visualization supporting the very common and critical task of ranking items based on multiple heterogeneous.
As another example, ValueCharts  is a visualization that has been applied to elicit user preferences in decision making in different domains, as well as a component of a sophisticated interface to query event sequences.
We argue that our results may well generalize to these more complex and extremely useful visualizations based on bar graphs.
Third, most of the interventions considered in the paper can be applied to other visualizations besides bar graphs.
Thus, our results may well generalize also to these visualizations.
For example,  demonstrated several example applications of reference line, bolding, and de-emphasis in pie charts and line charts in addition to bar graphs.
Average reference lines have been used to visually compare individual marks to a predetermined value in various charts .
Bolding and de-emphasis form a perceptual group based on the Gestalt principle of similarity  and thus have been applied in various visualizations to relate items .
We now discuss the user study results with respect to our original research questions.
In the study we wanted to ascertain  if highlighting interventions can improve visualization processing;  if there is a highlighting intervention that is the most effective, and  if questions 1 & 2 are affected by user characteristics, task complexity, and intervention timing.
We investigated these questions in the context of performing visual tasks with bar graphs.
This main effect and the trends of the relative ratings between interventions correspond exactly to those for intervention on task performance found in the previous section , showing a strong connection between objective and subjective effectiveness of the tested interventions.
It is also worth noting that users found all the interventions more useful than no intervention, regardless of task type.
This was not the case for task performance.
There was, however, an interaction between intervention type and visual WM, as shown in Figure 8.
This is in line with previous work linking Visual WM to user subjective  scores .
Pairwise comparisons show that users with either low or average Visual WM rated the usefulness of Avg.
Ref Lines significantly lower than users with high Visual WM.
A possible explanation for this result is that the added reference lines may have been `visual distractors' for lower Visual WM users, given that the lines do not run only through the relevant bars, but also through any other bars between the average and the last relevant bar.
We also find that users with average Visual WM rate Bolding significantly higher than users with either low or high Visual WM.
While this finding further confirms the influence of Visual WM on subjective ratings, we currently do not have an intuition as to the directionality of the result.
As for question 1, our results show that all the highlighting interventions we tested, except for Avg.
Ref Lines, can improve visualization processing compared to receiving no interventions, both in terms of task performance and a user's perceived usefulness.
Thus, these interventions should be further investigated as means of providing users with dynamic support during visualization tasks.
As for question 2, results show that no single highlighting intervention is the most effective in general.
De-Emphasis always performed at the top, in terms of both performance and rated usefulness, but it was absolute best only with the simpler RV tasks, and when it was present from the beginning of the task .
Hence, we did find significant effects of task complexity and delivery time on intervention effectiveness .
When considering task performance, there was no longer a significant difference between De-Emphasis and Bolding during complex tasks, or among De-Emphasis, Bolding and Connected Arrows when the interventions were delivered dynamically.
For the long-term goal of providing adaptive highlighting interventions, this latter result suggests that future studies should focus on further investigating the effectiveness of De-Emphasis, Bolding and Connected Arrows in dynamic delivery conditions, and in particular in conjunction with delivery criteria based on actual user needs .
It is already a very encouraging result however, that delivering the interventions dynamically did not neutralize their effectiveness compared to no intervention, suggesting that their benefits outweigh their potential intrusiveness.
Still in relation to question 3, we also found an impact of user characteristics, in terms of an effect of Visual WM on ratings for perceived intervention usefulness.
This result is in line with previous findings that Visual WM affects subjective ratings for visualizations .
Our results suggest that, if information on a user's Visual WM is available, higher perceived usefulness may be achieved by using Bolding as a highlighting intervention for users with average Visual WM.
Interesting effects of individual differences were also found when analyzing the interaction with task complexity for task performance.
In particular, for each of the three cognitive abilities tested in the study, we found no significant difference in performance among participants with different levels of these abilities for simple tasks .
In contrast, for complex tasks  participants with high measures performed significantly better, indicating that complexity can significantly impact user performance depending on cognitive abilities.
Similar results were found in previous work for Perceptual Speed , but this is the first study that extends them to Visual and Verbal WM, likely because of the increased complexity of our tasks.
The fact that there were no interaction effects between cognitive abilities and the different highlighting interventions targeted in the study suggests that perhaps other types of interventions should be explored to help users with low-medium cognitive measures.
For instance, previous work linking gaze patterns to performance when processing bar graphs , suggests that users with low Perceptual Speed may benefit from help in processing a graph's legend, whereas users with low Verbal WM may benefit from interventions that facilitate processing of the verbal elements of a graph.
Also to note, we did not find any significant results for the personality trait Locus of Control.
A likely explanation is that most findings for this user characteristic were found when comparing list-like visualizations and visualizations with a strong containment metaphor , which were not the target of our interventions.
We also did not find any significant results for the visualization expertise measures we collected from users.
This could be due to the fact that some users were possibly biased when self-reporting their expertise, or that previous expertise was not a relevant factor with regard to a user's performance/preference with the visualization tasks administered in our study.
Our next step involves an analysis of user eye gaze behavior in order to verify and better qualify our findings, and to suggest further interventions for adaptive help.
We also plan to run similar experiments on more complex visualizations and on a broader set of interventions.
Visual Prompts and Graphical Design: A Framework for Exploring the Design Space of 2-D Charts and Graphs.
Muir, M. & Conati, C.: An Analysis of Attention to Student - Adaptive Hints in an Educational Game.
What Are the Shapes of Response Time Distributions in Visual Search?
Rotter, Julian B. Generalized Expectancies for Internal Versus External Control of Reinforcement.
Steichen, B., Carenini, G., Conati, C. User-Adaptive Information Visualization - Using eye gaze data to infer visualization tasks and user cognitive abilities.
Toker, D., Conati, B., Steichen, Carenini, G. Individual User Characteristics and Information Visualization: Connecting the Dots through Eye Tracking.
Toker, D., Conati, C., Carenini, G., & Haraty, M. Towards Adaptive Information Visualization: On the Influence of User Characteristics.
Stochastic modelling of elementary psychological processes.
Is working memory capacity task dependent?
Van Zandt, T. How to fit a response time distribution.
Velez, M.C., Silver, D., & Tremaine, M. Understanding visualization through spatial ability differences.
The Aligned Rank Transform for Nonparametric Factorial Analyses Using Only ANOVA Procedures.
Ziemkiewicz, C., Crouser, R.J., Yauilla, A.R., Su, S.L., Ribarsky, W., & Chang, R. How Locus of Control Influences Compatibility with Visualization Style.
