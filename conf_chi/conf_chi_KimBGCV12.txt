In this paper, we present TeleHuman, a cylindrical 3D display portal for life-size human telepresence.
The TeleHuman 3D videoconferencing system supports 360 motion parallax as the viewer moves around the cylinder and optionally, stereoscopic 3D display of the remote person.
We evaluated the effect of perspective cues on the conveyance of nonverbal cues in two experiments using a one-way telecommunication version of the system.
The first experiment focused on how well the system preserves gaze and hand pointing cues.
The second experiment evaluated how well the system conveys 3D body postural information.
We compared 3 perspective conditions: a conventional 2D view, a 2D view with 360 motion parallax, and a stereoscopic view with 360 motion parallax.
Results suggest the combined presence of motion parallax and stereoscopic cues significantly improved the accuracy with which participants were able to assess gaze and hand pointing cues, and to instruct others on 3D body poses.
The inclusion of motion parallax and stereoscopic cues also led to significant increases in the sense of social presence and telepresence reported by participants.
The effect of these cues on remote communication may be difficult to measure, and may not affect typical parameters, such as task performance .
However, we believe that differences in user experience of telecommunication versus face-to-face communication may be attributed to subtle violations of such nonverbal communication .
Since the Talking Heads system , researchers have worked on preserving cues in telecommunication to enhance human telepresence .
However, very few systems approach the richness of direct face-to-face communication.
Most only preserve a partial set of visual cues or suffer from costly and complex implementations .
One approach has been the use of animated 3D avatars of users  and head-mounted 3D virtual reality systems .
In such systems, a 3D model of the user is produced once, then animated in real time by measuring the user's behavior.
Since only animation parameters are transmitted in real time, these systems typically require little bandwidth.
However, they do so at a cost in realism that results in an Uncanny Valley effect .
While recent advances in 3D avatar systems offer highly realistic renditions , we believe there are significant advantages to using 3D video instead.
Video-based systems differ from avatar systems in that they capture a realistic 3D video model of the user every frame, which is then broadcast and rendered in real time across the network .
This results in a highly realistic replication of behavioral cues, but at a cost of network bandwidth.
The capturing and transmission of 3D video has, to date, required many special considerations in terms of camera placement and projection environment .
The associated requirements of such environments are prohibitive for the typical workplace.
Current videoconferencing systems range from the popular, low-end, small displays of Skype and FaceTime to expensive, large-screen business systems such as Cisco TelePresence and Polycom RealPresence, the latter of which can support life-size display.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Research initiatives in electronic transmission of human telepresence trace back to as early as the late 1940s with Rosenthal's work on half-silvered mirrors to transmit eye contact during video broadcasts .
In the 1970s, Negroponte developed the Talking Heads project .
Driven by the US government's emergency procedures prohibiting the co-location of its highest-ranking five members, Talking Heads proposed a five-site system where each site was composed of one real person and four plastic heads mounted on gimbals that replicated user head orientation.
Properly registered video was projected inside a life-size translucent mask in the exact shape of the face, making the physical mask appear animated with live images.
However, the system was a mockup that, in practice, would have required head mounted cameras for appropriate registration of faces.
The BiReality system  consisted of a display cube at a user's location and a surrogate in a remote location.
Both the remote participant and the user appeared life size to each other.
The display cube provided a complete 360 surround view of the remote location and the surrogate's head displayed a live video of the user's head from four sides.
By providing a 360 surround environment for both locations, the user could perform all rotations locally by rotating his or her body.
This preserved gaze and eye contact at the remote location.
Although this system presented a life size tele-operated robotic surrogate, only the remote user's head image was rendered realistically.
As implemented, the BiReality display was not responsive to viewer position, and thus, did not support motion parallax.
Kinects for capturing 360 3D video models of the users.
3D models are efficiently broadcast over the network by adding a grayscale depth map frame to each frame of video.
3D video images are then synthesized locally through texture mapping, in response to actual viewer perspective.
The 3D video models are rendered with perspective correction and stereoscopy on a life-sized cylindrical display, using an off-the-shelf 3D projector .
The chief contribution of TeleHuman is that it provides 360 motion parallax with stereoscopic live-sized 3D images of users, using a lightweight approach.
Motion parallax is provided via perspective correction that adjusts views as users move around the display.
Stereoscopy is provided through shutter glasses worn by the user.
There is evidence to suggest that motion parallax and stereoscopy play an important role in the experience of telepresence .
To evaluate how these factors might aid in the preservation of basic body orientation cues used in deixis  and in pose estimation tasks, we conducted two experiments.
The first focused on how well the system preserves gaze directional and hand pointing cues.
The second experiment evaluated how well the system conveys 3D body postural cues.
For both tasks, the TeleHuman was tested in three different viewing conditions: conventional 2D, 2D + motion parallax, and motion parallax + stereoscopy.
Results show the presence of both motion parallax and stereoscopic cues significantly improved the accuracy with which participants were able to assess gaze and hand pointing cues, and instruct others on 3D body posture.
These cues also led to significant increases in the sense of telepresence reported by participants.
A lightweight approach to preserving gaze directional cues was provided by Hydra .
Hydra used multiple cameras, monitors, and speakers to support multiparty videoconferencing.
It simulated a four-way round-table meeting by placing a camera, monitor, and speaker at the position of each remote participant, preserving both head orientation and eye contact cues.
Although initial prototypes suffered from vertical parallax due to the spatial separation of the camera below the monitor, subsequent designs reduced this considerably by placing the camera directly above the display.
Another limitation of Hydra was the use of small screens, which limited the size of remote participants.
The size of the rendered interlocutor may indeed affect the sense of the social presence .
The MAJIC  and Videowhiteboard systems  projected life size images on semi-transparent surfaces by placing cameras behind the screen.
However, these systems did not support 3D stereoscopic cues or motion parallax.
The GAZE  groupware system provided integral support for conveying eye gaze cues using still images.
Instead of using multiple video streams, GAZE measured where each participant looked by means of a desk-mounted eye-
This technique presented a user with the unique view of each remote participant, emanating from a distinct location in space.
Each persona rotated around its x and y axes in 3D space, thus simulating head movements.
Later, motion video was added via the use of half-silvered mirrors in GAZE-2 .
Our main consideration in the design of our capture and display system was to support 3D cues.
These aid in the preservation of information related to head orientation pose, gaze, and overall body posture of a human interlocutor.
In this context, we identified a number of relevant design attributes: 3D Cues - TeleHuman supports 3D both through optional use of stereoscopic shutter glasses and motion parallax.
The latter results in a change of view and relative shifts of objects in the visual field due to changes in the observer's tracked position, allowing users to walk around and observe a virtually projected interlocutor from any angle.
Form Factor - Providing full 360 motion parallax required the use of a cylindrical form factor display  proportionate to the human body.
Since this offers an unobstructed 360 field of view, it enables a user to explore different perspectives by natural physical movement.
Directional Cues - Being able to determine where users are looking or pointing has been shown to be an important cue in videoconferencing .
These cues can help regulate conversation flow, provide feedback for understanding, and improve deixis .
The use of 3D video models, as opposed to the direct display of a single 2D video camera output, facilitates preservation of eye contact.
However, stereoscopy through shutter glasses inhibits estimation of eye orientation in bi-directional scenarios.
We believed that motion parallax alone may suffice for estimation of gaze or pointing direction, as users are free to move to the location in which gaze and arm orientations align to point at the user .
Size - Prior work, such as Ultra-Videoconferencing  and that of Bocker et al.
This motivated the conveyance of life-size images in our design.
A variety of technical solutions have been devised to explore the preservation of 3D depth cues and motion parallax.
Harrison and Hudson presented a method for producing a simple pseudo-3D experience by providing motion parallax cues via head position tracking .
Their system required only a single traditional webcam at each end for both scene capture and the creation of head-coupled pseudo-3D views.
This system utilized a 2D display that did not provide stereoscopic vision .
Some CAVE-like environments provide an immersive VR experience, providing motion parallax for a single user.
They typically also require the use of shutter glasses, thus precluding the possibility of eye contact transmission.
For example, BlueC, an immersive projection and communication system , combines real-time 3D video capture and rendering from multiple cameras.
Developing a novel combination of projection and acquisition hardware, it created photorealistic 3D video inlays of the user in real time .
The use of auto-stereoscopic display technologies  provides similar capabilities, but without the need for special eyewear and often, adding the ability to support multiple users simultaneously, each with their own perspective-correct view.
However, these are restricted to specific optimal viewing zones, may result in significantly reduced resolution, and rely on a flat form factor.
We should note that the above examples all rely on planar screens, limiting the ability of users to walk around the display of a remote interlocutor as is, e.g., possible with LiteFast displays .
Another technology, swept-surface volumetric display , supports 3D display with motion parallax in a form factor often more suitable for this purpose, but recent examples have been too small to render a full human body at life size.
Although the benefits of including motion parallax and stereoscopy in the presentation of graphic interfaces have been demonstrated , systematic evaluation of the impact of these factors in the context of task performance during video communication, specifically, in assessing pointing or poses of a remote interlocutor, is sparse.
Bocker, Rundel and Muhlbach  compared videoconferencing systems that provide motion parallax and stereoscopic displays.
While their results suggested some evidence for increased spatial presence and greater exploration of the scene, the studies did not evaluate effects on task performance.
Subsequently, the provision of motion parallax was shown to generate larger head movements in users of video conferencing systems, suggesting that users do utilize such cues .
Figure 2 shows the cylindrical display deployed in TeleHuman.
The display consists of a 170 cm tall hollow cylinder with a diameter of 75 cm made of 6.3 mm thick acrylic.
The cylinder was sandblasted inside and out to create a diffuse projection surface.
The cylinder is mounted on top of a wooden base that holds the projector, giving the entire system a height of approximately 200 cm.
These dimensions were chosen to allow for a range in size of remote participants.
A DepthQ stereoscopic projector  is mounted at the bottom of each display, pointed upwards to reflect off a 46 cm hemispherical convex acrylic mirror.
These obtain images from the side and back of the user.
Images from the Kinects are accessed using OpenNI  drivers.
Each camera provides a 640x480 pixel stream at 30 fps with both RGB and depth images.
When a user approaches to within 2 m of the TeleHuman, the system starts tracking and broadcasting.
The system tracks the location of users around the display until they step out of range.
Each Kinect is connected to a PC, which sends the user's position via Open Sound Control , along with the user's RGB image and depth map to a Microsoft XNA application that controls the projection.
The XNA application calculates the angle between the user and the cylinder and updates the displayed model accordingly.
To maintain an appropriate frame rate, we use 1 PC per 2 Kinects, using a total of 5 PCs for preprocessing image data.
In order to create a 3D representation of a user, depth values are used to position vertices in a 3D XNA application.
Using the depth and RGB streams, the system calculates a four-channel image via OpenCV .
This image contains RGB information in the first three channels and depth information in the fourth channel.
Images are then sent via a TCP connection with the XNA projection application running on a separate machine.
Currently, our system sends images over a gigabit LAN connection, relying on the associated high network speeds to provide multiple live streams with low latency.
Note that future versions will use more efficient UDP protocols.
Using the depth map, the XNA display application creates vertices corresponding to each pixel of the user.
The depth value is used to determine the vertex locations along the z axis.
Depth values are also used to remove the scene behind the user, via a basic depth threshold.
Vertices are placed in a vertex buffer.
The content of this buffer is read and rendered by the XNA application.
Based on the distance of the viewer from the cylindrical display, the model is rendered such that the center of mass of the TeleHuman appears to be in the middle of the cylinder, which we treat as the origin.
The RGB values from the input image are used to texturemap the resulting mesh model.
This allows projections of images across the entire surface of the cylinder.
The DepthQ projector has a resolution of 1280 x 720 pixels.
However, since only a circular portion of this image can be displayed on the surface of the cylinder, the effective resolution is described by a 720 pixel diameter circle, or 407,150 pixels.
An Nvidia 3D Vision Kit  is used with the projector to create an active stereoscopic display.
This kit provides an IR emitter that connects to a 3-pin sync port on our system's graphics card.
Compatible shutter glasses are synced with the IR emitter and projected image, refreshing at 120 Hz.
As a result, when viewing the display, a distinct image is shown to each eye, and disparity between these two images creates stereoscopy.
By combining depth cues with perspective corrected motion parallax  the remote participant appears to be standing inside the cylinder.
We used Microsoft Kinect depth-sensitive cameras  to determine the location of users around the cylinder.
Six Kinects are mounted on the top of the cylinder, pointed downwards .
These track the location of the user around the cylinder, and obtain frontal images.
Four Kinects are located in a square around the cylinder,
The view of a user on the cylinder is rendered from the perspective of a virtual camera targeted at his or her 3D model.
The angular position of the user controls the angle with which this virtual camera looks at the 3D model of the interlocutor.
As a user's position changes, the position of the camera changes accordingly, allowing him or her to view a motion parallax corrected perspective of the 3D video model of the other user.
This camera view is rendered and stored as a texture.
3D information is preserved during this process allowing the texture to be viewed with stereoscopy.
The projected image is rendered using Microsoft's XNA 4.0 framework.
A custom distortion class was developed, creating a two-dimensional semi-circular object.
Top-view drawing of perspective conditions: conventional 2D , motion parallax , motion parallax + stereoscopy .
In the case of motion parallax, the display would show the remote individual from a slightly side perspective.
The black circle represents the cylinder, the person with a green shirt is the perception of the remote participant.
The local user is wearing a blue shirt.
The distortion model is textured using the previously rendered camera view .
When reflected off the hemispherical convex mirror, this creates an undistorted projection of the remote participant on the surface of the cylinder.
When the user moves around the display, the distortion model ensures that the remote participant remains at the center of the user's field of view.
As this projection changes based on user position, it creates a cylindrical Fish Tank VR view that preserves motion parallax .
Note that our approach does have the side effects of causing both resolution and brightness to drop off at lower elevations of the cylinder.
We used a within-subjects design in which we evaluated the effect of two fully factorial independent variables: perspective and pointing cue.
To allow for a more realistic scenario, and a richer set of cues, we also varied the participant's location in front of the display: left, center, and right, and the TeleHuman's pointing angle: left, center and right, between conditions.
We designed two experiments to evaluate effects of stereoscopy and 360 motion parallax on the preservation of nonverbal cues in our TeleHuman system.
Our first experiment focused on how stereoscopy and motion parallax might aid in the preservation of basic body orientational cues.
The second experiment focused on how stereoscopy and 360 motion parallax around the display might aid in conveying body postural cues.
The perspective factor consisted of three levels: conventional 2D, motion parallax, motion parallax + stereoscopy .
For the conventional condition, the TeleHuman was shown from the perspective of a frontfacing camera, centered on the human.
In the motion parallax condition, the TeleHuman was displayed with continuous perspective correction based on the location of the participant relative to the display.
In the motion parallax + stereoscopy condition, participants additionally wore shutter glasses that provided them with a fully stereoscopic image of the TeleHuman, giving the impression that the human was inside the cylinder.
Participants were asked to indicate where a TeleHuman model was looking or pointing.
To ensure equal conditions for all participants, we used a static prerecorded TeleHuman 3D video model in all conditions.
We used a simplified, asymmetrical setup in which only one TeleHuman pod was used.
At each position, participants were first asked if the TeleHuman was pointing or looking directly at them.
The pointing cue factor had three levels: gaze, hand, and gaze + hand.
In the gaze condition, the TeleHuman indicated the pointing direction by both eye gaze and head orientation directed towards the same location on the wall.
In the hand condition, the TeleHuman pointed at the target with their arm, hand and index finger.
In this condition, the gaze of the TeleHuman was fixated directly to the center, unless the actual target was the center, in which case, gaze was oriented randomly to the left or right of the target.
In the gaze + hand condition, the TeleHuman's arm, hand and index finger all pointed in the same direction as the eyes and head.
Figure 4 shows a participant standing in front of the TeleHuman.
The display was placed 2 m from a wall behind the participant.
This wall showed a tape measure with markings at 5 cm intervals from left to right.
To ensure presentation of consistent stimuli to all participants, we used a recorded still 3D image to constitute the pointing cues factor.
These were rendered according to the perspective factor, as shown in Figure 4.
For each condition, participants were asked to stand in between the display and a wall behind them, approximately 190 cm from the display and 10 cm from the wall.
Participants experienced the perspective and pointing cue conditions from three locations, distributed between-conditions: directly in front of the cylindrical display, 45 cm to its left, and 45 cm to its right.
In addition, in each condition, the TeleHuman pointed in a different angle, selected from left, center, or right.
Note that while pointing targets were not visible within our display setup, targets could be projected in the environment in a real videoconferencing scenario.
To evaluate the degree of telepresence and social presence experienced, participants completed a seven-point Likert scale questionnaire after each perspective condition .
Telepresence was defined as the feeling of "being there", while social presence was defined as the perceived ability to connect with people through the medium.
In the questionnaire, a 1 corresponded to strongly agree and 7 to strongly disagree.
Each participant carried out a total of 9 trials, by factorial combination of 3 perspectives  with 3 pointing cues .
To allow for a richer set of cues, we also varied the locations of the participant  and the directions of pointing between conditions .
We did not perform a fully factorial presentation as it would have led to 81 trials per participant.
The order of presentation of conditions was counterbalanced using a Latin square.
All participants were presented with the same set of stimuli, in different orders.
The experimental session lasted one hour.
Visual assessment allowed us to determine any effects of a more stationary perspective on the accuracy of pointing direction estimates.
We expected visual alignment to provide the most accurate method for determining where the TeleHuman pointed or looked, as it allowed users to align themselves such that the TeleHuman appeared to be looking or pointing directly at them.
Each measure was calculated as the angular difference between reported viewing direction and the actual TeleHuman pointing direction.
Bonferroni post-hoc tests showed that mean accuracy of visual assessment was 1.8 times higher in the motion parallax + stereoscopy condition than in the conventional 2D condition .
However, there were no significant differences between other conditions.
Post-hoc pairwise Bonferroni corrected comparisons of the perspective conditions show that mean accuracy was significantly greater in the motion parallax condition  and in the motion parallax + stereoscopy condition , compared to the conventional 2D condition.
There was no significant difference between the motion parallax and motion parallax + stereoscopy conditions .
Participants were asked to walk around the TeleHuman to examine the pose, and around the poser to examine the result, in all conditions.
Note that while participants were allowed to ask the instructor to rotate to show her back conventional 2D conditions, none did, as this would have interfered with her ability to perform the pose.
The coach and the poser were co-located in the same room as the TeleHuman system; but only the coach could see the TeleHuman system.
The instructor was in a separate room, and displayed using a live 3D 360 video model on the TeleHuman system.
We used an asymmetrical version of the system that allowed for full 360 motion parallax, in which the coach could see and hear the instructor as represented by the TeleHuman, but the instructor could not see the coach.
The instructor was not allowed to interfere with the directions of the coach to the poser.
Once the coach was satisfied with the poser's posture, the instructor would go to the poser's room to evaluate the poser's stance, while the coach filled out a questionnaire.
We used pairs of participants, unfamiliar with yoga, alternating as coach and poser.
To alleviate learning effects, a different yoga pose was used for every condition between pairs of participants, for a total of six yoga poses.
All yoga poses, preselected by the yoga instructor, were of the same intermediate level of difficulty as judged by the instructor, and focused on upper body positioning .
All poses had limb elements positioned on the back, front and sides of the instructor.
The choice of yoga pose was randomly assigned to each coach and condition, and no feedback was provided by the instructor to the poser about the quality of any poses.
The three visual perspective conditions were counter-balanced for each coach.
The poser was never instructed on the perspective level at hand.
A remote instructor, displayed on the TeleHuman, first positioned herself in one of the predetermined yoga poses , one per condition.
The remote instructor was blind to the conditions.
At that point, the main participant  instructed a co-located partner  to reproduce the pose as accurately as possible, within a 3 minute time limit.
Statements It was as if I was facing the partner in the same room.
The instructor evaluated the similarity between her pose and that of the poser on a scale from 0 to 10 .
In this process, she took into account limb angles and orientations, as well as overall posture.
After each condition, coaches completed the same questionnaire administered in the first experiment, which evaluated the degree of telepresence and social presence experienced.
Table 3 shows the mean pose similarity score and standard error for each perspective condition.
Results show that posture similarity scores were significantly different between perspective conditions =4.224, p=0.03.
Post-hoc tests using Bonferroni correction show that scores in the motion parallax + stereoscopy condition were significantly different from scores in the conventional 2D condition .
Results from our first experiment confirmed a strong effect of perspective on the accuracy of assessment of remote pointing cues.
Motion parallax + stereoscopy increased the accuracy of angular judgment by a factor of 1.8 over traditional 2D conditions in cases where participants were stationary.
As expected, motion parallax alone, in this situation, was limited, and thus, the addition of stereoscopy was important.
When participants were allowed to move, motion parallax was shown to provide the dominant effect, with participants achieving four times higher accuracy on average in angular judgment of remote pointing cues as compared to 2D conditions.
In this case, stereoscopy appeared to provide little additional benefit.
Note that the type of pointing cue: gaze, hand only, or gaze + hand, had no significant effect on accuracy measures.
Qualitative measures support the above analysis.
Social presence rankings were significantly higher in conditions where motion parallax cues were supported, with no significant additional effect for motion parallax augmented by stereoscopy.
As for the degree of telepresence or immersion, the combined effect of motion parallax and stereoscopy was critical for obtaining significant differences from 2D conditions.
Stereoscopy therefore appears to be beneficial for judgment of pointing angle when motion parallax cannot be exploited.
Results for our second experiment, in which we evaluated the effects of perspective cues on preservation of postural cues, were in line with those from Experiment 1.
The presence of motion parallax + stereoscopy cues increased the accuracy of pose scores by a factor of 1.6 over conventional 2D conditions.
These results suggest that both motion parallax and stereoscopy needed to be present in order to judge and convey poses accurately.
Surprisingly, the presence of motion parallax cues alone only marginally improved scores.
This was likely due to the fact that while motion parallax allowed users to see the sides and back of poses, stereoscopy helped improve their judgment of the relative angles of the limbs.
Qualitative measures indicate little additional effect of the presence of stereoscopic cues.
Social presence rankings were significantly higher in conditions where motion parallax or motion parallax + stereoscopy were supported.
As for the degree of telepresence, rankings were significantly higher in cases where motion parallax or motion parallax + stereoscopy were supported.
However, there appeared to be little additional effect of the presence of stereoscopic cues over motion parallax only.
While the presence of stereoscopy did not significantly affect qualitative measures, we can conclude that in this task both motion parallax and stereoscopy were required.
One example is in remote sports instruction.
As Experiment 2 demonstrates, examination of the mechanics of limb movement may benefit from the ability to review movement and posture from any angle.
For example, this may be helpful in teaching golfers to improve their swing.
Applications also exist in telemedicine and remote medical instruction, for which the benefits of arbitrary view control were demonstrated previously in the context of surgical training .
TeleHuman could similarly offer doctors the ability to examine remote patients from any angle, but at full scale.
This may be particularly beneficial for orthopedic or postural conditions, where the patient cannot reorient herself for a side view.
Finally, applications exist in gaming, as the ability to render a 3D gaming character or another online gamer in a 360 view allows for a more immersive gaming experience in first-person shooter scenarios.
In the near future, we hope to leverage TeleHuman for multiparty teleconferencing scenarios.
To support such experimentation, we will be replacing the current TCP communication layer with a UDP-based alternative, suitable for low-latency interaction over larger distances.
Support of a teleconference with n users requires n2-n setups and, barring multicast support, a similar number of data streams.
This entails significant bandwidth requirements for transmission of 3D video models.
However, our design allows for such scaling without modifications to the TeleHuman hardware.
Our first study was limited by the fact that the TeleHuman was a static 3D image, and communication was not reciprocal.
Although this permitted us to evaluate the effect of stereoscopy on pointing cue assessment, it necessitated an artificial communication condition in which the shutter glasses had no detrimental effect on perception of eye contact.
There is an obvious tradeoff between supporting eye contact between interlocutors and presentation of a stereoscopic display requiring the use of shutter glasses.
However, other display technologies, such as autostereoscopic and volumetric displays do support glasses-free stereo viewing.
We hope to conduct future experiments to evaluate the added benefit that such technologies might offer in terms of eye contact perception with TeleHuman.
Note that participants in our study did not ask the instructor to rotate in the 2D condition.
There may be cases in which such rotation would provide adequate information to complete a 3D pose task.
To avoid introducing confounding factors, we did not specifically compare results with traditional 2D flat display conditions.
However, we believe that the results of our 2D conditions would generalize to such conditions.
In this paper, we presented the TeleHuman system, a cylindrical display portal for life-size 3D human telepresence.
The system transmits telepresence by conveying 3D video images of remote interlocutors in a way that preserves 360 motion parallax around the display, as well as stereoscopy.
We empirically evaluated the effect of perspective on the user's accuracy in judging gaze, pointing direction, and body pose of a remote partner using an asymmetrical version of the system.
Results for pointing directional cues suggest that the presence of stereoscopy is important in cases where the user remains relatively stationary.
However, when users move their perspective significantly, motion parallax provides a dominant effect in improving the accuracy with which users were able to estimate the angle of pointing cues.
As for pose estimation, the presence of both 360 motion parallax cues and stereoscopic cues appeared necessary to significantly increase accuracy.
Both motion parallax and stereoscopy appear important in providing users with a sense of social presence and telepresence.
We conclude that we recommend inclusion of both motion parallax and stereoscopic cues in video conferencing systems that support the kind of tasks used in our evaluation, with the caveat that tools such as shutter glasses, which obstruct views of the remote participants eyes, are most likely not recommendable for bi-directional communication systems.
