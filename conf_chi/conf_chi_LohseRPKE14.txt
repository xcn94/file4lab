Gestures are important non-verbal signals in human communication.
Research with virtual agents and robots has started to add to the scientific knowledge about gestures but many questions with respect to the use of gestures in humancomputer interaction are still open.
We conducted a 2 x 2  experiment.
The results indicate that robot gestures increased user performance and decreased perceived workload in the difficult task but not in the easy task.
Thus, robot gestures are a promising means to improve human-robot interaction particularly in challenging tasks.
Complex gesture models for virtual agents have been developed and employed in HCI research .
The evaluation of these models focused, among others, on the perceived naturalness of the gestures , on their expressivity , on users' trust in the agent , and on gesture production depending on the cognitive load of the speaker .
All these studies showed that virtual agents are perceived more positively when producing co-verbal gestures.
Gesture research with agents has attracted further attention in the recent years with the increasing number of robot research platforms that cannot only produce gestures in a virtual space but in the physical world that they share with humans .
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Thus, HRI research has supported the findings from the virtual agents community.
Our research focuses on an effect of gestures that has largely been ignored in HCI in general as well as in HRI in particular: the influence of gestures produced by a system  on the perceived workload and task performance  of the user in tasks with varying difficulty.
While the impact of gesturing on objective task performance has found some attention in previous work , perceived workload has not been taken into account.
However, perceived workload is a highly important concept because it captures the users' subjective impression of the task .
It has been shown that perceived workload affects the affective and physiological state of humans.
High perceived workload can cause affective distress and increased blood pressure  and has also been shown to influence job satisfaction .
To address this gap in the literature, we present an experiment on the influence of deictic  gestures that are redundant with speech in a direction-giving task.
Users listen to directions provided by a robot in an easy or a difficult task.
The descriptions are only accompanied by gestures for half of the participants.
Based on our results, we show the importance of robot gestures particularly in difficult tasks.
Thus, our study contributes first research on the influence of robot gestures on perceived human workload and task performance.
We add to the understanding of user behavior and cognitive capabilities in tasks with different levels of difficulty.
In this respect, our work lays the ground for the design of future robot systems.
Metaphoric gestures present an abstract idea rather than a concrete object.
Deictic gestures are used to indicate objects, people, and locations in the real world that do not necessarily have to be present.
Finally, beat gestures are beats with the rhythm of speech regardless of content.
As has been mentioned above, our experiment focuses on a direction-giving task.
In this context, deictic gestures are the most relevant type of gestures because they connect utterances to the physical setting and the physical world in which the conversation takes place is the topic of the conversation .
Pointing is the most obvious way to create deictic references.
For the use of gestures in route direction tasks, Kita  found that when people utter an expression such as "turn left", they also perform gestures to describe the direction of "left".
Kendon  identified different pointing gestures in interaction data, extending the index finger being the most commonly used when a speaker singles out a particular object or place.
Thus, in our experiment the robot will perform pointing gestures with its index finger extended while providing the directions to the user .
Most of the research on human gestures follows one of two main traditions: the one claiming that the main function of gestures is to help the speakers think and structure their speech , or the one stressing the communicative importance of gestures .
We here follow the second tradition and summarize related work on the influence of gestures on the perception of a person by others.
Gesturing has been found to strongly affect human listeners because they pay close attention to information conveyed via such non-verbal behaviors .
The influence of the gestures is largely determined by their relation to the information encoded in verbal utterances.
This relation can be reinforcing or supplementing .
Especially deictic gestures such as pointing, references to objects, locations, and actions, can be reinforcing .
The gestures label what is pointed at, but the referent can be understood with speech only.
In contrast, when the gesture is supplementing, the referent that is pointed at is not clear without the gesture .
For our experiment, we use reinforcing deictic gestures.
Thus, the referent is also clear without the gesture which allows us to design and compare conditions with and without the robot gesturing .
There is a vast body of research on gestures in human interaction.
Based on Kendon's definition , they are understood here as deliberate movements with sharp onsets and offsets.
They are an excursion in that a part of the body  moves away from and back to a certain position.
The movement is interpreted as an addressed non-verbal utterance that conveys information.
Gesture research has resulted in categorizations of types and functions of gestures.
These are described in the following sections because they help to set the frame for our experiment.
They define workload as a hypothetical construct that represents the cost incurred by a human operator to achieve a certain level of performance.
This view is human-centered rather than task centered and captures a subjective experience rather than an inherent property of a task.
In other words, the perceived workload depends on the circumstances, skills, behaviors and perceptions of the user.
Perceived workload is a well-studied concept in HCI which is underlined by the fact that validated standard scales like the Nasa TLX exist.
In , Hart reviews a selection of 550 papers that have used the Nasa TLX.
It needs to be mentioned that the review was conducted some years ago, in 2006, and even at that time papers were selected and the overall number was higher.
According to Hart's analysis, the research areas where the scale has been employed include manual control tasks such as flying , driving , data entry  and others.
However, there are much fewer studies addressing the perceived workload caused by virtual agents and robots.
The Nasa TLX has mainly been used to measure human performance and workload in teleoperation scenarios  .
The perceived workload concept has also been mentioned in some other contexts.
Gu and Badler  propose to research the effects of workload in studies with virtual agents by manipulating the number of agents one interacts with.
However, they present no evaluation.
In HRI, Yoshiike et al.
Again, there is no study to test the effect of this minimal design on the perceived workload of the user.
H1: The participants' task performance  will improve significantly when the robot produces redundant gestures in the difficult task but not in the easy task.
However, task performance is not our main interest here.
In contrast to previous studies, we are also interested in the effects of gestures on the perceived workload of the users.
As has been mentioned above, we are not aware of any related work addressing this relation directly.
However, it has been shown that visual feedback can decrease perceived mental workload .
Also gestures are a visual cue which leads us to our second hypothesis: H2: The participants' perceived workload will be significantly lower when the robot produces redundant gestures, both in the difficult and in the easy task.
We designed a 2  x2  between subjects experiments , in which the participants played a memory game with the robot Nao.
The robot told the participants a seemingly random set of directions that they were later asked to recall.
The directions consisted of phrases used to navigate a building.
Each direction included two pieces of information: one directional information  and a number specifying the amount of levels the participant needed to go up or down, or the number of turns the participant needed to take to left or right.
The phrases we used were: * "Take the 'nth' left" * "Take the 'nth' right" * "Go up 'n' levels" * "Go down 'n' levels" In all utterances, n was a number between one and five.
The amount of directions per round was varied between difficulty levels .
In the following, we establish a connection between gestures, performance and perceived workload, and introduce our hypotheses.
For human interaction, Kendon  proposed that gestures along with speech make utterance units more effective.
In an HRI direction-giving task , Okuno et al.
Also McNeil, Alibali, and Evans  suggested that redundant gestures facilitate understanding in adult-child interaction.
They assumed that indexical  gestures may facilitate comprehension most when speakers make reference to particular objects and locations.
However, their findings only partially underlined these assumptions.
They found that speakers' reinforcing gestures facilitated listeners comprehension of the accompanying speech when the verbal message was highly complex  but not when it was simple.
Also for HRI, H aring et al.
This leads us to our first hypothesis:
The robot used in this experiment is the Nao robot by Aldebaran Robotics .
It is a bipedal humanoid robot that measures 57.3 cm in height when in an upright, standing position.
The robot has 25 degrees of freedom to move its head, arms, hands, legs, and hips.
It has two speakers integrated in its head.
In our experiment, the robot was connected to a laptop from where the researcher could remote control its behavior.
Scripts for the utterances and gestures were prepared beforehand and played during the interaction.
All participants received the same set of directions in a randomized order.
We designed two different robot behaviors with respect to gesturing.
In the "no gesture" condition , the robot stayed in one posture throughout the experiment and did not gesture .
For the condition with gestures , we developed movements that were in accordance with the directions that the robot provided to the users.
Thus, all gestures were pointing  gestures redundant with speech .
Their design was based on work by Kanda et al.
However, as the Nao robot cannot rotate its torso, we replaced this motion by the rotation of the head.
This should allow the participants and the robot to establish a joint perspective.
We implemented the following gestures: 1.
The robot points its arm and rotates its head to its right  or left .
The robot points its arm and tilts its head upwards.
The robot points its arm and tilts its head downwards.
Figure 3 depicts examples of the gestures of the robot.
All participants of the pretest should be able to solve the easy task.
The difficult task should cause some problems to some of them but still be solvable for the majority of participants.
Based on the results of the pretest, we decided for two directions  in the easy task and four directions  in the difficult task.
This decision is in line with Miller's Law that claims that humans are able to hold 7  2 objects in their working memory .
Next to the pieces of information within one direction, we also analyzed how long the participants paid attention to the task and the robot before getting bored or distracted.
Based on this information, we decided for five rounds that the participants had to complete with the robot in both conditions.
We chose a direction-giving task because the embodied robot could indeed be credible in such a task pointing out directions in space.
Furthermore, this task allowed for manipulation of the difficulty by influencing the amount of information provided to the participants.
As has been mentioned above, each direction consisted of two pieces of information.
We conducted a pretest with five students of our university to determine how many directions appeared easy or difficult to them.
We employed a scale on perceived shared reality because it has been found that robot gestures and shared reality are closely intertwined in HRI .
Thus, this scale basically serves as a manipulation check to ensure that the manipulations between the static and animated conditions were actually perceived by the users and caused the potential differences in the task difficulty and perceived workload ratings.
We used the experienced shared reality scale previously employed by Salem et al.
The task performance was an objective measure.
We counted the number of correct recalls of directions by the users based on video data that we recorded during the trials.
The maximum score was 5  because the users played five rounds and only if the recall of the information presented in one round was 100% correct, the recall was counted as being correct .
When the participant completed the round successfully, the robot would say "That is correct".
If the participant made an error, the robot said "That is not correct."
Both events were triggered by the experimenter.
Each new round started with the robot announcing "Let's begin a new round".
After having completed five rounds, the experimenter asked the participant to fill out the questionnaire and thanked him/her.
To capture the users' subjective impression of the workload , we used the Nasa TLX scale which is a standard in recording subjective task workload .
We included all subscales  into the analysis, each of them being rated on a 10-point Likert scale ranging from "low" to "high".
We employed the weighted version of the Nasa TLX which allows to take the importance of each subscale in a specific scenario into account.
Thus, the questionnaire included the scale to weigh the subscales which asked the participants to compare all individual subscales with all others and to indicate which one they found more important or relevant.
The experiment was conducted with 32 participants distributed equally between the conditions.
All participants were students of the university .
Their age ranged between 18 and 28 years with a mean age of 21 years  = 2 years.
The participants had hardly any experience with robots: 27 had never seen a robot in real life, 3 had only seen one, and just 2 had interacted or worked with robots before the experiment.
Before analyzing the data, we checked for normality of distribution.
If the data was normally distributed, we used two-factorial ANOVAs and we report main- and interaction effects.
In case of non-normally distributed data, nonparametric tests were applied, namely Kruskall-Wallis tests followed up by post-hoc Mann-Whitney tests as proposed by Field .
A Shapiro-Wilk test showed that the data for perceived shared reality were normally distributed.
Thus, we conducted a twoway ANOVA to discover differences between the conditions.
This ANOVA revealed that there was no statistical difference between the conditions regarding experienced shared reality, F  = 1.865.p = 0.159.
The means for the animated / easy and animated / difficult conditions were the same as well with a value of 3.58 .
Thus, the task difficulty did not have an impact on the perceived shared reality which is positive because only the gestures should have.
Based on this finding, we can conclude that the manipulation was successful and the gestures led to higher perceived shared reality.
At the start of the experiment, each participant was greeted by the experimenter who provided some explanation and asked the participant to fill out a consent form.
Thereafter, the participant sat down in front of the table on which the Nao was standing .
Thus, both were facing each other .
The experimenter explained that when the robot would either point to the right or left, it would comply to the spatial perspective of the participants, i.e.
After providing this information, the experimenter withdrew to a laptop that was used to control the robot in a Wizard of Oz fashion.
The participants could not see the experimenter from where they were sitting.
In all conditions, the interaction started with the robot uttering "Let's begin the game".
As mentioned above, the robot's actions were pre-programmed and triggered by the experimenter at the correct moments.
A Shapiro-Wilk test showed that the data on task performance were not normally distributed.
Hence, we conducted a Kruskal-Wallis test to compare the conditions.
The test revealed a significant difference between them  .
Post-hoc Mann-Whitney tests showed that participants who completed one of the easy conditions had significantly higher scores  than participants who completed one of the two hard conditions , U = 47.50, p = .002.
This finding indicates that the manipulation of the difficulty of the task was successful.
Tests between the individual conditions showed that the difference was statistically significant between the static conditions  as well as between the animated conditions .
To look at this in more detail, we compared all individual conditions.
We found that the gestures had no significant influence on task performance in the easy condition but in the difficult condition .
In other words, in the easy condition the recall of the participants did not improve if the robot gestured, but in the difficult condition it improved significantly.
This finding is in line with related work and H1  will improve significantly when the robot produces redundant gestures in the difficult task but not in the easy task.
As has been mentioned in the "Dependent Variables" Section, we used the weighted Nasa TLX scale to assess perceived workload.
The results indeed revealed that some subscales were more relevant than others to the participants, e.g., physical demand had almost no influence with a mean weight of 0.41 .
Table 2 depicts the weights of all subscales.
Again, we conducted a Shapiro-Wilk test for normality which showed that the data for perceived workload were normally distributed.
A two-way ANOVA revealed a significant difference of the perceived workload between the four conditions  = 6.905, p = .001.
There was no interaction effect between animation and difficulty.
In fact, unpaired T-tests showed that the difference in the easy task was not significant, while the difference in the difficult task was  = 3.031, p = .009.
Therefore, H2  was only supported for the difficult but not for the easy tasks.
This finding is in line with the results regarding task performance.
Indeed, we discovered a significant negative correlation between the weighted Nasa TLX scores and the task performance , r = -0.695, p = 0.000.
In other words, the higher the perceived workload, the less correct answers were provided by the participants.
This paper set out to answer the question whether robot gestures can decrease the users' perceived workload and increase their performance on a task.
To find an answer to this question, we conducted a direction-giving experiment in which we manipulated the gestures that a robot performed  and the difficulty of the task .
Our results showed that the robot gestures did indeed support human task performance and lowered the perceived workload of the users but only in the difficult task.
This result is in line with research on gestures in human interaction that came to the same conclusion .
We believe that this is due to the easy task being simple enough for the participants to be completed with very little information.
This little information could easily be kept in mind even if it was only presented with one modality.
However, the difficult task required the participants to recall more information and multimodality was useful to convey it.
But what does this imply for the design of robots or virtual agents?
As has been mentioned in the introduction, perceived high workload can have a serious negative impact on humans' psychological and physical well-being.
Thus, if we are building systems, we have to strive to keep the perceived workload low enough to avoid such effects.
This goal has to be achieved despite the fact that the difficulty of the task itself in many cases cannot be reduced.
However, we can influence how a system and a human solve the task.
Our results show that equipping systems with the capability to gesture is a promising way to reduce perceived workload while keeping the task difficulty stable.
Buisine, S., Abrilian, S., and Martin, J.-C. From brows to trust.
Evaluation of multimodal behaviour of embodied agents, 217-238.
Nudge nudge wink wink: elements of face-to-face conversation for embodied conversational agents, 1-27.
Designing persuasive robots: how robots might persuade people using vocal and nonverbal cues.
In Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction, HRI '12, ACM , 293-300.
Field, A. Discovering statistics using SPSS.
Fox, M. L., Dwyer, D. J., and Ganster, D. C. Effects of stressful job demands and control on physiological and attitudinal outcomes in a hospital setting.
Goldin-Meadow, S. Hearing Gesture: How Our Hands Help Us Think.
Belknap Press of Harvard University Press, 2005.
Visual attention and eye gaze during multiparty conversations with distractions.
H aring, M., Eichberg, J., and Andr e, E. Studies on grounding with gaze and pointing gestures in human-robot-interaction.
7621 of Lecture Notes in Computer Science.
Hart, S., and Staveland, L. Development of nasa-tlx : Results of empirical and theoretical research.
In Human mental workload, P.A.
Hato, Y., Satake, S., Kanda, T., Imai, M., and Hagita, N. Pointing to space: modeling of deictic interaction referring to regions.
Ilies, R., Dimotakis, N., and De Pater, I. E. Psychological and physiological reactions to high workloads: Implications for well-being.
Our work suffers from some limitations that will be addressed in future research.
First of all, the impact of gesture in interaction is highly dependent on the task.
Our research here is restricted to one direction-giving task and to deictic gestures.
Thus, future work needs to focus on the influence of other types of gestures  on performance and perceived workload in a variety of tasks.
Furthermore, future research would profit from larger sample sizes and samples that better capture representative user groups of robots, i.e.
Nevertheless, we believe that this work addresses a novel aspect in HRI that comes with valuable implications for the larger HCI community: it introduces the relation between robot gestures and perceived workload to the field and shows that the interaction can actually profit from robot gestures with respect to perceived workload, particularly in difficult tasks.
Bergmann, K., Eyssel, F. A., and Kopp, S. A second chance to make a first impression?
Bergmann, K., Kahl, S., and Kopp, S. Modeling the semantic coordination of speech and gesture under cognitive and linguistic constraints.
In Intelligent Virtual Agents, R. Aylett, B. Krenn, C. Pelachaud, and H. Shimodaira, Eds., vol.
8108 of Lecture Notes in Computer Science.
Breazeal, C., Kidd, C., Thomaz, A., Hoffman, G., and Berlin, M. Effects of nonverbal communication on efficiency and robustness in human-robot teamwork.
In Intelligent Robots and Systems, 2005.
Iverson, J. M., Capirci, P., Longobardi, E., and Caselli, M. C. Gesturing in mother-child interaction.
Kanda, T., Shiomi, M., Miyashita, Z., Ishiguro, H., and Hagita, N. A communication robot in a shopping mall.
Gesture and speech: How they interact.
In Nonverbal Interaction , J. Wiemann and R. Harrison, Eds.
Gesture - Visible Action as Utterance.
Cambridge University Press, Cambridge, 2004.
Towards natural gesture synthesis: Evaluating gesture units in a data-driven approach to gesture synthesis.
In Proceedings of the 7th international conference on Intelligent Virtual Agents, IVA '07, Springer-Verlag , 15-28.
Kita, S. Pointing: Where Language, Culture, and Cognition Meet.
The role of gesture in children's comprehension of spoken language:now they need it, now they don't.
McNeill, D. Hand and Mind: What Gestures Reveal about Thought.
University of Chicago Press, Chicago, 1992.
The magical number seven, plus or minus two: Some limits on our capacity for processing information.
