For example, a flashing cursor denotes that system is attending and what its focus is .
Such mechanisms have, by now, become conventions of commonplace and accepted genres for interaction.
Indeed it is easy to forget that each one had to be carefully designed, before it ever became a convention.
By genre here, we mean a set of design conventions anticipating particular usage contexts with their own conventions.
Examples of system genres include; games, productivity tools, and appliances and examples of interaction genres include, the GUI, voice activation and the remote control .
Genre makes design easier by pre-packaging sets of interaction conventions in a coherent manner that designers can use to leverage user expectations about the purpose and use of a device and to accommodate their existing skills.
By sticking to the GUI genre , using standardized toolkits, and by copying design ideas from existing solutions, designers now assemble myriad UIs for desktop, laptop, hand-held and other devices from preexisting components without needing to ponder basic interaction issues.
However, those working in areas such as Ubiquitous Computing , where input is sensed by means other than keys, mouse or stylus , have no such well-understood, pre-packaged answers to these questions.
Lacking these well-established precedents, designers of sensing systems must constantly confront these basic questions anew.
In the rest of this paper we present a framework for addressing the resulting design challenges inherent in sensing systems, drawing on lessons about human-human interaction  in social science.
Our approach is not the same as presenting methods and guidelines for HCI design such as  or Apple's wellknown Human Interface Guidelines .
Such texts are useful for designing systems within GUI-style interaction paradigms.
This paper borrows ideas from social science to inform the design of novel "sensing" user-interfaces for computing technology.
Specifically, we present five design challenges inspired by analysis of human-human communication that are mundanely addressed by traditional graphical user interface designs .
Although classic GUI conventions allow us to finesse these questions, recent research into innovative interaction techniques such as `Ubiquitous Computing' and `Tangible Interfaces' has begun to expose the interaction challenges and problems they pose.
By making them explicit we open a discourse on how an approach similar to that used by social scientists in studying human-human interaction might inform the design of novel interaction mechanisms that can be used to handle human-computer communication accomplishments.
Designers of user interfaces for standard applications, devices, and systems rarely have to worry about questions of the following sort: When I address a system, how does it know I am addressing it?
When I ask a system to do something how do I know it is attending?
When I issue a command , how does the system know what it relates to?
How do I know the system understands my command and is correctly executing my intended action?
How do I recover from mistakes?
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
However, these approaches tend to deal in specific interaction mechanisms rather than the general accomplishments they support, they do not fare well when applied to innovative genres of interaction beyond the GUI.
Instead, our aim is to revisit and bring together some fundamentals of HCI, borrowing concepts from the social sciences, to provide a systematic framework for the design of sensing systems.
We have, in the last decade, seen a number of innovations in interaction mechanisms best characterized overall as sensing systems; including Ubiquitous Computing  systems ; Speech and audio input ; Gesture-based input  Tangible Interfaces or `Phicons'   and Context Aware computing .
These sensing mechanisms have expanded what was previously a key-pressing, point-andclick interaction bottleneck, allowing systems to accept a far wider range of input than was previously possible.
However, by definition, designers of these systems cannot simply copy existing precedents for handling input and output, unlike standard GUI designers.
The point of their research is to tackle anew the many challenges that had to be addressed in the GUI and its cousins to make it over Norman's famous gulfs of execution and evaluation .
In contrast to Norman, our approach highlights communicative, rather than cognitive aspects of interaction.
We agree with the coverage of Norman's model-from human intent to assessment of system action-but focus our attention on the joint accomplishments of the user and system that are necessary to complete the interaction, rather than the user's mental model.
This stance is driven by a growing appreciation of two developments: The potential value of social science to the field of HCI.
However, rather than focusing on the findings of sociologists about the use of technology in social settings  we are using the kinds of questions addressed by social science in HHI as a model on which to pattern some of the science of HCI.
We understand, as we have said, that HHI and HCI cannot be regarded as identical problem spaces; however, we argue that despite the differences, many of the same communication challenges apply and must be recognized by designers.
A trend in HCI towards sensing systems that dispense with well-known interaction genres, requiring us to return to the basic communication problems that the prepackaged GUI interaction solutions so elegantly solved.
Goffman, an interaction analyst who has been particularly influential in social science, has written extensively on interpersonal verbal and non-verbal communication .
He provides a perspective on HHI that elucidates how people manage accomplishments such as addressing, attending to and politely ignoring one another.
For example, signals are used to communicate intention to initiate, availability for communication, or that a listener understands what is being said.
Surely attention to similar mechanisms for HCI could be valuable.
Further Goffman  also developed a notion of frames that are social constructs  that allow us to make sense of what might otherwise seem to be incoherent human actions.
Frames in HHI seem to parallel genre in HCI as defined above and may be useful constructs for informing design.
From Conversation Analysis, we know that successful conversation demands many basic accomplishments that most humans master.
Sacks et al.,  show how turn taking is managed as conversational participants organize their talk in an orderly fashion.
Schegloff et al.,  demonstrate how mistakes, and misunderstandings are repaired in communication.
Button and Casey,  examine how people establish a shared topic in conversation.
Similarly humans and systems must manage and repair their communications, and must be able to establish a shared topic .
Norman  proposes an "approximate model" of seven stages of action with respect to system interaction: Forming the goal Forming the intention Specifying an action Executing the action Perceiving the state of the world Interpreting the state of the world Evaluating the outcome It is important to notice that Norman's theory of action focuses on user cognition.
Moreover, it implicitly reflects a difference between HHI and HCI.
Humans and computers are not equal partners in dialog.
Computers are dumb slaves, have limited functionality, and rarely take the initiative.
On the other hand, they have capabilities that humans do not.
They can output precise information about their state, perform many rapid calculations simultaneously, emulate a vast range of tools and control multiple complex mechanical systems in parallel, and they can be guided and manipulated in many different ways by users.
The clever ploy embodied in the GUI is to exploit the different roles and relative strengths of computer and user and finesse the communication problem by forcing the user  to drive interaction, constantly discovering and monitoring which of many possible things the system is capable of and how it is interpreting ongoing action.
Norman's seven stages of execution, but with the emphasis now being on communication rather than cognition.
Address: Directing communication to a system.
Attention: Establishing that the system is attending.
Action: Defining what is to be done with the system .
Alignment: Monitoring system response .
Accident: Avoiding or recovering from errors or misunderstandings.
These issues may be posed as five questions that a system user must be able to answer to accomplish some action.
Table 1 shows how each question has a familiar GUI answer.
Further, each one poses some challenges that are easily solved by sticking to the existing GUI paradigm and its simpler hand-held counterparts.
However, for novel sensing systems, the challenges take center-stage as design issues again and we list some of them here, together with some potential problems caused by not addressing them.
In this section, we review some of the ways each of our five questions is mundanely addressed by conventions in familiar GUI applications.
We then consider alternative sensing approaches to interaction drawn from a number of recent research prototypes that expose the related challenges and either succeed or fail in addressing them.
In GUI applications, the "system" is a very clear concept; it's the box sitting on your desk.
Designers know that if the user intends to interact with the system, he or she will use the devices, such as a keyboard or mouse, attached to it.
There is little possibility for error, barring cables falling out of sockets, or users accidentally touching input devices.
Familiar GUI Answers Keyboard Mouse  Social control over physical access Graphical feedback  Assume user is looking at monitor Click on objects or drag cursor over area around object.
Select objects from menu .
Select actions from menu, accelerator keys, etc.
Exposed Challenges How to disambiguate signal-to-noise How to disambiguate intended target system How to not address the system How to embody appropriate feedback, so that the user can be aware of the system's attention How to direct feedback to zone of user attention How to identify and select a possible object for action.
How to identify and select an action, and bind it to the object How to avoid unwanted selection.
How to handle complex operations .
How to make system state perceivable and persistent or query-able How to direct timely and appropriate feedback How to provide distinctive feedback on results and state 
Such assumptions, however, are invalid when more "ambient" modes of input, such as gesture, are used, as well as when the notion of what precisely constitutes "the system" is a more amorphous concept.
In such settings, the following challenges arise: How to disambiguate signal-to-noise.
How to disambiguate intended target system.
How to not address the system.
Augmented Objects  tackle this challenge by the intuitive use of proximity of Augmented Objects to sensors; objects augmented with RFID  tags or IR  emitters can be waved at pickup sensors to initiate action.
Listen Reader  is an interactive children's storybook with an evocative soundtrack that the reader "plays" by sweeping hands over the pages.
Embedded RFID tags sense what page is open, and capacitive field sensors measure human proximity to the pages.
Proximity measurements control volume and other parameters for each page's sounds.
Listen Reader, unlike Augmented Objects, allows users to address the system without using RFID tagged objects or IR emitters.
Digital Voices  is computer-to-computer interaction  mechanism that uses audible sound as the communication medium.
A user can address a suitably equipped system using another Digital Voices-enabled device, as long as the devices can `hear' one another.
Moreover, the user hears the communication as it occurs.
One problem for sensing input approaches such as these is a risk of failure to communicate with the system if the sensing fails for any reason.
The converse problem is avoiding unintended communications with devices that the user does not want to interact with.
Simply getting too close can lead to accidental address, and so targets must be well spaced, and use limited sensing ranges or durational thresholds.
However, auditory feedback from Digital Voices informs the user which devices are responding and helps them to decide whether the response is appropriate.
Accidentally addressing a system could be more than annoying, it could be a serious hazard .
Potential danger arises when people talk or gesture normally and a system becomes activated unintentionally.
For example, a voice activated car phone triggered accidentally could compete for a driver's attention with serious consequences.
With sensing systems users may well be looking elsewhere than at a display.
The design challenges here are: How to embody appropriate feedback so that the user can be aware of the system's attention.
How to direct feedback to zone of user attention.
There are inherent problems with sensing UIs.
Unobtrusively attached tags and sensors, make it hard for users to distinguish objects that the system is attending to from ones that the system is ignoring .
Without visible affordances users can unintentionally interact or fail to interact.
Further, there could be privacy or security implications from unintended actions such as information being output simply because a user displaces an object and causes a system to become activated.
Lack of feedback about system attention is common in many proposed and experimental systems .
Conference Assistant  is a system that uses sensing technology to identify a user  and supply information about the context to that user .
The system also collects information about the user, including location, session arrival and departure times, and supplies this information to other conference attendees.
In this environment, the system is always attending whenever the user is within range.
This raises the serious issue of how to keep users aware of what their peers are learning about them.
In this design, there is no feedback to users to remind them that their actions are being monitored and recorded; in other words, the system does not provide feedback that it is accepting input from the user.
Questions of user privacy have always followed new technologies and will continue to be a tough challenge .
In contrast to Conference Assistant, EuroPARC's audiovideo media space  used monitors placed next to cameras in public places to tell inhabitants they were oncamera.
In this case, if people saw themselves on the monitor, they could tell that the system was, in some sense, attending to them.
Our second question is related to, but distinct from, the first.
The first question focuses only on addressing the system.
In addition to this, users must determine whether and when the system is attending to them.
Somehow the system must provide cues about attention, analogous to an audience sending signals of their attention  to a human speaker .
Even once the user knows how to address the system, and is aware that it is, or is not, attending, more questions remain.
The next is about how to effect action: How the user can establish what action she wishes the system to perform, how to control its extent  as well as how to specify  targets of that action?
In Conversation Analysis, researchers have addressed somewhat similar issues in relation to establishing and maintaining topic .
Graphical items, such as menus, icons, images, text and so on, indicate, in Norman's Theory of Action terms, what the system is capable of .
The problem of learning and memorizing how to express a meaningful command to a system  is translated into one of choosing from options.
Users can explore the UI without changing anything; opening windows, pulling down menus, dragging the scrollbar to inspect contents, and so forth, to get a feel for the range of functionality offered by the application and the objects  that can be acted on.
In Microsoft OutlookTM, for example, a set of menus and toolbars provide access to the functions of the application.
These actions can be bound to mail messages and folders, each of which is represented by an item in a list or, alternatively, by an open window.
When a message is selected from a list, the user can ascertain which operations are available and which are disallowed for that particular object .
In the window view, the set of operations that are allowable for the particular object are grouped together in that window.
In most cases, users perform an action on an object by first selecting the object and then selecting which action to apply to it.
The patterns exemplified by Outlook are GUI genre conventions common to many graphical applications.
The Listen Reader, like the GUI, uses "matrix" input; that is, it combines two kinds of input streams: four proximity sensors combined with an RFID reader.
Unique RFID tags are buried within each page, so that the natural action of turning the page triggers the new set of sounds that will be elicited by gestures.
The reader doesn't have to think about selecting new sounds; it's automatic.
In this case, the design is again constrained so that there are no "unwanted selection" or "action binding" issues and the set of possible actions is very small: The range of possible control over the sounds on each page is limited to relative volume, and perhaps pitch shift, but there are no "wrong" responses.
This design is aimed at naive users who will encounter the Listen Reader only once or twice .
As long as these objects are distinctive and suggestive of their action , the range of possible actions may be known.
Thus Tangible UIs in general  attempt to use physical traits of an object to communicate its virtual affordances.
For example, the physical shape of an object may suggest certain uses for it, certain ways it should be held, and so on.
Thus, sensing UIs such as these actually handle the challenges of binding actions to targets, and supporting selection of actions and targets, rather elegantly.
By embedding only a limited range of functionality into a set of suggestive physical objects, they provide a natural mechanism for users to bind actions to targets: They simply pick up or gesture at the object of interest.
Our question about action here exposes the inherent challenges associated with binding more than limited system actions to physical objects.
At the very heart of the vision for Ubicomp, the notion that "computers  vanish into the background" , lies a serious problem for interaction, which is communicating to the user which objects the potential for possible action is embedded in.
Sensor Chair,  is another gesture-based sound control system.
The Sensor Chair was designed for the MIT Media Lab's `Brain Opera.'
Unlike Listen Reader, which is constrained for naive, one-time users, the Sensor Chair is a musical interface with many layers of complexity and control.
It does allow "wrong" responses, typically, an inability to discover effective gestures  or a miscalculation of spatial requirements.
Systems for expert users, like the Sensor Chair, are difficult to use, require training and often rely on multimodal feedback, such as a variable light indicating strength of signal.
Of course, they also support much more complex tasks such as a rich and skillful musical performance.
Sensetable  is a newer Augmented Objects system that, unlike earlier prototypes, is able to support the dynamic binding and unbinding of actions to objects.
Sensetable uses augmented `pucks' that are sensed by a tablet surface.
With sensing systems the major challenges are as follows: How to identify and select a possible object for action.
How to identify and select an action, and bind it to the object How to avoid unwanted selection.
How to handle complex operations .
The first three challenges become apparent as soon as designers attempt to create "invisible interfaces," in which the UI "disappears" into the environment .
In such settings the user is not looking at a computer screen, thus genre and conventions cannot be communicated .
How, then, do sensing systems overcome these challenges?
This provides a simple, "unidimensional" input mechanism whereby each object only causes a single action to occur when placed near a particular sensor.
The space of possible actions is limited to the "actor" objects present in the environment.
The puck may represent something like a molecule and turning the dial represents the action of changing its charge.
This is a compelling GUI-Phicon hybrid solution to the challenges related to establishing an action and an object to apply the action to.
However, it still leaves open the question of how to apply actions to multiple objects simultaneously.
For sensing systems in general a persistent challenge is that abstract operations such as `copy' or `find' are likely to be awkward or severely restricted without some means to specify an argument .
It may be that such systems simply do not lend themselves to operations that may be best suited to keyboard input.
Or it may be that researchers have yet to establish new non-GUI ways to do these things.
Augmented Objects, gestural UIs and sonified I/O do not presuppose any mechanism to display state information in a manner that is consistent with the mode of input.
With respect to the first and third challenges, if a state change is a part of the function of a system, then these issues must somehow be explicitly addressed.
We might propose ongoing projection of graphical or audio information into the space of action.
Sensetable takes the former approach, displaying both distinctive and persistent information, however this is presently done at the cost of restricting users to working within the projected area.
Ideally, Augmented Objects themselves should be capable of displaying the states they have acquired through action.
With respect to the second challenge, Digital Voices has been designed to address the low-end of speed of digital communications, that is, interactions that occur at hundreds of bits per second and that usually take a few seconds to occur.
Therefore the timeframe of the machines' interaction is the same as the people's timeframe, and the user can perceive the interaction in real-time as it happens.
They can also do so without having to watch the system, for example, they might be attending to other matters, thus the audio channel can be an appropriate alternative to visual displays.
Likewise, Sensor Chair, in addition to playing sounds in response to user proximity to its sensors, gives additional visual cues, in the form of variable intensity lights.
Experts can use this timely feedback to further ensure that the system distinctly senses actions around each of its sensors.
As another example of alignment, the Speakeasy framework for Ubicomp  provides facilities to query and display the state of devices, such as projectors, PCs and printers, and also services in an environment .
Users can discover what these entities are doing, if they've failed, if they're available, and so on.
Sociologists pay a great deal of attention to the mechanisms that support coordination or alignment of speaker and listener as a conversation progresses .
Back-channeling is a term used by linguists to refer to feedback a listener gives as to her ongoing understanding, which is monitored by the speaker.
Similarly, systems users must be able to monitor system understanding of their input; in other words to bridge Norman's `Gulf of Evaluation.'
Graphical interfaces display current state, action and results, through feedback mechanisms such as echoing input text and formatting, rubber-banding, wire-frame outlines, progress bars, highlighting changes in a document, listing sent messages and so on.
In the rare instances where the system takes the initiative , the user sees the results in real time as they work .
The mechanisms above overcome the following challenges: How to make system state perceivable and persistent or query-able.
How to direct timely and appropriate feedback.
How to provide distinctive feedback on results and state .
Our first challenge is one of how the user may determine current state.
However, by definition, Ubicomp is everywhere, embedded in mundane objects.
So the goal of making state perceivable and persistent or query-able seems daunting without something very like a GUI.
With Augmented Objects, gestural UIs, `sonified' inputoutput  systems like Digital Voices, and other novel sensing systems, the risk is that users will not be able to tell whether the system understands or not what the user is trying to do.
Without a GUI equivalent, such as the one provided by Sensetable, how does the user know how the system is responding to their gesture?
Our final question deals with not only preventing mistakes in the first place, but also informing users about mistakes that have already occurred so they can correct them.
Conversation analysts  have dealt extensively with breakdowns and repair in HHI, observing that misunderstandings are much more commonplace than one might expect.
Likewise, error is an important and to-beexpected part of normal HCI.
Thus, as Norman  states, "interaction should be treated as a cooperative endeavor between person and machine, one in which misunderstandings can arise on either side."
In Word, certain errors, such as poor speling, can be highlighted or corrected automatically.
Many of the actions of the system are also visibly displayed and easily correctable, if the user notices them.
These feedback mechanisms occur after the action is completed.
Other tasks, such as a long print job or a software compilation, may be long-lived, taking several minutes or even hours to finish.
Tools designed to support such work often provide feedback during action to allow users to monitor  the task.
For example, an animated printer icon on the desktop may show that the printer is working, or has stopped working, and provides controls to allow the user to stop a print job.
Some actions, however, are rapid, do not lend themselves well to "preview" feedback, or to easy cancellation, and are inherently undoable.
In Outlook it is not possible to retract a message that has been mis-sent .
In Word, if the user accidentally saves a document over another document, the application cannot correct the mistake.
Experience with such problems means that designers are advised to make risky operations more difficult  or to present alert boxes before action to protect users from unrecoverable mistakes; however, alert boxes can be irritating and must be used sparingly.
As things stand in sensing systems, our accident-avoidance challenges, though serious, are largely unaddressed.
We believe this is because the field of sensing system research is in its infancy and the existing prototypes have so far been restricted to areas where erroneous behavior has limited consequences.
Future, more ambitious systems will most likely need to provide a wide range of mechanisms for dealing with the common problem of error.
We believe that the issues we have raised provide the beginnings for a systematic approach to the design of interactive systems without tried and tested precedents.
In particular, we have addressed our arguments to a novel class of systems that obtain user-input through sensing user action, rather than through standard input devices such as the keyboard, mouse or stylus.
By considering each of our questions and ensuring they have dealt with the corresponding challenges, designers should be able to avoid a number of potential hazards or pitfalls.
As just one example, automobiles are gradually acquiring a growing number of on-board systems such as hands-free phones, navigation and security systems, etc.
Looking to the future, we might anticipate a number of problems as the voice channel in the car becomes increasingly overloaded and displays proliferate.
Our framework is a starting point for those wishing to find innovative solutions without making hazardous design mistakes .
How to control or cancel system action in progress How to disambiguate what to undo in time How to intervene when user makes obvious error In order to correct mistakes, they have to be visible in time to take action before it is too late; perhaps during or immediately after a system response, and sometimes even before.
Feedback needs to be instantaneous, but without a GUI, ambiguity is a serious problem.
Both the action and its object need to be represented in a manner such that they can both be identified and specified as targets for undo.
There has been little discussion in the Ubicomp literature so far concerning failure modes and errors.
For example, the designers of Augmented Objects  and Sensetable  do not even mention the possibility of error!
In Listen Reader, a heavily constrained museum piece, error is designed out of the system; the user cannot do anything "wrong."
This is one possible route to go with sensing systems, but it works only in simple interaction situations .
More complex systems must allow for error as a trade-off against greater achievement.
Digital Voices applications, are appealing in that constant feedback is provided, which should allow the user to cancel an error in progress .
However, it is not clear how users could differentiate system communications that contain erroneous content from correct ones.
This paper also represents an invitation to social scientists, in particular, interaction and conversation analysts, to develop and improve on our analysis and to apply their understandings of human-human interaction to help designers develop systems that can communicate more naturally and effectively with people.
We are working in a time when systems are rapidly taking on many new forms and functions, faster even than people can find uses for them.
With so much design innovation ongoing, there is a wide range of opportunities for social scientists to team up with innovators in academic and commercial research settings to define and refine new mechanisms that will become the conventions of future interaction genres.
Our aim here is to open a new discussion on innovative design research for human-machine communication and we look forward to further efforts in this area.
