There is increasing evidence that users' characteristics such as cognitive abilities and personality have an impact on the effectiveness of information visualization techniques.
This paper investigates the relationship between such characteristics and fine-grained user attention patterns.
In particular, we present results from an eye tracking user study involving bar graphs and radar graphs, showing that a user's cognitive abilities such as perceptual speed and verbal working memory have a significant impact on gaze behavior, both in general and in relation to task difficulty and visualization type.
These results are discussed in view of our long-term goal of designing information visualisation systems that can dynamically adapt to individual user characteristics.
Similarly, Conati & Maclaren  and Toker et al.
These studies indicate that it is important to investigate the possibility of user-adaptive information visualization systems, namely, Infovis that can dynamically adapt to individual differences.
User-adaptive interaction has been shown to be effective in a variety of applications such as web search, desktop assistance, and e-learning , but it is largely unexplored in information visualization.
Notable exceptions are , which monitor a user's interaction data to detect and adapt to suboptimal usage patterns.
In contrast, our research goal is to investigate how to detect and adapt to longer-term user cognitive abilities, which have been shown to be relevant for effective information visualization processing.
While Conati & Maclaren  investigated the impact of these cognitive abilities on overall user performance with different visualizations, the research presented in this paper aims to gain a more fine-grained understanding of the impact that these cognitive abilities have on visualization processing.
One of the most informative  sources of real-time information on visualization processing is a user's gaze data, because visual scanning and elaboration are fundamental components of working with a visualization .
Therefore, in this paper we aim to determine if and how features in user gaze behavior are impacted by different user characteristics.
Information visualization  aims to assist users in exploring, managing, and understanding the evergrowing amount of digital information.
While visualizations have gained increasingly in terms of general usage and usability, they have traditionally followed a onesize-fits-all model, typically ignoring user differences.
However, recent research has shown that individual differences can indeed have a significant impact on task effectiveness and user satisfaction during Infovis usage.
For example, personality traits have been found to impact a user's performance with different Infovis designs .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Moreover, these answers can help drive the design of user-adaptive visualizations.
In particular, by finding that user characteristics influence a user's gaze behavior in a way that is detectable via eye tracking, we can consider exploring eye tracking as a source of real-time information to provide adaptive interventions targeting these characteristics.
For instance, if an Infovis system could detect from gaze data that the current user has low perceptual speed , it could then generate interventions to facilitate visualization processing, e.g., through highlighting or by explanatory material.
In exploring these research questions, this paper makes the following contributions.
First, we present a statistical analysis using Mixed-Models  to investigate how a user's gaze behavior relates to user characteristics, task difficulty, and visualization type.
We argue that a Mixed Model analysis is the most suitable statistical model to leverage at best the generally noisy eye tracking data.
Secondly, we present a novel definition of task difficulty, derived through applying Principal Component Analysis to a selection of both objective and subjective performance measures.
Thirdly, the results from the analysis show that user characteristics indeed have a significant influence on user gaze behavior, and that this influence is detectable through a variety of eye tracking metrics.
These results were extended in a recent study  showing that perceptual speed and visual/verbal working memory influence not only task performance, but also a user's subjective preference for two different visualizations.
While each of these studies clearly indicate that user differences should be considered in Infovis, they do not explain why or how these differences impact visualization processing, nor do they examine how this impact could be detected in real time.
In this paper, we address these issues by providing a detailed analysis of how a set of user characteristics  influences a variety of eye gaze features during visualization processing.
To the best of our knowledge, there is no established comprehensive theory connecting eye gaze patterns and individual user traits that could guide our investigation of gaze patterns during visualization processing.
Previous work has empirically identified relationships between eye gaze and individual user differences in attention-related tasks  , which is not directly relevant to our focus.
Our research adds to this body of empirical work by providing detailed evidence of how individual differences affect a user's gaze patterns during Infovis usage.
The closest to our research is work by Tai et al.
The scope of our work is broader, since we investigate a comprehensive array of user characteristics including cognitive abilities and visualization expertise, .
These characteristics are domainindependent, thus our results are more general across different Infovis tasks.
Furthermore, we perform a detailed analysis of gaze data in order to link different characteristics to standard Infovis components .
One approach to analyze eye tracking data is to apply data mining techniques, such as Hidden Markov Models , Scan-Path clustering , or specifically defined unsupervised algorithms .
While data mining methods can quickly identify clusters of similar attention patterns during visualization tasks, the results they return are often difficult to interpret, since unsupervised algorithms are typically applied as black-boxes.
By contrast, although traditional human-guided statistical analyses can be more time-consuming, its findings tend to be more transparent and easier to interpret.
Our paper presents such a human-guided analysis of how user gaze behavior relates to user, task, and visualization characteristics.
In particular, the paper provides finegrained insights on how a set of user characteristics interact with different visualization types, components, and task difficulty to impact gaze patterns.
The use of eye tracking has long been established in Psychology as a means for analyzing user attention patterns in information processing tasks .
Research in this field has also investigated the impact of individual user differences on reading and search tasks .
Researchers in human-computer interaction and information visualization have also started to use eye tracking technology to investigate trends and differences in user attention patterns and cognitive/decision processing.
This research has typically focused on either identifying differences in gaze patterns for different visualizations , task types , and activities within a task , or on explaining differences in user accuracy between alternative visualization interfaces .
While these studies provide valuable insights on how different tasks and/or activities affect a user's gaze behaviors, they have traditionally ignored individual differences among study participants.
Recent research, however, has shown that user differences can have a significant impact on a user's performance during Infovis tasks.
For example, Ziemkiewicz et al.
In this section, we describe the study that we conducted to investigate the relationship among user characteristics, task difficulty and gaze patterns while using different visualizations.
As case studies, we considered two basic visualization techniques: bar graphs  and radar graphs .
Bar graphs were chosen because they are one of the most popular and effective visualization techniques.
We chose radar graphs because, although they are often considered inferior to bar graphs on common information seeking tasks , they are widely used for multivariate data.
Furthermore, there are indications that radar graphs may be just as effective as bar graphs for more complex tasks .
The tasks were based on a set of low-level analysis tasks that Amar et al.
The tasks were chosen so that each of our two target visualizations would be suitable to support them.
A first battery of tasks involved 10 questions comparing the performance of one student with the class average for eight courses .
A second battery of tasks involved 4 questions comparing the performances of two different students with respect to the class average, e.g., "Find the courses in which Andrea is below the class average and Diana is above it".
The user characteristics that we investigate in this study consist of two measures of prior visualization expertise, one for each of the two visualizations, as well as three cognitive abilities: perceptual speed , verbal working memory , and visual working memory .
Visualization expertise was chosen because we hypothesized that users with different levels of expertise might exhibit different gaze behaviors.
For our study, participants self-reported their expertise by expressing their agreement with the statement "I am an expert in using radar graphs," on a Likert-scale from 1 to 5.
Perceptual speed and visual WM were selected because they were among the perceptual abilities explored by Velez et al.
We chose verbal WM because we hypothesized that it may affect performance in processing textual components of a visualization .
Thirty-five subjects , ranging in age from 19 to 35, participated in the experiment.
Participants were recruited via advertising at our university, with the aim of collecting a heterogeneous pool of participants with suitable variability in the target characteristics.
Ten participants were CS students, while the rest came from a variety of backgrounds, including microbiology, economics, classical archaeology, and film production.
The experiment was a within-subjects study, designed and pilot-tested to fit in a single session lasting at most one hour.
Participants began by completing tests for the three cognitive measures: a computer-based OSPAN test for verbal WM  , a computer-based test for visual WM  , and a paper-based P-3 test for perceptual speed  .
The experiment was conducted on a Pentium 4, 3.2GHz, with 2GB of RAM and a Tobii T120 eye tracker as the main display.
Tobii T120 is a remote eye tracker embedded in a 17" display, providing unobtrusive eye tracking.
After undergoing a calibration phase for the eye tracker, each participant performed the 14 tasks described in the previous section twice, once with each of the two target visualizations.
The presentation order with respect to visualization type was fully counterbalanced across subjects.
Each task consisted of presenting the participant with a radar/bar graph displaying the relevant data, along with a textual question.
Participants would then select their answer from a set of available options, and click OK to advance to the next task.
Before seeing the next task, participants were shown a screen asking them to rate their confidence in their answer on a Likert scale from 1 to 5.
The experimental software was fully automated and coded in Python.
The independent measures for our study consisted of the collected cognitive abilities and expertise measures , visualization type , and task difficulty .
Table 1 presents summary statistics on the user characteristics data collected from the study.
Kaiser's sampling adequacy was 0.55 and all variables showed a communality > 0.52 which was above the acceptable limit of 0.51 .
The component we generated had an eigenvalue over Kaiser's criterion of 1 and explained 62.22% of the variance.
In sum, we use the output component generated by this PCA  as the measure of task difficulty that we will investigate in our analysis.
Defining tasks as being easy or difficult a priori is challenging, since difficulty depends upon user expertise and perceptual abilities, which were varied on purpose in our study.
We therefore defined task difficulty a posteriori, based on four different measures  aggregated using a principal component analysis .
Because there was a ceiling effect on task correctness, our first objective measure of task difficulty is task completion time .
However, longer completion times may also simply be an indication of a task being longer while not necessarily being more difficult.
Therefore our second objective measure of difficulty is the standard deviation of completion time for each task, across all users.
A high value of this metric indicates a high variability among users' completion times, an indicator that the task may be difficult or confusing for some users.
Our two chosen subjective measures of task difficulty are based on the users' reported confidence of their performance, which was elicited after each task.
The first subjective measure is the average confidence reported by users on each task.
Intuitively, less difficult tasks would have higher values for this average.
However, we also want to take into account that some users may tend to be more confident overall than other users.
Therefore, our second subjective measure is the average deviation of confidence for each task across all users and is computed as follows.
For each user, we look at their average confidence across their tasks.
Then, for each task, we compute the deviation of confidence as the difference between the user's reported confidence for that task and the user's average confidence across tasks.
Finally, for each task, we average the deviation of confidence across all users.
This average indicates for which tasks users were giving confidence ratings that were above or below their typical input.
In order to combine the four variables above, we performed a Principal Component Analysis .
PCA is a form of dimension reduction that allows one to identify and combine groups of inter-related variables into components more suitable for data analysis .
A PCA on our four measures of task difficulty resulted in one output component.
An eye tracker captures gaze information in terms of fixations  and saccades , which can then be analyzed to derive a viewer's attention patterns.
In this paper, we use a large set of basic eye tracking features described by Goldberg and Helfman  as the building blocks for comprehensive gaze processing.
These features are built by calculating a variety of statistics upon the basic eye tracking measures that are described in Table 2.
Measure Fixation rate Number of Fixations Fixation Duration Saccade Length Relative Saccade Angles Absolute Saccade Angles Description Rate of eye fixations per milliseconds Number of eye fixations detected during an interval of interest Time duration of an individual fixation Distance between the two fixations delimiting the saccade  The angle between the two consecutive saccades  The angle between a saccade and the horizontal 
Among the measures described in Table 2, fixation rate, number of fixations, and fixation duration are widely used.
In addition, we included saccade length, relative saccade angle, and absolute saccade angle, as suggested by Goldberg and Helfman , because these measures are useful to summarize trends in user attention patterns within a specific interaction window .
The gaze features for our analysis are obtained by computing statistics such as sum, average, and standard deviation over the measures shown in Table 2, at two levels of granularity.
At the Task Level, features are computed over each task as a whole .
At the AOI level, All subsequent PCA results reported meet the required criteria, and for simplicity we only report the value of Bartlett's test.
To limit our analysis to a reasonable number of features, at the AOI level, we opted to calculate only proportionate features and did not include features related to path angles .
In total, we included 49 different features in our analysis , computed by processing raw data from the Tobii using customized Python scripts2.
A total of five AOIs were defined for each of the two visualizations.
These regions were selected in order to capture the distinctive and typical components of these two information visualizations.
Figure 3 and 4 show how these AOIs map onto bar graph and radar graph visualizations.
This area is the graphical portion of an Infovis that contains the relevant data values.
On the bar graph, it corresponds to a rectangle over the top half of the vertical bars ; for the radar graph, it corresponds to the combined area of the 8 trapezoidal regions covering the data points .
The selection of these five AOIs is the result of a trade-off between having detailed information on user attention by measuring very specific areas that are salient for task execution, versus keeping the number of AOIs manageable for data interpretation and analysis.
To account for correlations among measures, we used three PCAs on our initial set of 49 gaze features.
We grouped the gaze features into three non-overlapping families according to how the measures were intuitively related, namely  task-level features ;  AOI proportionate features  and  AOI transitions .
One PCA was performed on each of these three families, which allows us to discuss results in terms of high-level related gaze components rather than many low-level features.
The task level family consisted of 14 gaze features.
Table 5 shows the breakdown of the original 14 features into the five components.
Component Name Sum Measures Fixation Measures Path Distance Std.Dev.
GLM, however, is less suitable than a Mixed Model for eye tracking analysis, because it is less resilient to missing data.
This issue is due to the fact that GLM requires data to be in wide format, where all repeated measures  for each participant are listed in one data entry row.
When there is an invalid trial, GLM is forced to discard the entire data for that participant.
This can be costly in an experiment with several invalid trials, as is often the case when using unobtrusive eye trackers that do not constrain subjects' movements.
By contrast, a Mixed Model uses data in long format, listing each trial as a different data entry, and discarded invalid trials do not interfere with valid ones.
Thus, a Mixed Model analysis is able to leverage at best potentially noisy eye tracking data.
For each of our three families of gaze features  we ran a mixed model over each of the generated PCA components within that family4.
Each mixed model was a 2  by 2  model, with the user characteristics and task difficulty as the model's covariates.
We report statistical significance at the 0.05 level.
In the next section, we report the most salient results of the analysis.
When going over the results involving directionality, the reader should keep in mind that our dependent measures are PCA components, each consisting of a single value that represents a much larger collection of underlying measures.
Each component is generated by  calculating the weighted values of its underlying members;  aggregating and scaling these values into one number typically ranging from -1 to +1.
If an underlying member is positively correlated to its corresponding component the directionality will be the same, otherwise it will be opposite.
The AOI proportionate family consisted of 10 gaze features.
Note that PCA proved to be especially useful for reducing the many AOI transition features to a small set of meaningful components, each including features mostly related to a specific AOI.
AOI Proportionate Measures low prop.
In this section, we present results that provide answers to our original research questions: do individual user characteristics influence a user's eye gaze behavior in a way that is detectable by state of the art eye trackers?
If so, which gaze features are influenced by which particular user characteristics, and is the effect modulated by task and visualization type?
The analysis results are discussed per user characteristic.
Since the study data involved repeated measures , a suitable means for analysis is a Mixed Model .
Mixed models can handle both repeated measures as well as the mix of categorical and continuous independent measures that we consider.
An analysis of the underlying members of this component shows that users with high perceptual speed had a higher fixation-rate than low perceptual speed users, indicating that they were able to scan the screen more quickly.
They also had lower average and standard deviation of fixation durations, i.e.
These combined findings closely match the definition of perceptual speed, and are interesting because they show that individual differences for this cognitive ability may be captured via eye tracking measures that are not related to information on specific elements of the visualization.
The other two main effects of perceptual speed are at the AOI level , showing that this cognitive ability also affects eye gaze measures relating to specific visualizations elements.
The main effects are on the two components Legend Proportion and Legend Transitions: low perceptual speed users spent more of their time in the legend AOI and transitioned to it more often than high perceptual speed users.
This result indicates that users with low perceptual speed took more time to process/store legend-related information and looked at the legend more frequently .
There are significant interactions of task difficulty and perceptual speed on both the Legend Transitions and Label Transitions components .
For Legend Transitions, all users generate more legend-related transitions with difficult tasks than with easy tasks , likely due to the fact that an increased difficulty increases cognitive load and causes users to forget some of the information in the legend.
This effect, however, is higher for low perceptual speed users.
There are two main effects of verbal WM: one on the Text Proportion component and one on Standard Deviation of Path Angles .
Text Proportion relates to the most textual element in our visualizations, namely the question text.
An analysis of the members of this component shows that the proportionate amount of time spent on the Text AOI and the number of fixations in this area are lower for users with high verbal WM.
This effect indicates that high verbal WM users refer to the task question less often than their low verbal WM counterparts, which is consistent with the definition of verbal WM as a measure of storage and manipulation capacity of verbal information.
This result is interesting because it shows that differences in users' verbal WM can be directly captured by eye tracking features related to the primary textual elements of a visualization.
The power for the effect of Bar Graph expertise on the AOI Label is 0.67, and the power for the effect of Radar Graph expertise on AOI Legend is 0.64.
A commonly recommended value of power is 0.8 , and we would have to add 17%  more users to reach this value.
It may seem surprising that we did not find stronger influences of visualization expertise on gaze patterns.
This result, however, is consistent with findings in , which showed that bar and radar graph expertise may only have significant effects on user visualization preference, but not on performance.
These findings suggest that there might not be easily detectable differences in the visualization processing behaviors of experts and novices, as defined by our self-rated measures of expertise.
Visual WM We found no effects worth reporting for visual WM.
This lack of findings may be due to the fact that the study tasks were relatively easy and that the visualizations were static in nature.
It is thus likely that users did not require to reach their maximum visual memory capacity, especially since they could easily get an overview of the whole graph in a single look.
Moreover, individual tasks were independent of each other, thus users were not required to store any successive visual information .
Path Angles: this component essentially captures the consistency of a user's gaze patterns during a visualization task, because it is built upon features related to measuring the deviation of angles between subsequent saccades.
Users with low verbal WM had higher values for Std.
Path Angles than users with high verbal WM.
When these values are higher, it indicates that a user is frequently looking across different areas of the screen, rather than following more planned or consistent path directions.
Therefore, the finding that users with low verbal WM had higher values for Std.
Path Angles is consistent with the finding that low verbal WM users referred back to the question text more often.
In this paper, we chose to focus on the five characteristics listed in Table 11 and, as shown in this table, we found a number of effects  on various gaze measures.
Perceptual speed is the cognitive measure with the highest number of effects.
This finding provides encouraging evidence that this cognitive ability could be reliably detected in real time using gaze information.
This result is particularly important for our long-term goal of designing user-adaptive visualizations, especially in light of previous studies, which showed that low perceptual speed can negatively affect task performance, in terms of both accuracy  as well as task completion time .
We have shown that perceptual speed influences AOI-specific gaze measures relating to the legend, labels and High AOI.
These findings suggest that adaptive interventions could be particularly useful if they support the access and/or processing of such AOIs for low perceptual speed users.
In addition, the interaction effects we found for perceptual speed suggest that task difficulty and visualization type should be taken into account, if known, when providing adaptive interventions.
There are two non-significant main effects of both bar graph and radar graph expertise, which we discuss because of their large effect sizes.
There was a main effect of Bar Graph Expertise on the AOI Label proportion component,  = 6.042, r = 0.80, p = 0.1, showing that users with high bar expertise spent a greater proportion of their time looking at labels compared to non-experts.
Similarly, there was a main effect of Radar Graph Expertise on the AOI Legend proportion component  = 5.732, r = 0.78, p = 0.129, with radar experts spending less time looking at the legend when compared to non-experts.
However, we also found that this effect is exacerbated in the presence of difficult tasks.
Thus, while it may not be worthwhile disrupting a low speed user with a legend-related intervention for tasks known to be easy, it may be important to do so as task difficulty increases.
User Characteristic Perceptual Speed Verbal WM Bar Expertise Radar Expertise Visual WM Eye tracking measure component Fixation Measures  Legend Proportion  Legend Transitions  Label Transitions  High AOI Transitions  Std.
In fact, the majority of our results are effects that are actually independent of visualization design.
Similarly, while the study has focused on an artificial data set involving student grades , the actual tasks were derived from an established set of general, low-level analysis tasks for information visualization .
Lastly, while this work has focused on an analysis for the purpose of adaptive information visualization, similar user studies could be performed in other areas of HCI , to determine whether the influence of individual user differences can also be detected in those scenarios.
The results on verbal WM indicate, intuitively, that this cognitive ability affects eye-tracking features related to the main textual element of a visualization, and thus may be detectable in real time by tracking these features.
In terms of adaptation, it is plausible that users with low verbal WM may benefit if textual elements of a visualization were given more emphasis than the purely graphical elements.
However, because we do not have information on whether verbal WM affects performance during Infovis processing, it remains a topic for future research to investigate if and how adaptive interventions would impact visualization effectiveness for users with different levels of verbal WM.
We discussed two non-significant main effects of the expertise-related user characteristics because of their large effect sizes.
Bar expertise had a large effect on label access, while radar expertise had a large effect on legend access.
These results may indicate that non-experts could benefit from adaptive interventions that guide them to access these elements in a way that is more similar to experts.
However, we need to run further studies with more reliable, objective measures of expertise  before we can make a more informed decision on how to provide adaptive support for novice users.
In summary, we have identified a set of user abilities that have a strong impact on gaze measures related to specific AOIs of a visualization, and discussed how adaptive interventions driven by these abilities and targeting such AOIs may improve a user's experience with a given visualization.
We presented research aimed at investigating the relationship between a set of user cognitive and expertise measures, task difficulty, and user attention patterns when using different visualization techniques.
Our analysis reveals that some of the tested user characteristics do have a significant influence on user gaze behavior, and that this influence is detectable through a variety of eye tracking metrics.
Based on these findings, we provided general suggestions for adaptive visualization design in relation to components that are common to most types of visualizations, for example suggesting that low perceptual speed users may need support in processing legends.
Our results may therefore be of interest when designing systems for specific user groups that are known to have high/low cognitive abilities .
We see the analysis presented here as a first step towards understanding the complex relationships between user traits, visualizations, and gaze patterns.
However, additional studies are necessary to investigate these relationships at the level of more basic Infovis properties such as color, size, and shape.
Similarly, studies should be run to investigate these relationships in more complex visualizations such as time series, networks, as well as interactive visualizations.
Along these lines, we are currently applying the experimental design described in this paper to investigate the impact of user traits on different versions of a complex interactive visualizations involving multiple, aligned bar charts  for preference elicitation.
Because of the added complexity, we expect the impact of user characteristics, task difficulty, and visualization type to be even more pronounced than in the current study.
The next step of our research is to show that the relevant user characteristics can be detected in real-time to drive adaptive interventions benefiting users with those characteristics.
We are currently investigating a variety of machine learning techniques to perform this real-time inference task, and we already have encouraging results .
Lastly, we are in the process of running a user study to test different ways of providing adaptive interventions, both in general, and in relation to individual user differences.
Jarodzka, H., Scheiter, K., Gerjets, P., & Van Gog, T. In the eyes of the beholder: How experts and novices interpret dynamic stimuli.
Keith, R. Eye movements and cognitive processes in reading, visual search, and scene perception.
Eye Movement Research Mechanisms, Processes, and Applications.
Kruschke, J. K., Kappenman, E. S., & Hetrick, W. P. Eye gaze and individual differences consistent with learned attention in associative blocking and highlighting.
Plumlee, M.D., & Ware, C. Zooming versus multiple window interfaces: Cognitive costs of visual comparisons.
Rayner, K. Eye movements in reading and information processing: 20 years of research.
Using hidden Markov model to uncover processing states from eye movements in information search tasks.
Steichen, B., Carenini, G., & Conati, C. User-Adaptive Information Visualization - Using eye gaze data to infer visualization tasks and user cognitive abilities.
Tai, R.H., Loehr, J.F., & Brigham, F.J. An Exploration of the Use of Eye-Gaze Tracking to Study ProblemSolving on Standardized Science Assessments.
Tang, H., Topczewski, J.J., Topczewski, A.M. & Pienta, N.J. Permutation test for groups of scanpaths using normalized Levenshtein distances and application in NMR questions.
Toker, D., Conati, C., Carenini, G., & Haraty, M. Towards Adaptive Information Visualization: On the Influence of User Characteristics.
Is working memory capacity task dependent?
Velez, M.C., Silver, D., & Tremaine, M. Understanding visualization through spatial ability differences.
How Locus of Control Influences Compatibility with Visualization Style.
