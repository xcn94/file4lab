Finally, the small screen area limits the size devoted to user interface widgets, resulting in hard-to-hit, small targets.
The fat-finger problem exacerbates the issue .
Marking gestures provide a solution for eyes-free interaction, as they can be learned into the muscle memory .
However, gesture recognition requires a mode switch to distinguish pointing input from simple inking action, where a user input is by default treated as `ink'.
Recently, researchers  have explored using the bezel of the phone for eyes-free interaction.
Bezel refers to the physical touch-insensitive frame surrounding a touchscreen display .
Bezel menus are built on the periphery of the touch screen  without occluding the screen space .
The initiation of a bezel gesture starts from outside of the screen , solving the mode-switching problem.
They also found that mark-based bezel gestures as in marking menus  are faster and more accurate than freeform bezel gestures 
In this research, we build upon this previous work  by exploring the range of possible mark-based bezel gestures, along with evaluating crucial aspects of bezel menus like threshold distance, feasibility of off-axis items and performance of different bezel menu layouts, which has not been studied before.
Based on the obtained results, we developed a text-entry application to study the performance of bezel gestures in a real-world task.
Touchscreen phones tend to require constant visual attention, thus not allowing eyes-free interaction.
For users with visual impairment, or when occupied with another task that requires a user's visual attention, these phones can be difficult to use.
Recently, marks initiating from the bezel, the physical touch-insensitive frame surrounding a touchscreen display, have been proposed as a method for eyes-free interaction.
Due to the physical form factor of the mobile device, it is possible to access different parts of the bezel eyes-free.
In this paper, we first studied the performance of different bezel menu layouts.
Based on the results, we designed a bezel-based text entry application to gain insights into how bezel menus perform in a real-world application.
From a longitudinal study, we found that the participants achieved 9.2 words per minute in situations requiring minimal visual attention to the screen.
After only one hour of practice, the participants transitioned from novice to expert users.
This shows that bezel menus can be adopted for realistic applications.
Since the launch of the iPhone, touchscreen phones have dominated the smartphone market.
Touchscreen phones allow for direct manipulation, but require constant visual attention, thus making eyes-free interaction difficult.
For users with visual impairment, or when occupied with other tasks that require a user's visual attention causing situational impairments  , touchscreen phones can be difficult to use.
Furthermore, because touch input is overloaded in functionalities, it is hard to use without being able to see the interface.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The text entry user study demonstrates that the bezel menus are easy to learn and use in situations not permitting much visual attention to the screen.
Text entry requires more than 26 targets on the screen which is hard to achieve.
Also, users tend to prefer text entry with minimal visual attention .
Using bezel menus, we made the 26 letters accessible by at most needing to access two levels of menu operations in an eyes-free manner.
Further, typing requires the user to perform more input than other interactions .
Thus, being able to access various combinations of letters through the bezel menu system without needing to devote full visual attention towards the interface was used to evaluate the learning and performance of bezel menus.
Through a longitudinal study, we show that after an hour of practice, the user's transition to expert user.
They achieved 9.2 wpm  in situations requiring minimal visual attention to the screen.
We conclude with some design implications for bezel menus, such as accessibility considerations and optimal threshold distance for bezel invocation.
The bezel space has been previously explored by other researchers .
Barrier Pointing  relies on elevated bezels, and is designed for people having motor impairments.
It uses the bezel as an end point with the inside of the display area as a starting point, contrary to the approach adopted by others .
This can result in a mode-switching problem.
Manual Deskterity  uses the bezel menu to create a new object, implying making something out of nothing.
Bezel Swipe  restricts the bezel usage to support multiple selections, cut, copy, paste and other similar operations.
An item is selected by swiping the finger from the bezel to over the item and lifting the finger off the display, but the finger might occlude the item.
The bezel space has also been used for tasks not requiring visual attention , and it was found that users performed better in the eyes-free condition than looking at the phone.
The recently unveiled BlackBerry PlayBook  has a touch-sensitive bezel which can detect many of the bezel interactions described above; however, our approach does not require any hardware changes.
Much of the previous work has explored bezel menus for specific applications contexts, such as creating a new object , multiple target selections .
The general space of bezel-initiated marks has not been studied before.
Bezel-initiated marks are a combination of crossing-based interfaces  and marking menus .
Crossing-based interfaces require crossing a boundary for menu selection.
Marking menus , a variant of radial or pie menus , allow menu selection by either selecting an item from a popup radial menu appearing directly under the cursor or by drawing a mark in the direction of the desired menu item.
Marking menus enable an efficient transition from novice to expert, as every time a novice user makes a mark to select an item from the popup menu; he/she is basically rehearsing the gesture that an expert user would have made without popping up the menu.
However, on a small touchscreen phone, the finger can cause an occlusion, making it difficult to view the popup radial menu.
Bezel-initiated marks  are similar to marking menus but instead of using the display screen for menu invocation, the phone's bezel is used.
To select a menu item, a threshold distance starting from the bezel needs to be swiped, similar to a crossing-based interface.
Both marking menus and crossing-based interfaces use the display area for menu invocation which might result in mode-switching problems.
Invoking from the bezel helps in solving this issue.
Simple marking menus , in which series of separate strokes  are used were shown to be more accurate and faster than hierarchical marking menu.
However, similar to Bragdon et al.
Although many gesture-based text entry systems have been proposed , not many has been evaluated for eyes-free mobile phone usage scenario.
Graffiti  uses free-form unistroke gestures resembling English letters.
Because it leverages a skill that people are often familiar with, imaginably it can be operated eyes-free.
T-Cube  uses simple markbased gestures and supported ~20 wpm, but no eyes-free study has been reported for T-Cube.
EdgeWrite  provides an easier gesture-set for text entry.
It was built for people with motor impairments.
It uses a physical square guide imposed over the usual text input area, and text is entered by traversing the square's edges and diagonals.
It can potentially be useful in eyes-free scenarios; however, no evaluation of its potential in eyes-free usage scenarios has been reported.
Yfantidis and Evreinov's  technique uses very simple directional gestures on a pie-menu of characters.
Three out of 12 users were able to achieved 12 wpm after five trials on a touchscreen monitor; however, users' ability to achieve the same speed on a small phone screen has not been explored.
Most touchscreen phones support softkeypads.
Due to small screen size, the fat-finger problem, high visual demand, and lack of tactile feedback from physical keys, accessing virtual keypads is difficult .
The iPhone offers VoiceOver , which provides audio feedback to guide visually-impaired users to access the phone, including text entry using a soft keypad.
It has been shown  that entering text is easy with VoiceOver, but the text entry speed is very low at 0.66 wpm.
Recently, researchers have proposed techniques  enabling faster eyes-free text entry on touchscreen phones.
No-Look Notes  and Mobile Messenger  divides the phone screen into 8 or more easy-to-reference parts, and allocates 3-4 letters to each part, with Text-to-Speech providing feedback.
Performance of Mobile Messenger has not been reported, while No-Look Notes reports a low text entry speed of 1.32 wpm with minimal practice.
Most of the above text entry methods  were developed for visually-impaired users and, thus rely on speech, auditory, or vibration feedback to input text.
These systems were tested with either visually-impaired or blindfolded participants.
In contrast, our research focuses on situation-impaired users.
Everyone experiences situational vision impairments such as attending a meeting or walking through a busy shopping center, when paying high visual attention to the phone is difficult.
This design paradigm differs in that users can look at the phone once in a while to get visual feedback, unlike visually impaired users.
This design space has been previously explored , but have not examined text entry specifically.
Other text entry researches  have mentioned this design space but none have conducted a longitudinal study exploiting situational impairment.
Eight or fewer menu items can be accommodated in level-1 of the bezel menu design .
To select a level-1 item, the finger entering from the bezel needs to cross a threshold distance .
For more than 8 menu items, a level-2 selection is defined, such that after crossing the threshold distance in level-1, the user needs to swipe towards the correct intended direction, to select a menu item out of the 64  possible items .
On lifting the finger from the screen, the item gets selected.
For example, to select item-5 in level-1 and item-4 in level-2, the required mark is shown in Figure 2c.
Level-2 items can be submenu for level-1 items; e.g., "save" is an example of a level-2 menu item under the "file" in level-1 menu.
Starting from the bezel leads to a temporal mode change, but the gesture ends as soon as the finger is lifted--causing no mode-switching related errors.
To select a menu item again, the same process of swiping from the bezel needs to be repeated.
Using a threshold distance to transition to level-2 removes ambiguity of sequential strokes in the same direction  which has been pointed as one of the limitations of hierarchical marking menus .
To reduce accidental bezel menu invocation, no item is selected until the threshold distance is not crossed.
We divided the bezel into eight regions - 4 corners and 4 sides , and associated different menu items with each region, numbered clockwise from 1 to 8, starting from the top.
The 8 regions are distinguishable from each other, and allows for both portrait and landscape mode usage.
It is also in accordance with prior work on marking menus , which states that optimum performance is achieved with 8 items.
For generality, instead of directions we use numbers which do not have any existing mental mapping with the regions.
This enabled us to study novice behavior.
To enter the selection mode, the user has to start swiping the finger from the bezel inwards to the touchscreen .
Although the finger is detected only when it touches the screen, swiping from the bezel makes sure that the first point of contact is detected on a very narrow edge of the screen, called the activation area .
The activation area is kept as a thin strip of 30 pixels around the periphery.
Only the eight level-1 items were used for this study.
A cardboard arrangement was used to hide the phone screen.
A random number between 1 and 8 was spoken by the phone, as the question.
For a correct response, the participant needed to swipe his/her finger starting from the bezel corresponding to the spoken number and make a mark in the required direction.
No threshold was used; hence any mark of any length initiated from the correct bezel was taken as a correct mark.
A beep sound was played for mark initiation from the wrong bezel.
Participants were instructed to keep trying until a correct response was received.
The 8 items were randomized, forming a set of 8 tasks.
Each participant completed 24 sets of trials .
Thus, the total number of trials = 4 participants x 24 sets x 8 items = 768.
Participants took 20 minutes to complete the study.
Overall, the participants achieved a high accuracy with 97.33% of the marks drawn correctly on the first attempt.
For all further studies, 70 pixels have been used as the threshold distance.
Similar to , we did not provide any audio/vibration feedback, as we were not targeting the visually-impaired users.
Also the user was allowed to look at the phone once in a while to get visual feedback, thus not necessitating the use of vibrotactile feedback.
An ink trail, consistent with the metaphor of marking with a pen, is provided.
The trails remain till the user no longer touches the screen.
Andersen and Zhai  showed that an ink trail actually slows down the average speed in stroke production.
However, we used the trails as it reinforces learning which helps in correcting errors for subsequent use.
For smooth transitioning from novice to expert mode, marking menu based methodology is adopted.
On swiping from the bezel, a long pause  results in a pie menu appearing  to enable selection.
A similar pie menu also appears for level-2.
Over time, the user can get used to making such bezel marks, and smoothly transitions to the expert mode of selecting items eyes-free.
To solve the occlusion problem, the pie menu is displaced visually, such that it is always shown at the center of the screen, which also helps in avoiding the pie menu from getting clipped-off.
Kurtenbach  showed that the source of poor performance at higher breadths and depths is due to the selection of off-axis items.
We conducted a study  with the same 4 participants to check the feasibility of off-axis items of bezelmenu.
Only the off-axis level1 items  were used for this study.
The four level-1 items with four level-2 items form a set of 16 tasks.
A randomly selected  task  appears in the center of the screen and the participants were asked to keep on trying until a correct response was received.
A beep sound was played for incorrect responses.
Each participant took around 40 minutes to complete 16 trial sets  for each of the two conditions.
So total number of trials = 4 participants x 2 conditions x 16 tasks x 16 sets = 2048.
The participants were able to achieve high accuracy of 96.6%, showing that level-1 off-axis marks are feasible.
This result is in accordance with  that markings with no inflections can be ambiguous.
Also, any kind of inflection after crossing the bezel can help in reducing unintentional menu invocation.
As the touch points are not uniformly distributed, the resample method from the $1 recognizer  is used on the obtained touch-points.
Identification of the level-1 item is dependent on the entry point where the finger first touches the screen, and the direction of the mark until the threshold distance is reached.
Assuming the deflection point breaks a single stroke into two lines, we used a `two best fitting straight lines' algorithm  to obtain the angle at the point of deflection for calculating thelevel-2 item.
Similar to marking menus , all marks are treated as scale invariant.
We conducted a study to understand how different menu layouts, in terms of the number of items at each level, impact performance.
We studied accuracy, types of errors, and length of the marks, to gain an insight into the feasible directions for eyes-free interaction.
The study was conducted on an HTC Desire Android phone with a 533 x 320 pixels  capacitive touchscreen.
In L4x8, level-1 consisted of only the four on-axis items  and level-2 comprised of all the 8 items.
L8x4 consists of all the 8 items for level-1, but only the opposite axis items for level-2 , i.e., if the level-1 item is on-axis then the level-2 item will be off-axis and vice-versa.
This decision of taking opposite axis is in accordance with the results obtained from Pilot Study 2.
So both the menu layouts - L4x8 and L8x4 - have 32 items each, which we refer to as a set.
For the same reason, we did not study the L8x8  layout.
Participants were given verbal instructions with demonstrations on how to perform the menu selection.
To make the experiment ecologically realistic, the participants were free to hold the phone in any manner, though they were asked to use it in the landscape mode, using 2-hands.
We believe that complex realistic operations on phones are preferable in landscape mode, using both the hands as it allows easy access to all the parts of the screen.
The participants were asked to perform the task as fast and as accurately as possible.
At the completion of the task, participants were briefly interviewed about their preference and the problems faced.
A random menu choice  appears in the center of the screen.
It was ensured that no two same menu choices appeared consecutively.
A beep sound was played for each wrong gesture, and the participants were asked to keep trying until the correct gesture was received.
A long pause resulted in a pie menu appearing.
The task started with performing a trial session with 2 sets each of L4x8 and L8x4.
Next the participant performed 4 sets each of L4x8 and L8x4 with the phone screen visible , and 4 sets each of L4x8 and L8x4 in the eyes-free mode.
In the eyes-free mode, a cardboard arrangement was used to hide the phone screen, and audio-based question and feedback were provided.
The phone's inbuilt-speaker was used for the audio cues.
For complete within-subjects counterbalancing, an ABBA design was used such that each participant completed 2 sets each of L4x8 and L8x4 in eyes-on mode, followed by 4 sets in eyes-free mode, and then again 2 sets in eyes-on mode.
The order of presentation of the menu layouts were L4x8 followed by L8x4, as starting with the four on-axis bezels, provides a better learning curve.
Each participant took around 1.5 hours to complete the study, with breaks in between sets.
Total number of trials in eyes-free mode = Total number of trials in eyes-on mode = 12 participants x 8 sets x 32 items = 3072.
Three of the participants complained that "there was no direct mental model associated with the numbers and the bezel menu items"; hence it required "thinking before doing, leading to lag and more errors."
For the L8x4 eyes-free mode, the errors were further analyzed and it was found that marks originating from the upper left corner  contributed to the majority of the total errors .
This might be because all participants were right-handed.
The error results were also dependent on the way the phone was held.
Nine participants used both thumbs to perform the task, while the remaining held the phone with their left hand and used the right-hand index finger for interaction.
These three participants were among the four most inaccurate, and contributed to 41.2% of the total errors, much more erroneous than participants using both thumbs.
Marks, in which the correct response was received on the first attempt, were only considered.
This shows that participants were making long marks.
We believe that similar to marking menus, the mark length will reduce as user's transition from novice to expert.
This is not surprising, as similar results were obtained by .
This confirms that bezel-initiated marks are better suited for eyes-free interaction.
The post-test interview revealed the same as all participants preferred the eyes-free mode, with comments such as "It is easy to memorize", "Eyes-on mode is surprisingly tougher than expected, whereas eyes-free mode is surprisingly easier than expected."
Based on the results obtained from the bezel menu study, we designed a text entry application to evaluate the learning and performance of bezel-initiated marks in a realistic application scenario .
Using text entry to study the performance of bezel menu for realistic operations was a conscious decision.
Text entry requires more than 26 targets on the screen which is hard to achieve.
Also users tend to prefer text entry with minimal visual attention .
Using bezel menus, we made the 26 letters accessible by at most needing to access two levels of menu operations in an eyes-free manner.
The textbox in the center of the screen contains the text being typed so far.
To minimize screen usage, the visual hint showing the letters layout and the ink trail are not always visible .
A user can tap on the textbox to view the layout , and after entering a letter with visible ink trail, the layout disappears .
The default state of the screen is shown in Figure 5b.
This provides an added benefit that users would not need to switch between the content screen and keypad screen.
While the layout is visible, a long pause results in the appearance of a pie menu .
The alphabetical layout is visible only when a user taps on the textbox, situated at the center of the screen.
After entering a letter, the layout disappears.
We used the L8x4 layout to design the system as it was found to be more accurate.
A letter is assigned to each item of the L8x4 menu layout .
An alphabetical layout is used to minimize the learning curve.
Initially, we thought of using the T9-design , but due to the limitation of 8 bezels, we designed a modified version of T9.
On every bezel, the layout of the block of letters is alphabetical with letters in the clockwise direction starting from top/top-left .
For letters `a' to `l', we assigned four letters to each bezel such that the first letter of the bezel is a vowel, which would help the user to remember the layout.
For `m' to `z', we used the same layout as T9.
This design allowed us to effectively distribute the functional characters such as `Del' , `Sp' , `Ent' , `ABC' , and `.
We assigned `Sp' and `Del' to different thumbs, as confusing the two might result in many mistakes.
As marks originating from bezel number 8 are highly inaccurate , least-frequently used items were assigned to it.
Note that our goal is not to design the best layout for an eyes-free text entry system, but to gain insight in the performance of a bezel-based system for a realistic task requiring minimal visual attention.
Based on the results of Pilot Study 2, a mark requires an inflection to select a letter.
As the system is designed for righthanded users, most of the high frequency letters are assigned to the right thumb.
Eight right-handed participants  were recruited from the local university campus.
Only two were native English speakers.
Participants selfreported an average 9.8 hours  of daily computer usage, though only five of them perform touch-typing.
Except one, all had previously used a touchscreen phone on a regular basis for 6 months or longer.
None of them reported ability to interact eyes-free with their touchscreen phone.
Each participant was compensated $10 per session.
All the participants participated in eleven 40-minutes sessions spread over a period of 15 days, with each session separated by at least twelve hours and no more than two days.
Replicating the real-world scenario, where a typical user looks at his/her cellphone in burst of 4 sec , we decided to use a visual distracter task to assess situational vision impairment usage.
Similar to , as a visual distracter, participants were required to repeat verbally a periodically-changing random number .
This tries to replicate typical low attention mobile usage.
To avoid confusion, we ensured that no two same numbers appeared consecutively.
The number of times participants missed/misspoke a number was recorded.
The sessions were divided into three distraction-level:  no distraction: no visual distracter was used ,  low distraction: random numbers changing periodically at 5 sec interval , and  high distraction: numbers changing at 2.5 sec interval .
The distraction levels were based on previous research , and were not randomized for the study so that all the participants have similar learning curve.
For the study, the same phone HTC Desire was used, with all the hard buttons being disabled.
The experiment was conducted only with the lower case letters and with no punctuations and numbers, so `ABC' and `.
Each participant was given verbal and visual instructions describing the task and goal of the experiment.
The researcher demonstrated how to hold the phone and enter text - holding the cellphone with both hands in the landscape mode, and using both the thumbs for text entry.
Also we believe that realistic operations such as text entry benefit from two-hand usage for optimum performance, as two-thumb typing using mini-QWERTY has been studied and found to support 60 wpm, much faster than any one-handed technique .
Participants were told that the layout is alphabetical and for each block of letters assigned to a bezel, the letters are in clockwise direction starting from top/top-left.
A mark initiated from an on-axis bezel must end towards an off-axis bezel, and vice-versa, to input a letter .
The positions of the vowels were stressed for a quick start; also position of `Del' and `Space' was emphasized.
Participants were free to tap on the textbox to view the letter's layout.
A within-subject design was used.
For every session, participants were asked to enter as many phrases as possible in 30-minutes duration, similar to .
The instruction was to "enter the text as quickly and accurately as possible, as if typing an email to a colleague."
No error correction was enforced.
At the start of every session, the participants were required to type the practice sentence  twice.
Participants were encouraged to take a short break between phrases, anytime they wished.
They were facing two monitors, one showing the presented phrase and other showing the periodically changing random number.
For sessions with visual distraction, the participants were explicitly told that the primary task was to speak out the appearing number , and the secondary task was to enter text.
Participants were seated on a reclining chair and were asked to adjust the height of the chair to their comfort before starting the task.
The phrases were randomly chosen from a published phrase set for mobile phone text entry by Wobbrock , with no phrase longer than 27 characters in length.
All the participants received the phrases in the same order.
Participant's face including eye movements was video-recorded for all the sessions, totaling 58 hours 12 minutes of video, which was analyzed offline.
Approximately 8 wpm was achieved just after 1 hour of usage, showing a short learning curve and quick transition from novice to expert user.
The effect of introducing distracters is visible in Figure 6a, as there was a significant decrease in the text entry speed during session 5 and session 8, as low distraction and high distraction were introduced in these respective sessions.
Also, if the curve  from session 1-4 is extended to join the averagewpm of session 11, the curve obtained is very similar to the characteristic learnability curve for text entry system .
This shows that learning was not affected much by distracter.
We conducted a Distraction-level x Day RM-ANOVA  on the text entry speed, with the no distraction condition as baseline.
Session 1 and session 11 data were dropped, thus allowing us to analyze three days of data for each distraction-level.
To analyze this further, we conducted a post-hoc analysis with Bonferroni correction.
Among the three distraction conditions, the participants performed fastest at 9.07 wpm  with high distraction.
This can be attributed to the slow development of the muscle memory over time.
Also the reason could be that higher distraction requires higher attention, resulting in high speed.
The recorded videos were analyzed at the frame level by an independent coder, to reduce the possibility of bias.
Every time a participant looked at the phone and the associated duration were noted.
During session 10, the average time that the participants looked at their phone was 3.1 minutes , proving that the experiment successfully captured the minimal visual attention scenario.
A high speed of 9.2 wpm was maintained during session 10.
It shows that users can enter text comfortably while paying minimal attention to the screen.
To generalize, using bezel-initiated marks a user can interact with realistic touchscreen applications, even during minimal visual attention scenario.
Intra-time is the time taken to perform a mark, measured as the time difference between finger-down and finger-up; while Inter-time is the time in between two consecutive marks.
This shows that both, time taken to perform a mark and time taken to think before making a mark, reduce with practice over time.
These values are comparable with , which reports that the mean completion time of bezel marks was 1092.87 ms. Also, marking menu's response time was 1.69 sec , while for crossingbased menu, the average response time was 0.9 sec .
Interestingly, a strong correlation was found between the bezel-based text entry speed and the participants' computer typing speed with Pearson's r=0.79, p<0.05.
The average text entry rates over all the sessions across all the participants were 8.47 wpm .
Compared to Graffiti and Unistroke  where users are required to make free-form gestures, our technique uses basic marking gestures which might have contributed to the speed.
The video analysis confirms that not much visual attention is required to enter text with our bezel-based technique.
However, we recognize that a direct comparison is not fair, as previous work  were tested for complete eyes-free usage.
Even MSD of no distraction  and high distraction  have a main effect of p<0.01, showing that participants were correcting significantly more mistakes in the no distraction mode compared to the high distraction mode.
Interestingly, on a pairwise comparison, MSD for different distractionlevel were significantly different from each other  .
Speed, GPC and MSD were co-related, as for session 8-10, the speed and MSD increased considerably while GPC dropped.
This shows that as participants gain speed, error correction was minimized, highlighting the speed-accuracy trade-off.
For session 10, three participants have high MSD  which could be because of high speed, high distraction level, novice user, and/or non-native speaker .
The average number of misses in speaking out the number aloud was 1.79  in low distraction mode, and 6.6  with high distraction.
In low distraction, the number changes 360 times in a session, and 720 times in high distraction.
Hence the number of misses is not significant.
MSD accounts for the uncorrected errors in the final transcribed text; while GPC measures the corrected errors as every correction adds multiple gestures, i.e., delete character, re-enter character.
The calculation of GPC also includes non-character producing marks, which constitutes only 1.72% of the total marks.
Please note that a low value of GPC, close to 1, is considered as the ideal value.
Out of the total 1.72% of non-character producing marks, 0.15% consists of tap used to view the alphabet-layout, showing that on-screen visible menu items is not necessary as users can easily remember after practice; 0.07% marks missed the bezel activation area, highlighting that users can comfortably access all the 8 bezels; and the remaining 1.5% marks consists of such marks where the threshold distance was not crossed in spite of correct bezel initiation, hinting that the chosen threshold distance works.
This could be because as user speed increases, he/she tends to make smaller marks, and thus missed the threshold distance.
An adaptive threshold distance might work better.
Figure 7 shows the total number of times when a presented letter  was transcribed with an incorrect letter .
The most prevalent mistakes were: transcribing `h' instead of `e', and `j' instead of `i'.
Both of these confusing-letters are adjacent to each other in their respective bezel.
These errors can be either due to incorrect gesture by the participants, or incorrect gesture-recognition by the application, it is hard to differentiate them.
The number of wrong gestures for `e' only constitute 3.3% of the total gestures for `e' .
However a better recognition system might improve the performance.
To generalize, for building any novel bezel-based application, bezel numbered 1 to 5 are preferable; these bezels would work well in both landscape and portrait mode.
Lefthand users should also be taken into consideration.
One way to achieve this is to provide a way to switch the position of the menu items to its mirror image.
All, except one, were affirmative of using this technique for text entry, if it were available as an app.
Participants like the technique: "no need to worry about the small  keys", "eyes-free is possible", "faster", "quick to learn", and "mechanical, patterns such as `the' are very easy to perform eyes-free, once learnt".
Most participants were of the opinion that the alphabetical keypad layout was easy to learn and easy to remember, though three participants did complain about the positioning of the Delete key.
Only one participant was of the opinion that a single hand usage would be better.
We believe that the system can be easily modified for a single hand use scenario.
Three participants complained that the task was "taxing on the mind ".
On the contrary, on average, participants took only 0-3 breaks during a session, hinting that the task was not too tiring.
Our current system lacks a cursor, so a correction requires deleting even all the correct letters that were typed after a mistake.
Apart from the cursor, participants asked for features such as auto-correction, audio/vibration feedback, and a better gesture recognizer.
If menu items are 16 or fewer, an L4x4 design could be used with level-1 on-axis items and level-2 off-axis items.
For 32 items or fewer, an L8x4 design  could be used.
If possible, items at bezel 8 should be avoided, and preference should be given to bezel number 1 to 5 .
For more than 32 options, an L8x8 design with all the items in level-1 and level-2 can be used, providing 64 items.
As noted earlier, on-screen menus are not necessary and can be shown only when required, providing more screen space for the actual content.
For example, a few Android phones provide touch-sensitive buttons at bezel number 3.
In such phones, that specific bezel cannot be used for initiating a bezel menu.
Further investigation is needed to use the bezel technique for other touchscreen-based handheld devices having different screen size.
Participants stated that they were more comfortable accessing a bezel with their right-thumb.
Bezel menus enable interaction with a touchscreen phone with minimal visual attention, along with solving the occlusion and mode-switching problem.
They ameliorate the fatfinger problem.
Marks do not have to be very precise.
Bezel menus can work under direct sunlight, when it is difficult to access the on-screen controls.
They can make the display icon-free, resulting in more screen space for the actual content.
Complex realistic applications such as video editor, word processor, text entry, which requires numerous controls along with large content viewing area can take advantage of bezel menus.
One of the demerits is that the number of menu items is limited to 64, and only 32 for best performance, but we believe that 32 menu items is a reasonable upper limit for most mobile applications.
Also users would need to learn different command sets for different applications, but with regular practice, accessing frequently-used items eyes-free would be achievable.
The study shows that highly accurate eyes-free interaction is achievable with L8x4 layout.
To gain insight into the per-
We found it to be competitive with existing techniques in terms of speed, accuracy, and ease of learning and usage.
This shows that bezel-initiated marks can be used to interact with realistic touchscreen applications, while paying minimal visual attention to the screen.
While encouraging, these results must be interpreted with caution.
The small sample size, non-native speakers as participants, limited our analyses.
More participants are required to make a stronger claim.
As the accuracy of originating the mark from the correct bezel is very high, different variations of bezel menu such as  both level-1 and level-2 marks starting from the bezel similar to simple marking menus , and  marks starting and ending at the bezels, are worth exploring.
Bezel menu can provide a 2-layer interaction on a touchscreen phone, as the first layer can be on-screen controls, and the second layer of menus can be pulled out from the bezel.
The obtained results are not limited to text entry, and can be readily applied to other applications.
We hope that our work will inform future designers to design better bezelbased interaction techniques.
