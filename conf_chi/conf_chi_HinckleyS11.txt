We explore techniques for hand-held devices that leverage the multimodal combination of touch and motion.
Hybrid touch + motion gestures exhibit interaction properties that combine the strengths of multi-touch with those of motionsensing.
This affords touch-enhanced motion gestures, such as one-handed zooming by holding one's thumb on the screen while tilting a device.
We also consider the reverse perspective, that of motion-enhanced touch, which uses motion sensors to probe what happens underneath the surface of touch.
Touching the screen induces secondary accelerations and angular velocities in the sensors.
For example, our prototype uses motion sensors to distinguish gently swiping a finger on the screen from "drags with a hard onset" to enable more expressive touch interactions.
These gestures can include information from the touch, such as the number of contacts and their positions, as parameters to a motion gesture.
For example, the user can tilt the device  to zoom into a specific screen location .
These techniques use the incidental vibratory motion induced by finger contact to add nuances to the expression of touch, or to infer context of use .
Collectively these perspectives represent a "hidden dimension of touch" that we seek to explore in the design space of motion sensing techniques that we propose, the novel techniques that we implement, and in the informal user study evaluating these new techniques that follows.
Touch-screen input is ubiquitous in mobile interaction.
But mobile devices also include motion sensors such as accelerometers and gyroscopes to support contextual sensing , digital photography , and games.
The widespread availability of mobile devices with both touch and motion-sensing presents an opportunity to explore novel interaction techniques that leverage synergistic roles for touch and motion as complementary modalities.
Synaethesia is a secondary sensation, such as color, experienced by some people in response to an actual perception, such as listening to music.
In the context of touch input, there is a corresponding sensor synaesthesia of accelerations, angular velocities, and torques that mobile devices can perceive coincident to the contacts detected by a touch-screen.
These secondary sensor responses may offer hints about touch that enhance the expressiveness of touch-screen interaction.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The user's touch input cues the system to "listen" for motion gestures.
This delimits Tilt-to-zoom from other incidental movements of the device.
This also affords direct control over the center of expansion via the  coordinate indicated by the user's thumb.
This offers one-handed continuous zooming, unlike pinch-to-zoom, which requires one hand to hold the device, and the other hand to articulate the pinch gesture.
This locks the screen at its current landscape/portrait format, for usage scenarios such as browsing the web while lying in bed.
This integrates selection with the motion gesture itself .
Some motion gestures can use the touch point to indicate specific objects on the screen.
This enables motion gestures that apply to a specific icon , photo, or region of the screen.
Example: Hold-and-shake a specific icon to Delete an icon, or Undo that deletion.
Unlike a physical button, touch allows for multiple points of contact, which can be combined with motion to afford additional interaction states.
Example: Tip-to-select a region of the screen, as indicated by the bounding rectangle of two thumbs.
Fluidly interleave touch and motion.
Some techniques may interleave motion gestures with complementary directtouch movements.
For example, a user can hold his thumb still and tilt-to-zoom, but then slide his thumb to transition directly to panning.
Examples: tilt-to-zoom; tip-to-select interleaves two-thumb zooming with rectangular selection.
By leveraging the motion sensors in more of a background-sensing role , different nuances of touch can be sensed.
This also allows simply resting a finger on an object to be treated differently than a distinct tap on an object, for example.
Examples: Soft-vs-Hard-Tap, Swipe-vs-Hard-Drag, and Context of Touch techniques.
The four techniques above illustrate new synaesthetic touch-motion gestures.
The following techniques employ motion sensors to lend secondary nuances to touch.
For example, a soft tap on the calendar tile of our device's home screen navigates to the day view showing all appointments for the day, but a hard tap navigates directly to the current appointment.
Synaesthetic touch+motion gestures exhibit a number of desirable design properties that collectively offer designers a new implement in the toolbox of mobile interaction.
To explore the design space of touch+motion gestures, we carefully considered the design properties of this class of gestures.
We then deliberately chose the techniques enumerated above to populate this space of touch+motion gestures with examples that illustrate each property.
Here, we briefly discuss these properties, and indicate the corresponding techniques that illustrate each of them.
State transitions for motion gestures.
We use FingerDown and FingerUp events to delimit motion gestures from other movements of a device.
This affords chunking and phrasing  of touch+motion gestures in a kinesthetic mode that is salient to the user .
When motion sensing is used to enhance touch, the onset  of touch indicates when to pull the corresponding motion data out of the sensor stream.
Example: All seven techniques listed above.
A touch-screen indicates an  position in addition to the FingerDown and FingerUp events.
Meanwhile, tilt can apply a continuous control to the indicated location.
A comfortable and imprecise target to delimit motion.
Systems must distinguish motion gestures from other incidental handling of a device.
A button can serve this role but diverts attention  and requires constant muscular strain while moving the device.
Some of our touch-motion gestures let the user gently rest a finger anywhere on the screen while moving the device.
Such motions demand less attention, do not impose a particular hand-grip, and may be more comfortable to articulate.
Combining motion gestures with touch is a simple idea that has been little explored.
Hassan's "Chucking" technique  uses a simultaneous touch+motion gesture to toss a file from a mobile device to a wall display.
The user holds a finger on the file's icon, while indicating where to place the file via a motion gesture.
Rahman  also uses touch + motion to measure wrist deflection angles.
We contribute novel examples of touch+motion gestures, and we reveal the larger design space of touch+motion by articulating its properties and charting a taxonomy of related techniques.
On digital cameras, sampling device motion at the moment that the user depresses the shutter button can yield enhanced image deblurring .
Skinput  uses vibratory signals to infer the location of taps on one's forearm.
These techniques hint at the strategy of looking for the motion in touch that we adopt in some of our techniques.
Several efforts have considered accelerometers as a proxy for pressure input.
For example, accelerometer-inferred pressure is not satisfactory for the musical expression of piano-forte on touchscreen-based mobile devices , but can support a few discrete states for "expressive typing" on the keys of an accelerometer-enhanced laptop .
Motion has also been used as a cue for grip-sensing mobile devices.
For example, an accelerometer can trigger implicit grip sensing when a mobile device is held still .
Graspables  use accelerometers to sense the orientation of objects and to trigger grip sensing at the right moment.
Several efforts consider touch in combination with other modalities.
Herot  describes a touch screen that senses shear and torque forces.
PACER  interleaves touch gestures with camera-based motion gestures.
Many works have explored motion-sensing gestures, as well as background sensing of motion as a contextual cue.
As this implies, motions may be explicitly triggered by the user, but motions also occur incidental to handling and transporting a mobile device.
Indeed, delimiters and state transitions  are fundamental building blocks of interactive techniques.
Some motion gestures are distinctive enough to serve as delimiters.
TimeTilt employs tapping the back of a device, or jerking it forward and back, to signal state changes .
Particular patterns of hard contact forces  can provide motion gestures that are fairly robust to false positive recognition errors.
However, to avoid false positives, these approaches limit the space of motion gestures.
Articulating the delimiters also may slow down some of the resulting interactions.
This opens up a larger space of permissible motions, but requires that the user find and hold a pushbutton on the device.
This may force the user to adopt a particular hand grip, or reduce the range of hand and wrist motions  that are comfortable to articulate.
Some systems have used indirect touch , but few systems have considered direct touch  to delimit motion gestures.
For manipulative motion gestures, tilt dynamics are often based on rate mappings to compensate for the limited range of motion that is possible when tilting a device .
The exact transfer function and appropriate feedback for such mappings can be tricky to optimize .
Other systems use larger-scale absolute movements to support a metaphor of direct spatial navigation .
Virtual Shelves  Tilt for text  Expressive Typing  Tilt scrolling  Pivot-to-lock Tilt-to-zoom Measuring wrist angles  Hard-tap Hard-drag Piano forte  Hold+shake Tip-to-select Chucking  Auto screen rotation  Rock'n'Scroll  Whack gestures  TimeTilt  Absolute  Relative  Hard contact force Motion gesture Stability  Thumb vs. finger usage Tapping corners Dual-screen Reader 
The property sensed is the primary axis of organization.
We classify it as acceleration , angular velocity , vibratory signals, shear & torque forces, or position-based motion sensing.
Within the acceleration category, the rightmost column characterizes the techniques based on how they use the accelerometer.
The columns of the table classify techniques by the activation mechanism employed to trigger an interactive response to sensed motions.
Indirect activation mechanisms include mechanical buttons as well as touch, pressure, or grip sensors integrated with the device.
Direct activation mechanisms include direct-touch contact with the display, as well as "none" , for techniques that employ always-active motion sensing.
We populate each cell of the resulting matrix with references to representative systems and techniques.
The new techniques described in this paper are shown in bold.
Yet the ubiquitous pinch-to-zoom gesture requires both hands on mobile devices: one hand holds the device while the other hand pinches.
As such, an effective one-handed method for continuous zooming could prove valuable to mobile users.
Attention is a precious resource in mobile interaction ; indeed, limiting the visual demands of interfaces has motivated many motion-sensing techniques.
Several of our techniques are motivated by pain-points that we have observed in current mobile user interfaces.
These include the inability to articulate continuous one-handed zooming, the difficulty of browsing the web while lying in bed due to undesired automatic screen rotation, and the tedium of performing a succession of taps to drill down to commonly used options or views.
We also intentionally chose examples that populate our design space of touch+motion gestures, so that each technique illustrates specific design properties.
Most of the techniques were implemented in a WP7 photo browser; we chose this as a demonstration application because all of the touch+motion gestures of interest have useful semantics in the context of browsing, organizing, cropping, and viewing individual photos or collections of photos .
The state transitions show how we incorporate the motion signal into the interactive technique, in terms of Buxton's 3-state model , where state 0 is out-of-range  and state 2 is dragging .
The touch parameter is the aspect of the touch signal that the technique leverages, which can be any of the following:  Entire screen: a touch anywhere on the screen.
Drag path: the path followed by the finger.
Select object: the user touches a specific icon, object, or region of the screen to select it.
Rectangle : a bounding box.
Contact / Shape: the geometry of the touch.
Our techniques currently do not include any examples of Contact / Shape parameters for touch+motion gestures, as our prototyping devices do not report this information.
Nonetheless these parameters suggest paths for future work.
The motion parameter specifies the contribution of the motion signal to the mixed-mode parameters of a gesture.
These use the same nomenclature as the attributes from the rightmost column of Fig.
2; we also note which axes of the accelerometer each technique employs.
We implemented our techniques as 3rd party applications for Windows Phone 7 .
We also implemented some techniques on a Zune HD.
Both devices have capacitive multi-touch screens and an integrated three-axis linear accelerometer.
WP7 supports sampling the accelerometer at 30Hz, whereas the Zune HD supports 60Hz.
We used both platforms because of limited availability of WP7 devices, and to user-test Tilt-to-zoom at the 60Hz sampling rate.
As noted earlier, pinch-to-zoom is difficult to perform, onehanded.
As such, touch-screen devices often use double-tap for one-handed zooming, but double-tap only supports discrete zooming.
Furthermore, in the context of zooming within a web browser, the first of the two taps is prone to accidentally activating links.
This forces the user to hunt for "empty" space, or text that is not hyperlinked, to successfully zoom via double-tap.
Thus, a reliable and effective way to zoom with one hand is desirable for mobile devices.
Because one-handed continuous zooming would open up many mobile usage scenarios, such as a user referring to a map while encumbered with shopping bags or holding a child's hand, such a technique complements pinch-to-zoom even if it is not necessarily faster.
The user zooms into a point on the screen by holding his thumb at the desired location, and tilting the device.
Tilt-tozoom thus affords directly zooming into locations such as the corners of a web page.
Pinch-to-zoom cannot directly express this because there is not room to expand two fingers at the edges of the screen.
Yet our technique is compatible with pinch-to-zoom; a device can support both techniques.
This lets the user employ the approach that best suits his current usage context.
We implemented Tilt-to-zoom in a simple map navigation application.
Touching down a finger on the screen starts an ambiguous "pan-or-zoom" mode.
We say the mode is ambiguous because at the moment of the FingerDown event, the system does not yet know if the user will pan or zoom the display.
Sliding the thumb beyond a minimum distance threshold triggers the standard panning behavior.
If the user instead tilts the device while the finger remains still, this triggers tilt-to-zoom.
The user can also hold the thumb still at the end of a panning motion, and then tilt, to transition directly to zooming.
Our technique therefore fluidly interleaves both panning and zooming.
Tilting the top edge of the screen away from yourself is mapped to zoom-out, while tilting the top edge of the screen closer to yourself is mapped to zoom-in.
In early pilot testing, we found that slightly more than half of users preferred this mapping, but some users with a different mental model preferred the opposite mapping.
The zooming speed is a rate-based function of the change in forwardback tilt angle .
The zooming speed is therefore determined relative to the angle at which the user starts touching the device: that is, whenever the user touches the screen, this resets the "resting" orientation  to the current forward-back tilt angle.
We chose a transfer function with a rate of expansion that seemed "about right" in early pilot tests, balancing rapid zooming against the desire for fine control of the zoom.
Attempting to read a web page while lying down on a couch, or in bed, are common examples that many of us have experienced.
To address this, the Apple iPad and iPhone include a global screen lock  to disable automatic screen rotation, but this makes it tedious to change orientation.
The MobileRSS iPhone app  pops up an icon when the user reorients the screen.
Users can touch this icon to toggle between the lock and automatic screen rotation option.
This icon appears for two seconds every single time the user reorients the device.
This may distract the user, and it interferes with reading the underlying text.
To avoid accidentally triggering tilt-to-zoom with small motions, the user must pass through a dead-band of 5 around the starting orientation before zooming starts.
However, it is possible to tilt the device without significant deviation of the wrist by adopting a precision grip on the device, and using the fingertips to subtly tilt it forward and back like a miniature seesaw.
The rate-based transfer function enables zooming a long distance with a small change in the tilt.
There has been little exploration of visual languages for motion gestures  that show the desired direction or degree of tilting required by an interaction.
To convey that tilt-to-zoom requires forward-back tilting, we provide a perspective box near the borders of the map that appears axis-aligned at the start of a tilt motion, but becomes perspective-distorted as the user tilts further from the starting angle.
The vanishing point appears to be either somewhere above the screen, when zooming in, or somewhere below the screen, when zooming out.
This conveys the current zoom direction as well as the tilt axis that responds to zooming.
Our pilot testing suggested that the perspective box enforces the metaphor, while providing feedback of the correct direction to tilt the device.
For this technique, the location of the touch point does not matter.
Thus it is an example of a touch-motion gesture that uses the imprecision of a touch anywhere on the entire screen as a way to delimit motion-sensing gestures from other incidental movements of the device.
Although this is the only example of such a gesture that we implemented, it is clear that motion-sensing gestures for other "global commands" that act on the entire screen  could be added, so long as the required motions do not conflict with one another.
We implement standard automatic rotation to any of the four canonical screen orientations, but we check to see if the user is holding a finger on the display before we respond.
Like the Tilt-to-zoom technique, on a FingerDown event Pivot-to-lock starts in an ambiguous mode.
If the user touches down and slides his finger, this again triggers panning.
If the device rotates substantially with little movement of the finger, we turn off the panning interpretation.
When the device has rotated far enough to represent a new viewing orientation, we trigger Pivot-tolock instead of automatically rotating the screen.
Visual feedback in the form of a padlock icon  appears to indicate that the screen has entered the lock state.
The screen remains locked after the user lifts his finger.
To prevent the screen lock from becoming a heavyweight mode, we opted to leave the lock on as long as the user remains at the current viewing orientation.
As soon as the user rotates the device to a new orientation , this turns off the lock and restores the automatic screen rotation functionality.
This solution ensures that the user cannot become "trapped" in a mode that they do not know how to escape.
The user can tip by this amount in any direction.
The user then returns to the original orientation within a short time-out.
Hence, Tip-toSelect represents a gesture that is recognized as a whole, rather than continuously interpreted as a direct manipulation.
The motion component of Tip-to-Select is related to the jerk gesture employed by TimeTilt .
Selecting or clipping content from the current screen view can be problematic for touch-only interfaces, if only because many of the obvious input events and state transitions available to the interaction designer must be assigned to other, higher-priority tasks.
For example, to select a region of text on the Apple iPad, the user must tap and then manipulate small blue selection handles to indicate the desired passage.
Indicating a rectangular selection by framing it with two fingers is an oft-demonstrated technique in the literature, but in typical mobile device interfaces, two-finger manipulation is already consumed by the pinch-to-zoom gesture.
Bringing up the menu via tapand-hold is a standard solution.
The user must search through the menu for the desired option, and then tap on it via hand-eye coordination.
Hence context menus are slow to activate, and demand significant visual attention.
However, a problem with shaking as a pure-motion gesture is that it is prone to unintended activation.
As a result, a system must be careful what it accepts as "shaking," which makes it more difficult to articulate the gesture when it is intended.
Our photo browser supports two-fingered zooming on individual photographs.
However, with the Tip-to-Select technique, if the user holds both fingers stationary and quickly tips the device away and back, this triggers a state transition to the selection mode.
When the user lifts his fingers, this crops the selected region from the photo.
Note that Tip-to-Select fluidly interleaves two-fingered zooming with the selection mode.
The user can start by zooming to the desired portion of a photograph, for example, and then keep the fingers in contact with the display while tipping the device to trigger selection.
Tip-to-Select responds to a change in orientation from the start of the motion signal.
We instead explore Hold-and-Shake as a contextual touch+motion gesture.
This gesture simultaneously demonstrates two interaction properties of interest.
First, it illustrates how holding touch indicates a mode switch from the "neutral" mode, where incidental motions can be ignored, to a "gesture" mode where the system looks for motion gestures.
Second, Hold-and-Shake illustrates how this transition to gesture mode can be integrated with object selection, by selecting the underlying object or icon that falls underneath the  coordinate of the touch.
Clearly the concept of touching an object to integrate mode switching with object selection  could be applied to other contextual motion gestures, such as holding a photo and moving the device in various directions .
The user can hold a photo and shake the device to delete the indicated photo.
Instead of asking the user to confirm whether the photo should be deleted, our prototype just replaces the photo with a red square that says Deleted.
The user can later undo the operation by holding on the former location of the photo and again shaking the device to Undo the action.
This leads to a nice symmetry between the original action and the gesture needed to reverse its effects.
Like our other gestures, Hold-and-Shake starts in an ambiguous mode.
For example, touching down but then sliding the finger on the screen scrolls the list of photos, and deactivates motion-sensing mode.
If the user instead touches down while the finger does not slide, we then recognize the shaking gesture by a heuristic that looks for three successive peaks in the motion signal .
Once motion starts, but before the shaking gesture has completed, we relax the distance threshold  because the touch contact point may move while the user shakes the device.
Note that the standard tap-and-hold interaction can still be supported in the presence of Hold-and-Shake.
The system just recasts tap-and-hold as a touch that occurs while the device is not sensing significant motion.
Many views on mobile devices can be scrolled or panned, but at the same time, the user may want to rearrange the items in the view by standard drag-and-drop actions.
Examples of such views include home screen icons, photos in an album, or web bookmarks.
The problem is that making a swipe motion to scroll conflicts with drag-anddrop, so the system typically must introduce a special "edit" mode to support rearranging items.
Our technique offers a new way to finesse this problem by using motion sensing to distinguish gentle swipes from hard drags.
In our prototype, gentle swipes scroll through a list of photos.
Hard drags "grab" the photo that the user lands on, to drag it to a new position.
The user thus has direct access to both scrolling and dragging.
This technique is similar to Soft-vs-Hard-Tap, except that the finger remains in contact with the display and specifies the dragging path.
The techniques detailed in the sections above focus on touch-enhanced motion gestures.
We now turn our attention to the complementary perspective of techniques that sense motion consequent to touching a device's screen.
We implement Soft-vs-Hard-Tap and Swipe-vs-Hard-Drag to demonstrate this approach.
We conclude this section with some further possibilities for augmenting touch with gyroscope + accelerometer motion sensing.
We have fewer example techniques for motion-enhanced touch, but we believe this perspective is equally important.
Our devices only include integrated accelerometers, which limits their sensitivity to some types of motions .
Users employ various hand grips on mobiles, such as interacting with a thumb, versus the index finger of the opposite hand.
If we can sense such nuances, they may form valuable contextual cues that can be used to fine-tune the handling of the touch input signal itself.
For example, the software might adjust interaction thresholds to make scrolling a web page with a thumb more forgiving.
Or, a soft-keyboard might be able to optimize its input model if it knows whether a given tap is a thumb, or an index finger.
To explore such possibilities, we attached an external gyroscope to a Zune HD and produced sensor traces.
We found the gyro signal  more sensitive to these types of subtle motions than the accelerometer.
Furthermore the gyro senses motion in the reference frame of the device, as opposed to the world coordinate system of the accelerometer, thus making it easier to recognize the device-centric motions of interest.
This suggests that such contextual inferences could be made with appropriate recognition techniques, and a sufficient corpus of training data.
We implemented a simple user-specific recognizer as a proof of concept, but more work must be done to generalize and validate this.
Touch interfaces for small-screen devices often tradeoff breadth for depth: fewer options are presented on any one screen, so that more taps are required to navigate to a particular option.
To address this, we implement Hard-vsSoft-Tap in a mock-up of the WP7 home screen "tiles" and a Calendar application with daily agenda and current appointment views.
This exhibits the depth-first nature typical of navigational structures on mobile phones.
The home screen offers a context where making the interactions less tedious  seems particularly appropriate.
By introducing a distinction between soft-taps and hardtaps, and the design rule that two soft taps equals one hard tap, our technique allows users to selectively jump ahead: softly tapping an item activates it, but tapping an item more forcefully "drills into" it to allow direct navigation to a secondary target.
For example, in our prototype, a soft-tap on the calendar tile in the device's home screen navigates to the day view with all appointments for the day, but a hard tap navigates directly to the current appointment.
This technique also enlivens interaction with the home screen, while providing feedback for hard vs. soft taps, by animating the tile that the user taps in proportion to the force exerted.
Lightly tapping an icon subtly expands the tile to show that it has been activated, but striking it more forcefully causes it to animate with more of a "splat."
We distinguish hard vs. soft taps by looking at the difference between the accelerometer peaks in the first 200ms of the touch.
If the separation is larger than 0.3g, it is a hard tap.
We also tried recognizing finger taps on the inert rear casing of the device.
Our video illustrates the recognizer responding to the user's finger tapping the case at the topleft corner, and the top-right corner.
Thus, the gyro and accelerometer signals can be used to localize "touch" interactions to some degree, without any true touch sensing being used at all .
Our recognizer uses a typical training-data approach.
We manually register several templates for the desired motion.
Template matching is performed on a candidate temporal data sequence.
If the sum of Euclidean distances between the input values and the template is within an error threshold, the recognizer reports a match.
The experimenter demonstrated each of the six primary touch+motion gestures discussed above.
Participants then tried each feature, and spent about 10 minutes using each one.
For the Tilt-to-Zoom gesture, users tried both the WP7 prototype  and the Zune HD prototype .
After trying each technique, users filled out 7-point Likert scale questions, and we asked users what the best and worst thing was about each technique.
After trying all six techniques, users rank-ordered them for overall preference.
Overall, the Likert questions revealed positive to strongly positive responses to memory load, learning curve, and ease of control.
Many users commented that the features were simple, fluent, seamless, or intuitive.
The general area that indicated the most concern by users was ease of discovery, which tended towards neutral to mildly positive responses on the Likert scale questions.
Although our current focus is not on self-revelation mechanisms for motion gestures , this does suggest we should improve discoverability.
In general, this is a little explored topic in motion sensing.
The physical metaphors that many of our techniques exploit appeared to help users learn the techniques, while also making the interaction memorable.
For example, one user commented that Pivot-to-lock "mimics a natural technique to prevent things from moving."
Another user described Hold-and-shake as a "salt and pepper" metaphor.
Hard-tap and Hard-drag correspond to real-world experiences with striking objects: as one user stated, "the splat animation is a natural mapping between gesture and the resulting action."
Although all users found the gestures easy to learn, we observed that in the early going, in general there are four steps that most users progress through:  Learn the device reacts to motion.
One user stated "I'm not accustomed to shaking the phone," while another's initial reaction was "tilting may not be natural."
However, most users had seen or used motion-sensing before.
Because of the limited screen real estate, users had a tendency not to touch the screen unless there was a clear purpose.
However, most users had no problem holding the screen while moving.
With Tilt-to-zoom, some users made anticipatory tilting motions before their finger actually touched down on the screen, which made the technique seem less responsive.
While we believe these are intriguing possibilities, they nonetheless remain to be adequately explored and validated by future work.
For example, the motion signals coincident with touch interactions also might form valuable sources of contextual data for probabilistic interaction frameworks, such as that proposed by Schwarz et al.
In addition to our early pilot studies which helped to iteratively refine the initial implementation of our techniques, we conducted an informal evaluation to garner feedback on our techniques from test users.
The goal of the evaluation was to assess each technique's value proposition, as well as its strengths and weaknesses, with test users.
We also sought ideas for possible enhancements by observing users' experiences, as well as their comments.
Our rate-based zooming function appeared to be a bit over-sensitive, leading to overshoot, and hence should be further refined .
A couple of users suggested the alternative of imparting discrete zooming steps by making "whip" tilting motions.
Nonetheless, despite these limitations, Tilt-to-zoom  resonated very strongly with most test-users.
Likewise, the simplistic recognition heuristics that we currently use for some of our gestures, such as Hard-tap and Hard-drag , or Hold-and-shake , currently requires the user to make more forceful or effortful motions than perhaps is necessary.
It is important to map touch+motion gestures to suitable interaction semantics.
There were several users who did not like Hard-tap, but reacted positively to Hard-drag, which is based on a very similar motion.
These users commented that if they make a mistake with Hard-tap, they first have to realize this, and then figure out how to go back.
On the other hand, if the user intends to do Hard-drag but scrolls a bit by mistake instead, the cost of this error is essentially zero, and the user can just try hard-dragging the item again.
We observed a similar issue with Pivot-to-lock.
All users liked being able to lock the screen orientation, but several users wanted the lock to be persistent .
Several users wanted the device to remain at a fixed screen orientation if they showed the screen to someone else, or passed the device to that person.
Users' preference rankings for the techniques reflected the everyday problems that they experience using their mobile phones.
The need to support effective one-handed zooming, as well as to avoid undesired automatic screen rotation, strongly resonated with many of the users.
Five of the ten users explicitly mentioned "one-handed" interaction as the best thing about Tilt-to-zoom.
Another user loved the fact that the center of expansion is controllable, unlike pinching.
Users of the iPhone in particular felt that having a simple gesture to lock the screen orientation was valuable.
Several users particularly appreciated shortcuts for common interactions .
Hold-and-shake was the only technique that nobody chose as their favorite technique , largely because shaking took too much effort, and took too much time.
However, because our study only includes 10 subjects, keep in mind that the difference between the rankings is not significant =16.74, p>0.05.
Users appreciated that they could use movements of the device for Pivot-to-lock and Tilt-to-zoom, or intensity of the movement for Hard-tap and Hard-drag, to address common problems that they experience in mobile device interaction.
Compared to tedious sequences of time-outs or touch gestures, users liked how they could combine wrist, finger, and device motion to articulate lightweight interactions with less "friction" in the user interface.
For example, users commented that the techniques were "intuitive and easy to transition to different modes," "easy and magical," or that "the icons are alive!"
On the other hand, for motion based interaction to be useful, the interactions must carefully consider appropriate grips as well as the wrist's range-of-motion .
For example, although we designed Pivot-to-lock with the idea that users would "pinch" the device between their fingers, and spin the device , many users performed Pivot-to-lock by adopting a rigid grip on the device while using ulnar/radial deviation of the wrist to turn the device on its side.
This is somewhat uncomfortable as the wrist has limited mobility along this axis.
However, in a more naturalistic usage context, where the user actually lies down in a bed while rigidly gripping the device, no wrist motion would be required.
Nonetheless, this leads us to consider teaching users the appropriate grip on the device as part of the self-revelation strategy for our gestures.
Our interactions are designed to take unique advantage of the properties of handheld mobile devices, while the user is interacting with the device at hand.
However, because of this, it is unclear how touch+motion techniques would translate to other form-factors, such as slate devices, or to other usage contexts, such as employing a device while it rests flat on a desk.
For example, the motions for Tilt-tozoom, Pivot-to-lock, and Tip-to-select cannot be applied when the device is resting flat on a surface such as a desk.
In our current prototype, we deliberately picked interactions that work well with an accelerometer.
However, integrating accelerometers with gyros, proximity sensors, or vibratory and shear/torque sensors could greatly enrich the vocabulary of motion at our disposal.
For example, combining accelerometers and gyros into an inertial reference frame enables each sensor to make up for shortcomings of the other, which could help to improve our existing techniques, or open up possibilities for others.
We have also explored motion-enhanced touch techniques that tease out further nuances of touch when the user strikes the screen.
Sensors such as accelerometers and gyros can pick up the subtle ripples in the motion stream that spread out from the point of impact.
We have implemented hardtapping and hard-dragging as examples of this perspective, while also presenting sensor traces that suggest further contextual sensing techniques may be possible.
Collectively, all of these techniques help to reveal the hidden dimension of touch.
Direct interaction is about much more than just the touch modality itself.
This work provides one example of how touch interactions can be extended and enriched by considering the interplay of touch with other modalities, including touch + motion sensing.
Bartlett, J.F., Rock 'n' Scroll Is Here to Stay.
Boring, S., Baur, D., Butz, A., Gustafson, S., Baudisch, P. Touch Projector: Mobile Interaction Through Video.
Buxton, W., Lexical and Pragmatic Considerations of Input Structure.
Buxton, W. Chunking and Phrasing and the Design of HumanComputer Dialogues.
Buxton, W. Three-state model of graphical input.
Amsterdam: Elsevier Science Publishers B. V. .
Card, S., Mackinlay, J., Robertson, G. The Design Space of Input Devices.
Navigation Techniques for Dual-Display EBook Readers.
Multi-Context Photo Browsing on Mobile Devices Based on Tilt Dynamics.
Chun Yat Li, F., Dearman, D., Truong, K. Virtual Shelves: Interactions with Orientation-Aware Devices.
Synergistic use of direct manipulation and natural language.
Eslambolchilar, P., Murray-Smith, R. Tilt-Based Automatic Zooming and Scaling in Mobile Devices.
Essl, G., Rohs, M., Kratz, S. Use the Force  Pressure and Pressure-Like Input for Mobile Music Performance.
Fitzmaurice, G., Buxton, W. The Chameleon: spatially aware palmtop computers.
An exploration of manipulative user interfaces.
Harrison, C., Hudson, S. Scratch Input: Creating Large, Inexpensive, Unpowered and Mobile Finger Input Surfaces.
Harrison, C., Tan, C., Morris, D. Skinput: Appropriating the Body as an Input Surface.
Hassan, N., Rahman, M., Irani, P., Graham, P. Chucking: A One-Handed Document Sharing Technique.
Herot, C., Weinzapfel, G., One-Point Touch Input of Vector Information from Computer Displays.
Hinckley, K., Pierce, J., Sinclair, M., Horvitz, E. Sensing techniques for mobile interaction.
Manual Deskterity: An Exploration of Simultaneous Pen + Touch Direct Input.
Hudson, S., Harrison, C., Harrison, B., LaMarca, A. Whack Gestures: Inexact and Inattentive Interaction with Mobile Devices.
Expressive Typing: A New Way to Sense Typing Pressure and Its Applications.
Joshi, N., Kang, S.B., Zitnik, C.L., Szeliski, R. Image Deblurring using Inertial Measurement Sensors.
Hand Grip Pattern Recognition for Mobile User Interfaces.
Proceedings of AAAI/IAAI-2006: Innovative Applications of Artificial Intelligence.
Kratz, S., Ballagas, R. Unravelling Seams: Improving Mobile Gesture Recognition with Visual Feedback Techniques.
Liao, C., Liu, Q., Liew, B., Wilcox, L. PACER: Fine-Grained Interactive Paper via Camera-Touch Hybrid Gestures on a Cell Phone.
Mahoney, R., Nonlinear Complementary Filters on the Special Orthogonal Group.
Olsen, D.R., Clement, J., Pace, A. Spilling: Expanding Handheld Interaction to Touch Table Displays.
Rahman, M., Gustafson, S., Irani, P., Subramanian, S. Tilt Techniques: Investigating the Dexterity of Wrist-Based Input.
Roudaut, A., Baglioni, M., Lecolinet, E. TimeTilt: Using Sensor-Based Gestures to Travel through Multiple Applications on a Mobile Device.
Handheld and Ubiquitous Computing .
Schmidt, D., Chehimi, F., Rukzio, E., Gellersen, H. PhoneTouch: A Technique for Direct Phone Interaction on Surfaces.
Schwarz, J., Hudson, S., Mankoff, J., Wilson, A.D. A framework for robust and flexible handling of inputs with uncertainty.
Schwesig, C., Poupyrev, I., Mori, E. Gummi: A Bendable Computer.
Sellen, A., Kurtenbach, G., Buxton, W., The prevention of mode errors through sensory feedback.
Taylor, B., Bove Jr., V. Graspables: Grasp-Recognition as a User Interface.
Wigdor, D., Balakrishnan, R. TiltText: Using tilt for text input to mobile phones.
