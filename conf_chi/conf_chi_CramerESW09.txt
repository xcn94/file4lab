Even though adaptive  spam filters are a common example of systems that make autonomous decisions on behalf of the user, trust in these filters has been underexplored.
This paper reports a study of usage of spam filters in the daily workplace and user behaviour in training these filters .
User observation, interview and survey techniques were applied to investigate attitudes towards two types of filters: a user-adaptive  and a rule-based filter.
While many of our participants invested extensive effort in training their filters, training did not influence filter trust.
Instead, the findings indicate that users' filter awareness and understanding seriously impacts attitudes and behaviour.
Specific examples of difficulties related to awareness of filter activity and adaptivity are described showing concerns relevant to all adaptive and  autonomous systems that rely on explicit user feedback.
Users need to spend time and effort to train their filter.
They have to be convinced to keep on training their system and need to understand the way the system learns.
Studies on spam filters can therefore provide interesting information about the usage of spam filters in specific, but also about interaction with autonomous and adaptive systems in general.
Despite this opportunity, spam filter usage is a surprisingly underexplored area.
Ways to develop spam filters are widely available, but information on user interaction with spam filters is scarce.
Research is available on interaction with e.g.
Research into trust in adaptive filters has focused mostly on collaborative recommender systems , and e.g.
Using spam filters might also be seen as more risky, as in contrast to recommenders, they do not recommend additional information items, but instead potentially delete information sent by others.
This paper presents an effort to gain more understanding in the ways that users trust systems that make  autonomous decisions on their behalf by evaluating how people interact with spam filters.
It investigates user attitudes and trust toward adaptive, trainable, as well as toward non-adaptive non-trainable filters.
Spam filters can help users deal with unwanted, unsolicited email.
These filters are a widespread example of systems or agents  that make decisions on behalf of the user.
In that capacity, spam filters offer a fine opportunity for studying user interaction with and trust in  autonomous and adaptive systems in real-life contexts.
Users for instance need to trust a spam filter's competence, as they risk losing communication that is relevant to them.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Whether tasks such as deleting spam email, are delegated to a system is guided by trust in a system .
Besides trusting a system's intentions or internal goals, users evaluate the trustworthiness of a system by assessing its competence .
Risk and users' own competencies also play a role.
Users will also not depend on a system when they expect to outperform it themselves .
Trust especially guides reliance when a complete understanding of a system is difficult .
Understanding however also affects acceptance and trust .
When studying user trust in adaptive systems, awareness and understanding should be taken into account.
Users first of all need to be aware that the system exists.
For adaptive, trainable systems, users may not be aware that they can provide feedback, or may not fully understand what effects training has on the filter.
We argue that awareness of the system's existence and training facilities is a relevant pre-condition to studying trust in adaptive and autonomous systems.
Participants were observed while using their regular email clients at their own workplace.
Participants were asked to refrain from using email on the day of the session so that their usual routine of checking email and dealing with spam messages could be observed.
The number of email messages, number of spam messages and the filter settings were recorded.
Afterwards, a semi-structured interview was conducted.
Twelve open-ended questions concerned users' experiences in interacting with  spam filter, their attitudes towards spam and spam filter use.
In addition, we asked participants to explain the way they thought their email filter worked and to explain why they did  train their spam filters.
A survey that measured acceptance of and trust in information filters and in the training of information filters concluded each session.
The questionnaire consisted of 7 items relating to the participants' background and 22 items based on  and , concerning perceived filter usefulness, perceived ease of use, attitude toward the spam filter and dependability of the filter .
The questionnaire addressed both using and training the spam filter using two separate sections.
To learn which messages are spam and which are not, an adaptive spam filter needs user feedback.
Ideally, training would also increase trust through improvement of performance.
However, understanding of systems and providing useful feedback to improve their performance can be challenging .
Trainable filters also do not immediately offer a high level of performance, but improve over time.
The nature of trainable spam filters is that these systems rely on correction of errors, further impeding trust.
Unfortunately, trust rapidly decreases when users notice errors and only slowly increases as a system performs without errors .
The study reported in this paper aims to investigate user attitudes and behaviours when using trainable and nontrainable spam filters.
We are especially interested in understanding the role of trust and the factors that motivate users to spend time and effort training a spam filter system.
Participants' use of their own spam filters was evaluated.
Used were the Mozilla Thunderbird email client's built-in adaptive filter , a rule-based, non-adaptive filter installed on a central mail server , or both .
Both filters can label, move or delete spam messages.
The Thunderbird client offered an adaptive, Bayesian spam filter, which can be trained by correcting its mistakes.
The server-side filter was not adaptive or personalised.
The server-side filter assigned scores to emails, based on which and how many of the indicators a message is spam  are fulfilled.
Users could not correct the spam filter if it made a mistake, but could add their own server-side rules.
28 worked in an area related to computer science, the others did not.
To see whether training of the filter affected trust, we compared users who actively trained or corrected their adaptive filter, or added rules to their server side filter  with those who did not .
Scores for participants who trained their filters were significantly higher  than for participants who did not do so .
These results indicate that a user's decision to train or correct an adaptive spam filter has an important relation with trust.
It's not trust in the system that plays a decisive role; instead it is trust in the training process in specific that is associated with training behaviour.
During interviews, participants did report that training the spam filter increased their trust in the system as they could notice that it improved.
However, even though they had spend considerable effort in training their filters, participants who corrected their filters were not found to delegate `higher risk' tasks such as automatically moving or deleting spam.
To investigate whether the level of trust in a filter was related to the level of delegation to a filter, Kruskal-Wallis tests were used to compare three groups: participants that allowed their filter to label emails , to move emails  or to delete emails .
Jonckheere's test revealed a significant trend in the data: participants who would allow for more autonomy of the system  were more positive on usefulness, ease of use of the filter and attitude towards it.
Trust however, was not found to be related to the choices for delegation.
From the interviews, the social context, in which it was either acceptable or unacceptable to lose email messages from others appeared to play a large role, e.g.
Scale might also play a role; most participants received a modest amount of spam and e.g.
The observation and interviews yielded interesting insights into the importance of awareness.
A number of participants expressed uncertainty about the settings of filters and some worried that there might be more filters active than they knew about.
They feared `invisible filters' might be deleting emails before they even had the chance to exert any control over filter settings.
Furthermore, participants often reported other filter settings than actually observed.
In this study, all of the participants who had a server-side filter were aware of the filter's activities, while a considerable portion  of the participants who had an active Thunderbird filter were not.
Results from the observation studies indicated that showing a unambiguous and hard to miss `possible spam' addition to an email's subject line, as the server-side filter did , worked better to make users aware a spam filter was active than showing an icon in the mailbox list of emails.
Such lack of awareness of participants of both filter activity and interface items led to less-than-optimal training behaviour in a number of ways.
First of all, even recognising the filter was active did not guarantee correct usage of interface items related to training.
In an extreme case a user did know about the filter and its ability to learn, but did not understand its ability to learn from example spam messages.
Sometimes, the spam button and icons were misinterpreted as delete buttons.
This led to inadvertent training of the filter when users used the button to delete no-longer needed, but legitimate email.
If the interface was understood and participants did train their filter, they still occasionally consciously decided to not mark specific spam messages as such.
This decision concerned spam messages that in their opinion were very similar to non-spam messages, and was made to `not confuse the filter'.
Ironically, these messages would be most informative for the filter to improve and not make the subtle mistakes these users were worried about.
This clearly indicates a gap in awareness relevant to all systems that rely on explicit feedback.
More user support for training has to be provided, in which overall understanding and such boundary cases have to be taken into account.
This raises the question whether this investment is having other positive effects on user attitudes.
Finally, qualitative findings indicate that facilitating awareness about system activity and adaptivity is extremely important in ensuring trust and useful training behaviour.
The findings above appear straightforward, but become more interesting when generalised to other adaptive autonomous systems.
They show system designers need to pay special attention to ensuring awareness about system activity and adaptivity.
Trust in the effectiveness of training was found to play an important role in the user's willingness to invest effort in a system that can be taught to improve over time.
Systems that depend on explicit user feedback need to be designed in such a way that this trust is optimised.
An overview of filtering activity should be available to the user.
How the system learns and can be taught should be made clear, but only on a level necessary to use the right interface items, avoiding problems with complete control .
Interface items should be recognisable as specifically dedicated to training of the system.
Training a system on `borderline cases' has to be encouraged when necessary.
Risks can perhaps be decreased by providing an opportunity for users to tell the system a case is special, e.g.
Even while some of the found problems may seem mundane, this study shows they are still open challenges, impeding adaptive system success.
The findings reported in this paper indicate that a more positive attitude toward a system and a more positive assessment of a system's ease of use and usefulness increases the likelihood that a user delegates higher risk tasks to a system; in this case automatic deletion of messages marked as spam.
Adaptivity did not appear to play a decisive role in reliance on the system.
Instead, risks associated with the social context of filtering email appeared more influential.
Users' choice to actively offer feedback to  an adaptive system relates to trust in the trainability of the filter.
It was not directly affected by ease of use, usefulness of or trust in the filter itself.
