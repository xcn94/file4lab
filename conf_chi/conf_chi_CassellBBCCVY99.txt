We believe that interfaces that are truly conversational have the promise of being more intuitive to learn, more resistant to communication breakdown, and more functional in high noise environments.
Therefore, we propose to leverage the full breadth and power of human conversational competency by imbuing the computer with all of the conversational skills that humans have; to whit, the ability to use the face, hands, and melody of the voice to regulate the process of conversation, as well as the ability to use verbal and nonverbal means to contribute content to the ongoing conversation.
In addition, we argue that the only way to accomplish such a goal of embodying the interface is to implement a model of conversational function.
This means that particular conversational behaviors  are generated and understood in terms of the functions that they fulfill in the ongoing conversation .
To provide a practical example of this approach, we present Rea, an embodied conversational agent whose verbal and nonverbal behaviors are designed in terms of conversational functions.
Rea is not designed with the metaphor of the interface as a conversation, but actually implements the social, linguistic, and psychological conventions of conversation.
Rea differs from other dialogue systems, and other conversational agents in three ways: * Rea has a human-like body, and uses her body in human-like ways during the conversation.
That is, she uses eye gaze, body posture, hand gestures, and facial displays to organize and regulate the conversation.
The underlying approach to conversational understanding and generation in Rea is based on discourse functions.
Thus, each of the users' inputs are interpreted in terms of their conversational function and responses are generated according to the desired function to be fulfilled.
Such models have been described for other conversational systems: for example Brennan and Hulteen describe a general framework for applying conversational theory to speech interfaces .
In this paper, we argue for embodied conversational characters as the logical extension of the metaphor of human - computer interaction as a conversation.
We argue that the only way to fully model the richness of human faceto-face communication is to rely on conversational analysis that describes sets of conversational behaviors as fulfilling conversational functions, both interactional and propositional.
We demonstrate how to implement this approach in Rea, an embodied conversational agent that is capable of both multimodal input understanding and output generation in a limited application domain.
Rea supports both social and task-oriented dialogue.
We discuss issues that need to be addressed in creating embodied conversational agents, and describe the architecture of the Rea interface.
The metaphor of face-to-face conversation has been successfully applied to human-interface design for quite some time.
One of the early descriptions of this metaphor gave a list of features of face-to-face conversation that could be fruitfully applied to HCI, including mixed initiative, non-verbal communication, sense of presence, rules for transfer of control, and so forth .
However, although these features have gained widespread recognition, human - computer conversation has never become more than a metaphor.
That is, designers have not taken the metaphor seriously in such a way as to design a computer that could hold up its end of the conversation.
She is being designed to generate these cues, ensuring a full symmetry between input and output modalities.
This is a step towards enabling Rea to participate on more of an equal footing with the user in a human-computer conversation.
An embodied conversational interface can take advantage of this and prompt the user to naturally engage the computer in human-like conversation.
If the interface is well-designed to reply to such conversation, the interaction may be improved As we shall show in the next section, there has been significant research in the areas of conversational analysis and multimodal interfaces.
However there has been little work in the recognition and use of conversational cues for conversational interfaces, or the development of computational conversational models that support nonspeech input and output.
A prime motivation for our work is the belief that effective embodied conversational interfaces cannot be built without an understanding of verbal and nonverbal conversational cues, and their function in conversation.
Developing an embodied conversational agent is a complex endeavor that draws on many fields.
We begin this paper by describing several motivations for building embodied conversational agents.
We then review past work in relevant HCI areas, and in several theories of conversation.
Examination of these theories leads us to believe that a conversational function approach may be the most appropriate for a conversational agent.
We then present Rea, and describe how we have begun to implement conversational function in an embodied interface agent.
There are many challenges that must be overcome before embodied conversational interfaces reach their full potential.
These range from low-level issues such as capturing user input to high level problems such as agent planning and dialogue generation.
In this section we review related work in three areas; multimodal interfaces, models of conversation, and conversational agent interfaces.
Embodied conversational agents may be defined as those that have the same properties as humans in face-to-face conversation, including: * * * * The ability to recognize and respond to verbal and non-verbal input The ability to generate verbal and non-verbal output.
The use of conversational functions such as turn taking, feedback, and repair mechanisms.
A performance model that allows contributions whose role is to negotiate conversational process, as well as contributions whose role is to contribute new propositions to the discourse.
There are a number of motivations for developing interfaces with these attributes, including: Intuitiveness.
Conversation is an intrinsically human skill that is learned over years of development and is practiced daily.
Conversational interfaces provide an intuitive paradigm for interaction, since the user is not required to learn new skills.
Redundancy and Modality Switching: Embodied conversational interfaces support redundancy and complementarity between input modes.
This allows the user and system to increase reliability by conveying information in more than one modality, and to increase expressiveness by using each modality for the type of expression it is most suited to.
The Social Nature of the Interaction.
Whether or not computers look human, people attribute to them humanlike properties such as friendliness, or cooperativeness .
Embodied conversational agents are similar to multimodal systems in that information from several modalities must be integrated into one representation of speaker intention.
One of the first multimodal systems was Put-That-There, developed by Bolt, Schmandt and their colleagues .
Put That There used speech recognition and a six-degree-offreedom space sensing device to gather input from a user's speech and the location of a cursor on a wall-sized display, allowing for simple deictic reference to visible entities.
More recently, several systems have built on this early work.
Koons allowed users to maneuver around a twodimensional map using spoken commands, deictic hand gestures, and eye gaze .
In this system, nested frames were employed to gather and combine information from the different modalities.
As in Put-that-There, speech drove the analysis of the gesture: if information is missing from speech, then the system will search for the missing information in the gestures and/or gaze.
Time stamps unite the actions in the different modalities into a coherent picture.
Wahlster used a similar method, also depending on the linguistic input to guide the interpretation of the other modalities .
Bolt and Herranz described a system that allows a user to manipulate graphics with two-handed semiiconic gesture .
Using a cutoff point and time stamping, motions can be selected that relate to the intended movement mentioned in speech.
In all of these systems interpretation is not carried out until the user has finished the utterance.
Johnston describes an approach to understanding of user input based on unification across grammars that can express input from multiple modalities.
While the system does treat modalities equally  it is still based on a mapping between combinations of specific gestures and utterances on the one hand, and user intentions  on the other hand.
In addition, all behaviors are treated as propositional -- none of them control the envelope of the user-computer interaction.
Although these works are primarily command-based rather than conversational, there are some lessons we can learn from them, such as the importance of modeling the user and developing interfaces which use existing deeply ingrained conversational behaviors .
They also highlight areas of potential difficulty, such as the fact that humans do not naturally use gesture according to a grammar with standards of form or function, and the problem of recognition errors in speech and gesture.
Missing from these systems is a concept of non-verbal function with respect to conversational function.
That is, in the systems reviewed thus far, there is no discourse structure over the sentence .
Therefore the role of gesture and facial expression cannot be analyzed at more than a sentence-constituent-replacement level.
Gestures are only analyzed as support for referring expressions .
What is needed is a discourse structure that can take into account why one uses a verbal or nonverbal device in a particular situation, and a conversational structure that can account for how non-verbal behaviors function in conversation regulation - such as turn-taking - as well as conversational content.
In a different context these behaviors may carry a different meaning, for example a head nod can indicate back-channel feedback or a salutation rather than emphasis.
Despite the fact that different behaviors may fulfill the same function, it is striking the extent to which such non-verbal behaviors coordinate and regulate conversation.
It is clear that through gaze, eyebrow raises and head nods both speakers and listeners collaborate in the construction of synchronized turns, and efficient conversation.
In this way, these non-verbal behaviors participate in grounding the conversation , and fill the functions that Brennan & Hulteen  suggest are needed for more robust speech interfaces .
An important aspect of the grounding of a conversation is evidence of understanding .
A conversational model that uses both positive and negative feedback enables an agent to recognize a misunderstanding and initiate the appropriate repair mechanisms.
To further clarify these types of roles fulfilled by discourse behaviors, the contribution to the conversation can be divided into propositional information and interactional information.
Propositional information corresponds to the content of the conversation.
This includes meaningful speech as well as hand gestures and intonation used to complement or elaborate upon the speech content .
In short, the interactional discourse functions are responsible for creating and maintaining an open channel of communication between the participants, while propositional functions shape the actual content.
Although the way in which conversation incorporates speech and other movements of the body has been studied for some time, there have been few attempts by the engineering community to develop embodied computer interfaces based on this understanding.
On the contrary, embodied conversational characters have, for the most part been built with hardwired associations between verbal and non-verbal conversational behaviors, without a clear flexible notion of conversational function underlying those behaviors.
In interfaces of this sort, there is no possibility for one modality to take over for another, or the two modalities to autonomously generate complementary information.
Even though conversation is considered an orderly event, governed by rules, no two conversations look exactly the same and the set of behaviors exhibited differs from person to person and from conversation to conversation.
Therefore to successfully build a model of how conversation works, one can not refer to surface features, or conversational behaviors alone.
Instead, the emphasis has to be on identifying the fundamental phases and high level structural elements that make up a conversation.
These elements are then described in terms of their role or function in the exchange.
Typical discourse functions include conversation invitation, turn taking, providing feedback, contrast and emphasis, and breaking away .
It is important to realize that each of these functions can be realized in several different manners.
The form we give to a particular discourse function depends on, among other things, current availability of modalities, type of conversation, cultural patterns and personal style.
Input events in different modalities may be mapped onto the same discourse function, while in different conversational states the same function may lead to different conversational behaviors, based on state, as well as the availability of input and output modalities.
Noma & Badler have created a virtual human weatherman, based on the Jack human figure animation system .
In order to allow the weatherman to gesture, they assembled a library of presentation gestures culled from books on public speaking, and allowed authors to embed those gestures as commands in text that will be sent to a speech-to text system.
This is a useful step toward the creation of presentation agents of all sorts, but does not deal with the autonomous generation of non-verbal behaviors in conjunction with speech.
Other efforts along these lines include Andre et al.
The work of Thorisson provides a good first example of how discourse and non-verbal function might be paired in a conversational multimodal interface .
In this work the main emphasis was the development of a multi-layer multimodal architecture that could support fluid face-toface dialogue between a human and graphical agent.
The agent, Gandalf, was capable of discussing a graphical model of the solar system in an educational application.
Gandalf recognized and displayed interactional information such as head orientation, simple pointing and beat gestures and canned speech events.
In this way it was able to perceive and generate turn-taking and back channel behaviors that lead to a more natural conversational interaction.
However, Gandalf had limited ability to recognize and generate propositional information, such as providing correct intonation for speech emphasis on speech output, or a content-carrying gesture with speech.
In this case the challenge was to generate conversation between two artificial agents and the emphasis was on the production of non-verbal propositional behaviors that emphasized and reinforced the content of speech.
Since there was no interaction with a real user, the interactional information was very limited, and not reactive .
Rea is an attempt to develop an agent with both propositional and interactional understanding and generation, which can interact with the user in real time.
As such it combines elements of the Gandalf and Animated Agents projects into a single interface and moves towards overcoming the limitations of each.
In the next section we describe interaction with the Rea agent and its implementation.
Other researchers have built embodied conversational agents, with varying degrees of conversational ability.
Each successive iteration of their computer character has made significant strides in the use of these different aspects of an embodied dialogue system.
Although their current system uses a tightly constrained grammar for NLP and a small set of prerecorded utterances that their character can utter, it is expected that their system will become more generative in the near future.
Their embodiment takes the form of a parrot.
This has allowed them to simulate gross "wing gestures"  and facial displays .
The parrot's output, however, is represented as a set of conversational behaviors, rather than a set of conversational functions.
Therefore, modalities cannot share the expressive load, or pick up the slack for one another in case of noise, or in the case of one modality not being available.
Nor can any of the modalities regulate a conversation with the user, since user interactional behaviors cannot be perceived or responded to.
Loyall and Bates build engaging characters that allow the viewer to suspend disbelief long enough to interact in interesting ways with the character, or to be engaged by the character's interactions with another computer character .
Associating natural language with non-verbal behaviors is one way of giving their characters believability.
In our work, the causality is somewhat the opposite: we build characters that are believable enough to allow the use of language to be human-like.
That is, we believe that the use of gesture and facial displays does make the characters life-like and therefore believable, but these communicative behaviors also play integral roles in enriching the dialogue, and regulating the process of the conversation.
It is these latter functions that are most important to us.
In addition, like Ball et al., the Oz group has chosen a very non-human computer character-- Woggles, which look like marbles with eyes.
Researchers such as Ball and Bates argue that humanoid characters raise users' expectations beyond what can be sustained by interactive systems and therefore should be avoided.
The system currently consists of a large projection screen on which Rea is displayed and in front of which the user stands.
Two cameras mounted on top of the projection screen track the user's head and hand positions in space.
Users wear a microphone for capturing speech input.
A single SGI Octane computer runs the graphics and conversation engine of Rea, while several other computers manage the speech recognition and generation and image processing.
And the house tour continues... Rea is designed to conduct a mixed initiative conversation, pursuing the goal of describing the features of a house that fits the user's requirements while also responding to the users' verbal and non-verbal input that may lead in new directions.
When the user makes cues typically associated with turn taking behavior such as gesturing, Rea allows herself to be interrupted, and then takes the turn again when she is able.
She is able to initiate conversational repair when she misunderstands what the user says, and can generate combined voice and gestural output.
For the moment, Rea's responses are generated from an Eliza-like engine that mirrors features of the user's last utterance , but efforts are currently underway to implement an incremental natural language and gesture generation engine, along the lines of .
In order to carry on natural conversation of this sort, Rea uses a conversational model that supports multimodal input and output as constituents of conversational functions.
That is, input and output is interpreted and generated based on the discourse functions it serves.
The multimodal conversational model and the underlying Rea architecture are discussed in the next sections.
Rea's domain of expertise is real estate and she acts as a real estate agent showing users the features of various models of houses that appear on-screen behind her.
The following is a excerpt from a sample interaction: Lee approaches the projection screen.
Rea is currently turned side on and is idly gazing about.
As the user moves within range of the cameras, Rea turns to face him and says "Hello, my name is Rea, what's your name?"
Rea says with rising intonation at the end of the question.
Do you want to see the master bedroom?".
Lee says, overlapping with Rea.
While Rea is capable of understanding speech, and making reasonable contributions to an ongoing conversation about realty, to date our primary effort has been in the interactional component of the conversational model.
This component manages several discourse functions.
Turntaking function - Rea tracks who has the speaking turn, and only speaks when she holds the turn.
Currently Rea always allows verbal interruption, and yields the turn as soon as the user begins to speak.
If the user gestures she will interpret this as expression of a desire to speak , and therefore halt her remarks at the nearest sentence boundary.
Finally, at the end of her speaking turn she turns to face the user to indicate the end of her turn.
Recognizing emphasis is important for determining which part of the utterance is key to the discourse.
For example, the user may say "I'd like granite floor tiles," to which Rea can reply "granite is a good choice here;" or the user might say "I'd like granite floor tiles," where Rea can reply "tile would go well here."
We are developing a gesture classification system to detect the 'beat' gestures that often indicate emphasis.
On the output side, we plan to allow Rea to generate emphasis using either modality.
These conversational functions are realized as conversational behaviors.
For turn taking, for example, the specifics are as follows: If Rea has the turn and is speaking and the user begins to gesture, this is interpreted as the user wanting turn function.
If Rea has the turn and is speaking and the user begins to speak, this is interpreted as the user taking turn function.
If the user is speaking and s/he pauses for less than 500 msec., this is interpreted as the wanting feedback function.
If the user is speaking and issues a declarative sentence and stops speaking and gesturing, or says an imperative or interrogative phrase, their input is interpreted as a giving turn function.
Finally, if the user has the turn and continues gesturing after having finished uttering a declarative sentence, or if s/he begins another phrase after having uttered a declarative sentence, with a pause of less than 500 msec, this is interpreted as a holding turn function.
This approach is summarized in Table 1.
For example, when the user first approaches Rea , she signals her openness to engage in conversation by looking at the user, smiling, and/or tossing her head.
When conversational turn-taking begins, she orients her body to face the user at a 45 degree angle.
When the user is speaking and Rea wants the turn she looks at the user and utters a paraverbal .
When Rea is finished speaking and ready to give the turn back to the user she looks at the user, drops her hands out of gesture space and raises her eyebrows in expectation.
Table 2 summarizes Rea's current interactional output behaviors.
Gesture Wanting turn Speech Taking turn User speaking Pause of <500 msec.
Wanting feedback Imperative phrase Giving turn Interrogative phrase Giving turn Declarative phrase & Giving turn pause >500 msec.
Functional interpretation of turn taking input Thus, speech may convey different interactional information; it may be interpreted as taking turn, giving turn, or holding turn depending on the conversational state and what is conveyed by the other modalities.
A similar approach is taken for generation of conversational behaviors.
Output Functions By modeling behavioral categories as discourse functions we have developed a natural and principled way of combining multiple modalities, in both input and output.
Thus when REA decides to give feedback, for example, she can choose any of several modalities based on what is appropriate at the moment.
However the different modalities are integrated into a single semantic representation that is passed from module to module.
This representation is a KQML frame .
The categorization of behaviors in terms of their conversational functions is mirrored by the organization of the architecture which centralizes decisions made in terms of functions , and moves to the periphery decisions made in terms of behaviors .
In addition, a distinction is drawn between reactive and deliberative communicative actions .
The Input Manager and Action Scheduler interact with external devices and together with the Reaction Module respond immediately 
Performing head nods when the user pauses briefly is an example of a reactive conversational behavior.
The other modules are more "deliberative" in nature and perform non-trivial inferencing actions that can take multiple real-time cycles to complete.
These modules are written in C++ and CLIPS, a rule-based expert system language .
The input manager currently supports three types of input: * Gesture Input: STIVE vision software produces 3D position and orientation of the head and hands.
Grammar Based Speech Recognition: IBM ViaVoice returns text from a set of phrases defined by a grammar.
In all cases the features sent to the Input Manager are time stamped with start and end times in milliseconds.
The various computers are synchronized to within a few milliseconds of each other.
This synchronization is key for associating verbal and nonverbal behaviors.
Latency in input devices can have a significant impact on the functioning of the system, since delays of milliseconds can have significant meaning in conversation.
For example, if Rea delays before giving a "yes" response it can be interpreted by the user as indecision.
Thus, our goal is to minimize input device and processing latencies wherever possible.
Low level gesture and audio detection events are sent to the reaction module straight away.
User-testing of Gandalf, capable of some of the conversational functions also described here, showed that users relied on the interactional competency of the system to negotiate turn-taking, and that they preferred such a system to another embodied character capable of only emotional expression.
However, Gandalf did not handle repairs gracefully, and users were comparatively more disfluent when using the system .
Our next step is to test Rea to see whether the current mixture of interactional and propositional conversational functions, including turntaking and repair, allow users to engage in more efficient and fluent interaction with the system.
The functional approach provides abstraction that not only serves theoretical goals but also gives important leverage for multi-cultural scalability.
The inner workings of the system deal with a set of universal conversational functions while the outer modules, both on the input and output side, are responsible for mapping them onto largely culturespecific surface behaviors.
The architecture allows us to treat the mappings as an easily exchangeable part in the form of a specification file.
In this paper we have argued that embodied conversational agents are a logical and needed extension to the conversational metaphor of human - computer interaction.
We demonstrated our approach with the Rea system.
Increasingly capable of making an intelligent contentoriented - or propositional - contribution to the conversation, Rea is also sensitive to the regulatory - or interactional -- function of verbal and non-verbal conversational behaviors, and is capable of producing regulatory behaviors to improve the interaction by helping the user remain aware of the state of the conversation.
Rea is an embodied conversational agent who can hold up her end of the conversation.
Cassell, J., Torres, O. and Prevost, S. Turn taking vs. Discourse Structure: how best to model multimodal conversation.
In Shared Cognition: Thinking as Social Practice, J. Levine, L.B.
CLIPS Reference Manual Version 6.0.
Technical Report, Number JSC-25012, Software Technology Branch, Lyndon B. Johnson Space Center, Houston, TX, 1994.
Finin, T., Fritzson, R. KQML as an Agent Communication Language.
In The Proceedings of the Third International Conference on Information and Knowledge Management , ACM Press, November 1994.
A. and Smith, I. Unification-based multimodal integration.
In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, .
The negotiation of context in face-to-face interaction.
Integrating simultaneous input from speech, gaze and hand gestures.
In Intelligent Multi-Media Interfaces M.T.
Loyall, A. and Bates, J. Personality-rich believable agents that use language.
Nagao, K. and Takeuchi, A.
Social interaction: multimodal conversation with social agents.
Proceedings of the 12th National Conference on Artificial Intelligence , , AAAI Press/MIT Press, vol.
On Conversational Interaction with Computers.
In User Oriented Design of Interactive Graphics Systems: Proceedings of the ACM SIGGRAPH Workshop , ACM Press, 681-683.
In Proceedings of the IJCAI'97 workshop on Animated Interface Agents - Making them Intelligent, , Morgan-Kaufmann Publishers, San Francisco.
User-Centered Modeling for Spoken Language and Multimodal Interfaces, IEEE Multimedia, 3, 4, , 26-35.
Integrating Reactive and Scripted Behaviors in a Life-Like Presentation Agent.
Azarbayejani, A., Wren, C. and Pentland A. Real-time 3-D tracking of the human body.
Lifelike computer characters: the persona project at Microsoft Research.
Beskow, J. and McGlashan, S. Olga - A Conversational Agent with Gestures, In Proceedings of the IJCAI'97 workshop on Animated Interface Agents - Making them Intelligent, , MorganKaufmann Publishers, San Francisco.
Bolt, R.A. Put-that-there: voice and gesture at the graphics interface.
Bolt, R.A. and Herranz, E. Two-handed gesture in multi-modal natural dialog.
In Proceedings of UIST `92, Fifth Annual Symposium on User Interface Software and Technology, .
Interaction and Feedback in a Spoken Language System.
Cassell, J., Pelachaud, C., Badler, N.I., Steedman, M., Achorn, B., Beckett, T., Douville, B., Prevost, S. and Stone, M. Animated conversation: rule-based generation of facial display, gesture and spoken intonation for multiple conversational agents.
Cassell, J. and Thorisson, K. The Power of a Nod and a Glance: Envelope vs.
Emotional Feedback in Animated Conversational Agents.
Journal of Applied Artificial Intelligence, in press.
Thorisson, K. R. Communicative Humanoids: A Computational Model of Psychosocial Dialogue Skills.
Wahlster, W., Andre, E., Graf, W. and Rist, T. Designing illustrated texts.
A computer program for the study of natural language communication between man and machine.
