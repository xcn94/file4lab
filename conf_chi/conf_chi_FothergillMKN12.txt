Entertainment and gaming systems such as the Wii and XBox Kinect have brought touchless, body-movement based interfaces to the masses.
Systems like these enable the estimation of movements of various body parts from raw inertial motion or depth sensor data.
However, the interface developer is still left with the challenging task of creating a system that recognizes these movements as embodying meaning.
The machine learning approach for tackling this problem requires the collection of data sets that contain the relevant body movements and their associated semantic labels.
These data sets directly impact the accuracy and performance of the gesture recognition system and should ideally contain all natural variations of the movements associated with a gesture.
This paper addresses the problem of collecting such gesture datasets.
In particular, we investigate the question of what is the most appropriate semiotic modality of instructions for conveying to human subjects the movements the system developer needs them to perform.
The results of our qualitative and quantitative analysis indicate that the choice of modality has a significant impact on the performance of the learnt gesture recognition system; particularly in terms of correctness and coverage.
The availability of body movement sensing technology in commodity entertainment and gaming systems such as the Nintendo Wii, Sony Playstation Move, and Microsoft Kinect has now made such interfaces available to a much larger audience.
Whilst popular for controlling entertainment consoles, applications of such systems also exist in tutoring , security  and healthcare .
These motion sensing systems enable the estimation of movement of various body parts from raw inertial motion or depth sensor data.
However, the interface developer is still left with the challenging task of creating a system that recognizes these movements as embodying meaning.
During development of games driven by human movements, developers generally tackle this problem using a trial and error approach.
They start by defining a map from body part movement to a set of gestures.
This is generally done by specifying a set of rules or conditions on the movements of the body parts under which a particular gesture would be deemed to have happened.
An example of such rule would be: `if both feet simultaneously move upwards, then a jump gesture should be detected'.
These initial rules are refined by hand by testing their performance on a set of test subjects .
This approach does not scale well to more complex gestures and is also not guaranteed to lead to a continual increase in system accuracy or performance.
The machine learning  approach for the gesture recognition problem requires the collection of data sets that contain examples of movements and their associated gesture label.
In essence, a machine learning algorithm tries to teach the system what movements can represent a particular gesture.
The accuracy of the system is therefore influenced by the anthropomorphic and behavioral kinematic variation in the set of example gestures that are used to train it.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
For an accurate and responsive form of interaction, not only must the set of performers providing training data be representative of the target population but the dataset of movements used for training the system must reflect what is ideally or most likely to occur during system deployment.
In other words, not only must there be examples of only desired gestures , but in order to cope with a wide array of users and their corresponding abilities the dataset must include common, desired variants of the particular movements associated with the gestures .
We explain the desirable properties of a training dataset using Figure 1.
In the left-most picture, the movements performed by eight human subjects lie outside the circle.
In other words they do not belong to a common space of movements.
Thus, they are correct, but with little coverage.
In the center-right picture, the movements cover the space of movements, but some of them lie outside the circle and are an inaccurate reflection of the gesture.
They have coverage, but some are outside of the space of movement and, thus, would be perceived as incorrect.
In the right-most picture, the set of movements collected from the eight human subjects are accurate and also cover a space of movements.
They are correct and have coverage.
Developers usually use human subjects to generate the data used to train and test the machine learning system.
To convey the body movements a designer associates with each gesture, they give the subjects some instructions.
These instructions or signs can be of different semiotic modalities including text, images, video, and combinations of the above .
However, there has been no study of what biases are introduced by these different modalities on correctness and coverage.
In this paper, we investigate the questions: R1: Does the semiotic modality of instructions for collecting training data affect the performance of the gesture recognition system?
R2: In what way does semiotic modality of instructions affect correctness and coverage?
We investigate what is the most appropriate semiotic modality of instructions for conveying to human subjects the movements the system developers want to associate with particular gestures in order to achieve  Correctness and  Coverage .
We analyze the questions from both the performers' perspective and through the accuracy of the trained gesture recognition system.
The problems of detecting and recognizing gestures from human body movements captured using videos or 3D skeletal data have been extensively studied in the computer vision and machine learning communities .
They are primarily focused on the developments of mathematical models that can generalize the semantics-kinematics mapping learned from a set of training examples to unseen data.
These studies have generally ignored the problem of how to collect the set of movements associated with a gesture.
There is little work in the Human-Computer Interaction  and Computer Vision  literature on the problem of how to specify which movements need to be performed by subjects generating data for training a gesture recognition system.
A number of datasets of body movements corresponding to different gestures have been collected by the Computer Vision community; however, compared to the Cambridge Gestural Performance Database 2012  collected for this study, they do not provide details on how the performers were instructed, as shown in Table 1.
The primary meanings of instructions are to convey the kinematics .
We are interested in both the semiotic semantics and immediate pragmatics of the different modalities.
The effect they should have on those who interpret them is to instruct these people in the performance of kinematics signified while allowing for them to perform the movement as they feel is most natural.
In the field of semiotics, Peirce  outlines three phenomenological categories of signs emphasizing the way it denotes the object of reference: icon, index, and symbol.
There are examples of using iconic and symbolic instructions to convey kinematics pictures, movies, or text that aim for correctness, but not coverage.
For instance, methods such as Labanotation provide a detailed approach of describing dance movements, but the complexity of the notation puts it beyond the reach of novices .
Teaching dance moves has been investigated , but only from the point of view of correctness at deployment when they use correctness.
In addition, they only investigate differences in iconic video instructions for sequences of iconic gestures.
In addition to correctness, coverage requires a mechanism that supports the gathering of a wide assortment of performances for each gesture.
This has been far less addressed in research as it has less importance in other areas of instruc-
But in machine learning, gathering a variety of samples for training data significantly improves the performance of a recognition system during testing and deployment.
For instance, the importance of similarity between training and test data has already been recognized for speech recognition.
It is understood that a speech recognition system trained using speech samples collected from people in a very nervous or overly excited state would not have good performance on speech samples obtained from people in a relaxed state.
Other challenges that arise in data collection for voice and speech recognition are described in  where the authors tried to collect continuous and spontaneous speech samples occurring in day-today life.
They asked human subjects to summarize a written passage in their own words, rather than reading a passage or using isolated words.
They further characterise training data in terms of whether there is a human audience or whether the speaker gets feedback.
These characteristics are pertinent to our task of detecting human body movements but beyond this paper's scope although collecting gestures tend to be more contrived as it is harder to record them as they occur in day-to-day life.
In addition to investigating the effect of instruction modality, we are also looking at whether the type of gesture makes a difference in the affect of modality.
We introduced two gesture types based on McNeil  categorize of gesticulation.
The first was Iconic gestures - those that imbue a correspondence between the gesture and the reference.
The second was Metaphoric gestures - those that represent an abstract content.
For the former, we borrowed six gestures from a first person shooter game  and for the latter, we borrowed six gestures for a music player .
The three were  descriptive text breaking down the performance kinematics,  an ordered series of static images of a person performing the gesture with arrows annotating as appropriate, and  video  of a person performing the gesture.
We wanted mediums to be transparent so they fulfill their primary function of conveying the kinematics.
Static images correspond with any static drawings and video is also analogous to live demonstration; although these pictorial modalities may again be biased by the performer they depict.
The two combinations of modalities were the simultaneous juxtaposition of descriptive text with each pictorial modality.
Textual descriptions with varying degrees of verbosity were possible over all gestures and each description was determined by the authors.
The videos were all of the first author performing the gestures as defined by each application's designer and started and stopped with the beginning and end of the gesture.
They were filmed in front of a white background ensuring all body movements were within frame .
For the static images, individual video frames were extracted at points the designer considered necessary to fully define the gesture.
Two types of structured questionnaires  and an openended final interview were used to gather participant subjective data.
The former was administered after every gesture was performed.
It consisted of 11 psychometric questions on how well the participant felt they understood the instructions  and if the participant felt they were able to perform the gesture freely  - all questions were rated on a 4-point Likert scale from Strongly Disagree to Strongly Agree.
The latter was administered at the end of the study and consisted of 11 questions on the same concerns as the after-instruction questionnaire; however, the participants were asked to rank the instruction methods  for each question.
Questions regarding participants' understanding of instructions were posed in terms of the clarity of the movements, importance of body parts, amount of information about desired movements, effort required for their interpretation, their ambiguity, the correctness of performances they allowed, and the amount of practice they required.
Questions regarding participants' feeling of freedom were posed in terms of feeling of confident whilst performing, feeling inhibited, being in control, and feeling odd.
The performances and final interview was also recorded using a normal video camera for later review and transcription.
A markerless motion capture system was used  to record the 3D position of skeletal joints at 30Hz to within approximately 10cm accuracy .
They repeated this process for 20 gesture instructions.
At the end of the study, they were asked to complete the ranking questionnaires and then the participants were interviewed by the experimenter with open-ended questions regarding their experience and their opinions of the different semiotic modalities.
Thirty participants were recruited from a multicultural, industry research lab and a university computer science department in the UK.
Although some of the participants were familiar with the domain of machine learning and computer vision, none of the participants were privy to the workings of the machine learning algorithm of the study we were conducting.
Each participant performed each gesture based on at least one semiotic modality.
Since two of our conditions were combinations of other modalities, we had participants do some gestures in two conditions.
For example, a participant would shoot a pistol instructed by descriptive text first and then, after completing an after-instruction questionnaire, be instructed to shoot a pistol with descriptive text plus static images.
We ensured that participants did not receive a multimodal instruction followed by a unimodal instruction for the same gesture in order to handle any significant learning effect.
The 11 post-action questions were grouped in two clusters, one for understanding and one for freedom.
The reliability analysis shows that the items in each cluster were highly intercorrelated: the Cronbach's alpha values were .92 for Understanding and .86 for Freedom.
For high reliability, Nunnally and Bernstein  suggest to use a cut-off of .7, thus, we could compute aggregated scores for each factor.
The long history of automatically recognising gestures from visual or kinematic measurements is reviewed in    .
We address only the recognition of relatively simple human gestures of a few seconds, not activities of minutes or hours.
For these short gestures, Schindler and van Gool  have shown that short windows of measurements are sufficient to obtain state-of-the-art recognition performance.
We assume a small vocabulary of gestures A is given.
Each gesture a  A has associated to it an action point that is characteristic for the gesture.
As an example, for a punch we can define the action point as the first point in time at which the arm is straight out in front.
The experiment was conducted in a large private space with one experimenter.
Participants were told that we were investigating instructions for performing gestures for the sole purpose of training a gestural interaction system.
They then were asked to stand and face a 30" LCD TV with a Kinect sensor in front of it.
When they indicated they were ready, the first gesture's instructions appeared on the screen in a PowerPoint slideshow.
The participants had as much time as they desired to read or watch the instructions.
Questions were not addressed by the experimenter and instead the participants were told to 'do what they wanted'.
When they indicated they were ready to begin, they were instructed to perform the gesture ten times and to ensure that there was a pause between each repetition of the gesture.
When all ten repetitions of the gesture were completed, the participant returned to sit at the table and completed an after-
The performance of the system is measured in terms of precision and recall.
To achieve a high precision, the training data should only contain movements that users of the deployed system will associate with the gesture .
To achieve a high recall, the training data should contain all movements that the designer wants to associate with a gesture .
We assess the quality of our predictions using ground truth annotations.
To this end, we define a performance measure that captures the characteristics of the system in an online setting.
These are, its precision--how often is the gesture actually present when the system claims it is, its recall-- how many true gestures are recognized by the system, and its latency--how large is the delay between the true action point and the systems prediction.
Latency-aware measure of predictive performance for a single gesture: a fixed time window of size 2 is centered around the ground truth  and used to partition the three predicted firing events into correct  and incorrect predictions ; precision = 0.5, recall = 1.
We now show how random forest classifiers  can be straightforwardly adapted to the problem of recognizing gestures.
Our approach is similar to that of Yao et al.
We use 35 skeletal joint angles, 35 joint angle velocities, and 60 xyz-velocities of joints for a 130-dimensional feature vector at each frame.
The feature vector t is evaluated by a set of M decision trees, where simple tests f : Rd  {left, right} are performed recursively at each node until a leaf node is reached.
The parameters    of each test are determined separately during the training phase, to be described below.
We are interested in measuring the inter-person generalization performance of our gesture recognition system.
To this end, we follow a "leave-persons-out" protocol: for each instruction modality we remove a set of people from the full data set  to obtain the minimum test set that contains performances of all gestures.
The remaining larger set constitute the training set.
After training on this set the generalization performance is assessed on the people in the test set.
This is repeated ten times for fixed disjoint sets of test persons.
The average test performance over the ten runs is a good estimator of the generalization performance of the system trained on this instruction modality.
We perform two separate experiments as follows.
We assess the intra-modality generalization performance: training and testing using the same instruction modality.
Hence we take only those sequences for training and testing that originate from performances with the respective instructions.
As results we obtain five F-scores, one for each modality, and each being an average over all 10 repetitions and 12 gestures .
We also report separately the F-scores achieved on the first-person-shooter gestures  and the music player gestures .
We assess the inter-modality generalization performance: training on one modality, for example text, but testing on a different modality, for example videos.
We evaluate all possible training-testing combinations where training and testing modalities differ.
As a results we report there sets of 5-by-5 average F-scores, one for each gesture set: all , music , and first-person-shooter .
The five rows correspond to the training modalities, and the five columns correspond to the testing modalities.
If for a gesture class a  A we have p:t    , we fire the gesture as being detected at the current time t. We use a fixed value of  = 0.16 for all experiments.
This value has been determined from previous runs.
Our goal is to learn a set of M decision trees that classify the action points in these sequences correctly by means of .
We use the standard information gain criterion and training procedure .
Hence, we greedily select a split function f for each node in each decision tree from a set of randomly generated proposal split functions.
The tree is grown until the node is pure, that is, all training samples assigned to that node have the same label.
A series of paired t-tests with a Bonferroni correction showed that Video+Text was better understood than Text and Images+Text, Video was better than Images and Text, and Images+Text was better than just Text .
In addition, a one-way within subjects ANOVA on Understanding of iconic gestures showed that the means are significantly different between the five conditions  = 3.439, p.01.
A series of paired t-tests with a Bonferroni correction showed that Video+Text was better understood than Images and Text, Images+Text was better understood than Text, and Videos were better understood than Text .
A series of 2x2 repeated measures ANOVAs on Understanding further showed that the means are different for Text vs. No Text and Static vs.
Dynamic for All Gestures  and metaphoric gestures , but there were no significant differences to report for the iconic gestures .
Thus, like the F-score analysis, we see that the modality of Video  yielded a better understanding of what was to be performed.
This analysis is corroborated by a review of the interviews.
Participants generally related that the videos were the clearest and one knew exactly what to do.
In addition, many of the participants appreciated that the addition of text specified exactly what was the important aspect of the gesture for the system recognition.
However, participants also explained that the videos were not as necessary for the iconic gestures since they felt they could understand what was being requested of them from previous experiences.
Tukey's HSD post-hoc analyses revealed that Video alone was more effective than Text alone, and Video+Text was more effective than Text alone, Images alone, or Images+ Text .
In addition, for the metaphoric gestures, there was a significant difference between the five condition means, F=9.643, p.01.
A post-hoc analysis revealed that Video alone was more effective than Text or Images and Video+Text was more effective than Text, Images, or Images+Text .
However, for the iconic gestures, there was no significant difference between the five condition means.
Thus, although the instructions' semiotic modality made a difference in F-scores for the metaphoric gestures, it made no difference in the F-scores for the iconic gestures.
A series of 2x2 ANOVAs between Static vs.
However, there was no difference for the iconic gestures.
Thus, Video alone and Video+Text were better than the other semiotic modalities in terms of achieving correctness in performing the metaphoric gestures.
However, Video alone was statistically just as effective as Video+Text.
On the other hand, there is no discernible difference between the instructions' semiotic modality for the iconic gestures.
In the analysis of the questionnaire data on Understanding the instructions, a series of one-way within subjects ANOVAs were performed on All gestures, metaphoric gestures, and iconic gestures.
A series of paired t-tests with a Bonferroni correction confirmed this in that it showed that Video alone was better understood than Images or Text; Images+Text was better understood than Images or Text; and Video+Text was better understood than Images or Text .
Thus, we present the analyses from the F-scores averaged across the five testing modalities.
For All gestures, a one-way, between subjects ANOVA showed a significant difference between the five condition means, F=7.327, p.01 .
Tukey's HSD post-hoc analyses revealed that training on Videos+Text was more effective than Text alone , Images ,
In addition, for the metaphoric gestures, a oneway, between subjects ANOVA showed a significant difference between the five condition means, F=6.604, p.01 .
Tukey's HSD post-hoc analyses revealed that Video+Text was more effective than Video , Images , or Text  and Images+Text was more effective than Videos alone .
However, for the iconic gestures, there was no significant difference between the five condition means .
If we look at each testing modality separately we see that some training modalities are better than others for covering the variation.
For instance, for All gestures, testing on a data set instructed through Text or Images, we found that Images+Text yielded a significantly better result .
However, testing on a data set instructed on Videos, we found that Videos+Text yielded a significantly better results .
Thus, we see that in terms of capturing natural variation less information was optimal.
In the analysis of the questionnaire data, a one-way within subjects ANOVA on Freedom Questions showed that the means are significantly different between the five conditions  = 5.390, p.01 .
A series of paired t-
Although not a strong difference between instruction methods, the general trend is that as more information was provided, the sense of inhibition lowered.
But the instructions' semiotic modality with the most information, Video+Text, yielded a slightly less sense of freedom than Images+Text.
Freedom Questions for the metaphoric gestures showed that the means are significantly different between the five conditions  = 3.469, p=.013 and a series of paired t-tests with a Bonferroni correction showed that Images+Text are significantly less inhibiting than Text .
Again, not a very strong difference, but Images+ Text provide a sense of more freedom than the other semiotic modalities.
Finally, Freedom Questions for iconic gestures showed that the means are the same between the five conditions.
However, if one looks at the graph, again, we can see the trend is for Images+Text to be providing a greater sense of freedom.
A 2x2 repeated measures ANOVA on Freedom for All, metaphoric, and iconic gestures showed that the means are the same for Text vs. No Text Added, Static vs.
Dynamic, and the interaction between dynamicity and text-added.
From the interviews, we start to understand what was occurring: Participants asked about Freedom were referring less to an ability to improvise than to not feeling apprehensive or embarrassed from potentially misunderstanding or disagreeing with what was being asked of them.
In this latter case feeling more free would retract undesirable coverage from awkward gestural performances and may even instill confidence that encourages more controlled improvisation.
When the discussion turned to doing the gesture as they saw fit to give better coverage, then they generally agreed that less information allowed this more effectively for metaphoric gestures.
This is primarily as a reaction to the videos, which prescribed exactly what to do whereas other mechanisms were seen as being more open for interpretation.
So I wasn't thinking of any creativity at all."
I tried to match the violent movement you had.
Whereas for the ... in the text , the gesture I had for the slide was sort of clear but I could still do it my way and I felt confident."
It breaks you in your freedom and text is more free ..." However, in the overall ranking analysis, Video followed by Text was preferred on average by the participants with respect to both Understanding and Freedom, as shown in Figure 9.
Unfortunately, we can not speak to whether this ranking differs for metaphoric or iconic gestures.
This study aimed to shed light on how the importance of using a particular semiotic modality to instruct participants can play a significant part in the development of gestural interaction applications.
Namely, to understand the robustness of different ways of eliciting movements based on correctness and coverage requirements of a corpora of machine learning training data that must be collected.
Such understanding allows developers to choose a training data collection methodology that suits their needs, without having to empirically justify their chosen kinematics.
We investigated three different semiotic modalities - Descriptive Text, Static Images, and Dynamic Video - as well as combinations of Images with Text and Video with Text.
We have shown that different semiotic modalities of an instruction alter the amount of variation in a set of training performances.
That this factor has not been addressed in any prior literature now questions the precision of all their results to date and future data sets should include this characterisation.
For a robust system, it is necessary to balance accurate recognition with a need to generalise recognition over an unknown population whilst matching the degree of flexibility the application designer's gestural definitions' allow.
Our analysis revealed considerations for developers on how and when instructions' semiotic modality makes a difference.
Intra-modality F-scores show Video  is best for correctness and Understanding is promoted most by Video + Text.
Inter-modality F-scores show Image + Text is best for coverage and it also gives the strongest feeling of Freedom.
We also learned a bit regarding sequences of instructions.
Overall, Video followed by Text was preferred in ranking, but this was only for All Gestures.
In addition, multimodal instructions are favored over unimodal, which is supported by .
Although we still think the two-step process yielded interesting results, we also realize that the learning method of enaction may also have had an effect on the improvements under both combinations .
In other word, the process of enacting the gesture the first time provided a learning advantage to enacting the gesture the second time despite our lack of giving feedback as to the correctness of the first gestural enactment.
In addition to these general findings we learned more regarding the difference in effect of modality on different types of gestures.
Few significant differences existed for the Iconic gestures for which people possessed a priori associations of kinematics.
Using Video+Text modalities promotes correctness but Images+Text does seem to be a good modality for coverage.
The ranking questionnaire allows us further investigation of the best recommendation by forcing performers to give a preference over modalities whilst recalling their performances of all gestures.
Taking the median of all questions relating to understanding or all questions relating to Freedom and then taking the median of the rankings of modalities over all participants gives a final, non-gesture specific ranking of modalities .
For both Understanding and to a less extent for Freedom, Video+Text is preferred.
One reason for this could be the appeasing nature of the participants to accept being told what to do and their lack of a desire or ability to improvise.
Additionally, the participants suggested providing a two tier instruction, no doubt as an outcome of the way we ran the study.
Participants experienced, for example, Text alone followed by Video+Text or Images alone and then with added Text for the same gesture.
They felt their performances changed with the addition of a modality but were divided over which should come first during the interviews.
Some preferred initial room for interpretation, others preferred constraints to be relaxed.
Correctness and coverage of performances of metaphoric gestures whose semanticskinematics mapping had to be 'taught' benefited from being able to read the description as well as see an example.
However, our results reflect our societies current conventions based on a moderate number of participants with wide demographics.
They may not be applicable in 20 years.
We also saw evidence of a performer's tolerance of ambiguity as playing a roll.
Those with initially high inhibitions could be more apprehensive and effort should be made to lower them for good coverage; this is encouraged by  who discourages using mirrors.
It may be sensible to let the performer choose the order of a sequential, multimodal instruction.
This would be an area of further investigation, though.
Aggarwal, J., and Ryoo, M. Human activity analysis: A review.
Toward a theory of instruction.
Belknap Press of Harvard University Press, 1966.
Teach me to dance: Exploring player experience and performance in full body dance games.
Fothergill, S., Harle, R., and Holden, S. Modelling the model athlete : Automatic coaching of rowing technique.
In Structural, Syntactic, and Statistical Pattern Recognition, vol.
Furui, S., Nakamura, M., Ichiba, T., and Iwano, K. Why is the recognition of spontaneous speech so hard?
In Text, Speech and Dialogue, V. Matouek, P. Mautner, and T. Pavelka, Eds., vol.
3658 of Lecture Notes in Computer Science.
Gorelick, L., Blank, M., Shechtman, E., Irani, M., and Basri, R. Actions as space-time shapes.
Guest, A. H. Labanotation, or, Kinetography Laban: The System of Analyzing and Recording Movements.
Hwang, B.-W., K. S., and Lee, S.-W. A full-body gesture database for automatic gesture recognition.
Reading Images: Grammar of Visual Design.
Kuehne, H., J. H. G. E. P. T., and Serre, T. HMDB: a large video database for human motion recognition.
In Proceedings of the International Conference on Computer Vision  .
Learning realistic human actions from movies.
Lin, Z., Jiang, Z., and Davis, L. S. Recognizing actions by shape-motion prototype trees.
