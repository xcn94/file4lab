To maintain a mobile form factor, the screen real estate of a mobile device canIn this paper we present SurfacePhone; a novel configuration of a projector phone which aligns the projector to project onto a physical surface to allow tabletop-like interaction in a mobile setup.
The projection is created behind the upright standing phone and is touch and gesture-enabled.
Multiple projections can be merged to create shared spaces for multi-user collaboration.
We investigate this new setup, starting with the concept that we evaluated with a concept prototype.
Furthermore we present our technical prototype, a mobile phone case with integrated projector that allows for the aforementioned interaction.
We discuss its technical requirements and evaluate the accuracy of interaction in a second user study.
We conclude with lessons learned and design guidelines.
This creates demand for bigger displays which inevitably leads to increased form factors.
To overcome this problem, an increasing number of devices focuses on multidisplay solutions.
Besides devices based on multiple physical displays such as , projector phones set out to enable the exploration of large-scale content and support collaboration in a mobile setting.
In such mobile multi-display environments  the information is displayed across different screens, that are often spatially seperated and not in the same field of view.
While a MMDE consisting of multiple physical displays easily allows for the extension of the screen real estate, it is still limited by the maximum size of the device.
A MMDE consisting of a combination of physical and projected displays, however, allows an increase in screen estate to a much higher degree while still keeping the small form-factor, and can be used for collaboration and sharing.
However the display setup of current projector phones such as the Samsung Galaxy Beam is often characterized by two displays being visually separated.
This setup precludes many of the prevalent sharing and collaboration techniques that are well known and investigated for example in today's tabletop systems.
In contrast, our setup allows to recreate such tabletop-like interactions in mobile scenarios with a private and a public display.
In this paper we present SurfacePhone, a novel configuration of a MMDE that consists of a physical and a projected display.
It is able to project a second display right behind itself, while it is standing on a surface .
The investigated setup allows for collaboration and sharing as well as advanced single-user interactions.
The projected display is touch- and gesture-enabled and additionally orientation aware.
This allows connecting multiple projected displays into one combined display .
In this paper we present the design process of the SurfacePhone.
Starting with the considerations for such a system and the envi-
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The initial concept prototype allowed us to easily evaluate the concepts and ideas.
The technical mobile phone case prototype  was developed to show and evaluate the technical feasibility of the SurfacePhone concept.
We conclude with design guidelines based on our study results.
There has been an ever increasing interest in mobile projection interfaces in the last five years .
Initial investigation of the advantage that a mobile phone with integrated projector can deliver has been done by Hang et al.
All these systems demonstrate that pico projectors integrated into a mobile device allow for an increased display size and that the explored applications and interaction benefit from it significantly.
Therefore the SurfacePhone integrates a mobile projector instead of a secondary physical display.
This allows to maintain a small form factor while generating a significantly bigger secondary display.
MMDE devices have been not only explored in research , there are also commercially available devices such as the Nintendo DS or the Kyocera Echo.
With Codex , Hinckley et al.
Besides, they also explored different application scenarios for different configurations as well as novel interaction techniques.
Nevertheless the display arrangement of the here presented SurfacePhone has not been mentioned nor explored in the related work.
Even though the SurfacePhone also makes use of a similar configuration, one of its main aspects is to explore collaboration on a potentially mutual interactive surface with multiple devices.
As a consequence, we present new sharing techniques between displays and devices and dynamic merging of their projections which lead to new public/private scenarios.
Furthermore, the SurfacePhone improves on touch accuracy and portability and for the first time, evaluates touch and gesture recognition which could not have been carried out before.
The ad-hoc combination of multiple mobile devices is another possibility .
For instance, the work of Lyons et al.
While there is evidence that different spatial arrangement of displays can have an impact in a mobile setting , it has been ignored in most MMDE investigations .
A large body of literature also investigated partially mobile configurations with one fixed display and one mobile .
Those configurations mainly have been employed to create a personal display which interacts with a shared public display .
In those settings the orientation of the displays to each other can be fluently changed since they are not coupled.
The SurfacePhone takes a very similar approach, it combines a public and personal display that keep their spatial alignments.
The most related work to SurfacePhone is PlayAnywhere .
The design of the SurfacePhone concept encompasses the position of the projected surface in relation to the phone, the position and orientation of one SurfacePhone to other SurfacePhones in the environment, and the modalities to interact with screen and projected display in either scenarios.
Further, we distinguish between single device/single user , single device/multi user , and multi device/multi user  scenario groups.
The projection in front of the mobile device would resemble the laptop posture or the display configuration, it is a very private setup.
This is because the projection is mainly visible to the user facing the device.
In such a configuration the projection could show a soft keyboard.
A projection to either sides of the phone would imitate the setup of Bonfire , where the projected surface is still within easy reach of the user, but more public than in the laptop scenario.
These two configurations have been explored intensively but a projection behind an upright standing phone has been neglected so far.
This setup - the SurfacePhone - consists of a public projected display and a private display  and presents a more collaboration oriented setup.
To some extent, it resembles the Battleship setup of Codex , albeit the difference that the primary user is able to see both the phone display and the projected display.
In this setup, there is a clear separation between the private phone and the public projection that is visible and within reach to people in the near vicinity.
This comes at the expense of a slightly more difficult interaction with the projection as the user has to circumvent the phone to touch the projection.
Additionally, this MMDE setup is in line with the findings of Cauchard et al.
When the user is sitting in front of the upright standing phone, the phone's display as well as the projection are in the same field of view.
This allows the SurfacePhone to split the information between these two displays without risking visual separation effects.
Details of our technical prototype can be found in section Implementation.
But to give an idea of the size and position of the projection early-on: through experimentation we found an optimal  projection behind the phone to be around 17cm x 14cm in size, 14cm behind the phone and 4cm to the left of the center of the device.
The projection, thus, is three times as big as the 4" screen of the iPhone 5.
Figure 2: SDMU:  One user is presenting pictures to another user.
MDMU:  Sitting next to each other on the same side with the projections merged at the short side.
This configuration can be used, for instance, to overcome the fat-finger problem on mobile devices by outsourcing e.g.
Apart from that, the projected display could be used as general secondary display, for instance, showing a task manager or notifications of applications currently running on the device.
Finally, phone screens are very useful for augmenting the reality of the user, but cannot serve publicly visible augmentation.
The projection on the other hand could be used to augment a real playboard with projected tokens.
For example it could project chess tokens on a real board to play against the computer or a human opponent.
On the opposite, sitting FACE TO FACE  merging the long side of the projections is the most distant setup.
It suits users unfamiliar with each other, as well as competing opponents in a game for instance.
In both cases, users have private interaction on their mobile display, using it to selectively share content on the projected surface.
Also, the own projected display is likely not within easy reach of other parties making it more personal for each user.
Sitting face to face, but at the same time NEXT TO each other  combines properties of both aforementioned setups.
In this setup, users keep their private view on their mobile screens, but expose their projected surface to be easily reachable by the other party.
Therefore, the setup particularly emphasizes familiar use cases of interactive surfaces, encouraging participants to manipulate all objects on the surface.
Two users may also sit round the corner of the table which is in general equivalent to the previous case, but allows more easily to come round and take a look on the other user's private display when both users desire so.
Finally, groups > 2 merge projections at ARBITRARY SIDES in their center.
Obviously, no general rule for the visibility of phone screens or reachability of projections can be determined.
However, like people do when playing games involving hand cards, users can arrange to ensure the required visibility and privacy.
Leveraging the inherent differences in publicity of the displays, the SurfacePhone can be used for several sharing tasks in small groups .
For instance, the projection of the phone can be used to present pictures or slides to a small group of people.
The screen of the SurfacePhone can be used to browse the content and decide which content should be shown on the projection.
Advantages of using SurfacePhone in this scenario include that users do not have to give out their phone to other people; that the content can be presented to all people simultaneously; and that only specific pictures or slides for presentation can be selected to address time or privacy constraints.
Finally, the projection can also be touch- or gesture enabled, giving the viewers the possibility to interact with the pictures or slides.
Similarly, the setup is also suitable for games such as blackjack: The person playing the bank controls the game from the screen.
Other players sit in front of the projection and use touch interaction.
Finally, when more than one user brings their SurfacePhone to the table, projections can be merged at different sides forming larger shared surfaces.
These can be used for collaboration, e.g.
Depending on the scenario and the familiarity of the participants, different setups support different degrees of collaboration.
Sitting next to each other on the SAME SIDE  is the most intimate setup as both the projections as well as the phone screens are visible to both users.
This setup, for example, could be useful to collaboratively search for holiday trips.
Users can first explore offers on their personal devices, then share it to the surface.
Being able to also see other users'
In the following we will discuss required interaction techniques for the SurfacePhone that suit aforementioned application and usage scenarios.
Here we draw from users' experience and familiarity with smartphones and tabletop systems.
The technical feasibility of the here described techniques will be addressed in the implementation section of the technical prototype.
With today's prevalence of multi-touch interaction users would expect to be able to interact with the projected content using direct touch.
This includes long touches and doubletouches, to allow for a richer input set through different touch modalities.
Furthermore, gestures like directional swipes are common on tabletops and should be supported as well.
As the phone camera is watching the scene from above, mid-air gestures above the projection could also be considered.
Another interesting space of interaction lies around the projection.
Similarly, gestures that cross the edges of the projection could be supported, for example, to move content to another user's projection that is currently not merged.
As the SurfacePhone is a mobile device, movement of the device can be measured using the built-in motion sensors and the optical flow of the camera's video stream.
The projection could, for example, be changed from showing displayfixed content that moves with the device to showing a dynamic peephole into world-fixed content.
Any table could thus become the personal virtual desktop that is explorable by moving the SurfacePhone like a flashlight over the table.
A regularly occurring task when using the SurfacePhone is to transfer content from the screen to the projection and viceversa.
Following on  we can distinguish between three main categories of transfer-techniques that can be supported: direct, binned, and mediated transfer.
For this category we propose to use Human Link.
The user touches the content that they want to transfer on the phone and then, simultaneously or in quick succession, touches the point in the projection where they want to place it or vice versa .
For instance, to place a whole word in the scrabble game, users can position the letters on the bench  on their phone screen in correct order and then transfer them altogether by swiping over the target positions on the projection .
Similarly, users could select pictures on their phone to a bin and then fan them out on the projection with a finger swipe.
To transfer a object simply drag&drop it on the proxy .
The hardware setup consists of a Samsung PixelSense table running Microsoft's Windows 7 and Surface SDK; further two HTC HD 7 running Windows Phone 7.5 which offer a stand to arrange the phone on a table more easily.
Markers placed below the phones allow them to be tracked by the table.
Our software framework creates a 23cm x 18.5cm sized virtual projection 9cm behind and 3cm to the left of the phone.
This size exceeds the projection size that is supported by our technical prototype by 33%.
As phone manufacturers surely are able to build devices that support projections of these dimensions by using short-throw lenses or curved mirrors, we assume the projection size fits a realistic usage scenario.
The devices communicate over Wi-Fi.
As soon as phones are moved such that projections intersect, a merged projection is created.
This merged projection can either be a graphically highlighted union of the individual projections, or something different like a shared playboard within the concave hull of the projections' corners.
The "escape" game represents the SDSU category by supporting external controls on the projection in a single-user game.
The task of the game is to escape monsters by moving the character horizontally and vertically on a play field without other obstacles.
When playing the game on the mobile phone, the on-screen controls and finger of the user cover parts of the play field on the phone.
By "outsourcing" the controls to the projection behind the phone, thus providing free sight on the whole play field, we assume users will perform better in the projected mode .
In this application the SurfacePhone is used to present pictures or slides to a small group of people in two different ways: Either the user publishes thumbnails to the projection by dragging the thumbnail on the proxy at the top of the phone screen.
The audience can then use standard multi-touch techniques for rotating and enlarging the pictures to their will .
The exemplary picture sharing - which would similarly work with other content types - is very similar to the SDMU presentation application.
Users publish their thumbnails to the surface by using the proxy or Human Link techniques as in the presentation application.
As soon as more than one device and user merge their projections by intersecting them, the merged space can be used to share all sorts of personal data.
Thumbnails then belong to the joint surface, allowing all participants to explore pictures through multitouch operations and transfer them to their phone using one of the aforementioned techniques.
When one of the participating users withdraws from the merged state the merged view is split and the separate projections retain prior items and positions on their side.
If items have not been moved to the phone, these items are moved back to the projection of the owner.
This feature shall give users a simple means of privacy control as they can withdraw with items that they only want to present but not give away.
We follow a qualitative approach using think aloud, structured interviews, and video analysis as no similar system is available for comparison.
We had 16 participants who took part in pairs to create a more realistic collaborative environment.
Their average age was 26 years,  and six of them were female.
All participants except one owned a mobile touchscreen phone and three of the participants had prior experience with multi-touch tables.
First we explained the concept of the SurfacePhone by showing them a concept design  of the technical prototype and to convince them that similar devices can be built we demonstrated a Samsung Galaxy Beam projector phone.
Finally, the experimenter briefly explained the prototype, how it works, and the different configurations  which also represented the different phases of the study.
After that, both participants tried all four applications  for approximately eight minutes each.
Before each application participants were given time to test the concepts relevant in that phase, for instance, merging of projections and different transfer techniques, until they had no further questions.
In single-device applications they took turns in acting as user or audience/spectator.
In multi-device applications both users operated their own device simultaneously.
To ensure a constant learning curve, the order of applications was always the same, going from single-device and single-user to multidevice and multi-user applications, thereby constantly gaining in complexity.
Before each multi-user application, users were allowed to choose device positions  that fit the task according to their opinion.
For the study the participants had to use all aforementioned applications.
For the picture presentation applications  both participants acted as presenter and observer in turns.
For the picture presentation in MDMU mode we added two tasks.
One task was to share pictures that contained Waldo with your partner and the other was to solve a 3x3 puzzle collaboratively on the merged projection space.
While participants were continuously motivated to share their experiences aloud, after each configuration  they filled out a questionnaire regarding the configuration and contained tasks.
The questionnaire asked for experience with the applications as well as physical demand, fatigue, visibility of content, feelings regarding privacy, etc.
The scrabble application  particularly emphasizes the private display on the mobile phones.
It shows a standard scrabble playboard on the merged projections.
The phone screens show the letters available to the users and a virtual bench on the bottom where words can be arranged with the letters using drag&drop.
On their turn, users either use the Human Link technique to place any letter, no matter if on the bench or not, by touching the letter and the target position on the playboard.
Alternatively, they first put the letters to place in correct order on the bench and then swipe over the empty fields on the board to place these letters.
Depending on whose turn currently is, the board changes its orientation to face the corresponding user.
Letters can be taken back to a precise position on the phone using Human Link or to a random position by double tapping them.
With the first user study we assess the quality of the overall SurfacePhone concept and its several components using the presented concept prototype.
In terms of preferred interaction technique, to transfer information between the two screens, most participants favored touch-swipe.
Comparing touch-swipe and Human Link in the scrabble game, 15 of the 16 participants preferred touchswipe.
When comparing touch-swipe, Human Link and proxy, nine participants preferred touch-swipe and five would rather use the proxy technique.
This is also reflected in the physical demand.
Ten participants stated that Human Link has the highest physical demand and four found proxy to have the highest demand.
The same was reflected in their rating of success, 12 participants said they were most successful with the touch-swipe and two thought they would be better with either Human Link or proxy.
In their comments the poor performance of Human Link became pretty obvious.
A majority  stated that when using the Human Link they had the problem that when they touched the phone's screen and tried to touch the projection the phone would slide away.
This was especially seen as an advantage of touch-swipe.
We asked the participants whether they developed a strategy to solve the puzzle and image tasks.
All participants agreed that they followed a certain strategy.
From the video analysis and the comments two strategies were particular promising.
Three couples would actually change their original sitting position so that they would sit next to each other, allowing both participants to view each others phone displays, helping them to identify the correct pictures before putting them on the surface.
Three other couples divided the work between in each other so that one participant would move the pictures from the phone to the projection and the other would arrange the puzzle parts in the projection.
To evaluate a possible adoption of such a device we asked the participants whether they would recommend such a device to their friends and if so what would be the necessary circumstances.
All but one participant answered that they would recommend it.
Most of the participants found fulfillment of hardware constrains such as reasonable size and battery life to be mandatory.
Privacy was desired by all participants as a needed feature and most of them thought that presenting pictures by selecting them on the phone screen and have the projection show them is more reasonable than the other way round.
Ten participants stated that they like that they don't need to pass their device around when presenting to groups.
The preferred combination of devices and users was MDMU.
Participants found the possibilities that arise from having a mobile device that ad-hoc can create a complex mobile multidisplay environment very attracting.
Besides games such as Battle Ships, Poker and Black Jack the collaborative editing of documents, e.g.
Two participants mentioned the case of ad-hoc meetings for example to collaboratively investigate construction plans on a construction site.
The results of the PSSUQ are underlining these results.
The positive results of the first study motivated us to build a technical SurfacePhone prototype that can support aforementioned interactions.
Our aim with this prototype is twofold: Firstly, we want to investigate the technical requirements and challenges for such a device and find solutions for them.
Secondly, we want to conduct another user study that delivers quantitative measures how well touches and gestures can be performed by users and detected by the system.
Additionally we want to provide everything needed to create a SurfacePhone prototype to the community.
From this we hope to spark further discussions.
As no similar device configuration has been presented so far, we started from scratch.
We chose the iPhone platform, since it was the only mobile platform that allowed two different outputs on screen and projection at that time.
We solved the problem by attaching a mirror to the top of the phone and the projector to its bottom.
This way the distance from projector to surface is more than doubled and sufficient to create a projection much larger than the phone screen.
Our prototype consists of an iPhone 5 plus projector case as depicted in Figure 5a.
Both projector and mirror are 4cm to the right of the iPhone camera which is the minimum distance required for projector and mirror not to appear in the wide-angle view of the phone camera.
Obviously, specifically the resolution in Y direction is quite small and the projection is not centred in the image.
Nonetheless, this is the best compromise we found between maximizing the size of the projection and still completely seeing the projection in the camera.
Following our previous design considerations, the prototype should support direct touch on the projection, different touch modalities, gestures, and tracking of other nearby SurfacePhones.
The software of the SurfacePhone is implemented in Objective-C and C++ on iOS with the help of OpenCV and openFrameworks modules.
First we calibrate intrinsic and extrinsic camera parameters using printed chessboard and projected chessboard patterns respectively.
Having the parameters we can map the projected area from object space to an interpolated orthogonal view of the projected region  and use this for tracking.
In a final SurfacePhone device this would only have to be performed once.
Figure 5: Design and instantiation of the SurfacePhone prototype that tracks finger touches using in-built camera and accelerometer.
The red border  in  is the relic of perspective counter distortion.
The phone shows raw camera image , background-image  and finger tracking with green dot at recognized fingertip .
For the tracking to work robustly at arbitrary locations we must make sure that different lighting conditions are handled.
We can let the iPhone automatically adjust exposure and focus of the camera to the center of the image to adapt to different conditions.
However, we need the user's finger for a correct estimation.
Therefore, in step 3, we ask the user to present their finger for 2 seconds to the center of the camera while we lock correct exposure and focus for future interaction.
In step 4, we capture a still frame for subsequent background subtraction.
As the background of interaction can be arbitrary we use background subtraction to separate moving fingers from the background.
This step is automatically performed whenever the device comes to rest on a plain surface.
Since we constantly measure the accelerometer at 100Hz we can quickly recognize whenever the user starts and stops moving the device.
Our following tracking process runs at 22 FPS.
To eliminate shadows as best as possible we first convert the camera image to the HSV space and then work on the saturation channel.
The literature recommends working on the hue channel to eliminate shadows, but we found that desk colors are often very similar to skin colors which is why we use the saturation channel that works more reliably in our scenario.
After background subtraction we find blobs using openCV's contour finding algorithm.
Because of the steep camera angle in our setup, blob area sizes can range from a few pixels to sizes that fill half of the image.
This makes the classification of correct blobs more difficult.
Further, we cannot rely on standard CV techniques like convexity defects for finding fingertips as often there is only one finger plus parts of the thumb in our image which do not provide the defects information.
Instead, the algorithm we developed first computes the convex hull of the contour and its normalized approximation.
Then, for each point on this new contour that has a tangential slope of less than 15 with its surrounding points, it calculates a probability that this point is the fingertip by minimizing equation:
A finger should create a very rectangular bounding box where the fingertip lies almost at the center of the smaller side of the rectangle yielding small distances to the two closest corner points; * where DCP is the summed distances between the point and corner points of the bounding box that lie on or outside of the edge of the camera frame.
A correct fingertip of a pointing finger will always have maximum distance to the hand center.
As we do not see the hand the corner point is only an average guess; * where SA is the estimated area of the fingertip above the current point.
This is calculated as the sum of distances between up to 15 surrounding points on the contour to both sides.
Correct fingertips should yield smaller results than arbitrary shapes with peak endpoints; * where AREA is the size of the blob; * and where WSCD and WDCP are weights found by experiment set to 10 and 5.
Finally we have to decide which blob represents the primary finger, which one is a possible second finger and which ones are not of interest.
Our blob sorting and filtering algorithm favors blobs with fingers, high finger probabilities, less circular shape  and lower Y position .
This approach seems the most promising as the hardware is readily available in most  phones - in contrast to e.g.
However, their camera angle was almost orthogonal which simplified finger tracking.
Further, they used the accelerometer of a laptop that shares a much larger space with the surface than our prototype and therefore allowed to work with simple thresholds.
Correctly classifying touches with the SurfacePhone is thus bigger challenge.
Furthermore, when relying on surface vibrations for touch detection we cannot distinguish between touch down and up events to, for example, classify a touch as "long touch".
Similarly, double taps cannot be recognized as successive events since the second vibration could overlay the first one.
We can, however, recognize the intensity of the touch quite well since light and strong touches create distinct vibration patterns .
These two modalities, light and strong, can, for instance, be used to start dragging of an item using a strong touch.
For the touch detection we measure the current acceleration in X direction every 10ms.
Then we compute the touch vibration as the difference between the averaged sum of the absolute amplitudes of the recent 150ms  and the previously calibrated sensor noise.
Based on thresholds we then decide whether the measured vibration corresponds to a strong, a light, or no touch at all.
The default strong threshold is twice the default light threshold.
As not all surfaces transport vibrations equally, we also implemented a detection procedure that vibrates the SurfacePhone with a constant pulse and measures the resulting phone vibration.
Through this procedure we can adjust the default thresholds to increase touch accuracy on different tables.
In the first part of the study, we asses multi-modal finger touch.
The projection shows circles the user has to touch.
Additionally, each target exists once for light and once for strong touch, marked with a big L or S .
Thus, in total there are 3 x 12 x 2 = 72 different targets split across three circle groups.
After a test round, each participant performs 2 successive study rounds .
The order of circle size sets is counterbalanced and the display order of targets randomized.
Participants can take as much time as they need to perform the touch.
In the second part of the study participants performed the gesture  that was written on the projection.
Again, users performed one test and two study rounds of 4x5 gestures in random order .
Since in our setup we only see small parts of the user's hand, gesture support of hand postures does not make much sense.
However, we can well recognize gestures that are based on a trajectory of movement such as directional gestures  or more complex gestures like a circle.
Finger trajectories that do not end in touches are simply analyzed for long directional movements or otherwise handed to the 1$ gesture recognizer by Wobbrock et al.
Overall, 93% of user touches were recognized , 71% of these were hit with the right intensity and 77% of targets at the right position .
Furthermore, we measured that clearly misclassified fingertips  have been responsible for about 12% of false position recognition.
Factorial repeated-measures ANOVA on the touch data reported significant main effects of circle radius, target position in Y direction, but not target intensity at the p < .05 level  on positional accuracy.
Posthoc analysis using Bonferroni corrected pairwise comparison of means revealed significant differences  between small and middle and small and large sized circles as well as target heights 140px/340px and 240px/340px.
Thus, the larger targets have been and the farer they have been away from the device, the better they have been hit in terms of position.
Touch intensity recognition is statistically independent of both target position and circle radius.
Left and right gestures yielded recognition rates around 90%, down and circle gestures around 80%.
Only the up gesture performed significantly worse than all others with only 43% .
Although not fully integrated into our prototype yet, we evaluated the use of Qualcomm's Vuforia on the SurfacePhone using projected frame- and image markers.
Our interest was to see how the steep camera angle and the much lower resolution of the projected image compared to printed markers affects the recognition algorithm.
Fortunately, the recognition worked better than expected.
Frame markers of a size of 1/8 of the projection size are perfectly recognized as soon as they are completely visible in the scene.
Image targets are recognized even up to a 1/50 of the projection .
Through our explorations and studies we learned the following lessons and concluded guidelines for the SurfacePhone and its interface design: 1.
In our study all participants cherished having the private display.
Therefore, applications designed for the SurfacePhone should always consider its proper usage in innately not very private mobile scenarios.
Where possible, interaction techniques should be used that do not require simultaneous interaction on both displays.
Besides the visual separation, though small, the user has no hand available to keep the phone in place.
The proxy was seen as an advantage in the SDMU case where the screens are divided between the users and nobody wanted to intervene on the display of the other.
In multi-device scenarios with merged and thus larger displays, transfer techniques that support precise placement  should be favored.
Interface elements placed to the top left side of the projected display will not be affected by the fat-finger problem.
Specific to our implementation, interactive elements should also have a radius of at least 50pixel to ensure a good accuracy.
Users should receive feedback about their touch intensity  to support their mental touch model.
Further, touch thresholds should be personally adaptable to account for anatomic differences.
Multi-modal touch decreases the accuracy of touch recognition as sometimes users touch too light while they try to keep the intensity below the threshold for strong touches.
Thus multi-modal touch should be disabled whenever the interface gets by with single touch plus gestures to increase the accuracy to at least 93%.
The proposed automatic calibration of touch thresholds only makes sense up to the physical limits of surface vibrations.
For thicker surfaces this especially means that light and strong touch thresholds move closer together, possibly resulting in more falsely recognized strong touches.
The system should account for overshooting with an offset function to increase accuracy of finger position tracking.
44% of up gestures with the circle gesture.
This may be due to the fact that after performing the correct up gesture participants moved their hand down and to the right to their default position, which the recognizer that evaluates the gesture after the hand has left the frame may have misinterpreted.
Figure 8 depicts all performed touches except for the far outliers .
From the touch distribution and their mean marked by the crossing of the fit lines we can conclude that the target position indeed has an effect on over/undershooting in both X and Y direction.
Targets to the lower-right are more overshot than targets to the upper left.
However, overshooting is only compensated for farer targets without transforming into obvious undershooting as on touchscreens .
We assume the reason for this is that due to the steep viewing angle of the user on the projection the fat-finger problem only exists close to the device and decreases with increasing distance from the device.
Also, perspective misjudgment may counter-balance overshooting.
The issue of overshooting may thus also explain the significant effect of Y direction on touch accuracy mentioned before.
Regarding personal experiences, all participants thought that the device is already usable in many scenarios but maybe not for tasks like text entry .
Similarly, 10 participants stated that the difference between light and strong touches was difficult to learn, maintain , or to perform.
For three female users the threshold for strong touch was set too high, for two male users rather too low.
Overall, light touches have been slightly but significantly better  recognized than strong touches .
Overall, the results of the exploration and study of the second prototype reveal that a working SurfacePhone with the features  used in the first study can be built with today's mobile phone hardware.
In this paper we presented the SurfacePhone, a novel configuration of a physical display and a projector that are aligned to allow ad-hoc Tabletop interaction in a mobile setup.
We explored its design space and identified new singleand multi-user application scenarios with tailored interaction techniques.
Our evaluation using our concept prototype indicates that the device is suited for a variety of everyday scenarios.
We also learned which setups and transfer techniques users preferred in different scenarios.
Later we demonstrated how the SurfacePhone can be built with only today's commodity phone hardware and the help of a specialized case and state-of-the art techniques for finger tracking and multi-modal touch recognition.
Overall, we can conclude that the SurfacePhone makes for a new and interesting device for ad-hoc collaboration.
In this work we concentrated on re-creating Tabletop scenarios with personal mobile data.
However, there is a large potential of mobile scenarios that include movement of devices on or off the table as interactions.
For instance, could the phone projection be used as peephole on a large desktop in joint learning scenarios, or games could involve tasks of which some can better be performed while holding the phone in hand and others better while placing the phone on the table.
We leave this for future work, likewise further improvements to our technical prototype.
The SurfacePhone software, STL print files of the hardware, and assembly instructions are available for download http://uulm.de?SurfacePhone.
