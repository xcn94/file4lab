However, studies aiming to correlate performance or responses on different tasks rely on meaningful data from each participant.
Rather than aggregating noisy data, an alterative strategy would be to develop a reliable method of screening participants to remove the subset of those gaming the system.
In this paper we discuss a screening process used in conjunction with a survey administered via Amazon.com's Mechanical Turk.
We sought an easily implementable method to disqualify those people who participate but don't take the study tasks seriously.
By using two previously pilot tested screening questions, we identified 764 of 1,962 people who did not answer conscientiously.
Young men seem to be most likely to fail the qualification task.
Those that are professionals, students, and non-workers seem to be more likely to take the task seriously than financial workers, hourly workers, and other workers.
Men over 30 and women were more likely to answer seriously.
Turkers tend to be younger, female, and lower-income than the average Internet user .
Payments on mTurk are suggested to follow a reasonable hourly rate, with an example of $8 per hour or about 13 per minute .
In practice, many mTurk tasks pay much less overall, with the median study paying just 5-10 for a task taking "a few minutes," like watching and providing feedback on 3 short  videos, summarizing a website, and evaluating hypothetical and real market products.
Indeed, "wages" this low have been shown to result in lower quality output than could be had for no payment at all, by pure volunteers .
Studies using mTurk generate user data quickly and at a low cost, but special consideration needs to go into creating the study materials.
As guidance, Kittur and his colleagues provide a set of recommendations for mTurk users to maximize the usefulness of their data , some of which will be necessarily limited to certain kinds of tasks.
For example, tasks could be designed so that a good-faith effort requires similar or less effort than random responding.
However, merely reading the material is a non-negligible amount of work for many surveys.
The temptation to choose a convenient response from a multiple-choice set, or to type a superficially appropriate response in a text box, requires considerably less effort than any good-faith response.
Thus, other strategies may be effective at identifying respondents not acting in good faith.
Kittur also highlights the need to include items that can be explicitly verified, both to identify appropriate responses and to indicate to the user that responses will be scrutinized .
Such items might be mixed in with other items, as in a social desirability scale, to serve as an external indicator that can vouchsafe the trustworthiness of less evaluable responses.
Indeed, multiple indicators of suspicious performance are recommended, such as time spent on task, and responses to different types of questions.
Amazon.com's Mechanical Turk  is an online marketplace where individuals can perform very small tasks for micro payments, making it an attractive market for researchers to run studies quickly and cheaply through crowdsourcing .
Crowdsourcing allows many people to participate with minimal recruitment and administration costs .
However, the cash payouts, anonymity and lack of participant accountability may entice people to complete as many tasks as possible without fully engaging in them.
If arbitrary clicking pays as well as thoughtful participation, some people may attempt to maximize their profits while minimizing their effort.
Indeed, a paper on foreignlanguage translations found that prolific mTurk users, known as Turkers, performed barely above chance .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We will discuss the proposed reorganization of the Human Resources department to better serve the faculty and staff at BRU.
During this conference call, we will also discuss the decisions reached at the 11am meeting of the University Benefits department.
It is critical that all attendees of the University Benefits department, especially those who attended the morning meeting, also attend this conference call, to ensure that necessary recommendations of this committee are incorporated into our procedural changes.
Details for the conference call are listed below.
Also, please confirm your participation via email to me.
Date: Thursday, May 14 Time: 2:00 PM  Number: 1-800-555-1200 8533123  Thanks, Ginger Holmes Administrative Coordinator Recruiting and Staffing Baton Rouge University www.bru.edu Easy question: Who is the email message sent to?
Such transparent tools only catch the most egregious of participants, violate Gricean norms by requiring careful attention to normally predictable information, and set a tone of distrust for the remainder of the task.
In contrast, our screening task was designed to appear as a formality, following the logic of the study task.
It included 2 questions of varying difficulty, to assess whether there were differential predictors of adhering to strict versus weak criteria.
The hallmark of both screening questions was to have a conspicuous distractor.
Questions were piloted and refined prior to the current study.
Participants were asked about demographics  and office work , followed by two qualification questions referring to an email message detailing an upcoming teleconference .
The intention was to convey that demographic background or prior participation with teleconferencing may be a prerequisite for qualification, but there was no indication what kind of answers would disqualify.
One qualifying question was relatively easy, and could be answered correctly by simply reading the full question and looking up the answer in the email recipient line, with a conspicuous distractor corresponding to the sender's name.
The more difficult question required not only interpretation of the question but a close reading of the email text, as the answer was in the body of the email.
In this case, the conspicuous distractor corresponded to a piece of information in the signature line of the email.
Those looking quickly for answers would likely be inclined to select these distractors.
An mTurk HIT  was posted for a 30-minute task  with a $4 payment, contingent upon qualification, and 20 for those not qualifying.
This relatively large payment is roughly equivalent to the federal minimum wage, and corresponds to Amazon's suggested pay rate , although it is larger than typical.
We chose to pay a more equitable rate both for fair compensation for participants' time and to appeal to a broad array of groups.
Fortyfive respondents  answered the demographics but neglected to answer both qualifying questions; their responses are included but considered incorrect.
Reliability between the two questions was moderate, with 95% of those answering the difficult question correctly also answering the easy one correctly, 2=184.43, p=.001.
Only 1,266  answered the difficult question correctly, with an equivalent skip rate to the easier question  but far more respondents answering incorrectly .
Older participants were more likely to qualify than younger ones.
Professionals  and students  were more likely to answer the difficult question correctly compared to hourly workers , financial workers  and other occupations .
Those who were retired or not working were in between, not significantly different from any of the other groups .
A similar pattern emerged with the easy question, although fewer comparisons were significant perhaps due to ceiling effects.
For the easy question alone, there was a slight trend toward an interaction between gender and occupation, F= 1.77, p=.12.
As Figure 2 shows, this appears to reflect very small gender differences for nonworking and financial workers, but large differences for hourly and other workers.
We explored the use of time stamps as a mechanism to identify participants who are clicking quickly rather than conscientiously to see if a simple assessment of time on task might predict performance on the qualifying questions.
On average, participants spent 2 minutes  completing the task, ranging from 4 to 1,548 seconds, with a standard deviation of 119 seconds.
Setting a speed threshold at the 90th percentile , we find that those who qualified were only slightly more likely to spend more than 45 seconds on the task  compared to those who didn't qualify .
Time on task was a better predictor of the easy question than of overall qualification, with only 76% of non-qualifiers taking longer than the 90th percentile threshold of 45 seconds , but still the majority of those spending very little time on task would have qualified and thus appear to be answering the questions conscientiously.
Although time on task did not differentiate qualifiers from non-qualifiers very well, there was a mean difference between the groups, with non-qualifiers completing the task about 20 seconds more quickly than qualifiers, t=3.39, p<.001.
Unfortunately, the considerable variance in both groups prevents time on task from being a reliable tool to differentiate these groups.
We speculate that variability in computer load time and mouse maneuvering adds enough noise to overwhelm meaningful differences in cognitive processing time.
Furthermore, people trying to game the system might not be lightening fast in their clicks, but rather could be acting distractedly, perhaps while doing something else simultaneously.
In contrast, some conscientious participants may have quick computer response times and near-instant mousing or tabbing behavior.
These data suggest that a threshold for time on task may not adequately identify non-conscientious participants, and may inadvertently disqualify many others.
Further piloting of similar kinds of qualifying questions would be informative.
The usefulness of easy versus difficult questions has not been sufficiently explored by this study alone.
Future work could attempt to create similar conditions that make lack of conscientiousness profitable in the lab, and determine whether performance on easy vs. hard questions is predictive of later performance.
Additional studies could also identify parameters establishing optimal question design.
One risk of moving forward with a cookie-cutter approach is that if many mTurk studies start using a similar, predictable design, then devious subjects might take to strategically answering screening questions, but not real survey questions.
A system of embedding periodic screening questions could remedy that problem.
Indeed, such an approach might have the optimal outcome of encouraging Turkers to complete tasks more conscientiously, rather than merely screening out those who don't.
Some respondents may be participating in mTurk studies for quick cash rather than inherent interest, and may not be inclined to answer conscientiously.
This screening tool provides a preliminary description of how people answer an easy and a difficult question, and who is likely to perform poorly.
The easy question was a proxy for answering arbitrarily without even a cursory attempt to respond to content.
The difficult question was a proxy for careful participation.
No special knowledge or skill was required to answer either, just a willingness to do the task as presented.
Getting either wrong is an indication that the participant may have been attempting to "game" the mTurk system.
Young men seem to be most likely to try to game the system, with fewer than half of men younger than 25 qualifying by getting both questions right.
Men over 30 and women of any age were much more likely to qualify.
Professionals, students, and non-workers seem to be the most likely to take the task seriously.
It's possible that they tend to do mTurk tasks for the inherent interest and distraction, whereas hourly and financial workers may be trying to earn quick money while working at their normal jobs.
Although the hourly rate offered by mTurk is small, if augmenting another income  it can be a nontrivial source of additional money.
The gender difference seems particularly strong among hourly workers, but cannot be explained by age or by differences between administrative and service jobs.
Thus, a strategy to discourage younger people from participating might only need to focus on those who are not students.
