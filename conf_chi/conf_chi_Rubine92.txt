A gesture, as the term is used here, is a handmade mark used to give a command to a computer.
The attributes of the gesture  can be mapped to parameters of the command.
An operation, operands, and parameters can all be communicated simultaneously with a single, intuitive, easily drawn gesture.
This makes gesturing an attractive interaction teehnique.
There is no opportunity for the interactive manipulation of parameters in the presence of application drag operations indirect feedback that is typical of interfaces.
The user drags around the latter endpoint  until the mouse button is released.
The other creation gestures work similarly, utilizing the starting point of the gesture as one parameter and interactive dragging to determine another.
The gestures that operate on existing objects  work similarly.
The copy gesture begins on an object, After it is recognized  a copy of the object appears and maybe dragged.
The start of the rotate-scale gesture determines the object to be rotated as after it is recognized the user rotating and scaling well as the center of rotation; the object.
All the gestures in GDP are single strokes.
Thus the physical tension and relaxation of the interaction correlates nicely with the mental tension and relaxation involved in performing a primitive task in the application.
For Though not shown in the video, it is possible to tie other gestural attributes to application example, one version uses the length of the create-line gesture to determine the thickness of the resulting line and the orientation of the create-rectangle gesture to determine the initial orientation of the rectangle.
GDP is a mouse-based drawing program that utilizes the two-phase interaction to create, copy, move, rotate, state, delete, and group lines, rectangles, ellipses, and text.
For example, the create-line gesture consists of positioning the mouse, pressing the button and making the gesture .
The mouse must remain still for an interval of time  before recognition occurs.
In some contexts, forcing the user to stop is awkward.
It seems desirable to have the gesture recognized as soon as enough of it has been seen to do so unambiguously.
This is the goal of eager recognition.
Thus the user would begin the turn the knob, the knob-turning gesture would be recognized, and then the knob would start to turn.
No stopping or other explicit indication of the end of the gesture is necessary, What begins as a gesture smoothly becomes a direct-manipulation interaction, l%e second example shows GDP with eager recognition enabled.
Since the create-rectangle gesture, an `L', is the only expected gesture that begins with a downward stroke, it is recognized almost immediately, a rectangle created, and direct manipulation of its comer begins.
The two-phase interaction may be used with multi-finger input.
The video shows MDP, a version of the drawing program that uses a Sensor Frame as a multi-finger input device.
The mouse gestures of GDP me mapped to singlefinger gestures in MDP.
After the gesture is recognized, additional fingers may be brought into the sensing plane of the Sensor Frame to control additional parameters.
For example, after recognizing create-line, the first finger rubberbands one endpoint of the new line , and additional fingers control the line's color and thickness.
Multiple finger gestures are also recognized.
The training of the undo gesture is shown, and later the use of undo is demonstrated.
The two-phase interaction allows the amount of "undoing" to be determined interactively after the gesture is recognized.
Also interesting is the two finger rOkttescale-translate gesture.
After recognition, each of the two fingers attaches to a point on the object.
The line gesture, a straight segment, is not eagerly recognized.
This is because the system recognizes that the gesture in progress may also be a move gesture, an arrow drawn with a single stroke.
The rotate-scale and delete gestures are eagerly recognized.
GSCORE has separate gestures for whole notes, half notes, quarter notes, eighth notes and sixteenth notes .
While it would be possible to have a single note gesture and then interactively control the duration and stem direction of the note, having separate gestures appears to result in faster interactions.
It is possible to edit the set of gestures and their meanings  to try out various interfaces, so the two approaches maybe compared.
The video shows how anew gesture is added to GSCORE.
Adding a new gesture involves pressing the "new class" button and then entering fifteen examples of the new gesture.
There is a click-anddrag interface to an Objective-C interpreter through which the semantics of the gesture are specified.
The new gesture may be tried immediately.
Although not shown, it is possible to evaluate the new classifier by testing all the training examples.
Eager recognition, a technique for smoothing the transition between the gesture and the direct manipulation phases of the interaction, is atso shown.
While the video demonstrates the potential of combining gesture and direct manipulation, if this potential can be user testing is needed to determine reatized,
