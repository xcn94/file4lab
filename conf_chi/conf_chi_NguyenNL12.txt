This paper presents Video Summagator , a volume-based interface for video summarization and navigation.
VS models a video as a space-time cube and visualizes the video cube using real-time volume rendering techniques.
VS empowers a user to interactively manipulate the video cube.
We show that VS can quickly summarize both the static and dynamic video content by visualizing the space-time information in 3D.
We demonstrate that VS enables a user to quickly look into the video cube, understand the content, and navigate to the content of interest.
VS allows a user to look into the video cube, summarize the video, quickly identify the content of interest and navigate to it.
The main idea behind VS is to visualize a time sequence in 3D.
VS is inspired by recent research that summarizes a video clip into a 2D image .
VS extends the 2D display space of these methods to 3D, which provides more visualization space and better conveys space-time video content.
It also extends these methods with interactive video manipulation and visualization, and handles a wider variety of camera and object motion.
An important aspect of VS is direct manipulation for video browsing inspired by .
But VS is different from these methods because it visualizes the whole video, enables a user to manipulate the entire video cube, and then navigate to the interesting part by clicking the corresponding region in the cube.
Video capturing and displaying devices are ubiquitous now.
Capturing, sharing, and watching videos has become a common practice.
Meanwhile, the human video interaction tools, mainly video players, remain almost unchanged in user interface design.
A typical video player consists of a display window to show a video one frame at a time and a slider to navigate through the video.
While such a player prevails, it is often inconvenient for video browsing.
It cannot fully and quickly convey important aspects of the video content.
This paper describes Video Summagator , a 3D volumebased interface for video summarization and navigation.
VS considers a video as a cube, with time as the third dimension, as shown in Figure 1.
It employs real-time volume rendering techniques to quickly visualize the cube in a 3D volume .
VS seamlessly combines video summarization and navigation.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
A rich literature exists on video summarization that selects a key frame sequence or key segments from an input video as a concise video representation.
A good review can be found in .
Our work is particularly relevant to video summarization methods that represent a video clip as a still picture like panorama and action synopsis .
These methods can create high-quality pictures; however, they are typically time-consuming.
Compared to these methods, our work models a video as a cube and uses real-time volume rendering techniques to show the video cube in 3D.
Our method supports interactive video cube manipulation and navigation.
Our work builds on video cubism research that models a video as a space-time volume and manipulates a cut surface through the volume to display the video content .
This method has also been extended for non-photorealistic rendering .
Unlike these methods, our method visualizes the whole video volume instead of the cut surface to more concisely summarize the video.
Our method also allows interactive volume manipulation to visualize complex motion.
Bennett and McMillan extend video cubism for convenient space-time video editing .
Their method does not work for visualization.
Daniel and Chen visualize a video in a transparent cube or other pre-defined volume .
This technique has been extended to visualize human activities .
Our work extends it to support interactive video cube manipulation to facilitate video summarization and browsing.
Our work is relevant to research on video player design.
Adaptive fast-forwarding is designed for quick video browsing with predefined semantic rules .
The video speed and playing speed are decoupled and content analysis is adopted to play interesting shots at an intelligible speed .
Besides the player bar, maps have also been used for browsing domain-oriented videos, such as lecture videos  and tour videos .
Our work is inspired by direct manipulation that allows users to browse videos by directly dragging the video content and frees them from the player bar .
Unlike these methods, our method enables a user to manipulate the whole video cube and navigate to the interesting part by clicking the corresponding region in the cube.
Our work is also relevant to video collection browsing in 3D that reconstructs the 3D scene and uses video-based rendering to create novel views .
Our method is different in that it does not require 3D scene reconstruction although it visualizes the video content in 3D.
Our 3D visualization typically does not correspond to the 3D scene structure.
VS supports a user to globally shear the video cube to create a panoramic summarization .
The user can also use a spline interface to deform the cube to better arrange the video content in the 3D space.
For the example in , the user define three control points  to define a new spline to deform the cube.
A voxel takes its color from the corresponding pixel.
Its coordinates and opacity are controlled by a user for video summarization and navigation, as described in the next subsection.
With the volume features, VS uses the state-of-the-art volume rendering techniques from the Visualization Toolkit 1 to quickly render the video cube.
Two common types of user interaction with videos are quickly skimming through a video for overview and slowly navigating in interesting segments for detail .
We develop Video Summagator , a 3D volume-based interface, to support both video summarization and navigation.
Below we first describe how we model and render a video as a cube , with time as the third dimension.
We then describe how VS supports a user to interactively manipulate the video cube to visualize video content.
We finally describe exemplary scenarios of using VS for summarization and navigation.
A user interacts with VS by controlling three features of a video cube: opacity, shape, and video frame sampling.
The voxel opacity value ranges from 0 to 1, with 0 being fully transparent and 1 fully opaque.
VS uses off-the-shelf computer vision algorithms to detect dynamic voxels and assigns higher opacity values to them than the stationary ones to clearly convey the video dynamics.
VS allows a user to control the opacity values of the dynamic, static, and boundary voxels using a slider interface.
VS supports a user to translate, rotate, scale, and deform a video cube to clearly visualize its content.
These user interactions are mapped to the coordinates of the voxels.
We use the standard 3D interface for translation, rotation and scaling.
One is to shear the cube globally, as shown in Figure 2 .
We consider a video as a cube V that contains a set of voxels: V = {vx,y,t |1  x  w, 1  y  h, 1  t  n}  where voxel vx,y,t corresponds to pixel  at frame t. w and h are the video frame width and height, and n is the number of video frames.
Static camera with moving objects.
Figure 3 shows a street show video captured by an almost static camera.
Dynamic voxels are assigned bigger opacity values than the background ones to reveal the magician's activity.
For this video, since the camera is almost static and the magician moves in a small region, looking into the video cube suffers from selfocclusion, as shown in Figure 3 .
This problem can be simply solved by rotating the cube, as shown in Figure 3 .
For , the opacity values for the left, back, and bottom cube faces are set to a big value to better visualize the background scene.
Consider a video with the camera panning horizontally.
VS enables a user to easily create a panorama by shearing the video cube, as shown in Figure 4 .
For this application, the opacity values for all the voxels are uniformly set to 1.
If the video contains a moving object, VS then creates an action synopsis that visually depicts the object activity, as shown in Figure 4 .
For this example, the moving objects are automatically separated horizontally as the cube is being sheared.
The opacity value for the back and right cube face is set to 1 to visualize the background.
Consider a video with the camera following the athletes.
VS supports a user to first create a mosaic image by shearing the cube globally.
Then, the user refines the result with local adjustments using the skeleton-based interface, as shown in Figure 2 .
Since VS provides online visual feedback, this can be done quickly.
For this example, the opacity values for the voxels on the key frames are set to 0.9 and the others are set to 0.
The other is skeleton-based local shearing transformation.
We define a video skeleton as a spline that is perpendicular to each video frame.
For the original cube, its skeleton is a line parallel to the t axis.
We provide a spline-based interface for adjusting the skeleton to deform the video cube and create the output volume.
As shown in Figure 2 , a user can create control points and move them to define a new skeleton.
VS uses two methods to select key frames, as rendering all the video frames will lead to a cluttered visualization.
The first is frame-based uniform sampling .
A user can control the sampling rate through a slider-based interface.
Uniform sampling is used to create all the examples except Figure 5.
The second is importance sampling, which is more applicable for a long video.
We consider that the probability of a frame being a key frame is proportional to the amount of foreground motion.
Then, we select a sequence of key frames according to this distribution .
VS allows a user to get a quick overview of a video typically with two steps: adjust the opacity values of the stationary and dynamic pixels to see the activities and rotate the video cube if there exists self-occlusion.
Since VS renders the cube at an interactive speed, these two steps together take less than 5 seconds, which is typically shorter than the video length.
With a preview of the video, the summarization can be further improved by adjusting the cube parameters.
VS also provides an intuitive interface for video navigation.
VS associates the  time axis with a 3D volume-based summarization.
With the overview of the video, a user can quickly navigate to the interesting video segment by clicking and selecting the corresponding area in the summarization.
For example, given a surveillance video, a user rotates the whole video cube so that the evolution of video content shows clearly in the display window.
Looking into the video cube, the user spots the region in the cube with human activity and selects this region by drawing a line indicating the region of interest, as shown in Figure 5.
VS maps the selected region to the time axis by projection and automatically navigates to the corresponding video segment.
To handle a long video, we down-sample the video frame size and adaptively sample the video frames using importance sampling.
On a desktop machine with 6G memory and AMD Phenom II X6 2.8 GHz CPU, VS currently can allow a user to interactively manipulate a video cube with as many as 3000 key frames with frame size 480 x 270.
Figure 5 shows a video cube that samples 1500 frames from an input video with totally 10000 frames.
This paper described Video Summagator, a 3D volume-based interface for interactive video summarization and navigation.
We model a video as a space-time cube and visualize the video cube in a 3D volume using volume rendering techniques.
We show that Video Summagator supports a user to manipulate the video cube to quickly summarize a video, identify the content of interest, and navigate to it.
Currently, we use off-the-shelf computer vision algorithms for moving object detection, which is sometimes not very reliable and causes visual artifacts, as shown in Figure 1.
The advance in computer vision research can benefit our results.
In future, we plan to extend VS to handle a streaming video such as webcam videos.
We also plan to integrate more video analysis to better handle complex videos.
Our visualization is not necessarily physically correct, so we plan to design user study to more thoroughly evaluate how people perceive the video content from our summarization.
