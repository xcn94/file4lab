A number of experimental studies based on domain-specific tasks have evaluated the efficiency of navigation techniques for searching multi-scale worlds.
The discrepancies among their results call for a more generic framework similar in spirit to Fitts' reciprocal pointing task, but adapted to a task that significantly differs from pure pointing.
We introduce such a framework based on an abstract task and evaluate how four multi-scale navigation techniques perform in one particular multi-scale world configuration.
Experimental findings indicate that, in this context, pan & zoom combined with an overview is the most efficient technique of all four, and that focus + context techniques perform better than classical pan & zoom.
We relate these findings to more realistic situations, discuss their applicability, and how the framework can be used to cover a broad range of situations.
The usability studies that relied on domain-specific tasks such as searching for items on geographical maps , comparing hierarchical data structures , or reading textual documents  have typically produced inconclusive and sometimes contradictory results.
More precisely, experimental findings varied from experiment to experiment, but since application domains varied dramatically, neither these findings can be compared nor generalized.
Such results are to be expected since the performance of a given technique is indeed dependent on its context of use .
A better understanding of the fundamental aspects of multiscale navigation could help explain - or even predict - such results, therefore saving valuable time and allowing better exploration of novel techniques.
Fitts' pointing paradigm provides such a fundamental tool for exploring and understanding the elementary task of reaching a known target as fast as possible.
Originally devised to study pointing in the real world , it has been used repeatedly in HCI for evaluating a variety of pointing techniques and devices .
Fitts' law has proven remarkably robust, to the point of being used as part of an ISO standard for pointing devices .
Fitts' pointing task has also been used with multi-scale interfaces and it has been shown that Fitts' law still applies for pointing targets with pan & zoom .
In particular, it has been shown that Fitts' paradigm could address navigation, not just pointing, in interfaces that require scrolling  or zooming .
While Fitts' pointing paradigm is very powerful, it models a very specific task: that of reaching a target whose location is known to the user.
However, this scenario only captures one of several navigation tasks in multi-scale worlds.
Users might only have partial information about the target's location and appearance, thus requiring them to search for potential targets and get more details about each one until the actual target is identified.
Consider for example a user searching for Brisbane on a multi-scale world map, only knowing that it is a large Australian city.
The strategy first consists in zooming towards Australia to then inspect each large city one by one, zooming in to discover that it is not the right one, zooming out, maybe as far as the whole continent, and zooming in to the next potential target until the right city is found.
Exploring large spaces in search of a particular target differs from pure pointing, as it requires users to perform additional motor actions to identify the target.
Multi-scale interfaces  have generated a growing interest over the past decade as a powerful way of representing, navigating and manipulating large sets of data.
A number of multi-scale navigation techniques have been designed and implemented, ranging from the original pan & zoom  to various focus+context techniques .
Up until now, the efficiency of these techniques has been evaluated with two kinds of experimental studies: usability studies based on domainspecific tasks and controlled experiments based on multiscale versions of Fitts' pointing paradigm.
In the same way as Fitts' reciprocal pointing task operationalizes the task of reaching a known target, we propose in this paper to operationalize the above search task in a way that is easily amenable to controlled experiments.
We then evaluate how four multi-scale navigation techniques perform in one particular configuration of a multi-scale world: classical pan & zoom, overview + detail, and two focus + context techniques, namely distortion  lenses and a variation on the DragMag image magnifier which, to the best of our knowledge, has not yet been evaluated.
Our results indicate that in this context overview + detail outperforms the other three, and that the two focus + context techniques outperform classical pan & zoom.
However, this multi-scale world configuration is only one particular case in a range of situations.
We discuss the limits of this preliminary study, describe the design space that is covered by our abstract search task and present an environment that we have developed to help explore this design space.
The pointing task is a well-known example in the field of HCI, initially operationalized in psychology by Fitts : to study the performance of pointing techniques, researchers act on the index of difficulty  variable and measure the movement time on a reciprocal pointing task.
We seek to operationalize multi-scale searching in a similar way.
In a multi-scale world, users navigate and look at objects until they find the target.
Users have to navigate in both space and scale to a position that reveals enough details about each object, in order to decide whether it is the target or a distractor.
Initially, users make a blind choice of a "potential target" at a high scale and navigate to it to acquire enough information.
If it is a distractor, they have to navigate to another object, typically by zooming-out, panning, then zooming-in .
Since we are interested in studying the performance in time and error rate to find a target according to the required "quantity" of exploration from a purely motor perspective, we abstract the representation from any semantic or topological relationship among objects that could help participants identify the target in an uncontrolled manner .
To quantify exploration, our experimental setup consists of a multi-scale world containing a set of n objects, one of them being the target and the others distractors.
We define the "quantity" of exploration as the number k of distractors that users have to visit before finding the target: the larger the number of visited distractors, the larger the quantity of exploration.
We control this parameter by forcing participants to visit a predefined number of objects before finding the target; if we chose a priori which object is the target, participants could find it immediately by chance, or on the contrary they could spend a lot of time searching for it, and this uncontrolled factor would have a significant impact on our measurements.
We design our experiment to ensure that the target is the k th object visited, no matter the order of exploration chosen by each participant.
Making the system aware of these two pieces of information in a fully reliable manner requires answering the following two questions: * What minimal scale provides enough information?
This depends on visual acuity, which is user-dependent.
Assuming that the user visually scans the whole screen is too strong an hypothesis, and probably an unfounded one.
Also, if only part of an object is in the viewport, the system cannot know for sure whether or not the user has seen it.
A number of experimental studies have compared the performance of different multi-scale navigation techniques and have reported contrasted results.
Classical pan & zoom was compared with fisheye and overview + detail on high-level cognitive tasks involving electronic documents : writing an essay after having explored a document, and finding answers to questions within that document.
Classical pan & zoom was the least efficient technique; participants read faster using the fisheye; they wrote better essays using the overview + detail, but took more time to answer questions.
In North and Shneiderman's experiment , participants had to browse the database of U.S. states and counties to answer questions using a detail-only scrollable interface or an overview + detail interface.
The overview + detail interface outperformed the detail-only interface by 30-80% depending on the task.
However, in another study , pan & zoom or overview + detail were not significantly different when participants had to navigate a large node-link representation and make topological comparisons.
On the contrary, Hornbaeck et al.
This, however, was true for only one of the two geographical maps that participants had to explore in their experiment.
The findings of these experiments show that the use of domain-dependent tasks makes it difficult to get consistent results that can be generalized, even more so when they require a significant amount of cognitive effort from the participants.
Identifying and isolating lower-level, domain-independent tasks can help reach more generalizable results.
From a motor perspective, one recurring task performed by users of multi-scale interfaces is to search for targets among sets of objects by navigating through space and scale.
This article describes an experimental setup for the controlled evaluation of various interaction techniques considered as appropriate for the task of searching a multi-scale world.
We address these problems as follows.
First, we set a minimum scale  at which the user can collect enough information to detect a target: all objects seem identical except for the target, which reveals a different piece of information when displayed at or above minScale.
In order to avoid differences among participants, all objects are displayed identically at all scales until the user explicitly asks to reveal the disambiguating piece of information.
This explicit "unveiling" action is available only when the scale is minScale or more.
Second, we make sure that the user cannot reveal several objects simultaneously.
Once an object has been revealed, the user has to process the information and, provided that the object is the target, take an additional explicit action to tell the system that this object is the target.
While we cannot be sure that participants actually look at targets when unveiling them, it is in their own interest to do so in order to perform the search task as fast as possible.
Therefore we believe that this design operationalizes a realistic search task without having to use more complex devices such as eye trackers.
Before presenting a first experiment based on this task, we introduce the multi-scale navigation techniques that we have tested.
Figure 1 illustrates these techniques using spacescale diagrams .
The first technique is the classical Pan & Zoom .
In order to get more detail about specific elements of the representation, users have to move the entire viewport both in space and scale, respectively by panning and zooming.
No contextual information is provided; this method is therefore prone to user disorientation.
One way to address disorientation consists in using overview + detail techniques.
One such technique, Pan & Zoom + Overview , enhances classical Pan & Zoom by providing users with an inset containing a representation of the region surrounding the area currently seen in the main viewport at a lower scale.
The overview is located inside the main viewport, typically in one of the four corners.
The goal is to minimize the visual interference caused by occlusion of the elements in focus, but this introduces the problem of divided attention .
In overview + detail representations, more screen real-estate is dedicated to the focus than to the context.
Conversely, focus + context techniques allocate more screen real-estate to the context than to the focus.
We selected two techniques that we consider relevant to the multi-scale searching task: constrained distortion lenses  and a variation on the original DragMag Image Magnifier .
Constrained distortion lenses  provide a detailin-context representation through the local magnification of a region of the screen .
This focus region is integrated in the surrounding context by distorting the representation in the transition region.
The distortion is defined by a drop-off function .
We chose a Gaussian profile as it provides a smooth transition between focus and distortion, and between distortion and context.
Our lens also features a flat top because many tasks require the focal region not to be distorted .
The in situ magnification of these lenses solves the problem of divided attention but introduces a distortion that can cause recognition problems.
The DragMag  can be considered a special case of fisheye lens often called Manhattan lens, featuring a perpendicular drop-off function.
There is no distorted region between the focus and the context, but as a result the region immediately surrounding the focus is occluded.
To address this issue, the focus region is translated by a usercontrolled offset .
This results in the occlusion of another region of the context, which is often considered less important than the immediate surroundings of the focus.
However, this reintroduces the problem of divided attention encountered with overview + detail representations, and the occlusion can be more cumbersome to handle than with the overview.
The task consisted in finding a target among a set of objects as quick as possible while minimizing the number of errors.
The virtual scene contained nine light gray squares organized into a 3x3 grid layout and embedded inside a large, darker gray square.
We used a grid layout so participants would easily know where the potential targets were.
A dark red grid was superimposed on the display in order to minimize desert-fog  .
The grid was adaptive to scale, i.e., new grid lines would fade in when zooming in and some grid lines would fade out when zooming out so that the display would always contain a reasonable number of grid lines.
All nine objects had square corners except for the target which had rounded corners.
The rounded corners could only be seen when the target was displayed at a large enough scale, called minScale .
Participants thus had to zoom in onto each square in order to find out whether it was the actual target or not.
Zooming in was not sufficient however: once minScale was reached, a black border was displayed around the square in focus.
Participants could then use the space bar to unveil the object: this would permanently reveal whether the object was the target  or not.
Note that this "unveiling" step does not affect the ecological validity of our task since it penalizes all techniques equally.
Figure 2 shows a storyboard of the task: participants started each trial by pressing a button located at the center of the screen .
The view was initialized so that the region containing potential targets  was not centered on the screen, requiring participants to reach the region by panning and zooming.
The goal was both to better simulate a multi-scale navigation & search task and to avoid a learning effect with respect to the participant's initial move.
Participants had to navigate to that region  and then inspect each object more closely by magnifying it using the current navigation technique .
Participants were allowed to zoom-in further, but zooming in too far would have the object fill the display and make it impossible to find out if it was the target.
Once minScale was reached for an object, participants could unveil that object by pressing the space bar.
The object's border flashed green for 400 milliseconds, informing the participant that the object had actually been unveiled.
If the object's corners remained square, this meant that the object was not the target and participants had to navigate to the next potential target using the current navigation technique.
Note that figures 2-c and 2-d are cropped versions of the viewport, aimed at illustrating the actual display size of objects at minScale on the monitor used for the experiment.
Participants were instructed to go as fast as possible to complete a trial , but they were allowed to rest between successive trials.
They were also instructed to minimize the number of visits to the same object and the number of misses, i.e., hitting F1 when the object was not the target.
Such misses terminated the trial and were counted as errors.
The first independent variable we manipulated in our experiment was the technique.
The first technique was pan & zoom .
Participants could pan the view by moving the mouse while holding the left mouse button, and zoom in/out by rotating the mouse wheel.
These three degrees of freedom could be controlled simultaneously.
The magnification factor per wheel step was tuned so as to get an average zooming speed of 8x per second, as advocated in .
With this technique, participants panned & zoomed the entire view to get enough details about each object.
Each of the other three techniques allowed participants to pan & zoom using the above commands.
The second technique was overview + detail .
The region seen through the main viewport was represented by a bright green rectangle in the inset containing the overview .
This rectangle could be dragged, resulting in changing the content of the main viewport.
With these additional two degrees of freedom, participants could do finegrain panning in the main viewport and coarse-grain panning in the overview.
The representation in the overview was dynamic: it was not necessarily showing all objects in the virtual world, as it followed the camera associated with the detailed view in space and scale when the scale difference between the overview and the detailed view was larger than a factor of 24.
The overview implemented by Google Maps1 demonstrates such a behavior.
The third technique featured a constrained distortion lens, also called graphical fisheye lens .
It allowed for magnification of the region around the mouse cursor .
We used a 100-pixel radial lens defined by a Gaussian drop-off function and the L distance metric  with a 60pixel radius flat top.
The lens was not activated at the start of a trial.
Participants could activate it by clicking the left mouse button, and deactivate it by clicking the right mouse button.
The lens was always centered on the mouse cursor.
When the lens was active, participants were still able to pan the context by dragging outside the lens with the left mouse button.
The default magnification factor within the flat top was set to 4 times the scale factor of the context .
Participants could change the lens' magnification by using the mouse wheel, within the limits of twice and twelve times the scale factor of the context.
This technique therefore featured five degrees of freedom .
Lens activation and deactivation were both animated by smoothly increasing the lens' magnification factor from 1.0 to its default value  over a period of 300 milliseconds for the sake of perceptual continuity .
The lens thus seemed to "emerge" from the flat surface when activated, and flatten itself when deactivated.
The last technique was inspired by the DragMag Image Magnifier , but interaction with the windows differed significantly from the original prototypes .
Figure 4-c shows the two windows composing the DragMag: the mag window outlines the region magnified in the zoom window.
Participants could activate and deactivate the DragMag by clicking on the right mouse button.
The mag window would then appear centered around the mouse cursor, the zoom window being offset by a default distance of 200 pixels to the southeast of the mag window.
As with the previous technique, both DragMag activation and deactivation were smoothly animated over a period of 300 milliseconds, with the zoom window "emerging" from the mag window.
Participants could drag the mag region, thus changing the content of the zoom window; they could also drag "through" the zoom window for small scale adjustments, though this feature was not very useful in the context of the experiment.
Participants could also move the zoom window by dragging the thick bar at its top.
This feature was useful to reveal objects occluded by the zoom window.
The mouse wheel was used to control magnification.
Operating the mouse wheel while the cursor was in the zoom window controlled that window's magnification factor.
Operating the mouse wheel anywhere outside this window controlled the scale of the context.
The technique therefore featured six degrees of freedom .
The default magnification factor in the zoom window was 4 times the scale factor of the context, as for the distortion lens.
The zoom window was not resizable.
For the purpose of comparing the techniques, the overview of OD, the lens of FL, and the zoom window of DM all used the same amount of screen real-estate: a 200 x 200 pixels region, which represented 4.5% of the total available display area.
Before starting, the experimenter checked that they could perceive the rounded corners at minScale, showing them squares with squared and rounded corners successively.
The experiment was divided into four blocks, one block per technique.
Before each block, participants were shown how to achieve the task using the corresponding technique.
They were then asked to practice on randomly-chosen trials until they felt comfortable with the technique.
The experimenter observed participants and encouraged them to keep practicing until they were familiar enough with the technique.
Our predictions were as follows: * Time is linearly dependent on the rank k of the target.
We hypothesized that, whatever the technique, the user has to navigate to inspect objects one by one and that each navigation incurs the same cost.
Since the cost of revisiting an object is fairly high, we hypothesized that the number of revisits would be very small.
Therefore the overall task completion time should be linearly dependent on the "quantity" of exploration, i.e., the target's rank k in the sequence of visited objects.
With PZ, navigating from one object to the next typically consists in zooming out to acquire the next object then zooming in and panning to magnify it.
With DM, FL and OD, it simply consists in moving the focus onto the next object.
Since the position of the focus can be controlled from the context, we hypothesized that the zoom-out/pan/zoom-in sequence of PZ would take more time than relocating the focus within the context.
With OD, DM and FL, navigating from object a to object b consists in moving the focus from a to b.
This movement can be seen as a pointing task.
With DM and FL, pointing is achieved by relocating the focus  while with OD, pointing is achieved by relocating the detailed view.
According to Guiard et al.
Since the detailed view is significantly larger than the lens' focus and the DragMag's zoom window, we predicted that OD would outperform the two Focus + Context techniques .
We used a Dell Precision 380 equipped with a 3 GHz Pentium D processor, an NVidia Quadro FX4500 graphics card, a 1280x1024 LCD monitor  and a Dell optical mouse with a scroll wheel.
The application was limited to a 1080x824 window with a black padding of 100 pixels in order to accommodate instruction messages and simulate screen real-estate that would usually be taken by control and information widgets.
Each condition was replicated 3 times so that each participant performed 9  4  3 = 108 trials .
The initial position of the area containing the objects was different for each of these 3 replications and was counterbalanced among blocks with a Latin square.
We grouped the trials into 4 blocks, one block per technique, to minimize negative skill transfers.
To minimize ordering effects, we computed four different technique orders using a Latin square and composed 4 groups of 3 participants , one group per ordering.
We also counterbalanced the presentation order of the different values of k within a block: we used a Latin square to compute 9 possible orders for presenting the values of k and concatenated 3 orders to compose a block .
Three block compositions  were obtained through a Latin square.
We mapped one block composition per participant within a group.
Table 1 summarizes our counterbalancing strategy among participants.
While we told participants that the target was selected randomly by the program, this was not, in fact, the case: instead, the program counted the objects being visited by the participant during the trial, and displayed the target when the k th object was unveiled by the participant.
This allowed us to fully control the rank variable.
Note that even if the participants had known  the actual working of the program, this would not have given them any advantage.
For each trial, the program collected the completion time, whether it was a hit or a miss, the order of visit of each object and the time at which it was unveiled.
It also logged cinematic data from the cameras associated with the focus and context viewports.
We also collected the participants' preferences among the techniques in a post-hoc test.
For our analyses, we first removed 14 miss trials  and then 31 outliers .
We verified that misses and outliers were randomly distributed across participants, techniques and ranks and that there was no effect of technique presentation order on time.
We isolated the rank variable  by analyzing it separately for each technique.
We computed the linear regression of time relative to the rank, treating participants as a random variable.
We obtained the high correlation coefficients listed in Table 2.
This supports our first prediction: completion time is linearly dependent on the rank .
As expected, the number of revisits was extremely low  and participants optimized the order in which they visited the objects so as to minimize traveled distance.
Most participants explored the objects following an S-shaped pattern, some used a spiral; very few made diagonal moves, except for one participant who adopted a very erratic search pattern across all blocks .
Table 2 also reports slopes  and intercepts with the y-axis  for each linear regression.
We note that the value of b is lower for PZ.
The cinematic logs explain this difference: with DM, FL and OD, participants initially spent more time adjusting the scale and position in order to optimize their future interactions.
Since we have evidence that time is linearly dependent on rank, we now analyze rank as a continuous factor.
Analysis of variance with the REML method for repeated measures revealed a significant simple effect on time for rank, i.e.
Figure 5-b illustrates these results: the larger the rank, the larger the differences among techniques.
Tukey post-hoc tests reveal that each technique is significantly different from the others: OD is the most efficient technique, followed by FL, then DM and finally PZ .
This supports our second and third predictions: the Overview + Detail technique outperforms the two Focus + Context techniques, which themselves outperform classical Pan & Zoom.
We believe the lower performance of FL, compared with DM, could be due to the visual distortion introduced by the lens .
We note however that the difference between the means of these two techniques  is much smaller than with the other two , as shown in Figure 5-c.
The subjective preferences we collected in the post-hoc questionnaire match these results.
At the end of the experiment, participants were asked to rank the techniques according to their preference: 11 ranked PZ as the worst technique, and 9 ranked OD as the best technique.
The goal is similar to that of Fitts' pointing paradigm and its use in HCI: to assess the limit performance of searching multi-scale worlds and to come up with predictive performance models and novel navigation techniques that improve multi-scale searching.
Our search task covers a large design space whose main dimensions are the amount of information the user has to acquire in order to decide which object is the target and the structure of the multi-scale world.
Our experiment tested an extreme situation in this design space.
First, the user had to look in detail at each target by navigating to it, therefore excluding the kind of visual search that occurs, e.g., in a Fitts' pointing task with distractors.
Second, we used a specific configuration of the multi-scale world: a "small-world", i.e.
Therefore, the results reported in the previous section cannot be generalized to all search tasks and we need to devise a strategy to explore the design space and operationalize other situations.
Unfortunately, few theoretical models are available to help us structure this design space.
Therefore we have developed an environment for testing re-
This environment  displays a multi-scale version of NASA's Blue Marble Next Generation world map  overlaid with geographical features such as countries, states, cities, parks and lakes.
The geographical features can be any set of localized items found in the Geo-Names3 on-line database.
The environment provides a set of navigation techniques, including those tested in the study reported in this article.
A variant of this environment was used to run the experiment reported in the previous section.
We conducted several pilot studies with this environment using a set of 1825 cities, 63 states and provinces, and 192 countries.
Participants were asked to search for geographical features by locating first the country, then possibly the state or province and finally the city.
Obviously, this task relies on cognitive skills such as the participant's geographical knowledge or contextual hints such as large water bodies.
It was extremely useful however for observing users and collecting quantitative data and subjective evaluations and helped us identify interesting multi-scale world configurations.
For example, the configuration that we tested in the experiment described in the previous section corresponds to, e.g., finding a large city in Australia.
So the task consists in finding a city among a relatively small, well-identified, set of objects of the same relative size.
In this context, participants found the most useful technique to be Overview + Detail, followed by the constrained distortion lens and the DragMag.
For the latter two, the commonly adopted search technique consisted in panning & zooming to make the entire continent fit the viewport , and then activate a lens or DragMag to inspect the potential targets while keeping the context fixed.
The same behavior was observed with the abstract task, as reported earlier.
It is interesting to note however that the negative effects of distortion were less frequently mentioned for the geographical task than for the abstract task, probably because continuous representations such as world maps withstand distortion better than other types of representations, at least for searching.
Other observations of the participants' behavior with the geographical task have helped us identify situations that seem interesting for subsequent experiments.
For instance, densely populated regions such as mainland Southeast Asia , which feature many cities, were most commonly explored with the Overview + Detail technique because the main viewport can accommodate more cities at the scale where their names become readable , thus facilitating visual scanning.
These behavior patterns lead us to hypothesize that Overview + Detail techniques work better when exploring dense regions while Focus + Context techniques are also efficient when searching for a target among a sparse set.
Providing empirical evidence for this claim requires running more experiments within the framework by varying parameters such as density.
Another area for future work is to test configurations in which objects have different minScale values, corresponding to situations where users have very limited information about the target, including the scale at which it is visible.
Since such situations presumably prompt for more zooming actions than the one we tested, it is possible that the best navigation technique would be different.
This paper has introduced a new framework based on an abstract searching task for multi-scale interfaces that operationalizes the situation where one has to look for the target before selecting it.
We have used this framework to compare four multi-scale navigation techniques in the context of one specific multi-scale world configuration , showing that in this case a fixed overview afforded better performance than Focus + Context techniques and that traditional pan-and-zoom was the worst.
These results cannot be immediately generalized to all multi-scale world configurations, and additional evaluations are required to cover a broader range of situations by varying parameters such as density, topology and the relative size of targets.
Our framework allows for the systematic exploration of this design space.
Moreover, the geographical environment we have developed can help identify interesting situations and formulate hypotheses about them.
These situations can then easily be translated into configurations of the abstract task and tested with controlled experiments.
C. Appert, M. Beaudouin-Lafon, and W. Mackay.
Context matters: Evaluating interaction techniques with the CIS model.
OrthoZoom scroller: 1D multi-scale navigation.
Human Factors in Computing Systems, pages 21-30.
S. K. Card, J. D. Mackinlay, and B. Shneiderman, editors.
Readings in information visualization: using vision to think.
M. S. T. Carpendale, D. J. Cowperthwaite, and F. D. Fracchia.
M. S. T. Carpendale and C. Montagnese.
A framework for unifying presentation space.
ACM Symposium on User Interface Software and Technology, pages 61-70.
S. Carpendale, J. Ligh, and E. Pattison.
Achieving higher magnification in context.
ACM symposium on User Interface Software and Technology, pages 71-80.
The information capacity of the human motor system in controlling the amplitude of movement.
G. W. Furnas and B.
Y. Guiard and M. Beaudouin-Lafon.
Target acquisition in multiscale electronic worlds.
Y. Guiard, M. Beaudouin-Lafon, J. Bastin, D. Pasveer, and S. Zhai.
View size and pointing difficulty in multi-scale navigation.
Quantifying degree of goal directedness in document navigation: Application to the evaluation of the perspective-drag technique.
Human Factors in Computing Systems.
Improving focus targeting in interactive fisheye views.
K. Hinckley, E. Cutrell, S. Bathiche, and T. Muss.
Quantitative analysis of scrolling techniques.
Human Factors in Computing Systems, pages 65-72.
B. Bederson, and C. Plaisant.
Navigation patterns and usability of zoomable user interfaces with and without an overview.
K. Hornbaek and E. Frokjaer.
Reading of electronic documents: the usability of linear, fisheye, and overview+detail interfaces.
Critical zones in desert fog: Aids to multiscale navigation.
ACM Symposium on User Interface Software and Technology, pages 97-106, 1998.
D. Nekrasovski, A. Bodnar, J. McGrenere, F. Guimbreti ere, and T. Munzner.
An evaluation of pan & zoom and rubber sheet navigation with and without an overview.
Human Factors in Computing Systems, pages 11-20.
C. North and B. Shneiderman.
Snap-together visualization: a user interface for coordinating visualizations via relational schemata.
K. Perlin and D. Fox.
Pad: an alternative approach to the computer interface.
Computer Graphics and Interactive Techniques, pages 57-64.
A Toolkit for Addressing HCI Issues in Visual Language Environments.
M. Posner and S. Petersen.
The attention system of the human brain.
G. Ramos and R. Balakrishnan.
Zliding: fluid zooming and sliding for high precision parameter manipulation.
ACM symposium on User Interface Software and Technology, pages 143-152.
Information visualization using 3d interactive animation.
R. W. Soukoreff and I. S. MacKenzie.
Towards a standard for pointing device evaluation, perspectives on 27 years of fitts' law research in hci.
R. Stockli, E. Vermote, N. Saleous, R. Simmon, and D. Herring.
The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS.
Published by the NASA Earth Observatory, 2005.
C. Ware and M. Lewis.
In CHI '95 conference companion, Human Factors in Computing Systems, pages 407-408.
