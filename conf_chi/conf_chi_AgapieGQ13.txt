Although longer queries can produce better results for information seeking tasks, people tend to type short queries.
We created an interface designed to encourage people to type longer queries, and evaluated it in two Mechanical Turk experiments.
Results suggest that our interface manipulation may be effective for eliciting longer queries.
Our interface design creates a halo around the query text box that varies in color and size with the length of the query being constructed.
We chose the halo as the feedback mechanism because it is a familiar interface element that is visually unobtrusive and does not compete for attention in text-based query construction tasks.
The initial state of the text box is shown in Figure 1: a soft pink halo with a radius of about 20 pixels surrounds the text box.
As the user starts to type a query, the halo becomes progressively less pink.
After the query reaches a certain minimum length, the interface settles on a cooler, bluish tone .
Keyword queries are a familiar way of representing information needs.
Research literature shows that longer keyword queries are more effective at retrieving useful documents in exploratory search .
Although users formulate more diverse queries when having difficulty finding results , it is well documented that people tend to run short queries .
It is also interesting to note work on shortening long queries to improve precision of search results .
While shortening to improve query clarity  and coherence  may be useful, in many cases longer queries may be desirable either to improve recall or to refine results in topics with many documents.
However, shorter queries become less reliable in situations with insufficient information on the relative utility of relevant documents.
In these more complex search situations, longer queries are more likely to efficiently retrieve the desired information.
We hypothesized that this propensity to create short queries could be mitigated through a novel interaction design that used a halo to reflect the length of the query being constructed.
We hypothesized that a pleasant, affective design  that modifies the visual characteristics of the text input area would nudge people to type more query terms.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In our web-based implementation, the box-shadow CSS property was used to set the color and size of the halo.
The color was interpolated between the two extremes, with queries of seven words or longer showing the bluish color.
While it is trivial to compute the query length in words and to set the halo to the associated color, we thought that a mechanistic application of the function might undermine its persuasive quality.
Instead, we chose to mask the relationship between query length and halo color by animating the change through a variable duration .
We are testing the performance of a new search engine we developed.
To test our search engine we will ask you to use it to answer search puzzles.
Do not use other search engines or resources other than the ones provided by the search engine results to find the answer to the task.
You can only use the search engine we make available to you in the content of the hit.
A-Google-a-day  search puzzles were used to simulate complex information needs.
These puzzles are designed to encourage people to learn how to solve complex information needs, and thus they were good proxies for our experiment.
We selected the following three older puzzles  to reduce the chances of our participants recognizing them: 1.
How many churches were built by the architect of the world's first triple-domed cathedral  after the Great Fire of London?
What tree does a mammal with fingerprints  rely on for food?
What material fuses with lime and soda to create an item on your dinner table that's considered to be an amorphous solid?
The experiment had a 2 x 2 factorial between-subjects design.
The factors were the presence or absence of a halo  on the search box and the presence or absence of a statement  following the experimental instructions telling participants that "our system performs better with longer queries."
Each subject performed three search tasks in random order in a randomly-assigned experimental condition.
Subjects were paid $1.55 through Mechanical Turk upon completion of all tasks.
We restricted participants to be based in the United States and required them to have a 98% or better HIT completion rate.
Because we were interested in queries people created, 91 queries that were copies of task questions or duplicates submitted within a second of each other were excluded from the dataset; analysis of the remaining 451 queries follows.
Our search system included a web-based interface for eliciting queries, for presenting search results, and for collecting the answers to the search tasks .
In addition to reporting the answer to the search question, we wanted people to record where or how they found the information.
The goal was to discourage people from spamming the experiment and from having them fill in the answer as a guess or based on prior knowledge.
Queries were executed using the Bing search API and results were filtered to remove any reference to the terms `google' and `google a day.'
We also discouraged people from running the puzzle question as a query because we wanted to elicit query-formulating activity rather than copyand-paste activity.
Participants were shown the following instructions prior to starting the experiment:
One hundred participants started the experiment, 61 of whom completed it.
Table 1 lists the breakdown of participants by experimental condition, the average number of queries by condition, and the average query length per condition.
It is worth noting that the average query length for this experiment was considerably longer  than those typically reported for web searches , which range from two to four words.
A two-way ANOVA test was conducted to assess if the Halo and Instruction variables affect the word length of queries typed by the participants.
We found a significant main effect of Halo  = 5.1, p < 0.05 indicating that participants type longer queries in the presence of a Halo than in its absence.
This does not provide enough evidence to conclude that the presence of Instructions results in longer queries.
I'm a serious piece of music known for comedy.
I was played by a famous cat.
I've been played by a woodpecker, dueling ducks and two vaudevillian brothers.
There may be 39 signatures on the U.S. Constitution, but there were only 38 signers.
Which state 's absent delegate had his name signed by a colleague?
You are standing in the farthest west U.S. town with a population of one person.
What is the speed limit?
To understand the interaction effect, we performed a Tukey HSD post hoc test.
It showed that the Halo with no instruction condition outperformed all others: its queries had 2.1 words more on average than Halo with instruction , 2.3 more words than the No halo with no instructions condition , and finally, 1.2 words more than the No halo with instructions .
These results further strengthen the conclusion from the main effect that a halo results in longer queries.
We also found that the No halo, Instruction condition on average had 1.1 more words than the No Halo, No instruction condition .
This result suggests that when no other factor is involved, people will enter longer queries with textual instructions.
Our experiment had a one-factor between-subjects design.
The factor had four levels: red fading to blue halo , blue fading to red halo , static blue halo , and no halo .
Each subject performed three search tasks in random order in a randomly-assigned experimental condition.
Subjects were paid $1.03 through Mechanical Turk upon completion of all tasks.
We restricted participants to be based in the United States and required them to have a 98% or better HIT completion rate.
Ninety-two people participated in Experiment 2.
Three of the participants were discarded because they were able to see the tasks several times before performing them, and five others were excluded because they spent less than a minute finding the answer and they provided random answers.
Data from the remaining 84 participants were used in the analysis.
Because we were interested in queries the searchers' created, we removed 83 queries that were close to or exact copies of task questions; analysis of the remaining 1077 queries follows.
The average query length for this experiment was considerably longer  than that of experiment 1 .
To test the hypothesis that the interactive halo would cause people to type longer queries, we performed a one-way ANOVA with the word length of the queries as the dependent variable and our experimental conditions as independent variables.
A Tukey HSD post hoc-test confirmed that the Halo condition outperformed the others with an average of 0.77 more words than the control and 0.89 more words than the Static Halo condition .
There was no significant difference between the Halo and the Inverted Halo.
We also did not find a significant difference between the Inverted Halo and the rest of the conditions.
As an initial exploration of this space, we created a novel interaction technique to encourage people to create longer keyword queries, and evaluated it with a Mechanical Turk experiment.
The encouraging results of our evaluation suggest that this is a promising area for further exploration.
What does all this mean?
Our results suggest that the Red fading to Blue halo was effective at eliciting longer queries as we had hypothesized.
Unfortunately, the story is not as clear-cut as expected.
We saw an interesting interaction with instructions in Experiment 1, and did not find a statistically-significant increase in query length due to the Blue fading to Red halo, although that interface condition showed a trend in the right direction.
We attribute some of the difference to cultural norms; the message of the Blue fading to Red halo was by design more ambiguous.
Some people may be more sensitive to the ambiguous message while others only react to the dynamic behavior of the halo.
The magnitude of the difference between halo and control in the first experiment and the Red fading to Blue halo and control in the second experiment is curious.
One possible explanation is that the search puzzles in the latter experiments were more difficult than in the first experiment, as suggested by the increase in the average query length for tasks in the second experiment over that in the first .
In addition, we compared the task correctness of answers between the two experiments and found that in the second study searchers had a significantly lower correctness score =6.8078, p<.001 than searchers in the first study.
Variability in performance of search systems and users due to search topic is well-described in the literature .
We would need to run this experiment over many more tasks to understand how robust these effects are.
One challenge that needs to be addressed is obtaining a largeenough pool of participants.
One way to address this issue is to pre-test a larger number of topics from which a subset with desirable properties 
While we used query length as a proxy for query quality, the halo can represent other metrics, such as diversity of results, novelty, etc.
These other metrics may represent more directly desirable outcomes from the perspective of a multi-query search session.
