We extend previous work on texture mapping video streams into virtual environments by introducing awareness driven video QoS.
This uses movements within a shared virtual world to activate different video services.
In turn, these services have different settings for underlying QoS parameters such as frame-rate, resolution and compression.
We demonstrate this technique through a combined conferencing/ mediaspace application which uses awareness driven video for facial expressions and for views into remote physical environments.
We reflect on the issues of spatial consistency, privacy, seamless shifts in mutual involvement and making underlying QoS mechanisms more visible, malleable and flexible.
Recent years have seen a growing interest in the integration of video into collaborative virtual environments .
Several systems have demonstrated the approach of displaying live video windows as dynamically updated texture maps within a virtual world .
We extend this work with the technique of "awareness driven video quality of service " where spatial actions in a virtual world drive the underlying QoS of textured video streams.
We demonstrate this technique through a combined conferencing/mediaspace application which uses awareness driven video for facial expressions and for views into remote physical environments.
There are three general motivations for this work: addressing problems of spatial consistency in video applications; providing information about the physical world in CVEs; and extending underlying video QoS mechanisms to be more visible, malleable and flexible.
In part, the integration of video with CVEs has emerged in the light of a growing dissatisfaction with more conventional video conferencing and media space technologies and their ability to support mutual awareness and flexible collaborative work.
For example, it has been found that the absence of a mutually available, or common, video space has engendered certain communicative asymmetries, undermining the participants' abilities both to produce action and draw the appropriate inferences from the conduct of others .
We anticipate that the integration of video and CVEs may ease some of these problems by situating video views of local physical domains within a consistent spatial environment.
This approach also builds on previous attempts to support gaze direction in video conferencing .
In contrast to these previous approaches which require the careful configuration of cameras and monitors into a mutually consistent spatial arrangement, the more software oriented approach of displaying video within CVEs may afford more dynamic configurability.
From the perspective of CVEs, video can provide information about physical world artefacts and locations.
An obvious role for video is adding facial expressions to embodiments.
A second role is to provide awareness of the real world context within which participants are located.
Drawing on studies of social interaction within the MASSIVE-1 CVE , activities in the physical world have an impact on those within the virtual world .
Video views into a local physical context may assist other users in coping with such problems.
However, in spite of considerable advances in the underlying QoS mechanisms, it has been noted that there has been little consideration of how to make QoS visible at the user interface .
Furthermore, current approaches to QoS have been strongly driven by the relatively static requirements of applications such as video on demand.
In contrast, human communication can be highly fluid, involving the negotiation of social interaction among dynamically changing groups of participants.
Awareness driven video QoS is intended to make QoS issues more visible to end-users, more easily controlled and also more suited to the dynamic requirements of human social interaction.
We return to these motivations in detail at the end of this paper.
Before that, the following sections introduce the technique of awareness driven video QoS and demonstrate its use in our conferencing/mediaspace application.
They can therefore be multi-valued .
They can also dynamically change shape and extent.
They have two general kinds of effects: adaptation of existing awareness relationships and the generation of secondary source views of groups of other objects.
Awareness driven video QoS involves the application of the previously defined spatial model of interaction  to the medium of video.
The inhabitants of a CVE move about a shared virtual world.
These movements are used to calculate different levels of mutual awareness through the spatial model mechanisms of aura, focus, nimbus and third party objects.
The underlying system  uses the awareness level associated with each video source to switch between different video services.
These services are defined with different levels of video QoS using parameters such as frame-rate, resolution and compression.
Users' spatial movements therefore provide a high level, medium independent way of dynamically managing the QoS of multiple video streams which can then be displayed within the virtual world.
As necessary background we now very briefly summarise the spatial model.
The spatial model of interaction allows the inhabitants of virtual spaces to negotiate social interaction in terms of levels of mutual awareness.
In turn, these levels of awareness drive the exchange of information across different communication media.
The key concepts of the model are: * Aura - a region of space which scopes an object's presence.
Awareness - a measure of mutual interest between an observing and an observed object that can be influenced by both using focus and nimbus.
Focus represents an observer's region of interest.
Nimbus is an observed's region of influence or projection .
The top-most level of the mapping is the interface level where users express their communication requirements through spatial movements.
The resulting video information is displayed as dynamic texture maps within the 3-D graphical environment.
The next level down is the computational awareness level.
Spatial actions from the interface level are mapped onto manipulations of video aura, video focus, video nimbus and third party objects in the video medium.
These manipulations result in the ongoing re-calculation of the awareness level associated with each video stream.
In some circumstances aura, focus and nimbus may be made explicitly visible at the user interface, providing feedback about current awareness levels or allowing users to explicitly manipulate them by choosing different settings or changing shape and extent.
The video medium level is concerned with providing both generic and application specific video services.
For example, a conferencing tool might define two levels of service, a high resolution service to be associated with the most active or interesting speakers and a low resolution service to be associated with passive or more peripherally interesting speakers.
As a second example, a mediaspace might support a low frame-rate porthole service and higher frame-rate glance, v-phone and office-share services.
As these examples suggest, different video services involve different settings of video QoS parameters such as frame-rate, resolution, colour, compression type and ratio, level of synchronisation with audio, price and acceptable jitter.
The choice and combination of parameters will depend on both the application and the operating environment .
In the former case, we might expect A to bear most of the responsibility for the communication and in the latter case, we might expect B to.
This has a particular bearing on the QoS parameters of price and compression.
If the overall awareness level is mapped onto the price paid for a given stream as part of a billing mechanism, then focus and nimbus levels might determine the relative financial contributions of the sink and source.
Similarly, the choice of compression technique will have different impacts on the source and sink.
For example, MPEG-2 places much higher load on the source than on the sink whereas JPEG requires a more evenly balanced distribution of effort.
Given a video stream where the source had a high nimbus and the sink a low focus, this might suggest the use of MPEG-2, whereas a more even balance between focus and nimbus might suggest the use of JPEG.
Finally, we use dynamic load monitoring to provide feedback from the video medium level to the computational awareness level.
The underlying system can monitor the load on the network and the processors that are connected to it.
If this exceeds an acceptable level, then aura, focus and nimbus can be automatically adjusted.
For example, a consequence of increasing load might be to automatically retract video aura, focus and nimbus in order to reduce the number of video connections or to reduce the quality of existing connections so as to bring the load back under control.
The key step in our mapping is the relationship between the computational awareness and video medium levels.
The awareness level associated with each video stream is used to switch between the different video services.
A simple mapping is to arrange the video services into a linear ordering according to increasing quality and to set awareness thresholds which determine the transitions between the different services .
A more sophisticated mapping is to compare awareness values across all available video streams and to allocate the highest levels of service to a restricted number of streams with the current highest awareness values.
This would ensure that only a limited number of high quality streams had to be processed at a time, but would dynamically reallocate these between different sources.
In some circumstances, information about the specific levels of focus and nimbus which are contributing to a given level of awareness might provide additional support for controlling video QoS.
Whereas awareness can be interpreted as an overall expression of desired QoS for a given video stream, focus and nimbus can be interpreted as the extent to which the sink and source associated with that stream wish to be responsible for that level of QoS.
The lowest level of our mapping is the network and operating system level.
This level is concerned with general  low-level QoS parameters such as bandwidth, error-rate, end-to-end delay and CPU time.
The settings chosen for the video specific parameters in the video medium level map to the settings of these network and O/S parameters .
Where only best-effort QoS is supported , network congestion and overloaded CPUs may mean that the desired video QoS cannot be obtained and feedback may be provided to higher levels through load monitoring.
Where guaranteed QoS is supported, then there will be a negotiation between the video and network and O/S levels as to whether the desired QoS can be provided and feedback will be provided in terms of acceptance or rejection of a given request .
This concludes our introduction to awareness driven video QoS.
In order to utilise this approach, an application developer has to take a number of key decisions.
How is video information to be presented in the virtual world?
What classes of video service are required by this application, how are they defined in terms of video QoS parameters and how do they relate to awareness levels?
Is guaranteed or best effort QoS available and what feedback can be obtained about network and system load?
The following section presents a demonstration of awareness driven video QoS which addresses these questions.
Our demonstration application combines the functions of traditional conferencing tools with those of mediaspaces.
The result is an integrated environment which supports personal communication between multiple participants as well as general awareness of the presence and activities of others in both the virtual environment and in local physical environments.
Our design has three key properties: a navigable 3-D interface which uses textured video windows for facial expressions and for views into physical offices; the use of spatial awareness to trigger three levels of video service: portholes, glances and communication, each with a different underlying video frame-rate; and the visible embodiment of those who are browsing the environment so as to promote mutuality of awareness.
Figure 2: The structure of the virtual environment Using third party objects from the spatial model of interaction, we have defined each virtual office as an independent sub-region of the virtual world.
The boundary of a virtual office operates so that those outside cannot see or hear what is happening inside, but those inside can still see and hear what is happening outside .
The inside of a virtual office therefore provides a private meeting space, but one where its occupants can still see and hear who is approaching from the public space outside.
Our application involves a shared virtual world within which participants can establish a personal territory, referred to as a "virtual office".
These virtual offices are arranged into a navigable 3-D structure, such as our current circular configuration as shown in figure 2.
Two video windows, each showing the same overview of its owner's physical office , are texture mapped onto each virtual office.
One is located on the roof and this offers what is termed a porthole style service - a low frame-rate video image at one frame every five minutes .
A user who levitates above the virtual offices and looks down  will therefore see the equivalent of a 2-D portholes style interface to a mediaspace .
The second video window is located on the front of the virtual office.
Most of the time this will also offer a porthole service which can be seen from ground level.
However, when someone who is browsing the environment stands directly in front of this window and looks in this is upgraded to what is termed a glance service - a medium frame-rate video window at one frame a second.
A key aspect of our design is the visible embodiment of those who are active within the application.
Each user is given a graphical embodiment which they move through the virtual world in order to look into different virtual offices or to meet other users .
This embodiment features a video face to display their facial expression as captured by a camera on their workstation.
When directly face-to-face with their embodiment, this offers what is termed the communication service - a high frame-rate video view of roughly twenty frames a second.
Note that our design requires two cameras in each physical office, one to provide an in context view to be displayed on the user's virtual office and the other to provide a close-up facial view to be displayed on their embodiment.
Through the use of a microphone and speakers or headphones, users are also able to speak to one another via their embodiments .
Each embodiment is also assigned a home position, directly in front of its owner's virtual office.
Home positions are the default resting place for an embodiment when its owner is not actively browsing the media space or engaged in a private conversation inside their virtual office.
An important aspect of embodiments is that they enable mutuality of awareness.
In particular, in order to glance into an office, a user must position their embodiment directly in front of it, making their action directly visible to the occupants of the office and to other passers by.
Just as in the real world, where it could be impolite to hang around someone's open door for a long time without taking any further action, so it could be in the virtual world.
Awareness provided User present/available in physical environment?
Engaged with visitors Currently browsing or in a private meeting Mechanism Appear in the video view of physical office Is embodiment sleeping?
Table 1: Available awareness information Figures 3 and 4 are screenshots from our application.
Figure 3 shows four virtual offices, embodiments and associated video windows.
Note the two sleeping embodiments, one whose owner is not logged in  and the other whose owner is logged in but is not currently active.
Figure 4 shows an embodiment glancing into a virtual office  with another nearby embodiment facing us.
Crossing the "glance line" on the floor activates the glance service.
We now describe how our application makes use of awareness driven video QoS.
Previous mediaspace research has proposed the idea of the service as the basic unit of interaction.
For example, the Goddard interface to the RAVE media space introduced four main services: the glance, a time limited one way view into a physical office; the v-phone, a two way video phone call; an office-share, a long term connection between offices; and background, the equivalent of the view out of an office door or window .
To these can be added the portholes service which provided slowly updated frame-grabs from a camera in an office so as to promote background awareness of availability .
Our design builds on this approach.
We define three distinct levels of service: porthole, glance, and communication, which can be triggered by actions within the virtual world.
In our interpretation, a porthole provides background awareness of activity; a glance provides a moderately detailed view of this activity; and communication is used for facial expressions.
In terms of QoS, these services are distinguished by their frame-rate.
A porthole corresponds to one frame every five minutes; a glance to one frame a second; and communication to twenty or more frames a second.
We have chosen frame-rate because it is relatively easy to manipulate, it has a major impact on underlying bandwidth and hence scaleability; and it reflects privacy requirements in that a low-frame rate view provides some awareness of presence but without details of specific activities.
As described above, three video textures are associated with each occupant of the virtual world: the face on their embodiment and the front and roof of their virtual office.
Spatial awareness is configured so that the roof can only ever display the porthole service and the front of the office can display the porthole or the glance service, with the latter only being enabled when the observer is standing directly in front of the office looking in.
Table 2 summarises the different video services that are defined by our application, their uses, their relation to awareness and their associated frame-rates.
Table 2: Conferencing/mediaspace video services Although not implemented at present, it would also be possible to introduce an office share style of service by extending focus and nimbus sideways from a virtual office so as to open up a permanent porthole, glance or communication service with one or more neighbours.
Additional video textures could be placed on the inside of each virtual office to display these views.
Each user could enter into multiple office-shares .
Unlike traditional mediaspaces, our officeshare wouldn't be so much a new service, as the long term use of an existing one caused by an appropriate spatial arrangement within the virtual world.
Thus, we make a clear separation between different levels of service corresponding to different underlying QoS and the various ways in which these can be activated within a virtual world so as to enable different styles of communication.
Figure 5: Multicast groups and different levels of QoS The source is transmitting video to three multicast groups corresponding to our three levels of service.
The sink currently has full awareness of the source and so has joined the highest QoS group in order to receive video at the highest frame-rate.
Although full details of the implementation of our application are beyond the scope of this paper, we do offer a brief glimpse of how awareness driven video QoS is realised in the network before passing on to the discussion of general issues raised by this work.
Our application has been implemented using MASSIVE-2, a CVE application development platform which realises the spatial model of interaction .
Like other recent CVEs, MASSIVE-2 utilises network multicasting to enhance scaleability.
Multicasting is a technique which allows a single message to be efficiently sent to any number of recipients such that the message never crosses any network link more than once .
The video medium in MASSIVE-2 exploits network multicasting as follows.
Each source of video is associated with its own separate multicast group for each level of video service that is offered for this particular application .
The source continually transmits video via all of these groups.
A recipient of video  automatically joins the appropriate multicast group according to its current level of awareness of the source.
Figure 5 shows how this operates between a single source and a single sink.
As noted in the introduction, the use of video can give rise to various environmental incongruities, especially the creation of dislocated spaces within which participants find it hard to establish mutual reference to one another or to shared resources.
Moreover, conventional video-conferencing and mediaspace has undermined participants' abilities to engage in more complex forms of collaborative work by principally focusing on the provision of face-to-face views to the exclusion of an individual's local physical environment .
The approach of situating video displays within a shared virtual environment may alleviate these problems.
The virtual environment is itself spatially consistent so that participants can establish mutual reference from the position and orientation of their embodiments.
Each local physical domain is then linked to this environment through a boundary which allows the inhabitants of each to see into the other.
Spatial consistency is also preserved across this boundary so that distance and orientation have a consistent meaning between the physical and virtual worlds .
Privacy, and the ability to see who looks at you and your respective domain, has been an important issue for video, especially in the design and deployment of mediaspace technologies .
Our approach addresses this issue in three ways.
First, the spatial model concept of nimbus allows a video source to directly restrict its availability.
By retracting one's nimbus, one can reduce the detail  of the video that is received by an observer or can prevent transmission altogether.
Second, the spatial model supports an explicit notion of virtual boundaries which may have different effects on mutual awareness.
Third, observers are embodied within the CVE, and typically have to approach a video source in order to access it in any detail.
In so doing, they themselves become visible to both the source and to other observers.
We argue that this provides the stimulus for the social negotiation of privacy.
We argue that current QoS mechanisms are limited by being too low level and by being medium specific.
Participants should not be expected to reason about video frame-rates, resolutions, compression, bandwidth, delay and the complex inter-dependencies between them.
Our approach addresses this problem by allowing participants to employ familiar spatial behaviours in order to control underlying QoS.
At the highest level participants are concerned with spatial actions such as movement or with the use of spatial structures such as boundaries.
Even if aura, focus and nimbus are made visible, we argue that they are relatively intuitive, being concerned with extent of presence, focus of attention and projection of information.
They are also medium independent so that users do not have to learn the mechanics of QoS in each available communication.
A further issue concerns how participants adjust their visual and spatial access with regard to changing demands of particular activities.
Some videoconferencing and mediaspace systems have begun to provide participants with access views other than the faces of the co-participants.
In many cases this simply consists of a document view, but may include access to the other's environment.
With almost all these systems, participants have to deliberately select the alternative view.
The problem which arises is that, in the course of an activity, the participants have to shift their involvement from the activity in which they are engaged, in order to select the appropriate views they require for engaging in that activity.
Shifts in alignment emporally rupture the task and interaction.
Our own approach is designed to facilitate significant changes in mutual access as an integral part of the ways in which the participants behave.
For example, simply moving from the overview of the environment towards a particular individual or domain, transforms your video access to the other, and of course the other's access to you.
In this way shifts in mutual access become a feature of the ways in which you organise your interaction and activity at hand, without requiring participants to temporarily suspend their involvement, in order to explicitly change the ways in which they are aligned to each other.
Related to these various social concerns is the corresponding technical issue of providing appropriate control over underlying video QoS so as to meet dynamically changing communication requirements.
We focus on four aspects of this problem: making QoS visible at the user interface; dynamically negotiating QoS; managing multiple video streams; and expressing QoS trade-offs between communications media.
Current approaches to QoS are based on a relatively static model of communication.
Even where QoS renegotiation is possible, it is seen as an exceptional event.
In contrast many social situations are highly fluid, involving dynamic groups of participants and rapidly changing communication requirements.
At the heart of awareness driven video QoS is the idea of an on-going negotiation of mutual awareness and hence QoS.
Furthermore, the separation of focus from nimbus in the spatial model directly reflects the idea of negotiating QoS between a video source and sink and the intervening network.
Another limitation of current approaches is that they focus on managing the QoS of a single video stream at a time.
Although this may be sufficient for services such as video on demand, collaborative applications will require users to allocate resources across a number of participants.
The problem then becomes how to achieve the best allocation.
The mechanisms defined at the user interface level  and the computational awareness of our framework are inherently multi-stream.
A single action in the virtual world or a single adjustment to the shape of an aura, focus or nimbus will affect the QoS settings for potentially many video streams.
Beyond the synchronisation of video and audio from a single source, cross-medium QoS issues have received little attention to date.
We argue that human communication involves a number of important tradeoffs between media.
We believe that these differences are highly significant to social interaction because they establish key distinctions between direct and peripheral awareness.
Thus, one can listen to someone talking while watching out for others who are approaching or can study an object while overhearing a peripheral conversation.
Thus, spatial movement may re-configure QoS across different objects in different media, but will maintain the overall balance between what is directed and what is peripheral in each.
We have extended previous work on texturing video streams into CVEs through the introduction of awareness driven video QoS.
Movements in a shared virtual environment are mapped onto different levels of mutual awareness via the spatial model of interaction.
In turn, awareness levels map onto different video services corresponding to different QoS parameters.
We have demonstrated this approach through an integrated conferencing and mediaspace application which uses textured video streams to display facial expressions and to provide views into local physical environments.
This uses awareness driven QoS to seamlessly move between three different video services  according to a user's movements.
We argue that the general approach of situating video displays within a 3-D virtual world offers three main advantages: it may help establish a degree of spatial consistency between different video views; it allows a direct representation of the viewpoints of those who are accessing video images ; and it uses natural movements to switch between different video views without disrupting the activity at hand.
Given these observations, our technique of awareness driven video QoS then introduces greater flexibility into the way that such video displays are managed within a virtual world.
Specifically, it allows users to establish an optimal allocation of limited resources  across multiple video streams and contains explicit support for privacy.
However, perhaps the most important feature of this work is that it forges a direct link between social concerns and lower level technical concerns .
Of course, a number of important issues require further consideration.
The overall communication space is still fragmented into multiple physical spaces connected by a shared virtual space - will users be able to reconcile these different spaces?
Will the increased overhead of rendering 3-D video textures as compared to 2-D video windows be outweighed by the benefits of this approach?
