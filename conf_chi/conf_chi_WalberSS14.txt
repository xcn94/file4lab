Manually selecting subsets of photos from large collections in order to present them to friends or colleagues or to print them as photo books can be a tedious task.
Today, fully automatic approaches are at hand for supporting users.
They make use of pixel information extracted from the images, analyze contextual information such as capture time and focal aperture, or use both to determine a proper subset of photos.
However, these approaches miss the most important factor in the photo selection process: the user.
The goal of our approach is to consider individual interests.
By recording and analyzing gaze information from the user's viewing photo collections, we obtain information on user's interests and use this information in the creation of personal photo selections.
In a controlled experiment with 33 participants, we show that the selections can be significantly improved over a baseline approach by up to 22% when taking individual viewing behavior into account.
We also obtained significantly better results for photos taken at an event participants were involved in compared with photos from another event.
Content-based approaches initially used pixel information of the photos to compute exposure, sharpness, and other photo properties in order to determine a photo subset .
These approaches were followed by context-based approaches, which exclusively or additionally analyze the contextual information of the photos.
This information could be technical parameters of the digital still camera, such as capture times or GPS coordinates, or information gained from social networks like blogs .
While acknowledging the achievements made by content- and context-based approaches, we claim that they miss the most important factor in the photo selection process: the user's interests.
Capturing the user's interests is important as the human photo selection process is assumed to be guided by very individual factors and is highly subjective .
Rodden and Wood showed that photo collections are browsed frequently .
However, the frequency of browsing decreases over time.
Thus, the viewing of photos usually happens shortly after the capturing or downloading to a computer.
We present a usage-based approach , where the very individual viewing behavior of the user is captured and used in the selection process.
We record the user's eye movements while viewing a photo collection.
Fixations in the gaze paths show the moments of the highest visual perception and indicate the user's attention.
Different eye tracking measures have been used to identify important photos.
They consider for example the duration of fixations, how frequently a photo is fixated, and the pupil reaction.
Photos with high measure values are assumed to be the most interesting to the user and thus should be part of a selection.
Our approach is reasonable as we expect a higher availability of eye tracking hardware in the future, as indicated by recent developments in the directions of less expensive professional hardware and eye tracking with low-cost hardware like webcams.
In our experiment participants first viewed a collection of photos and then manually created personal selections.
The manual selections served as ground truth and allowed for the evaluation of different selection approaches.
As we assume that the eye movements are strongly influenced by interest, we also investigated if the personal relevance of viewed photo sets influences the quality of the gaze-based photo selection results.
To this end, we showed photos of an event the user took part in or in which the user knew the participants  and photos of an event the user was not personally involved in .
The large number of personal digital photos makes the management of one's photo collection an increasingly challenging task.
Users easily take hundreds of photos during vacation or personal events such as weddings or birthday parties.
Often, selections of "good" photos are created to reduce the amount of photos stored or shared with others .
While users enjoy certain photo activities like the creation of collages for special occasions such as anniversaries or weddings, these tasks are seen as "complex and time consuming" for normal collections .
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Copyright is held by the owner/author.
Publication rights licensed to ACM.
The results of our experiment show that photo selection criteria are often subjective and manually created selections are diverse.
The usage-based approach, taking the individual user interest into account, significantly improves the automatically created photo selections based only on content and context information  by up to 17%.
Considering only photo sets of the home collection, we even achieve an improvement of up to 22%.
The three different tasks for manual selection had only little influence on the performance of the usage-based approach.
The related work is discussed below.
Subsequently, we describe the experiment design and the applied methods for creating photo selections.
We compare the participants' behavior when viewing and selecting photos and show the distribution of the selected photos.
Finally, the gaze selection results are presented and discussed before we conclude the paper.
It can be assumed that eye tracking will be available to the average user in the near future.
Nowadays, eye trackers can also be developed using low-cost hardware.
The results for the webcam system are satisfactory and comparable to the commercial system, although still with limitations concerning the comfort of use.
This rapid development in eye tracking hardware will allow using gaze information in everyday tasks like photo viewing.
Approaches for the automatic creation of photo selections typically use low-level image features  and/or photo meta data .
For example, Chu and Lin  selected representative photos by identifying near-duplicate photos.
Eye tracking technology can be used as an explicit input device.
The users explicitly control software by moving their eyes as presented, for example, in the evaluations of gaze interaction by Mollenbach et al.
We differentiate our approach from gaze-controlled approaches as the user is not asked to control his or her gaze in interacting with the system, e.g., by fixating on a photo to select it or by concentrating only on the "best" photos.
In contrast, we obtain information from users freely viewing the photos without concrete instructions.
Several approaches use eye tracking to identify attractive or important images in a search results list and use this information as implicit user feedback in improving the image search, e.g., .
From these works, we know that it is possible to use gaze information to detect images relevant to a given search task.
Support vector machines have been applied on eye tracking data together with content-based features to rank images .
Their goal was to find the most important image regions in order to exclude these regions from cropping.
This approach does not have the goal of creating selections of photos.
However, it shows that relevant information on the user's interest can be obtained from gaze.
We developed an experiment application that allowed the participants to view and select photos from a collection C = {p1 , p2 , ...pn }.
In the first part of the experiment, eye tracking data was collected from the participants while viewing photos.
Subsequently, ground truth data was collected by asking the participants to manually create three personal selections of these photos.
A total of 33 participants  completed the first part of the experiment.
Twelve were associated with a research lab A in North America and 21 with institute B in Europe.
Members of institute A and institute B did not know one another.
Twenty of them were graduate students and 4 postdocs.
The remaining 9 participants worked in other professions, such as secretaries or veterinary assistants.
Eighteen of the 33 participants  completed the second part of experiment.
The experiment photo collection C consisted of two collections of photos taken during two social events, one organized by each of the two research institutes the participants were associated with.
The activities during the events included teamwork situations, group meals, as well as leisure activities like bowling and hiking.
Event A lasted half a day and event B three days.
The photos were not preselected but taken directly from the camera.
Only two extremely blurry photos were removed.
Collection CA  consisted of 162 photos and CB 126 photos.
The photo collection C = CA  CB was split chronologically into sets of nine photos ci = {pi*9+1 , ..., pi*9+9 }.
Each set ci contained only photos of one of the collections.
The complete collection of 288 photos was thus split into 32 sets .
We assumed that the photos of the home collection are of higher personal interest for the participants than the photos of the foreign collection.
This assumption is supported by results from the questionnaire.
The participants were asked to indicate how interesting the two photo collections were using a Likert scale from 1  to 5 .
For the home collections, the question was answered with an average of 4.36  and for the foreign collection with an average of 2.72 .
A chi-square test was applied for testing the significance of the differences as the data was not normally distributed .
After having viewed all photos, the participants were asked to select exactly three photos of each set ci in the second step .
The photos were selected by means of a drag-and-drop interface as depicted in Figure 3.
The same sets as in the viewing step were again presented to the participants, but the photos were rearranged in a new random order.
The participants were asked in this second step to select the photos as they would do for their private photo collection.
We gave no specific instructions regarding the selection criteria for choosing the photos.
Thus, the participants could apply their own  criteria.
Also, in the third and fourth steps , the participants performed manual selections.
In the third step , the participants were asked to "select photos for their friends or family that provide a detailed summary of the event."
The fourth step  was to "select the most beautiful photos for presenting them on the web, e.g., on Flickr."
Eighteen of the participants completed these tasks.
The manual selections served as ground truth in the later analysis.
Finally, the participants filled in a questionnaire.
It comprised questions about demographical user data , the experiment data set, and the experiment task as well as a rating on different selection criteria.
The participants' gazes were recorded with a Tobii X60 eye tracker at a data rate of 60 Hz and an accuracy of 0.5 degree.
The distance between the participants and the computer screen was about 60 cm.
The setup  was the same for both groups.
The experiment consisted of four steps, one viewing and three selection steps.
In the first step , the participants were asked to view all photos of collection C with the goal "to get an overview."
Eye tracking data was recorded only during this photo viewing step.
Thus, unlike other item selection experiments , we clearly separated the viewing step from the selection steps.
This was crucial to avoid an impact of the selection process on the viewing behavior.
The order in which the two collections CA and CB were presented to the participants in the experiment was alternated.
No time limit was given for viewing the photos.
The participants were told that they would afterward, in the second step, create selections of the photos.
No details about the selection process were given at this stage of the experiment.
Each photo set ci was presented on a distinct page; the photos were arranged in 3 x 3 grids in the center of the screen.
The photos' maximum height and width were set to 330 pixels, corresponding to about 9 at the visual angle.
The minimum distance between the photo was 22 pixels .
By clicking on a button, the next set of nine photos was presented.
A "perfect" selection would be a selection identical to the manual selection.
Selections of j = 3 photos are created for each set.
An overview of the different photo selection approaches is shown in Figure 4.
They are presented in detail in the following sections.
We start by describing the content-based and context-based measures for photo analysis used in our baseline system.
Subsequently, we present the eye tracking based measures and then the combination of different measures by means of logistic regression.
The score is calculated as Q = strength entropy  with strength as the average gradient edge strength of the top 10% strongest edges and entropy  as the entropy of the normalized gradient edge strength histogram.
Related work, presented by Boll et al.
The four measures  to  are based on the analysis of depicted persons.
Measure  numberOfFaces simply counts the number of faces on a photo.
Also, measure  faceArea is based on this calculation.
It considers the size in pixels of the photo areas covered by human faces.
A Gaussian distribution of the face areas as proposed by Li et al.
Measure  personsPopularity considers a persons' popularity in the data set as presented by Zhao et al.
It assumes that faces appearing frequently are more important than the ones appearing less often.
The calculation is performed by the OpenCV's face recognition algorithm and considers persons appearing in each set ci of nine photos.
This measure is context-based as well as content-based.
Human gaze paths consist of fixations and saccades.
Fixations are short time periods when the gaze is focused on a specific point on the screen.
Saccades are the fast movements between the fixations.
Most of the visual perception takes place during the fixations.
Thus, we mainly analyze these parts of the gaze paths.
Fixations are extracted from the raw eye tracking data by applying a fixation filter.
This preprocessing is performed with the fixation filter offered by Tobii Studio3 with the default velocity threshold of 35 pixels and a distance threshold of 35 pixels.
A visualization of a sample gaze path can be found in Figure 5.
Fixations are visualized as circles, and the diameter indicates the duration of a fixation.
The obtained fixations are analyzed by means of eye tracking measures.
Each measure assigns a value to each photo p. An overview of all measures can be found in Table 2.
Measure  fixated determines if a photo was fixated or not.
Measure  fixationCount indicates how often a photo was fixated.
Measures  to  consider the durations of fixations on a photo.
A visit is a series of fixations on a photo.
Measure  saccLength considers the saccade lengths before fixating a photo.
The three measures  to  rely on the size of the user pupil while fixating a photo.
Related work shows that the pupil size can vary with emotional reactions .
This reaction could appear more often for interesting photos.
The first measure,  concentrationTime, relies on the assumption that many photos are taken within a short period of time when something interesting happens during an event .
This measure is context-based as the information when a photo was taken is obtained from the photos' metainformation.
By means of the first derivation of this function, a temporal representative value for each photo is calculated.
The next four measures are content-based as they analyze the photos' content at pixel level.
When analyzing subsets of the data  less data is available.
The test data size is reduced to three sets of nine photos.
The model is trained with the training data of all 33 users.
This number reduces to 3,699 samples when training the model only with those photos of the home sets.
1,998 samples were used when performing the training for the data from the experiment steps 3 and 4, which were completed by less participants.
The default parameter settings of the LIBLINEAR library  are used for training.
For every analysis, 30 iterations with different random splits are performed and the average results of all iterations are presented in this paper.
Three different measure combinations are investigated.
Selection Sb takes only the baseline measures  to  into account.
For the selection Sb+e , all 19 measures are considered in the logistic regression.
For Se exclusively the gaze measures  to  are used in the learning algorithm.
The logistic regression predicts a probability of being selected for each photo in set ci of nine photos.
Computing Precision P For comparing a computed selection to the ground truth, the percentage of correctly selected photos of all selected photos is calculated .
As three of nine photos are selected, a random baseline selecting three photos by chance would have an average precision of Prand = 0.3.
In this section, we first investigate the users' photo viewing times and photo selection times in our experiment.
Subsequently, the distribution of the manual photo selections of our participants is presented.
Finally, we show the users' rating regarding the importance of different photo selection criteria.
Different combinations of the content-based and contextbased measures and eye tracking measures are investigated.
The measures are combined by means of a model learned from logistic regression as presented by Fan et al.
The data of all users is split into a training set and a test set.
About 15% of the data are selected as test data, which correspond to five sets of nine photos for every user as test data and 27 sets of nine photos as training data.
The test sets are randomly chosen.
The shortest viewing time was below a second and the longest 121.1 s. The viewing times were on average higher for the sets belonging to the home collection with 13.3 s  compared with 11.8 s  for the foreign collection.
These values are calculated from the time the participants looked at the photo viewing pages in the experiment application.
The distribution of the viewing times significantly deviated from a normal distribution .
Thus, we applied a Mann- Whitney U test in comparing the viewing durations for the sets belonging to the home collection and the foreign collection.
The result is that the viewing durations are significantly longer for the home sets compared with the foreign sets .
The selection times were slightly shorter for the foreign sets with an average of 20.1 s  compared with those of the home collection with an average of 21.7 s .
Like the viewing times, the distribution of the selection times also significantly deviated from a normal distribution .
Applying a Mann-Whitney U test on the selection durations showed that the differences are not statistically significant .
The selection process clearly took longer than the viewing step .
Although the selection process was different from selections usually performed in daily life, it shows that the selection of photos is more time-consuming than the viewing.
The participants rated how difficult the creation of the selection was on a Likert scale from 1  to 5 .
The ratings were performed separately for the home collection and the foreign collection.
Shapiro-Wilk tests revealed that the data was not normally distributed .
The obtained results for Cohen's kappa comparing all user selections have a minimum of k = 0.5 and a maximum of k = 0.757.
The average Cohen's kappa over all users is k = 0.625.
The average result lies only about 12% above the by-chance probability of qr = 0.556.
This further confirms that the photo selections are very diverse.
In the second experiment step, where a manual selection was created for Task 1, no specific criteria regarding the selection of photos were given to the participants.
They were just asked to create selections for their private photo collection and could apply their own criteria.
In the questionnaire, we asked the participants to indicate how important different criteria were for their selections.
Nine criteria were rated on a five-point Likert scale.
Additionally, the users were given the option to add criteria as free text.
An overview of the criteria rated by the participants can be found in the following list: 1.
Attractiveness -- the photo is appealing 2.
Quality -- the photo is of high quality  3.
Interestingness -- the photo is interesting to you 4.
Diversity -- there are no redundant pictures 5.
Coverage -- all locations/activities of the event are represented 6.
Depiction of the persons most important to me 7.
Depiction of all participants of the event 8.
Refreshment of the memory of the event 9.
Representation of the atmosphere of the event Figure 7 shows the results of the ratings on a Likert scale between 1  and 5 .
The criteria are ordered by their mean results.
One can see that some of the criteria have a wide range of ratings, from 1 to 5.
Every criterion has at least one rating with five points.
In Figure 6, the numbers of selections for all photos are displayed.
On average, every photo was selected 3.7 times.
The highest number of selections was 24.
Approximately 75% of the photos were selected five times or less.
Thus, most of the photos were selected only by a minority of the participants.
We conclude that photo selections were very individual in our experiment and confirm results from previous work .
In this formula, qx is the observed agreement between two users.
This corresponds to the percentage of photos that were selected by both users.
The value qr = 0.556 is the probability of a by-chance agreement of two users on their photo selections.
The criteria were classified as "rather objective"  and "rather subjective" , expressing if a criterion is an objective measure and could  be calculated by computer algorithms.
Although this classification could be a subject of discussion, it serves the goal to better understand the nature of selection criteria.
In Figure 7, we can see that three of the five criteria with the largest range in the answers  belong to the objective criteria.
Also, the two criteria with the lowest mean results are rather objective criteria.
It is remarkable that the two criteria with the highest average rating and the smallest deviation, 3.
Attractiveness, are rather subjective criteria.
Also, four of the five highest-rated criteria are subjective.
Eight participants provided additional criteria as free comments like "the picture makes me laugh" or "the photo is telling a story."
All criteria added by the participants were very subjective.
We present the results for selections based on single measures, followed by the results from combining the measures with machine learning.
Subsequently, we show the influence of personal interest in the photos in the selection results.
Finally, the weak influence of different selection tasks is shown.
Figure 8 shows some sample photos with the highest and lowest measure results for three baseline measures.
The samples show that the measures basically succeeded in analyzing the content of the photos.
For example, the first row shows the most blurred photo  and the photo with the highest sharpness .
But it also shows the limitations of today's computer vision approaches as, e. g., the photo with the highest number of faces is determined with 7 faces, although almost 20 people are captured in this shot.
Please note that for measure  faceGaussian the examples with the lowest result of 0  are not considered in this overview.
As described in the previous section, we randomly split the data set into a training set and a test set in 30 iterations.
For the analysis of the performance of single measures in this section, no training was needed.
Thus, the training data set was not considered, but for ensuring compatibility to the following sections, we applied the measures only on the test data sets.
Figure 9 shows the average results for each user over the 30 iterations.
Precision P was calculated by using only a single measure for creating the selections of the test data sets.
The photos in the selections were the three photos with the highest measure values.
The results strongly vary between P = 0.202 and P = 0.56 for different users and measures.
Of all baseline measures,  faceArea performed best with a mean precision of P = 0.365.
It is interesting that photo quality as a selection criterion was ranked very high by the users , but the sharpness score, considering the photo quality, did not deliver good results.
We combined the measures by means of logistic regression.
Pairwise Pearson correlation tests showed that all correlation coefficients were below 0.8.
Thus, the correlations between the single measures were not too high, and we, therefore, decided not to exclude measures from the logistic regression.
We obtained the best average precision result of P = 0.428 for Sb+e , the selections created based on baseline measures and eye tracking measures.
The result for Se  is P = 0.426 and P = 0.365 for Sb .
Using gaze information improves the baseline selection by 17%.
The results of all users averaged over 30 iterations are shown in Figure 10.
Statistical tests were applied on the average precision values obtained from the 30 random splits for each user for investigating the significance of the results.
Consequently, the nonparametric Friedman was used for the analysis.
Figure 11 shows the results for the 30 random splits for one single user.
Precision results are between P = 0.267 and P = 0.6 and show the strong influence of the training data and test data splits.
The user selected for this example is the one with the precision result closest to the average precision over all users.
Precision of selection Sb+e was calculated separately for both collections.
The results can be found in Figure 12.
They show that P results for the foreign photo set have a larger range, and the average precision is lower with P = 0.404 compared with P = 0.446 for the home set.
Comparing the precision result for the home sets with the results for Sb leads to an improvement of 22%.
A Wilcoxon test showed a significant difference between the precision values of all users for the home and foreign photo sets, Z = -2.842, p < .004.
In the experiment, the participants were first asked to create a "selection for their private photo collection" .
Subsequently, we asked them to perform further selections for the task: "Select photos for giving your friends or family a detailed summary of the event"  as well as "Select the most beautiful photos for presenting them on the web, e.g., on Flickr" .
The participants created the selections in Task 2 and Task 3 only for the photo sets of personal interest , which were taken during the event they participated in.
As we were interested in the differences between the performances of the automatic selection compared with these three manual selections, we computed the precision results of the selections under each task .
The results are shown in Figure 13.
The results strongly vary between users and between different partitions of the data into training set and test set for the machine learning.
It is possible that this effect depends on the users and their individual viewing behavior or on the characteristics of the viewed photo sets.
For example, for sets including many interesting and good photos the viewing behavior is less obvious, because it is likely that several photo are intensively fixated, and it is more difficult to create a selection.
Automatic approaches, even when including gaze data, may probably be not sufficient for a "perfect photo selection," because of the complexity of human decision processes.
We think that the decision on how much support a gaze-based system should offer has to be made by the user.
Assistance in the creation of selections by suggesting photos is an option as well as applications that fully automatically create photo selections for the user without additional interaction.
One participant in our study concluded: "Dealing with only half of the photos of a collection would already be an improvement."
The viewing and the selection times were longer for photo sets of personal interest.
At the same time, the ratings from the questionnaire showed that the selection was rated as being less difficult for the photos of personal interest.
This indicates that on the one hand, users like viewing photos of personal interest, but on the other hand, the selection process seems to be even more time-consuming for these sets.
Our approach delivers significantly better results for photo collections of personal interest than for photo sets of less personal interest.
With other words, the prediction of the photo selections performs better when the photos' content is personally related to the users.
This suggests that our approach could work even better in real life with users viewing photos of strong personal interest, e. g., one's wedding, summer vacation, or a family gathering, compared with the data set in this experiment, which is taken from a working group situation.
Finally, we compared the results for different manual selections created under different selection tasks.
We found that the results are about the same.
This result indicates that the information gained from eye movements can be useful in diverse scenarios where photo selections are needed.
Based on our results, others features such as photo cropping based on gaze data  may be integrated into future research.
The results of our findings may be implemented in authoring tools such as miCollage  to enhance an automatic photo selection for creating multimedia collections.
We hope that our approach enhances research in the direction of helping users in their photo selection tasks and allowing them to spent more time on the pleasurable aspects of creating photo products like slide shows or collages.
The research leading to this paper was partially supported by the EU project SocialSensor .
We thank Chantal Neuhaus for implementing the baseline system and all volunteers for participating in our study.
This work has two main contributions.
From the analysis of the selection criteria, we conclude that the criteria judged by the users as most important are rather subjective.
At the same time, the more objective criteria which could at least theoretically be calculated by algorithms, such as the number of faces depicted or the sharpness of a photo, are less important to most users.
In addition, the manually created selections are very diverse; only few photos were selected by most of the users.
Thus, there is no "universal" selection that fits the preferences of all users.
Rather, a system supporting users in automatic photo selections by applying eye tracking data significantly outperformed these approach by 17%.
Considering only photo sets that were of personal interest, the improvement increased to 22% over the baseline approach.
Thus, our approach performed better for photos that are personally related to the user viewing them.
The overall best selection result with a mean precision of 0.428 were obtained when combining all measures  by machine learning.
It is noteworthy that a single eye tracking measure already delivered competitive results with a mean precision of 0.421 without any machine learning.
In our experiment application, users viewed sets of nine photos and navigated through the sets by clicking on a "Next" button to avoid scrolling.
This viewing behavior is different from real life photo viewing, where it is more likely that photos are viewed in a file viewer environment or in full screen mode.
It could be that the analysis of viewing behavior in these settings has to be adapted.
Bias effects like the concentration on the first photo of a page would be necessary to be considered.
