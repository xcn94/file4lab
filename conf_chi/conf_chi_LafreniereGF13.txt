Web-based tutorials are a popular help resource for learning how to perform unfamiliar tasks in complex software.
However, in their current form, web tutorials are isolated from the applications that they support.
In this paper we present FollowUs, a web-tutorial system that integrates a fully-featured application into a web-based tutorial.
This novel architecture enables community enhanced tutorials, which continuously improve as more users work with them.
FollowUs captures video demonstrations of users as they perform a tutorial.
Subsequent users can use the original tutorial, or choose from a library of captured community demonstrations of each tutorial step.
We conducted a user study to test the benefits of making multiple demonstrations available to users, and found that users perform significantly better using our system with a library of multiple demonstrations in comparison to its equivalent baseline system with only the original authored content.
A design possibility, which has received less attention, is to provide what we define as community enhanced tutorials, which continuously evolve and improve as more users work with them.
In particular, if a tutorial system could capture and store every user's workflow as they follow a tutorial, a new user could refer to an entire library of examples, instead of just the originally authored tutorial.
This could help users understand variations in the presented workflow, identify particularly efficient strategies, or find examples that more closely match their own specific task.
In this paper we present FollowUs, a community enhanced tutorial system.
FollowUs utilizes an Application-inTutorial Architecture, integrating a full-featured imageediting application into a web-based tutorial.
In doing so, FollowUs can track when users try to follow the tutorial and can capture video of those users' workflows.
Subsequent users can then view the author's original tutorial, or choose from a library of captured community demonstrations for each step of the tutorial.
Sorting and filtering mechanisms allow the user to identify demonstrations that may be particularly useful, such as those with similar content to their own, or those that use specific tools.
We performed an evaluation of FollowUs to understand a core question related to its design: Can the availability of multiple demonstrations aid user performance when following a tutorial?
Our study showed that users perform significantly better when using FollowUs with a library of multiple demonstrations, in comparison to its equivalent baseline system with only the original authored content.
Furthermore, users reported that the community-enhanced system was more useful and was subjectively preferred in comparison to the baseline.
In the remainder of this paper, we present a design space for tutorial architectures and how the Application-in-Tutorial Architecture enables community enhanced tutorials.
We then describe the FollowUs system, and present results of our evaluation.
We conclude with a discussion of the study's limitations and directions for future work.
Information interfaces and presentation : Graphical User Interfaces.
Software applications for content design and creation can be difficult for new users to learn , particularly when performing unfamiliar tasks.
While many means of seeking help exist, a common trend today is to consult webbased tutorials , which can help walk a user through the steps necessary to complete a desired task.
However, a number of factors can make tutorials difficult to follow.
In some cases, details are omitted or steps are not explained clearly.
In addition, a user's content may differ from that demonstrated in the tutorial , requiring the tutorial instructions to be adapted.
These difficulties have led researchers to explore new techniques for improving the presentation of tutorials, such as combining text with videos , or presenting tutorials in the application itself so they can react to the user's context .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Today, task-centric documentation is found most commonly in the form of web-based tutorials.
Recent work examining how web tutorials are used  has shown that they serve as help while performing a task, as resources for refining a user's skill set, or as guides that allow a user to perform a workflow beyond their current ability.
The HCI community has also explored a range of tutorial systems that are presented in the context of the application.
Stencils-based tutorials overlay step-by-step instructions on the interface and limit interaction to UI elements related to the current step .
Google SketchUp's self-paced tutorials are similar, but do not limit the user's interaction with the interface .
Sketch-Sketch Revolution provides inapplication tutorials that assist the user in completing drawing tasks to allow the user to experience success while learning .
Our approach combines the benefits of web-based tutorials and in-application tutorials by embedding a fully-featured application into a web-tutorial system.
More recent studies have started to delineate where animated assistance can be beneficial.
The MixT multimedia tutorial system showed that short, step-specific video demonstrations are useful for demonstrating dynamic actions that are difficult to express via text or static images alone .
Pause-and-Play  improves a user's ability to follow video demonstrations by automatically pausing and resuming video to stay in sync with the user's progress.
Our work builds upon this research by investigating how the availability of multiple short demonstration videos can be beneficial for users performing steps of a tutorial.
A number of systems have been proposed that automatically record a workflow performed by a user.
Some uses for recorded workflows that have been explored include: automatically creating step-by-step tutorials consisting of text and images , creating multi-media tutorials , allowing users to explore and replay the workflow history of documents , and creating or improving macros .
While work has explored how to create tutorials from captured workflows, we are unaware of previous work which augments existing tutorials by recording the workflows of users following the tutorial.
Our system is also based on variations within a collection of workflows, but augments the tutorial experience so users can view multiple demonstrations at each individual step.
In summary, there exist a number of previous projects related to our system.
However, we are unaware of prior research that has studied the design of tutorial systems that improve as more users work with them, or that has studied the value of multiple video demonstrations on how users learn to perform unfamiliar tasks in an application.
In the next section, we present a design space of tutorial system architectures including the Tutorial-in-Application architecture on which our system is based.
The emergence of cloud services and rich internet application technologies offer an opportunity to rethink how tutorial systems are architected.
In particular, new possibilities are opened up by fully-featured in-browser applications, such as Pixlr , and deviantART Muro .
We consider a new tutorial architecture enabled by these advances.
In this section we introduce a design dimension of tutorial Architectures, which was not considered in their design space.
The architectures we identify are distinguished based on the degree of integration between tutorial and software, and the domain in which the tutorial system and application are located: the desktop or the web.
The degree of integration between the tutorial and application is important because it affects how much the two can interact with one another.
The domain is important because of the qualitative differences between the web and the desktop.
Along these lines, work on the TApps system  has explored how existing web-tutorials can be converted into active interfaces for applying tutorial steps and experimenting with settings variations.
Reifying tasks into concrete artifacts also provides a convenient gathering place for a community of users to discuss techniques or experiences with performing those tasks.
In this work, we are interested in how an Application-inTutorial architecture opens up possibilities for creating tutorials that improve with use and contributions from the user community.
We expand on this idea in the next section.
Traditionally, web tutorials are created and do not change once posted on the web.
However, the Application-inTutorial architecture allows us to explore an alternative model where the initial posting of a tutorial acts as a seed, representing a task in an application.
Community content, in the form of demonstrations, comments, answers to questions, etc.
We see this being particularly useful under two scenarios.
First, the original instructions in the tutorial may not exactly match the goal that the user wants to accomplish.
For example, a tutorial on color correction may demonstrate a workflow using a photo with too much red, while the photo that the user wants to correct has too much blue.
If the user does not realize this mismatch, they may not understand why the tutorial instructions do not work for their photo.
However, if additional demonstrations were available, the user could realize that different photos need to be corrected in different ways.
Second, the quality of a tutorial may be poor, or targeted at users with a higher skill level.
If a question-and-answer feature was included with each tutorial step, common sticking points such as "How did the author select the man?"
In this architecture, the tutorial and application are in their separate domains , and cannot communicate with one another .
This is typical for current web tutorials, which users follow in a browser window separate from the application.
This architecture is limited in terms of design possibilities.
In this architecture, the tutorial and application still reside on the web and desktop respectively, but there is one- or two-way communication between them .
Oneway communication could be used to load an initial document into the application, or to customize the application's interface to suit the tutorial .
It could also be used to repurpose tutorials as auxiliary input devices for an application, as in Adobe's Tutorial Builder .
A two-way communication channel enables Reactive tutorials  that respond to a user's actions as they follow the tutorial, such as the Pause-and-Play system .
Communication between the application and tutorial also enables tutorial analytics, where detailed logs of in-application actions are reported to the server hosting a tutorial .
This architecture expands the design possibilities for tutorial systems, but it has two disadvantages.
First, the tutorial is still displayed in a separate window from the application.
This limits interaction possibilities, and is a known source of difficulty when following help .
Second, there are technical challenges involved in coordinating a desktop application with a tutorial that resides on the web.
The architectures we discuss next address these issues by embedding the tutorial in the application, or vice-versa.
In this architecture, the tutorial system is integrated into the application .
This allows a tutorial to be presented in-context, and to easily react to user actions.
Moreover, the system is implemented in one domain, easing technical challenges.
A potential downside is that the tutorials lose the benefits of having a web-presence.
This includes the ability to collect web analytics, the ease with which other relevant materials can be accessed through web search , and simple mechanisms for sending content to other users.
It also removes a key motivation for authors to create tutorials: the potential to earn recognition or ad revenue which are enabled by creating web content .
In this paper, we present a new tutorial architecture that contrasts in-application tutorials.
Instead of integrating the tutorial into the application on the desktop, the application is integrated into the tutorial system running on the web .
This enables qualitatively new interactions.
We hypothesize that community enhanced tutorials will have the most impact in the low-similarity or low-quality scenarios.
In the low-similarity scenario, community features could help generalize the tutorial to address a wider range of user goals.
In the low-quality condition, they could smooth flaws in the author's presentation, and adapt the instructions to a wider audience of users.
We now present a description of the features of FollowUs, our community enhanced tutorial system.
FollowUs provides instructions using text and images .
Similar to previous in-application tutorial systems , FollowUs displays one step at a time, and the user navigates between steps using Next and Previous buttons .
This step-by-step usage model allows the tutorial system to associate content captured in previous interactions with specific steps, and display community content to the user for the currently viewed step.
FollowUs is a new type of tutorial system that allows implicit and explicit contributions by the community to improve its tutorials over time.
An overview of the FollowUs interface is shown in Figure 2.
FollowUs is displayed in a web page, and provides instructions using text and images .
Unlike traditional web tutorials, it includes a fully-functioning built-in application .
A video player is displayed alongside the application  and users can choose between the author's video, and multiple community demonstration videos .
Pixlr is a full-featured image editor implemented in Flash that runs in the web browser.
Its feature set is similar to popular Desktop image editing applications such as Adobe Photoshop or GIMP.
Pixlr's source was modified to implement logging of interface events and to take snapshots of open documents.
The interface to FollowUs is implemented using standard web technologies  which easily integrate with one another.
Screen recording is implemented using ffmpeg  installed on the client system.
When not playing, an overlay over the loaded demonstration lists commands and settings used in that video .
These are intended to give the user a skimmable overview of the workflow presented in the video.
While playing, overlays on the video indicate commands, keyboard shortcuts, and mouse clicks.
Community demonstrations are displayed as a series of thumbnails, each showing the state of that user's document at the start of the step .
Hovering over a thumbnail shows the state at the end of the tutorial.
Clicking a thumbnail loads the demonstration into the main play area  where the user can play the demonstration for the current step.
Moving forward or backward through tutorial steps follows the currently selected demonstration, loading the demonstrating user's video for the current step.
This allows a user to select a desired demonstration and follow it through the length of the tutorial.
The thumbnail view also displays the duration of each demonstration's video for the current step, and the following set of badges that highlight particular attributes of the demonstration :
The demonstration includes at least one instance of the Undo command.
This may indicate that the demonstrating user made a mistake and corrected it, or experimented with settings.
The demonstration includes commands different from those used by the tutorial author.
This may indicate a variation on the task is demonstrated.
Images are compared by computing the magnitude of the difference between their histograms with the Euclidean norm.
To allow the user to explore variations in how a step was performed by other users, the system includes visualizations of setting and tool variations used at each step .
The Settings Variations visualization displays a density plot of setting values used by the tutorial author  and other users  for the current step.
Selecting a set of data points with the mouse filters the list of community demonstrations to only show those in which the selected range of settings was used.
The Tool Variations section lists tools used over and above those used by the author at the current step, and these can be clicked to filter community demonstrations.
The community enhancements we've described so far are generated implicitly based on screen recording and logs of users' actions as they complete a tutorial.
FollowUs also includes a number of explicit community feedback mechanisms.
Previous work has shown that comment systems on web tutorials play a support role for users, but are hampered because they are separated from tutorial content .
In FollowUs, we address this by including a User Questions area for each tutorial step where users can post questions and receive answers from other users .
Low-cost feedback mechanisms are included to allow users to flag content they liked or disliked.
Users can "Like" video demonstrations, up- or down-vote answers to questions, and flag particular tutorial steps as difficult.
An additional feature we included is the ability to open the document associated with any demonstration at any step.
This allows the user to start a tutorial partway through, skip portions that they aren't interested in, or skip a step and continue the tutorial.
While this feature won't help a user to apply tutorial instructions to their own content, it could help them to refine their skill set outside of a current task.
Users can sort community demonstrations by a range of criteria, including recency, views, likes, and video duration.
Users can also sort by image similarity between the document currently loaded in Pixlr and the demonstration thumbnails.
This allows the user to find demonstrations on content that matches their own.
In our prototype system, an author creates a tutorial by first authoring a series of steps in HTML format , and then records an initial video demonstration by performing the tutorial once in FollowUs while stepping through the tutorial steps.
The system segments the raw video based on the time codes for switching between steps, and combines the video data with a log of user interface events from the application to create the author's demonstration video for each tutorial step.
To create the community demonstrations, FollowUs automatically performs a screen capture any time a user starts a tutorial.
Interactions with the application are also recorded, to keep track of setting and tool variations.
Once the tutorial is completed, the system processes and segments the video and adds it to that tutorial's library of demonstrations.
As in the authoring process, video is segmented based on time codes for switching between steps.
We conducted an initial study comparing FollowUs to an equivalent system without community enhancements to see how users would react to the system, and to understand how the availability of multiple demonstrations would impact the use of tutorials to perform unfamiliar tasks.
We targeted participants with little or no experience with image editing software, since our interest was in how users would use the tutorial systems to figure out how to perform unfamiliar tasks.
Two additional participants were originally recruited, but later eliminated from the study.
One exhibited vision problems that affected his ability to complete the study tasks.
The other was a regular user of image manipulation software, and so did not meet our above criteria.
Participants were given a $50 gift certificate to an online retailer in appreciation for their participation.
We tested two versions of the FollowUs system.
In the baseline Author Condition, the community features of the system were removed, and the tutorial consisted only of the author's text/image instructions and the author's demonstration.
This condition is similar in spirit to the multimedia tutorials produced by the MixT system .
In the Community Condition, two additional community demonstrations were included for each task, as well as the described features for filtering and sorting videos.
To simplify our evaluation and provide greater control, the PerStep Q&A sections were not populated with content.
For both conditions, all demonstrations were created by the experimenters, in order to maintain control over the quality of the demonstrations and to simulate a range of specific scenarios, outlined below.
Eight tasks were designed to simulate applying one step from a larger tutorial.
We chose to simulate single tutorial steps because it allowed us greater control over the scenarios faced by participants.
The tasks were designed to cover the four quadrants of the task space described in Table 1.
In designing the tasks, we varied the quality of the author's text/image instructions and demonstration to attenuate the Tutorial Quality variable, and varied the user's goal document to attenuate Task Similarity.
Two sets of four tasks each were designed, with each task set spanning the four quadrants of the task space .
For High Quality  tasks the author text/image tutorial provided detailed instructions on how to perform the task, and the author's demonstration shows the procedure clearly, leaving out no details.
The additional demonstrations provided in the Community condition were designed to give slower and complete demonstrations of the task.
For High Similarity  tasks, the user's content was similar in nature to the author's, and the task could be achieved by applying the author's instructions verbatim.
For Low Similarity  tasks, an additional procedure or alternate command would be required beyond the author's instruction, due to the user's content not matching the author's content.
Additionally, for these tasks only one of the additional community demonstrations would be helpful for figuring out how to solve the users' task.
A within-subjects experimental design was used.
Users performed one complete set of 4 tasks for one condition, and then the other set of 4 tasks for the remaining condition.
The order of the conditions and the mapping of system to task sets were fully counterbalanced across participants.
The order of the four similarity/quality conditions was counterbalanced in a Latin square, but was the same for the first and second condition for each participant.
The experiment was conducted in an enclosed office on a desktop computer running Windows 7.
The tutorial system was loaded in the Chrome web browser in full screen mode.
Sessions lasted approximately 1 hour.
Each session began with a short questionnaire on the participant's demographic information.
The participants was then given an overview of the Pixlr image editor interface.
Before each condition, the experimenter showed the participant the associated tutorial system, and walked them through its features.
The "before" document was loaded into Pixlr.
The user was told that they could make as much or as little use of the features of each system as they wished, and that the main objective was to complete the task by recreating the "after" image.
An experimenter was present throughout the study, to make observations.
Assistance was only provided to reset the input image to its default state if this became impossible using Pixlr's Undo and History mechanisms, and to encourage the user to continue.
If the user appeared stuck, the experimenter would ask the user "Is there anything else you could try?"
If after several prompts the user was not making any further progress, the user was given the opportunity to go on to the next task.
The experimenter also noted instances where the participants showed visible signs of being lost or frustrated.
Between each task, the participant filled out a short questionnaire consisting of six 5-point Likert scale questions about their experience on that task.
After completing all tasks, the participant filled out a short post-study questionnaire consisting of 5-point Likert scale questions, and freeform questions about the two tutorial systems and the study as a whole.
The reversal for the HS/HQ case may be because the Author system provided the user with everything required to easily complete the task, and the additional demonstrations and features in the Community condition acted as a source of complexity or frustration.
Overall, these results partially support our hypothesis that multiple demonstrations are particularly useful when the user is contending with a low quality tutorial or applying a tutorial to content that differs from that of the tutorial.
We found a significant improvement when users faced both of these challenges and a trend toward an improvement when they faced each separately.
On each of the eight tasks, the experimenter noted whether the participant completed the task, and whether they showed visible signs of frustration during the process.
Participants completed 81% of study tasks, with the observed completion rate for the Community condition  higher than that for the Author condition , though this difference was not statistically significant.
We attribute the high overall completion rate to our experimental design, which prompted of participants to continue each task.
Instead of the completion rate, we analyze the rate at which tasks were completed without frustration.
This metric is meaningful for web tutorials, where a frustrated user may be quick to give up or seek alternative help.
Participants completed 46% of tasks without signs of frustration, with the observed completion rate for the Community condition , higher than that for the Author condition .
Comparing matched pairs of like-typed tasks for each user revealed that frustration-free task completion was significantly higher in the Community condition  = 9.38, p < .05,  = 0.33.
Figure 6 shows the number of tasks completed without frustration across the two conditions and four task types.
Across both conditions, videos were used extensively, with all participants watching at least one video during the study, and 10 of our 16 participants  watching a video for all eight tasks.
In 50% of the tasks performed in the Community condition, users watched two or more videos .
This suggests that users will take advantage of multiple videos when they are available.
Unsurprisingly, users viewed multiple videos more often in tasks that were not straightforward.
This is the most obvious for the LS/LQ tasks, for which 74% of users watched two or more videos.
The post-study questionnaire included three questions on the study as a whole .
The majority of users  preferred using the Community system, compared with 4 who expressed no preference and 3 who preferred the Author system.
Twelve of the participants agreed or strongly agreed to the statement that they could not have completed the tasks without the tutorial system.
We also asked five questions for each of the two systems .
In all cases, the trend favors the Community system, but only two of the questions showed statistical significance.
The result for frustration is notable because it reinforces the experimenter's observations of frustration during tasks, and matches our qualitative analysis.
We did not find significant differences for the other task types, but the data for the Author system in Figure 9 suggests a trend for all three questions in which the HS/HQ lies at one extreme, the LS/LQ at the other, and the two remaining task types are somewhere in-between.
That this is not the case for the Community condition may indicate that multiple demonstrations make a tutorial less sensitive to issues of tutorial quality and task similarity.
I guess the main reason for that is you're not going to be doing exactly the same thing as every tutorial.
So, you know, you pick up on pieces from different tutorials.
Participants also filled out a short questionnaire after each of the eight tasks they performed.
This questionnaire consisted of six 5-point Likert scale questions designed to probe three themes: the usefulness of the system for the task, the difficulty or frustration experienced during the task, and whether additional demonstrations were desired.
For each theme, a positively- and negatively-phrased version of the question was asked.
The aggregated results are shown in Figure 9.
Paired t-tests with Bonferroni correction were used to compare the two Systems for each task type.
Participants cited a number of ways that multiple demonstrations could be useful, including to show common mistakes, to demonstrate a task at a range of expertise levels, and to show the range of results tutorial instructions could produce.
One user also mentioned that multiple demonstrations would be useful because they could be used together, by drawing hints from several demonstrations.
Embedding mistakes in the videos was not an intentional part of our experimental design, but two of the community demonstrations we created included a case where the user experimented while performing an operation.
Two users pointed this out as a positive feature.
It just clogs up the interface.
I guess I could say that there's too much extraneous information on it .
Because everyone ends up, or I guess I just end up, just watching the fastest response.
The only of the community videos I watched was the person that did it in the quickest time.
So maybe you could just have one video on the community screen, the best, as opposed to everyone who's done it.
These are encouraging results, but they only scratch the surface of research questions raised by community enhanced tutorials.
In this section, we discuss limitations of our study, and outline opportunities for future work.
A potential limitation of our study is that all demonstrations were created by the experimenters, and the study tasks were on the granularity of individual tutorial steps rather than complete tutorials.
This allowed us to test the 2x2 task matrix and the impact of multiple demonstrations in a tightly controlled manner, but leaves open the question of the external validity of our findings.
Ideally, this could be addressed by deploying our system and studying the community demonstrations recorded by the system, and usage patterns for community demonstrations.
However, we feel it is reasonable to expect that high-quality demonstrations would be captured in a deployed system, because our approach of capturing demonstrations from every user of a tutorial would yield a large set of candidate demonstrations to choose from.
Moreover, our study has shown that demonstrations need not be perfect to be helpful to users.
This implicit, capture-everything approach to community contribution has the advantage that it requires no additional effort on the part of users to contribute; it is enough that they use the tutorial for their own needs.
However, it places additional responsibility on the system to identify and surface useful demonstrations.
This is a challenging problem for future work.
We anticipate that a fully working system would use of a number of complementary techniques in concert with one another.
First, automatic filtering heuristics could be used to identify good demonstrations  from the set of candidates recorded by the system, and conversely eliminate poor quality demonstrations.
Given a large number of candidate demonstrations, these heuristics could afford to be conservative about the demonstrations allowed into a tutorial's library.
Second, collaborative filtering could be used to take advantage of viewing patterns and low-cost explicit feedback from users to surface demonstrations that are popular with the user community.
This motivated our inclusion of the ability to "Like" demonstrations in FollowUs.
Finally, visualization and presentation techniques could be used to help users recognize and find demonstrations that are relevant to their own needs.
The badges on video thumbnails, and the image similarity sort features in FollowUs are examples of features in this vein.
Investigating and evaluating the techniques and approaches outlined above are all areas for future work.
This user's feedback provides validation for our surfacing of the "Fastest" attribute as a badge.
These comments also validate our decision to show only three videos at a time that match the current sort/filter criteria.
These users may be expressing that too much information is overwhelming.
Users also identified areas for improvement.
Two users felt that the video demonstrations were too fast, though they may have been reacting to author demonstrations in the LQ tasks, which were purposefully demonstrated quickly.
Two users mentioned that adding audio to the demonstrations would be helpful.
Because video is automatically recorded, and then segmented into individual tutorial steps, adding audio to videos is a non-trivial problem.
Some of the issues involved in this, and potential solutions, have been discussed in work on the MixT system .
Even when additional demonstrations were available, we observed many cases where a user would get demonstration fixation while watching a particular video.
When this occurred the participants appeared to be extremely focused on the video, and ignored the adaptive pop-up notifying them that other videos were available.
We observed this phenomenon even for videos that were particularly unhelpful.
We hypothesize that this phenomenon is an instance of the "paradox of the active user" identified by Carroll and Rosson, where a user will resist spending time learning and instead attempt to apply their current, incomplete knowledge to solving a problem .
Techniques for addressing this problem for multiple demonstration videos are an interesting subject for future work.
The results of our initial study are encouraging.
Users took advantage of multiple demonstrations when they were available, and the presence of multiple demonstrations allowed users to complete tasks with less frustration as compared to a control system.
Users also rated our system subjectively better than the control system, and expressed enthusiasm about its features, including identifying a number of potential uses for multiple demonstrations.
The Per-Step Q&A feature included in FollowUs is a simple explicit contribution mechanism in this vein.
Explicit mechanisms require effort on the part of users, who would have to be motivated to contribute.
However, they could allow for more targeted and specific refinement of tutorial content by users.
This approach has been explored to good effect by Berthouzoz et al.
While our work was focused on the tutorial following experience, community enhanced tutorials provide interesting opportunities for tutorial authors as well.
A tutorial dashboard component could to track open Per-Step Q&A questions, or visualize how users progress through a tutorial, including where they encounter trouble or give up.
Video recordings of user experiences would enhance this by allowing a tutorial's author to see how users performed the tutorial at each step.
We have presented FollowUs, a new system which leverages an Application-in-Tutorial architecture to bring enhanced community features to the tutorial experience, including additional demonstrations of each tutorial step.
An initial evaluation suggests that the presence of multiple demonstrations helps users to complete tasks with less frustration, particularly for tasks with poor author quality and where the user's task does not precisely match that of the tutorial.
