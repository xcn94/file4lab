When pairs work together on a physical task, seeing a common workspace benefits their performance and transforms their use of language.
Previous results have demonstrated that visual information helps collaborative pairs to understand the current state of their task, ground their conversations, and communicate efficiently.
However, collaborative technologies often impinge on the visual information needed to support successful collaboration.
One example of this is the introduction of delayed visual feedback in a collaborative environment.
We present results from two studies that detail the form of the function that describes the relationship between visual delay and collaborative task performance.
The first study precisely demonstrates how a range of visual delays differentially impact performance and illustrates the collaborative strategies employed.
The second study describes how parameters of the task, such as the dynamics of the visual environment, reduce the amount of delay that can be tolerated.
When designing tools and technologies to support such tasks remotely, we need to understand how the introduction of technological mediation impacts the coordination mechanisms typically relied upon in collocated physical environments.
A number of studies have demonstrated that visual information is a key element to successful collaboration.
Speakers and addressees take into account what one another can see .
They notice where one another's attention is focused , and they use cues available in the local visual context to help them coordinate both the language they use as well as the actions they engage in.
When mediating an environment, we need to understand how technologies impinge upon these processes in ways that can disrupt the mechanisms that support successful collaborative activity.
For example, how does the often unavoidable delay found in distributed applications, such as the type involved in network congestion, impact an individual's ability to maintain an awareness of their partner's actions?
How do visual delays disrupt the critical language processes required for successful communication?
And how do these delays impact the task strategies pairs use to successfully collaborate?
This paper aims to describe a basic function that governs the impact that delayed visual feedback has on collaborative task performance.
It provides detailed insight into the amount of visual delay that can be tolerated before impacting collaborative performance, and how this range of tolerance is dependent upon features of the task.
Finally, it presents both quantitative and qualitative descriptions of the strategic differences that occur across a range of visual delays.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
A number of studies have examined the impact audio delay has on communication and collaborative task performance.
As demonstrated by Krauss & Bricker , small audio delays of 300ms can have detrimental effects on communication processes, and delays as large as 900ms can severely impact a pair's ability to communicate.
Cohen  described how a simultaneous 705ms delay of audio and video resulted in longer conversational turns and decreased overlap between utterances, and Tang & Isaacs  found that a one-way delay of 570ms severely disrupted turntaking behaviors.
In summary, the work on audio delay and its impact on collaborative performance tends to find that delays below 300ms pose little problem.
Delays between 450ms and 700ms can severely impact communication and coordination processes.
And delays greater than 700ms drastically impact communication, coordination, and overall task performance.
While these studies examined audio on its own or combined with video, they do not provide insight into the communicative impact of the visual information itself.
Do delays in updating visual information, of the sort introduced by video compression or network lags, undercut its value in a similar fashion?
Gutwin and colleagues  examined delay1 in the context of a tightly-coupled motor coordination task in a shared workspace.
They reported that delays greater than 200ms led to a larger number of coordination errors in comparison to conditions with no delay.
However, they found no impact of visual delay on overall task completion time.
In addition, they do not report the differences between various levels of delay, nor do they describe the functional relationship between delay levels and task performance.
Our earlier work  examined the impact of delay on a collaborative task that had both language and motor components.
We demonstrated that delayed visual feedback impacts performance since it is not synchronized with the state of the task nor the language that it needs to support.
However, this work compared a rather long delay of 3000ms to no delay.
We found that the delay impacted task performance times as well as the communication processes and language patterns of the collaborative pairs.
In addition to our work, Vaghi and colleagues  performed a naturalistic study of a collaborative virtual ball game where they provide qualitative evidence that strategic adaptations occur in delays ranging from 150ms to 1000ms, and they suggest that delays of 500ms or more may cause disruptions to collaborative performance.
While these studies provide evidence to suggest that a phenomenon is present, they provide a wide variety of suggestions regarding tolerable ranges of delay.
We do not yet have a firm grasp on how visual feedback impacts collaborative task performance and strategies over a wide range of delays.
Is collaborative performance differentially impacted across a range of delays?
Finally, what are the different coordination mechanisms impacted by visual delay?
To address these questions, we apply a methodology that allows us to derive the form of the function that governs the impact of visual delay.
This provides a much more detailed account of the impact of visual delay on collaborative task performance and surrounding communication processes.
Before proceeding with the studies, we begin with a brief theoretical discussion of the ways pairs use shared visual information in order to ground our investigation of these questions.
Our theoretical understanding of the way pairs use visual evidence for collaborative purposes relies heavily on two psychological theories: Situation Awareness Theory and Grounding Theory .
Situation Awareness Theory holds that visual information helps pairs assess the current state of the task and plan future actions .
It primarily focuses on how visual information influences the ability of groups to formulate a common representation of the state of the task, which in turn allows them to plan and act accordingly.
This awareness supports low-level coordination for tightlycoupled interactions.
At a somewhat higher level, and having to do more with the language and communication surrounding a collaborative activity, Grounding Theory suggests that visual information can serve as an unambiguous source of evidence that allows conversational partners to generate efficient speech and more easily assess a level of understanding .
Visual information provides a means for coordinating language and generating efficient and understandable discourse surrounding a collaborative activity.
Together these theories predict that when groups have access to visual information they will coordinate their work better because, at a lower level, they can monitor the state of the task and plan and act appropriately, while at a higher level, they can deliver instructions and clarifications in an efficient and timely manner.
Delay in providing the necessary visual information should impact these coordination mechanisms and ultimately have a negative impact on both communication processes and task performance.
We now discuss the theories in more detail, highlighting the impact that delayed visual information has in each.
According to Situation Awareness Theory, visual information is primarily valuable for coordinating the task itself.
In order for collaboration to be successful, group members need to maintain an ongoing awareness of one another's activities, the status of relevant task objects, and the overall state of the collaborative task .
For example, Nardi and colleagues  describe how a scrub nurse on a surgical team uses visual information provided on an overhead monitor to assess the task state and anticipate the instruments a surgeon will need.
If the nurse notices the surgeon has nicked an artery during a surgery, she can immediately begin preparation of the cauterization and suture materials and have them ready to present before the surgeon asks for them.
However, if the visual information were delayed for some reason, such tight-coordination would not be possible and precious seconds could be lost.
In a similar fashion, but at an even finer temporal level, Gutwin and colleagues  describe how task coordination is supported by the availability of visual information during a tightly-coupled collaborative task in which pairs need to quickly move computational objects within a shared 2D workspace.
When the view of the shared workspace is delayed, the pairs have difficulty assessing the state of their partner and the state of the task, and there is an increase in the number of errors they make by grasping the same piece.
At a micro-level, situation awareness of what is currently happening likely influences the next move or action engaged in.
When pairs are performing tightly-coupled interactions in a distributed environment, a delay in the availability of the visual information may disrupt the formation and maintenance of such awareness, ultimately yielding coordination difficulties.
Visual information also impacts comprehension monitoring in a number of ways.
In a typical spoken interaction, partners can use explicit verbal statements 
In environments where visual information is available, the visual feedback itself can provide a critical resource for comprehension monitoring .
Evidence can be deliberate  or as a side effect of proper performance of a desired action , provided both parties are aware of what one another can see .
Recently, Clark and Krych  demonstrated that collaborative pairs use visual information to facilitate the precision timing required when discussants are introducing new entities to a discourse or changing their speech midsentence in response to their partner's actions.
However, delays of the sort introduced by video compression are likely to undermine the value of this visual feedback.
Our recent work demonstrated that large delays reduce the communication benefits of shared visual information .
Although immediately available shared visual information generally improves collaborative task performance by supporting situation awareness and conversational grounding, the benefits it provides in any given situation will likely depend on both the accuracy of the visual information  along with the requirements for coordination imposed by the task structure.
Any delays in the availability of the visual information are likely to impact these coordination mechanisms in different ways.
At a higher level, Grounding Theory suggests that visual information can improve collaborative task performance by supporting the verbal communication surrounding a collaborative activity.
It states that successful communication relies on a foundation of mutual knowledge or common ground .
Speakers form utterances based on an expectation of what a listener is likely to know and then monitor whether the utterance was understood.
In return, listeners have a responsibility to demonstrate their level of understanding.
Shared visual information serves to support both the initial generation of utterances as well as to provide evidence of comprehension .
Throughout a conversation, participants are continually assessing what one another knows and using this knowledge to generate subsequent contributions .
Clark & Marshall  propose that physical co-presence  allows speakers to anticipate what a partner knows.
Hence, a person can point to an object in a shared physical environment and refer to it using the deictic pronoun "that" if she believes her partner can also see the object and her gesture.
We can summarize this discussion about the influence of delayed visual feedback on collaborative task performance in terms of a set of hypotheses that describe our expected findings.
Study 1 aims to address the question of the functional form of the impact delayed visual feedback has on collaboration due to its impact on both lower level coordination tasks as well as higher level communication processes.
In particular, we should expect that: H1: A collaborative pair will perform their task more quickly when they share an immediately available shared view of the work space.
H2: A collaborative pair will perform their task more quickly as the linguistic complexity of the task objects decreases.
H3: An immediately available shared view of the work space will have additional performance benefits as the linguistic complexity of the task objects increase.
In addition to these general hypotheses about the overall impact of visual information, we should expect that the delays will differentially impact the coordination mechanisms on a different timescale, H4: The immediacy of the shared visual information will have differential impact across a range of delays.
As the collaborative task becomes more dynamic or tightlycoupled, we should expect the tolerance a collaborative pair has for delays to become reduced.
As the task becomes more tightly-coupled and dynamic, the pairs will experience performance deficits with shorter delays in comparison to less dynamic environments.
We examine this in Study 2 where we manipulate the dynamics of the task environment and expect that, H5: A collaborative pair will perform their task more quickly when the objects in the environment are less dynamic.
H6: A dynamically changing environment will reduce the tolerance a collaborative pair will have for delay in the visual feedback.
We use the collaborative puzzle task paradigm originally presented in  to test our hypotheses.
In this task, pairs of participants are randomly assigned to play the role of "Helper" or "Worker".
The Helper needs to successfully describe a configuration of pieces to the Worker so she can properly arrange the pieces in their work space.
The goal is to have the Worker correctly place the four solution pieces so that they match the target solution the Helper is viewing in the shortest amount of time.
In this task, the Helper and Worker are seated in separate rooms in front of a computer with 21-inch displays.
The pairs communicate over a high-quality, full-duplex audio link with no delay.
The experimental displays for the Helper and Worker are presented in Figure 1.
The Worker's screen  consists of a staging area on the right hand side where the puzzle pieces are held, and a work area on the left hand side where the puzzle is constructed.
The Helper's screen  shows the target solution on the right, and a view of the Worker's work area in the left hand panel.
We then temporarily slotted these times into three subranges for the sake of balancing participant assignment.
Participants were selected to receive two levels from each bin and these times were crossed with the levels of linguistic complexity2.
While the primary colors were likely to be part of a shared lexicon and therefore required very little grounding to name the objects, the plaid pieces were not and required the pairs to negotiate on the terms used to represent the various pieces.
Figure 2 presents examples of the task objects.
2 It is important to keep in mind that while we discuss bins here, the variable represents an essentially continuous range.
The bins were only temporarily used in order to assign each pair to a number of delays that fell somewhere in the low, middle and high ends of the distribution.
This was done to balance the pairs across the times and not to conflate any given pairs with the range of times they received.
Participants consisted of 27 pairs recruited from the Pittsburgh area.
They were randomly assigned to play the role of Helper or Worker and the pairs were balanced by gender.
Visual Delay  and Linguistic Complexity  were manipulated within each pair.
Each pair participated in a total of six experimental conditions, three different Visual Delay times for each level of Linguistic Complexity, counter-balanced.
Pairs solved four puzzles within each experimental condition.
This resulted in a total of 24 puzzles that were completed in approximately one hour.
This cycle continues until a pre-defined number of functions have been added.
At this point, the algorithm enters the second stage where it prunes the functions until it achieves an optimized tradeoff between the number of functions and the goodness of fit.
The Generalized Cross Validation  measure is used as the model measure of goodness of fit .
The GCV measure strikes a balance between model complexity and quality of fit in a fashion similar to that of the Aikake Information Criterion  commonly used in parametric regression models.
In the models presented in this paper, we allowed the algorithm to construct up to 100 functions for inclusion.
Each of the models was evaluated using a 10-fold cross validation technique.
That is, each model is created over 10 trials, with each trial using 90% of the data to train, and the remaining 10% to test the model's performance.
Performance is optimized based on the best fit as assessed by the GCV error measure.
The pairs were instructed to complete the puzzles as quickly and accurately as possible.
We used time to complete the puzzle as the primary measure of task performance.
Nearly all puzzles were solved correctly, so error rates were a less useful indicator of performance.
To detail how processes the pairs used at varying rates of visual delay, we transcribed their interactions and present representative examples to demonstrate qualitative evidence of the communication patterns witnessed.
We used a statistical technique known as Multivariate Adaptive Regression Splines   to model the impact that the visual delay had on collaborative performance.
This technique describes the impact of the independent variables  on the dependent variable  as an optimized sequence of piecewise linear regressions3.
The algorithm finds optimal breakpoints by examining points within the range of the independent variable where slope changes are most likely to occur.
The resulting piecewise segments can then be examined to explain, for example, how much delay can be tolerated before group performance begins to suffer or to describe the rate at which collaborative performance is impacted over a particular range of delays.
The MARS algorithm is a two-stage process.
The first stage begins with a forward selection process that adds functions  to the model.
For the first stage of the analysis we used the MARS method to discover the optimal partitioning of the continuous Visual Delay factor.
We found two major breakpoints at delays 939ms and 1798ms .
These results were then used to construct an appropriate random effects piecewise linear regression model where , , , Linguistic Complexity , Block , and Trial  were repeated factors.
We also included all 2- and 3-way interactions, for each Visual Delay segment, in the analysis.
Because each pair participated in 24 trials, observations within a pair were not independent of one another and were modeled as a random effect.
Consistent with H2, the manipulation of linguistic complexity had a large impact on the speed with which the pairs could solve the puzzles.
Consistent with H1, the quicker the visual feedback provided, the faster the pairs were at completing the puzzles.
However, this result was not consistent across the entire range of delays.
Similarly, the results addressing H3 were only found for delays greater than 1798ms.
As can be seen in Figure 3, the slope for this segment is relatively flat.
In this example, the Helper describes a piece and where to put it .
However, the delayed visual feedback causes him to reiterate his directive  since he assumes his partner did not hear or did not understand.
However, when the Worker hears this, her puzzle state already indicates the correct move , and therefore she interprets his reiteration as a clarification and incorrectly adjusts the piece to the lower left of the workspace .
The Helper then sees the delayed view and believes everything is fine and confirms the placement .
Unfortunately, the Worker believes this confirmation to be about her new placement.
Shortly thereafter, the Helper sees the incorrect move and they begin a repair sequence.
This example demonstrates how the delay led to misaligned views of the task state, ultimately resulting in coordination problems that impacted task performance.
In the highest range of delays , the differences between views becomes readily apparent, and the pairs demonstrate a strategic shift whereby they exhibit less behaviors that rely on tight integration between speech and the visual information.
At these higher levels of delay, the pairs tended to attempt to complete the puzzles simply using spoken language.
This was evidenced by their relative lack of use of deictic pronouns such as "that one," and "this."
Instead, the pairs relied on lengthier verbal descriptions to describe the objects and their arrangement.
However, this posed a greater problem for Plaids than it did for the Primary pieces.
This can be seen in Figure 3, where the slope for this segment is rather steep.
In this range, every 100ms increase in visual delay slowed the pair's completion time by an additional 2.3 seconds .
The impact of delay was equally important for both the Primary and Plaid pieces, as evidenced by the fact that we found no  x Linguistic Complexity interaction .
While Figure 3 illustrates the mean increase across the two levels of linguistic complexity, there was a significant  x Linguistic Complexity interaction .
In support of H4, decomposition of this interaction reveals that the slope for the Plaid pieces remains strong and positive, while for the Primary pieces it is flat to slightly negative.
In the higher range of delays, the impact of the delay appeared to additionally affect the Plaid puzzles.
This suggests that when the delays were greater than 1798ms, it appeared as though they impacted the conversational grounding processes required to talk about the plaid pieces, while having little additional impact on the primary colored pieces which were already a part of the pairs' shared lexicon.
We describe these differences in more detail with the following qualitative descriptions of the pairs' performance.
Plaid Pieces H: um horizontal white stripe... W: any blue?
H: and two and two...and two like hor- vertical gray stripes W: horizontal white stripe with two vertical gray stripes?
H: yeah W: this one?
We temporarily slotted these times into three sub-ranges for assignment.
Participants were selected to receive two levels from each bin and these were crossed with similarly binned levels of the Object Dynamics.
Object Dynamics : We manipulated the dynamic complexity of the task objects by allowing the colors of the blocks to cycle.
Each piece changed its color, smoothly moving through the color palette.
At the Moderate cycle rate, the pieces experienced a major perceivable color change  approximately every 6-8 seconds.
It took roughly one second of continuous observation to notice whether any given piece was changing.
In the Fast cycle rate, the pieces achieved a major perceivable color change approximately every 2-3 seconds.
While at the Very Fast cycle rate, the pieces were rapidly changing colors at a rate of approximately one perceivable color change every second or less.
It should be noted that these values fluctuate somewhat due to the fact that people do not perceive change equally across the color spectrum.
However, when negotiation about the names of the pieces was required, as was the case with the Plaids, the inefficiencies of the linguistic medium became much more evident .
As can be seen in Figure 5 , the Helper and Worker both became much more active in negotiating their descriptions.
However, when the visual evidence was needed for disambiguation or confirmation, they had to wait to receive the information.
This shift in strategy likely led to the additional impact of delay on task performance for the Plaids over the Primary colors in the high range of delays.
While the results in Study 1 are consistent with the findings in Kraut et al.,  they contrast with the timings presented by Gutwin and colleagues .
In this study we aim to clarify this inconsistency by examining how the dynamics of the task objects interact with the visual delay to impact the lower-level coordination mechanisms required for successful collaborative performance.
In this study we demonstrate how the dynamics of the task interact with the amount of visual delay that can be tolerated before impacting task performance.
Participants consisted of 27 pairs recruited from the Pittsburgh area.
They were randomly assigned to play the role of Helper or Worker and the pairs were balanced by gender.
Visual Delay  and Object Dynamics  were manipulated within each pair.
Each pair participated in a total of nine experimental conditions that varied across a range of delays crossed with a range of object cycle rates.
Pairs solved four puzzles within each experimental condition.
This resulted in a total of 36 puzzles that were completed in approximately an hour and a half.
This illustration presents a stylized view of the data.
It represents the initial breakpoints  across a range of object dynamics.
The lines up to the breakpoints are slopes that are not significantly different from zero, and the subsequent trajectories represent the slope changes.
From top-to-bottom the lines represent the various levels of change rate: Very Fast, Fast, Moderate , and Static .
We discovered two optimal breakpoints in delay at 431ms and 558ms, for the objects that changed at a Moderate rate.
These results were then used in a random effects piecewise linear regression model where , , , Block , and Trial  were repeated factors.
We included all 2- and 3-way interactions in the analysis.
Because each pair participated in 36 trials, observations within a pair were not independent of one another and were modeled as a random effect.
In other words, the slope for the range of delays between 100ms and 431ms was essentially flat.
Note however, that the dynamics of task objects did substantially shorten the range of tolerable delays in comparison to those found in Study 1 when the pieces were non-changing solid colors .
For the Fast paced changing objects the optimal breakpoints were found to be at 191ms and 1783ms.
We used the same model as above with the following changes, , , .
When examining the impact of visual delay at the Fast level, we found the range of tolerable delays to be greatly reduced in comparison to the Moderate change rate .
Once again, the slope for this initial segment was essentially flat.
Once the delay reached 191ms, the trend towards an impact on task performance appeared in the appropriate direction .
However, while the plots appear to indicate an upward swing consistent with the other models, indicating an impact of delay on task performance, the slope of this shift was not significant as it was in all the other models.
This may be due in part to an increase in the amount of noise in the data given the increasing complexity of the task.
At this point there was evidence of a drastic impact of the visual delay on task performance, with every 100ms increase in visual delay increasing the pairs' completion time by approximately 14.5 seconds.
For the Very Fast paced changing objects the optimal breakpoints were found to be at 154ms and 450ms.
We used the same model as above with the following changes, , , .
When examining the impact of visual delay when the objects were rapidly changing, we found the range of tolerable delays to be smaller than for any other condition .
This suggests that there was no impact of the delay on performance in the range between 100 and 154ms.
In this case, the pairs were much more reliant on the visual information to play a role in disambiguation and comprehension monitoring.
In Study 2, as is captured in Figure 6, and in support of H5 and H6, we found that when the dynamics of the task objects increased, visual delay began to have an impact at much shorter time intervals.
This was demonstrated by the tendency for the first breakpoints to move closer to the 100ms lower bound.
Together, these results provide evidence that the dynamics of the task objects and environment have a major impact on the range of delay that can be tolerated before collaborative task performance begins to suffer.
The range of tolerable delays found in this study are much more in line with those described in Gutwin's work .
In our moderately dynamic environment we found that pairs could accommodate up to a 431ms delay.
However, as the dynamics of the task increased to our fast rate, we found that the pairs appeared to suffer performance deficits once the delay reached 191ms , and for our very fast dynamic rate the pairs could only tolerate delays up to 154ms before their performance began to degrade.
Together, these results suggest it is not as simple as picking a single number to serve as a hard threshold for dictating whether or not a given delay is tolerable for collaborative task performance.
Instead, a detailed task-analysis needs to be performed in order to establish the collaborative requirements of the task.
In these studies we demonstrated the application of a statistical method that allows us to examine collaborative task performance over a continuous range of visual delays.
This method provides detailed insight into the range of delays within which collaborative task performance is not affected, as well as uncovers the points at which performance begins to break down.
In addition, examination of the corresponding slope coefficients provides an indication of the relative impact additional delays have on performance.
This method allows us to extend the findings of earlier work that examined discrete levels of delay but could not pinpoint the precise time at which collaborative performance breaks down in the presence of delayed visual information .
In Study 1 we found that the visual delay had no impact on task performance when it was less than 939ms.
However, for the range between 939 and 1798ms the delay impacted both the Primary and Plaid puzzles equally.
The conversational transcripts suggest that these deficits in performance may be due in part to the fact that the lowerlevel coordination processes supported by shared situation awareness are disrupted.
The delayed visual information can lead to misalignments in a pair's model of the current state of the shared task .
Such misalignments, or inaccurate mental representations of task state, can severely impact low-level coordination on the part of the pairs.
At delays greater than 1798ms, the impact of the delay seemed to shift to conversational grounding processes.
This was evidenced by the fact that the  x Linguistic Complexity interaction was significant, and that there remained an increasing slope for the Plaid pieces while the slope for the Primary pieces leveled off.
The transcripts revealed that this may be due to the fact that for the Primary pieces the pairs simply resorted to using linguistic terms to describe the objects and their placement and only used the visual information for delayed confirmation .
However, when attempting to use this strategy to describe the Plaid pieces, the pairs suffered a much greater penalty for not being able to use the efficiencies of the visual space for supporting grounding on the object terms.
We examined the effects that delayed visual feedback has on collaborative task performance.
Our results demonstrate that a number of factors come into play when assessing the tolerance pairs have for visual delay.
A solid understanding of the degree to which the collaborative pairs rely on situation awareness to perform their task, knowing the amount of shared domain they share, and understanding the complexity and dynamics of the task environment are all keys to understanding how well a given technology may serve a particular group.
This research was funded by National Science Foundation Grants #99-80013 and #02-08903, and the first author was supported by an IBM PhD Fellowship.
We would also like to thank Daniel Avrahami, Ryan Baker, James Fogarty and Cristen Torrey for their thoughtful comments on early drafts of this paper, and Rachel Wu for her efforts during data collection.
The effects of visibility on dialogue and performance in a cooperative problem solving task.
How conversation is shaped by visual and spoken evidence.
Approaches to studying worldsituated language use: Bridging the language-asproduct and language-as-action traditions, 95-129.
Cambridge University Press, New York, NY, US.
Speaking while monitoring addressees for understanding.
Definite reference and mutual knowledge.
Referring as a collaborative process.
Speaker interaction: Video teleconferences versus face-to-face meetings.
In Proceedings of Teleconferencing and Electronic Communications, 189-199.
Smoothing noisy data with spline functions.
Estimating the correct degree of smoothing by the method of generalized crossvalidation.
Toward a theory of situation awareness in dynamic systems.
Situation awareness analysis and measurement.
Gaze targets during collaborative physical tasks.
In Proceedings of ACM Conference on Human Factors in Computing Systems  , 768-769.
Action as language in a shared visual space.
In Proceedings of ACM Conference on Computer Supported Cooperative Work , 487-496.
Language efficiency and visual technology: Minimizing collaborative effort with visual information.
