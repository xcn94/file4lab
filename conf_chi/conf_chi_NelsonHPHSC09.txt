Les Nelson1, Christoph Held2, Peter Pirolli1, Lichan Hong1, Diane Schiano1, and Ed H. Chi1 11 2 Palo Alto Research Center  Knowledge Media Research Center 3333 Coyote Hill Road Konrad-Adenauer-Str.
In prior work we reported on the design of a social annotation system, SparTag.us, for use in sensemaking activities such as work-group reading and report writing.
Previous studies of note-taking systems have demonstrated behavioral differences in social annotation practices, but are not clear in the actual performance gains provided by social features.
This paper presents a laboratory study aimed at evaluating the learning effect of social features in SparTag.us.
We found significant learning gains, and consider implications for design and for understanding the underlying mechanisms in play when people use social annotation systems.
SparTag.us uses keyword tags and highlights as a means to collect paragraphs of interest in web pages.
A Click2Tag interface offers a low-cost option for the user to annotate paragraphs using simple interactions made directly on the content being read.
SparTag.us automatically extracts the annotated paragraphs from the page and inserts them into a system-created notebook, along with the URL of the page.
Moreover, users may subscribe to and follow the annotations of another user by designating that user as a friend.
Figure 1 shows a portion of a friend's notebook as viewed by the user.
The user's highlights are in yellow, and tags are in red.
The friend's annotations are displayed in light blue with tags attached to the end of the paragraph.
If there were multiple friends highlighting one paragraph, all the friends' highlights would have been aggregated.
Social annotation systems such as SparTag.us  and del.icio.us have been designed to encourage individual reading and annotation behaviors that, when shared, accumulate to build collective knowledge spaces.
In a recent longitudinal classroom study, Kalnikaite and Whittaker  report correlations suggesting a positive impact of social annotations on learning, but a causal relation remains to be shown.
Other studies suggest that while people often seek out previously highlighted and annotated content , they can be adversely affected by inappropriate annotations, even when warned about such adverse effects .
In the present study, we test whether annotations designed to represent the output of a subjectmatter expert accelerates the learning of users with access to those annotations during a given sensemaking task.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Our experimental approach to measure learning within a complex social annotation setting was to:  define an ecologically valid, realistic sensemaking task in a technical domain;  develop domain-specific knowledge tests as instruments to measure performance learning gains in that setting; and  vary the conditions to distinguish the impact of socially-constructed annotations.
This is a general category of tasks familiar throughout the academic and professional world that includes the activities of task forces, work groups, classroom group projects, and so on.
The conditions WS and SO were control conditions in which individuals read web content without access to others' annotations.
To provide for an ecologically valid comparison, WS participants could take notes in MS Word or with pen and paper.
In the SF condition, people independently read web content but also had access to social annotations created by an experimenter-simulated subject-matter expert .
Our hypothesis was that participants with access to tags and highlights made by an expert would perform better than participants without the same access.
We thus evaluate performance measures between subjects in the experimental condition, SF, with those in control conditions, SO and WS.
The way we constructed the SparTag.us friend for the study was to provide clear and succinct summaries of key content derived from social sources .
The overall organization of topics was then mapped to specific content that exemplified the topic.
The representation of this knowledge was given in-situ to the source materials and through clipped, annotated collections of the relevant information with back links to the sources.
The top 20 tags associated with the top 100 annotated URLs returned by a del.icio.us query on "enterprise mashup" were used as the target tag cloud in SparTag.us.
A set of URLs covering these keywords  were manually tagged using SparTag.us.
A sufficient number of URLs were tagged with a given tag to produce the target tag cloud.
This process created a repository of a friend persona named "mjones".
Each SF participant was given access to this friend notebook at the start of learning.
Participants  were solicited from various sources, including participants from other studies, company interns and a local university job list.
The only screening concerned having no previous use of the SparTag.us tool.
As participants arrived, they were assigned to one of the three groups in round-robin order to obtain six participants per group.
There was an even gender split.
Six participants reported having some education in computing.
We chose the study topic domain of "Enterprise 2.0 Mashups", which is a combination of the technology areas of "Enterprise 2.0" and "Web 2.0 Mashups".
This choice required participants to find and understand many web pages because at the time of the study there was no single good source of information on the topic area.
The area was relatively new, with visible activities in conferences and companies providing such capabilities.
Participants in all three groups were asked to find and read material in order to write reports on Enterprise 2.0 Mashups.
There were two writing tasks, each specified by three questions that needed to be addressed.
The writing tasks were aimed at eliciting what someone reasonably skilled in the area `should be able to answer'.
The questions to be addressed in the writing tasks were derived from a survey of experts.
We measured learning by assessing domain-specific knowledge about enterprise 2.0 mashups prior to the task  and after , in all three groups.
Two lists of 20 true-false questions were created, and each list was used as a Pretest for half the participants in each group and as a Posttest for the other half.
The true-false questions were elicited from experts.
Each list of 20 questions was designed to have an even distribution of easy and hard questions about enterprise mashups, as rated by 100 random people on Amazon Mechanical Turk  .
Questions were designed to minimize prompting of participants' subsequent learning.
Both tests were taken without access to tools or resources.
A two-day test schedule was established, refined through pilot testing and applied to 18 participants covering all conditions.
The first day involved a four-hour session of demographic and background survey, tool training, knowledge pretest, learning in the domain area, knowledge posttest, and essay writing.
Training in SparTag.us involved a short video and then a guided walkthrough of features.
Participants were given a brief written statement of learning objectives, instructing them to read from any sources and take notes as they felt appropriate regarding the definitions, standards, benefits, issues, and examples relating to the topic area.
Participants had one hour of unsupervised learning, with a break for lunch, and then 50 more minutes of learning.
The writing activity was separately prompted by a different set of questions and limited to 30 minutes.
A second session was held a week later involving a second writing task .
Sessions were logged, including URLs visited, content scrolling, and words written.
In debrief interviews, people were asked to talk about strategies used for learning, question answering, writing, and tool use.
Participants were randomly assigned to computers configured with Windows XP and Mozilla Firefox .
This score has the advantage of normalizing the observed gain  against the amount of possible learning that could be achieved .
The WS and SO groups were not significantly different.
Regression analyses identified two background questions showing significant relationships to the gains scores across all groups : * IT Learning: I enjoy learning about information technology and new developments in this field * Web Use: On average, how many hours per week do you spend on the World Wide Web?
Overall, the largest impact on gains was the presence of the expert friend annotations in the SF condition.
Using SparTag.us without access to these expert annotations  did not yield any learning gains over the use of the standard note-taking tools .
There is individual variability in the behavioral data, though also trending in interesting directions .
We see that on average SF participants visited fewer URLs, but spent more reading time on those they visited.
The SparTag.us users  scrolled content more on average, indicating more reading was occurring.
Again, these patterns were consistent after each round of participants.
Participants in the SF condition all show activity with the simulated friend.
All SF participants looked at the friend's repository early .
In debriefing , not all showed appreciation for the actual friend-annotated content, particularly when they believed that the questions of the learning task were not directly addressed by the seeded content.
Each did consider the information offered there, and followed it or found alternatives they thought were more appropriate.
Although not statistically significant, the performance measure of the use of domain terms in writing show a trend towards favoring the SF condition .
This pattern was consistently seen after each round of six participants was run and preliminary analyses were made.
SF learning gains were significantly higher than those of SO and WS, with SO not significantly different from WS.
Activity with the friend's seeded information is visible during learning.
Trends indicate SF participants visited fewer URLs, spent more time reading/scrolling those, and wrote more in the essay.
Id SF1 Quote about Friend I checked on that , but I couldn't find the kind of information I was looking for there...
I went back to the Friend afterwards  and found another thing to add .
I started off with the recommendations for websites that my friend saved taking into account that he seemed to be an expert.
I was knowing that a person had done this, even if he's not my friend, but he's just like a somewhat trustful person...just the fact that it was a human being made a huge difference to me.
But that being said, I never like looked at his tags, because he had a different tagging system, ...it was like this word, you know, "data sources", "client"...they were like too broad ... some of these could've been useful to me, maybe "SLA" and "SOA".
It's not what I would've thought of...
The first time I checked the friend was about 20 minutes.
Because first I wanted to get an idea myself, so I could evaluate how good my friend is.
I found only very few information that was useful I hadn't found already.
So I used him not very much.
But still, if I saw an article in his listing that I also tagged that made me feel better.
If Google thinks it's important, my friend thinks it's important, then it must be important.
That one  I didn't find particularly helpful.
Because for one thing this computer terminology is totally new to me.
So I had to go back and read.
I got what "RSS" stands for and all that stuff.
I did find that after that I began to appreciate  better and especially a couple of places with making money.
I found that interesting and helpful.
I mean I did sort of surprise myself using the Friends Web links, because that was new.
I did find myself highlighting a lot, but again I think it's more like the, it's more of the compulsive behavior, because when I highlight I really want to like physically write that stuff down.
The SparTag.us expert notebook functions as a kind of scaffold for learning, and serves as an advantage over participants without the scaffold.
The notebook not only provides sample reading material that might be useful, but the sample paragraphs and tag cloud also provides for a kind of preview of sample terms that might serve as navigational signposts.
These previews are all different kinds of schemata, and address possible organizational/abstraction processes at work.
In this paper, we describe a first step in grounding our understanding of the impacts of one social annotation technology on people's information foraging practices.
People with access to well structured artifacts left by others do show measurable learning improvements.
Our future plans are to explore the social sensemaking processes in action and expand the unit of analysis to groups collaborating synchronously and asynchronously, examining the timeliness and manner of delivery of expertise in the context of ongoing reading activities; and looking at the role of expertise and social familiarity in perceiving annotations.
We are currently investigating different analytical approaches we may apply to the data collected to better understand the role of social annotations.
In getting this result, we encountered the tradeoff between ecologic validity and statistical power in the experiment design.
Our priority was to increase ecologic validity, take multiple measures enabling the analysis of covariance, and control some sources of variability using a systematic procedure and the constant set of stimuli .
For example, by including IT Learning and Web Use as individual difference covariates in the reported analysis of covariance we were able to partial out a good portion of the variance due to participant differences and thereby reduced our error of measurement to get the significant result that was reported.
So we gained statistical power through additional subject measurements.
One line of further inquiry that may guide us towards testable explanations of the gains seen using SparTag.us may be found in `schema theory'.
If a user of SparTag.us does not already have the necessary background knowledge or schema, then she will be forced to obtain the background knowledge during reading.
