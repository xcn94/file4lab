Flexpad is an interactive system that combines a depth camera and a projector to transform sheets of plain paper or foam into flexible, highly deformable, and spatially aware handheld displays.
We present a novel approach for tracking deformed surfaces from depth images in real time.
It captures deformations in high detail, is very robust to occlusions created by the user's hands and fingers, and does not require any kind of markers or visible texture.
As a result, the display is considerably more deformable than in previous work on flexible handheld displays, enabling novel applications that leverage the high expressiveness of detailed deformation.
We illustrate these unique capabilities through three application examples: curved cross-cuts in volumetric images, deforming virtual paper characters, and slicing through time in videos.
Results from two user studies show that our system is capable of detecting complex deformations and that users are able to perform them quickly and precisely.
Projecting visual interfaces onto movable real-world objects has been an ongoing area of research, e.g.
By closely integrating physical and digital information spaces, they leverage people's intuitive understanding of how to manipulate real-world objects for interaction with computer systems.
Based on inexpensive depth sensors, a stream of recent research presented elegant trackingprojection approaches for transforming real-world objects into displays, without requiring any instrumentation of these objects .
None of these approaches, however, interpret the deformation of flexible objects.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Flexible deformation can expand the potential of projected interfaces.
Deformation of everyday objects allows for a surprisingly rich set of interaction possibilities, involving many degrees of freedom, yet with very intuitive interaction: people bend pages in books, squeeze balls, model clay, and fold origami, to cite only a few examples.
Adding deformation as another degree of freedom to handheld displays has great potential to add to the richness and expressiveness of interaction.
We present Flexpad, a system that supports highly flexible bending interactions for projected handheld displays.
A Kinect depth camera and a projector form a cameraprojection unit that lets people use blank sheets of paper, foam or acrylic of different sizes and shapes as flexible displays.
Flexpad has two main technical contributions: first, we contribute an algorithm for capturing even complex deformations in high detail and in real time.
It does not require any instrumentation of the deformable handheld material.
Hence, unlike in previous work that requires markers, visible texture, or embedded electronics , virtually any sheet at hand can be used as a deformable projection surface for interactive applications.
Second, we contribute a novel robust method for detecting hands and fingers with a Kinect camera using optical analysis of the surface material.
This is crucial for robust captur-
Since the solution is inexpensive, requiring only standard hardware components, it can be envisioned that deformable handheld displays become widespread and common.
The highly flexible handheld display creates unique novel application possibilities.
Prior work on immobile flexible displays has shown the effectiveness of highly detailed deformations, e.g.
Yet, the existing inventory of interactions with flexible handheld displays  is restricted to only low detail deformations, mostly due to the very limited flexibility of the displays and limited resolution of deformation capturing.
Flexpad significantly extends the existing inventory by adding highly flexible and multi-dimensional deformations.
We present three application examples that each leverage rich and expressive, highly flexible deformations.
We show how highly flexible displays support intuitive and effective exploration of the volumetric data set by allowing the user to easily define curved cross-sections, to get an overview, and to compare contents.
The second application enables children to deform and animate 2D characters .
These applications demonstrate the utility of Flexpad and introduce transferrable ideas for future handheld devices that use active flexible displays.
To evaluate the feasibility of our approach, both with respect to technology and human factors, we conducted two evaluation studies.
They show that the tracking provides accurate results even for complex deformations and confirm that people can perform them fast and precisely.
The remainder of this paper is structured as follows.
After reviewing related work, we present the concept and implementation of Flexpad.
This is followed by an overview of applications.
Finally, we present the results of two evaluation studies and conclude with an outlook on future work.
LightSpace  uses several depth cameras and projectors to augment walls and tables for touch and gesture-based interaction with projected interfaces.
KinectFusion  automatically creates a very detailed model of a static scene, enabling physics-based interactions on and above surfaces and objects.
Omnitouch  uses a depth sensor to support multitouch input on projected interfaces on virtually any noninstrumented surface.
However, none of these approaches model real-time deformation of objects.
Very recent work  uses proxy particles to model deformable objects, allowing for a range of physics-based interactions that are based on collision and friction forces.
However, tracking particles over a sequence of frames requires optical flow, which is not possible with untextured surfaces.
Other work  captures deformable surfaces from Kinect depth data, but the approach does not support capturing detailed deformations in real time, which is critical for interactive applications.
We present an approach that is capable of capturing the pose and detailed deformation of a deformable surface from depth data in real-time, to support very fine-grained interactions and natural deformations.
Due to a novel detection of hands and fingers, our approach is very robust to occlusions, which is crucial when users deform objects in natural settings.
Our research is also influenced by existing work on deformation-based handheld interfaces.
Here, two streams of work can be distinguished: First, prior work proposed using a deformable tape or sheet as an external input device, to control GUI applications by deformation.
Output is given on a separate, inflexible display.
An early and influential work is ShapeTape  that allows users to create and modify 3D models by bending a tape that is augmented by a deformation sensor.
Other applications use a deformable sheet for controlling windows on the computer desktop  and for AR purposes .
A second type of research, more directly related to our work, studies deformation of handheld displays.
Prior work has investigated interactions with interfaces of flexible smart phones and e-readers  and for volumetric datasets .
Most of this work uses projection or simulates flexible displays with existing inflexible displays; PaperPhone  and Kinetic  use an active flexible display.
A recent study  examined how device stiffness and the extent of deformation influenced deformation precision.
While no pronounced influence was identified, users preferred softer rather than rigid materials.
This stream of work represents an important step towards fully flexible displays, yet existing work is limited by quite restricted deformability.
It supports only slight bending and captures only simple deformations, which ultimately limits the expressiveness of deformation to mostly one-
Our work relates to a body of research that realizes handheld displays using optical capturing of the projection surface and spatially aligned projection, either with a static or a handheld projector, e.g.
Many methods exist for capturing the pose and deformation of a mobile projection surface - information required for accurate texture mapping.
Existing methods include passive or active markers, stereo cameras, structured light, time of flight, analysis of visible texture via features , color distances  or optical flow  as well as analysis of the object's contour .
The commercial availability of inexpensive depth cameras has recently stimulated a lot of re-
The display's weight is 12 grams.
The shape-retaining display retains its deformation even when the user is not holding it.
It consists of 2 mm thick white foam with Amalog 1/16" armature wire attached to its backside.
It allows for more detailed modeling of the deformation, for easy modifications, as well as for easy holding.
To characterize its deformability, we measured the minimal force required to permanently deform the material.
We affixed the sheet at one side; a torque as small as 0.1Nm is enough to permanently deform the material, which shows that the material is very easy to deform.
The sheets' weight is 25g.
Flexpad enables users to interact with highly flexible projected displays in interactive real-time applications.
The setup consists of a Kinect camera, a projector, and a sheet of paper, foam or acrylic that is used as projection surface.
Our current setup is illustrated in Fig.
A Kinect camera and a full HD projector  are mounted to the ceiling above the user, next to each other, and are calibrated to define one coordinate space.
The setup creates an interaction volume of approximately 110 x 55 x 43 cm within which users can freely move, rotate, and deform the projection surface.
The average resolution of the projection is 54 dpi.
The user can either be seated at a table or standing.
The table surface is located at a distance of 160 cm from the camera and the projector.
Our implementation uses a standard desktop PC with an Intel i7 processor, 8 GB RAM, and an AMD Radeon HD 6800 graphics engine.
While we currently use a fixed setup, the Kinect camera could be shoulder-worn with a mobile projector attached to it, as presented in , allowing for mobile or Fig.
2: Physical setup nomadic use.
The key contribution of Flexpad is in its approach for markerless capture of a high-resolution 3D model of a deformable surface, including its pose and deformation .
The system is further capable of grabbing 2D image contents from any window in Microsoft Windows and projecting it correctly warped onto the deformable display.
Alternatively, it can use 3D volume data as input for the projection.
The detailed model of deformation enables novel applications that make use of varied deformations of the display.
We will present some example applications below.
Existing image based tracking algorithms of deformable surfaces  rely on visible features.
In this section, we present an approach that requires only image data from a Kinect depth sensor.
This allows for tracking an entirely blank  deformable projection surface in high detail.
Moreover, since the depth sensor operates in the infrared spectrum, it does not interfere with the visible projection, no matter what content is projected.
A challenge of depth data is that it does not provide reliable image cues on local movement, which renders traditional tracking methods useless.
The solution to this problem is a more global, model-based approach: We introduce a parameterized, deformable object model that is fit into the depth image data by a semi-global search algorithm, accounting for occlusion by the user's hands and fingers.
Sheets of many different sizes and materials can be used as passive displays, including standard office paper.
In our experiments, we used letter-sized sheets of two materials that have demonstrated good haptic characteristics: The fully flexible display is 2 mm thick white foam .
It allows the user to change between different deformations very easily and quickly.
The bending stiffness index Sb* of this material was identified as 1.12 Nm7/kg3, which is comparable to the stiffness of 160g/m2 paper.
While the user is interacting with the display, hands and fingers partially occlude the surface in the depth image.
For a robust deformation tracking, it is important to classify these image pixels as not belonging to the deformable surface, so the model is not fit into wrong depth values.
The proposed tracking method is able to handle missing data, so the solution lies in removing those occluded parts from the input images in a preprocessing step.
Due to the low resolution of the Kinect depth sensor it is difficult to perform a shape-based classification of fingers and hands at larger distances.
In particular, when the finger or the flat hand is touching the surface, the resolution is insufficient to differentiate it reliably from the underlying surface.
Existing approaches use heuristics that work reliably as long as the back of the hand is kept at some distance above the surface .
These restrictions do not hold in our more general case, where people use their hands and fingers right on the surface for deforming it.
We introduce optical surface material analysis as a novel method of distinguishing between skin and the projection surface using the Kinect sensor.
It is based on the observation that different materials have different reflectivity and translucency properties.
This property causes the point pattern, which is projected by the Kinect, to blur.
The display surface, made out of paper, cardboard, foam or any other material with highly diffuse reflection, varies from skin in reflectivity as well as translucency; so low peak values or low gradient values provide a stable scheme to classify non-display areas.
The projector inside the Kinect has a constant intensity and the infrared camera has a constant shutter time and aperture.
Hence, the brightness of the point pattern is constant for constant material properties and object distances and angles.
As a consequence, the surface reflectivity of an object with diffuse material can be determined by looking at the local point brightness.
However, because the brightness in the infrared image decreases with increasing distance, the local gradients and peak values have to be regarded with respect to the depth value of the corresponding pixel in the depth image.
Pixels that cannot be associated to a dot of the pattern are not classified directly, but receive the same classification as the surrounding pixels by iterative filtering.
Results from an evaluation, reported below, show that this classification scheme works reliably at distances up to 1.50 m from the camera.
A drawback of this model is that it lacks the ability to shift the starting point of a deformation, e.g.
To add this ability to the deformation model while keeping it computationally efficient and robust through low dimensionality, an additional mapping function for the z components of the vertex position is applied after the deformed vertex positions are calculated.
The mapping can change from a square-like function, via the identity function to a square root-like function.
This parameterized function is applied to every zcomponent of each surface vertex to change the shape of the deformation.
This largely increases the set of deformations in the model by only adding one additional deformation parameter.
In summary, as shown in Fig.
4, the deformation model is a 15 dimensional vector, containing the angles of the 8 basic deformations, the z mapping parameter as well as 6 variables for the degrees of freedom that are required for the affine 3D transformations.
Figure 4  gives some examples of the complex deformations the model supports.
Without local movement cues in the image, such as feature movement or optical flow, and with incomplete depth data, it is important to formulate very precisely what the tracking method should find in the image, i.e.
We present a model that is generic enough to approximate a wide variety of deformations, including complex ones, while still being efficient to calculate.
The model is based on a rectangular 25 x 25 vertex plane at the size of the actual display surface.
Using 25 vertices per dimension proved to provide stable tracking results while keeping the computational costs small.
It is important to note that all of these basic deformations are combined by using weighted factors, such that the model supports complex curvatures.
Given this deformation model, the overall tracking goal can be defined as finding the parameters of the model that describe an object that, when synthesized as a depth image, matches the input depth image best.
Such approaches are known as Analysis by Synthesis  or direct tracking methods .
The advantage of such methods is the ability to work completely without information on feature movement or optical flow.
Such evaluation allows for formulating the tracking as an optimization problem, turning the tracking task into a mathematically well-defined objective of finding the parameter vector that produces the most suitable deformation.
To determine the deformation parameters, the AbS approach synthesizes the depth measurements of the 25x25 pixels to which the model vertices would be projected and compares the vertex-camera distance to the real input depth image.
Thereby, areas that were identified as skin by the occlusion handling step are ignored.
Due to the uneven measurements and the noise in the Kinect image, this error function contains many local minima, making it intractable for common local least squares optimization.
Hence, we apply the CMA-ES optimization scheme  to find an approximation of the optimum since CMA-ES combines two important properties: it is a global optimization algorithm, which allows for coping with large numbers of local minima, and it applies a smart distribution scheme minimizing the necessary number of function evaluations to find the optimum.
The optimization process is initialized at the first frame with a vertex set describing a planar surface of the display size and shape, placed at the center in the depth image.
In the beginning of each following frame, the optimization is initialized with the optimization result of the preceding frame.
This allows for initially finding the sheet not only at this position, but also at different locations and orientation within the instrumented volume.
Should the depth error of the optimization result rise above a given limit, it is most likely that the tracking failed, e.g.
In this case, the initialization step is automatically executed again, until the depth error falls below this limit.
To further increase the performance, our approach leverages the fact that the user cannot make high precision deformations while moving the object quickly, but only when the object is held steadily or slowly moved.
The number of iterative optimization steps per frame is automatically adjusted by the pose difference of the preceding frames.
During large movements, the number of iterations is reduced .
During very slow movements, the number is increased, smoothly reducing the frame rate , allowing for the highest possible accuracy of deformation capturing.
Flexpad supports many application and interaction possibilities.
Its unique strength is the high flexibility of the display and the detailed capturing of its deformation.
A direct 1:1 mapping between the deformed physical shape and virtual model space enables novel applications with expressive interactions that were not possible with rigid or slightly flexible handheld displays.
It is worth emphasizing that all proposed interactions also transfer to future active displays.
Analysis of volumetric images and datasets is important in many fields, such as medicine , geology and earth sciences .
Prior research has shown that curved cross-sections are required for analyzing important medical phenomena .
We contribute interactions that significantly go beyond existing work on visualization of volumetric datasets, which support only planar cross-cuts , only very restricted curvatures  or make curvature very hard to control .
Our application maps the volumetric dataset to a 3D volume in physical space and lets the user slice through that volume with the display.
By deforming the highly flexible display, the user can easily create non-planar cross-sections to analyze a large variety of curved structures that would not be visible on planar or only slightly deformed crosssections.
For detailed analysis of a cross-section, the user can lock the view by pressing a foot button.
For instance, this allows for flattening the view and easy measurement of distances that were on a curve in the original view.
It also allows for handing the view over to a colleague.
It is worth noting that this application is not only helpful for medical experts, but also supports non-experts who are interested in exploring the inner workings of the human body in an intuitive way.
In addition to following a layer, deformation also proves powerful in supporting curved cuts across layers.
Consider the example of a reservoir engineer exploring the best locations for a new well in an oil field.
The engineer can select the desired curvature of the well  by bending the display and then skim through the entire volume.
Deformation also supports better orientation and overview in the dataset.
Furthermore, deformation of the highly flexible display provides an intuitive way of comparing data across layers.
By deforming the display into a wave bend, the user sepa-
Our approach allows for robust real-time tracking of a large variety of deformations with a variety of deformable surfaces.
However, due to the real-time constraint and restrictions of the depth sensor, several limitations apply.
Obviously, it is only possible to track deformations that can be observed directly by the Kinect camera.
Folding, which occludes large parts of an object, as well as very steep bending angles that occlude parts of the object to the camera cannot be handled by the presented system.
By varying the bend, the user can smoothly select the distance between both slices.
Our proposed interactions can be easily integrated with functionality for cross-cuts of volumetric datasets that have been presented in prior work, such as additional views on external displays , panning and zooming, or saving of 2D snapshots for subsequent analysis and presentation.
However, this functionality is best illustrated in the video that accompanies this paper.
In contrast to the fixed display of Khronos, the flexible handheld display allows for defining a curvature, i.e.
Moreover, the handheld display allows for selecting any frame of the video to start with deforming, whereas deformations in Khronos always start at the topmost frame, going in only one direction.
This allows for novel, expressive interactions with videos.
To demonstrate the wide applicability of highly deformable handheld displays, we next present an application for children that leverages deformation as a simple and intuitive means for animating paper characters in an interactive picture.
High-resolution deformation allows very individualized and varied animation patterns.
For example, fish may move with different speeds and move their fins.
A sea star may lift some of its tentacles.
A sea worm may creep on the ground.
A flatfish may meander with sophisticated wave-form movements and seaweed may slowly bend in the water.
Such rich deformation capabilities can be easily combined with concepts from previous research on animating paper cut-outs  that address skeleton animation, eye movement, and scene lighting.
Also 3D models could get animated by incorporating as-rigid-as-possible deformation .
It is worth noting that our concept can be readily deployed with standard hardware.
A third application of Flexpad enables people to create slices through time in videos, inspired by the ingenious Khronos projector .
In contrast to Khronos, which required a very bulky, several cubic meters big, immobile setup with a fixed screen, Flexpad brings similar functionality to virtually any sheet of paper.
The user can load a video from YouTube, which is automatically mapped to a virtual volume, whereby time is mapped to the z dimension.
By moving and deforming the display within this volume, the user creates ever new combinations of different moments in time that open up new artistic perspectives on the video.
To evaluate the precision of tracking in a natural real-time scenario, we recruited 10 volunteer participants .
Their task was to use the slicing-throughtime application and freely create interesting renderings by deforming the display.
This application is particularly well suited for a technical evaluation of the system, for the unstructured nature of the interface stimulates users to deform the display in highly varied, complex ways.
We recorded the raw video stream from the Kinect camera while the user performed the task, overall 35 minutes of footage.
We used this data as an input for our algorithm, calculating the RMS error in each frame from the distance between the model surface to the corresponding depth image values.
This shows that the tracking performs very adequately even in challenging realistic tasks.
To test the abilities of the tracking method in a more distinct way, we additionally picked a set of 8 deformations, ranging from simple bends to complex deformations.
Each deformation was recorded 20 times at 90 and 150 cm distance to the Kinect camera.
Figure 8 shows the deformations and the average RMS error for each deformation.
In addition, we evaluated the preprocessing step, which removes hands and fingers from the input data, in an informal study.
The Kinect data of hands from 10 users  occluding three different display materials  was recorded.
A perspective rendering of the target shape was displayed before each trial.
The trial was solved when the target shape was held during 250ms.
The completion time for each trial was measured.
After each trial, the participant had to rate the perceived difficulty on a five-point Likert scale.
Each of these classes contains not only the basic deformation, but all variations of it, on all four sides of the display, and each also flipped vertically.
This set of deformations was informed by the deformations made in our application examples, and for reasons of feasibility of this study, focused on deformations along one dimension in landscape format.
From each deformation class, we randomly selected 2 deformations for each participant.
Each deformation had to be performed with two precision levels: +/- 8 degrees and +/- 6 degrees.
In a pre-study with 5 participants, we identified these precision levels as challenging, but still feasible, whereas a level of +/- 4 degrees turned out not to be reliably reachable.
The sequence of trials was randomized.
To compare influences of the display material, participants performed the tasks with the fully flexible and with the shape-retaining display, as described in the system overview section.
In summary, the within-subject, multifactorial experimental design was as follows: 5 deformation classes x 2 instances per class x 2 precision levels x 2 materials x 10 participants = 400 trials Before the experiment, participants could practice the tasks with both materials until they felt fully confident.
After the experiment, participants freely explored a medical dataset in the volumetric application as well as the slicing-throughtime application.
This was followed by a semi-structured interview.
Each session lasted approximately 1 hour.
For the feasibility of the Flexpad approach, it is essential that users are able to perform deformations with the display with ease and sufficiently high precision.
To evaluate these human factors, we conducted a controlled experiment with users.
Our aim was to evaluate how fast and precise users are able to perform deformations and to examine the influence of flexible vs. shape-retaining display materials.
Our hypothesis was that the task completion time increases with the complexity of the deformation and the level of precision required.
Moreover, we hypothesized that the shaperetaining material would increase task completion times.
We aimed at quantifying this influence.
As such, our study adds to an emerging body of experiments on the manipulation of paper-like displays, including target acquisition performance in 3D space with rigid paper displays  and performance of deforming slightly flexible displays .
We recruited 10 volunteer participants .
The two participants who performed the task fastest were offered a $10 gift card.
Figure 11  depicts the average trial completion times for the five basic deformations and both materials for the lower precision level.
With the fully flexible material, times range between 2.4 and 8.8 seconds; with the shape-retaining material, values range from 2.6 to 10.8 seconds.
Figure 11  depicts those values for the higher precision level.
In this case, times range between 3.6 and 15.6 as well as 4.8 and 16.5 seconds, for the two materials, respectively.
The rather high standard deviations reflect different levels of manual dexterity of users.
We first performed a multifactorial repeated measures ANOVA with all factors, including rotation and vertical flip of the deformations.
We found no significant effect of rotation and vertical flip; therefore, in the following analyses, we group the trials into the five basic deformations, regardless of their rotation and vertical flip.
To identify systematic dependencies, we performed a rANOVA with the factors deformation, precision level, and material.
As expected, more complex deformations, a higher precision level, and the shape-retaining material resulted in higher task performance time.
On average, completion time of trials that were performed with the shape-retaining material was 33% longer than of trials with the fully flexible material.
When the higher precision level was required, completion time of trials was 46% longer than of trials performed at the lower precision level.
To further analyze differences between the five deformation classes, we conducted post-hoc tests with Bonferroniadjusted alpha-levels of .01.
This shows a distinctive difference between deformations requiring bending only one versus both sides.
Simple  deformations had an average task completion time of 3.67 seconds .
Even the most difficult deformations, in high precision level, resulted in an average task performance time of 15.6 seconds .
Analyzed separately for all combinations of deformation, precision level and material, the average ratings range from 1.2 to 3.1, i.e.
Perceived difficulty and completion times of the trials showed a very high bivariate correlation .
Therefore, our following analyses are based on the completion times, but transfer to the difficulty of the task.
We identified one exception to the general finding that the deformations were performed without difficulties: the asymmetric wave deformation in the higher precision level, performed with the shape-retaining material.
While average time is in line with those of the other complex deformation, the standard deviation is much higher, as can be seen in Fig.
Statistical testing using Grubbs' Test for outliers showed that this SD is indeed an outlier 
This high SD shows that some participants were able to perform this deformation rather quickly, whereas other participants could only perform it reliably within a disproportionately long time.
As presented above, and in accordance with our hypothesis, the ANOVA found a significant effect of material.
Because of the advantages of the shape-retaining material for complex deformations that were stated above, we will now analyze this effect in more detail for double-sided deformations .
The penalty on completion time in the lower precision level that is due to the shape-retaining material averages out at 95% with symmetric wave, 56% with center and 24% with asymmetric wave.
For the higher precision level, the penalties introduced by the shape-retaining material are 77%,
21%, and 6%, for the three deformations respectively.
Most notably, the penalty introduced by the shape-retaining decreases with increasing difficulty of the deformation and increasing level of precision.
This relationship is illustrated by significant medium-sized negative correlations between the difficulty of the deformation  and the penalty on completion time, separated for precision level .
A similar result was obtained when difficulty of the deformation and level of precision combined were correlated with the penalty on completion time .
Results from the technical evaluation show that our tracking approach performs with high accuracy to support all of the proposed applications.
Even in a technically very challenging task that encouraged participants to make very complex arbitrary deformations, the average error was below 7 mm.
These values can be further decreased in a mobile setup, where the Kinect camera is closer to the projection surface.
Then, the average RMS error for many of the deformations is close to the random noise of the Kinect sensor.
The results of the controlled experiment show that users are capable of using highly flexible displays with ease.
Users can make various single and dual deformations with both materials quickly and easily, even with a high precision level of +/- 6 degrees.
The single exception is the asymmetric dual deformation, which, under high precision level, some participants had difficulties performing reliably with the shape-retaining material.
Therefore, we suggest for this deformation a lower bound of +/- 8 degrees as the maximum precision if time is an issue in the application.
The results further showed that use of a shape-retaining material has only a modest average influence on task performance.
While the fully flexible material is better suited for applications with only simple deformations, the shaperetaining material is well suited for use in applications with complex deformations.
Notably, the time penalty introduced by the shape-retaining material decreases with increasing complexity of the deformation and the level of precision required.
For the most complex deformation, it induces a temporal penalty of only 6 %, while it allows for easily holding deformations over time, for easily modifying them, and for modeling curvatures that involve more than two deformations.
These quantitative findings are underpinned by qualitative observations.
In the interviews, many participants commented that they preferred the fully flexible material for the task of the controlled experiment, whereas they preferred the shape-retaining material for analyzing curved phenomena in the medical dataset.
For instance, P3 commented: "The flexible one is easier to do the first assignment.
But this one  is easier to keep.
I can hold it even with just one hand.
And I know that I gonna have what I want.
P4 stated: "The less flexible material allows me to freeze a structure.
P7 commented: "I prefer the sensation of the flexible material, but the other one gives me more precision."
In future work, we plan to perform a qualitative analysis of our video recordings to analyze which deformations participants spontaneously made with both materials.
A logical extension of Flexpad is touch input.
With respect to this question, future work should address several challenges.
Technically, this involves developing solutions for Kinect sensors to detect touch input reliably on real-time deformable surfaces.
On a conceptual level and building on Dijkstra et al.
This is required to identify areas that are reachable for touch input and to inform techniques that differentiate between touches stemming from desired touch input and false positives that result from the user touching the display while deforming it.
An informal analysis from our study data showed that users made single deformations almost exclusively by touching the display close to the edges and on its backside.
This also held true for dual deformations with the fully flexible display material.
This suggests that touches in the center area of the display can be reliably interpreted as desired touch input.
In contrast, the participants touched all over the display for dual deformations with the shape-retaining material.
A simple spatial differentiation is not sufficient in this case; more advanced techniques need to be developed, for instance based on the shape of the touch point or on the normal force involved.
While this work focuses on projected displays, all of our application examples transfer to future active flexible displays.
Currently available prototypes are still very limited in their flexibility, so that they cannot be used to realize our concepts.
Given the rapid advancements in display technology, this is very likely to change in the future.
Future work should investigate smart materials for flexible displays.
As discussed above, both fully flexible and shaperetaining displays have unique capabilities.
A material that can programmatically switch between both of these states would combine all these advantages.
Moreover, future work should examine handheld displays that, in addition to being deformable, are stretchable.
This will further increase the expressiveness of interactions with flexible displays.
Alexander, J., Lucero, A. and Subramanian, S. Tilt displays: designing display surfaces with multi-axis tilting and actuation.
Balakrishnan, R., Fitzmaurice, G., Kurtenbach, G. and Singh K. Exploring interactive curve and surface manipulation using a bend and twist sensitive input strip.
Bandyopadhyay, D., Raskar, R. and Fuchs, H. Dynamic shader lamps: Painting on movable objects.
Barnes C., Jacobs D., Sanders, J. and Goldman B. D., Rusinkiewicz, S., Finkelstein A. and Agrawala, M. Video puppetry: a performative interface for cutout animation.
Benko, H, Jota R., and Wilson, A. MirageTable: freehand interaction on a projected augmented reality tabletop.
Cagniart, C., Boyer E., and Ilic, S. Probabilistic deformable surface tracking from multiple videos.
Cassinelli, A and Ishikawa, M. Khronos projector.
Multicue HMM-UKF for real-time contour tracking.
IEEE Transactions on Pattern Analysis and Machine Intelligence.
Dijkstra, R., Perez, C., and Vertegaal, R. Evaluating effects of structural holds on pointing and dragging performance with flexible displays.
Gallant, T. D., Seniuk, G.A., and Vertegaal, R. Towards more paper-like input: flexible input devices for foldable interaction styles.
Harrison, C., Benko, H., and Wilson, A.D. OmniTouch: wearable multitouch interaction everywhere.
Herkenrath, G., Karrer, T., and Borchers, J. Twend: twisting and bending as new interaction gesture in mobile devices.
Hilliges, O., Kim, D., Izadi, S., Weiss, M., and Wilson, A. HoloDesk: direct 3d interactions with a situated see-through display.
Hilsmann, A., Schneider, D. C., and Eisert, P. Realistic cloth augmentation in single view video under occlusions.
Holman, D., Vertegaal, R., Altosaar, M., Troje, N., and Johns D. Paper windows: interaction techniques for digital paper.
Izadi, S., Kim, D., Hilliges, O., Molyneaux, D., Newcombe, R., Kohli, P., Shotton, J., Hodges, S., Freeman, D., Davison, A., and Fitzgibbon, A. KinectFusion: real-time 3d reconstruction and interaction using a moving depth camera.
Jordt, A and Koch, R. Direct model-based tracking of 3d object deformations in depth and color video.
Khalilbeigi, M., Lissermann, R., Kleine, W., and Steimle J. FoldMe: interacting with double-sided foldable displays.
Khalilbeigi, M., Lissermann, R., Muhlhauser, M., and Steimle, J. Xpaaand: Interaction techniques for rollable displays.
Kildal, J., Paasovaara, S., and Aaltonen, V. Kinetic device: designing interactions with a deformable mobile interface.
Kildal, J. and Wilson, G. Feeling it: the roles of stiffness, deformation range and feedback in the control of deformable UI.
Konieczny, J., Shimizu, C., Gary Meyer, and D'nardo Colucci.
A handheld flexible display system.
Lahey, B., Girouard, A., Burleson, W., and Vertegaal, R. PaperPhone: understanding the use of bend gestures in mobile devices with flexible electronic paper displays.
Lee J. C., Hudson, S. E., and Tse, E. Foldable interactive displays.
Looser, J., Grasset, R., and Billinghurst, M. A 3D flexible and tangible magic lens in augmented reality.
Mark, R., Borch, J., Habeger, C., and Lyne, B. Handbook of Physical Testing of Paper Vol.
Mistry, P., Maes, P., and Chang, L. WUW - wear ur world: a wearable gestural interface.
Ostermeier, A. and Hansen, N. An evolution strategy with coordinate system invariant adaptation of arbitrary normal mutation distributions within the concept of mutative strategy parameter control.
Piper, B., Ratti, C., and Ishii, H. Illuminating clay: a 3-d tangible interface for landscape analysis.
Saroul, L., Gerlach, S., and Hersch, R. D. Exploring curved anatomic structures with surface sections.
Schwesig, C., Poupyrev, I., and Mori, E. Gummi: a bendable computer.
Sorkine, O. and Alexa, M. As-rigid-as-possible surface modeling.
Spindler, M., Martsch, M., and Dachselt, R. Going beyond the surface: studying multi-layer interaction above the tabletop.
Spindler, M., Stellmach, S., and Dachselt, R. PaperLens: advanced magic lens interaction above the tabletop.
Varol, A., Salzmann, M., Tola, E., and Fua, P. Template-free monocular reconstruction of deformable surfaces.
Wilson, A. D. and Benko, H. Combining multiple depth cameras and projectors for interactions on, above and between surfaces.
