Models of learning and performing by exploration assume that the semantic distance between task descriptions and screen labels controls in part the usersO search strategies.
Nevertheless, none of the models has an objective way to compute semantic distance.
In this study, participants performed twelve tasks by exploration and were tested for recall after a 1-week delay.
Latent Semantic Analysis was used to compute the semantic similarity between the task descriptions and the labels in the applicationOs menu system.
When the labels were close in the semantic space to the task descriptions, subjects performed the tasks faster.
LSA could be incorporated into any of the current models, and it could be used to automate the evaluation of computer applications for ease of learning and performing by exploration.
In the interaction of humans and computers, words are the link between usersO goals and the actions required to accomplish those goals.
For command-based environments, such as UNIX, users must memorize sets of keywords that they type to interact with the system.
Likewise, in displaybased environments, such as Mac OS or Win 95, users must point at and click on display objects labeled by words .
The right choice of words can successfully lead users through novel or rarely used applications such as library databases , telephone menu systems , or graphics applications .
The study described in this paper provides empirical evidence supporting the hypothesis that users act on those interface labels that are semantically related to their task goal.
Polson and Lewis  analyzed the exploratory behavior of novice users who have a goal in mind, and who have some experience with a particular application or operating system.
In this situation, users engage in search through a problem space  composed of the multiple application states.
Users employ some domain-independent method to guide their search without having to know much about the application.
This kind of method is called a weak method, and means-ends analysis is probably its most used instance.
Two variants of means-ends analysis are frequently observed in novice exploration: hill climbing, and back chaining.
In both cases, for each state, one action is chosen among the available alternatives using "perceptual similarity as a measure of distance" .
Engelbeck  observed that during exploration novice users tend to explore those menu labels that share one or more words with the experimenter-supplied description of the tasks .
Muncher  also found this behavior in novice users learning Lotus 1-2-3.
This heuristic has been called the label-following strategy , and it can be classified as a hill-climbing technique that uses semantics to compute distance.
Considerable evidence confirms that label following is an effective method for discovering the solution to novel computer tasks .
Users combine the label-following strategy with other searching techniques such as depth-first or breadth-first exploration.
Rieman analyzed searching in menu systems and concluded that an effective search algorithm would be a combination of label-following and a hybrid between depthand breadth-first search called depth-first iterative deepening .
Rieman suggested that this combination of label following and DFID should be called Oguided DFIDO, or gDFID.
Rather than using brute force, as in pure DFID, gDFID "heuristically limits its search to items semantically  related to the current task" .
The Task-Action Learning  model  simulates users who are familiar with basic operations of the mouse and keyboard, but unfamiliar with a particular menu structure, object labels, and actions required to accomplish a task.
TAL emphasizes the role of semantics, since it assumes that users analyze the instructions, the semantic features of tasks, and the labels of the screen, hoping to find a link between them.
The semantic associations are implemented via a function that takes semantic features of the tasks  and lexical items as parameters, and returns a Boolean expression indicating semantic matching.
The IDXL model  simulates learning by exploration.
It implements gDFID searching and assumes that the userOs attention mechanism focuses on one object at a time.
The model is supplied with a task description in working memory and it has knowledge about Macintosh conventions and about the correct and legal actions that can be taken in the menu system.
Scanning is the main operator used during exploration.
It allows the visual focus to shift right, left, up, down, and to jump from place to place.
Another operator comprehends the items that have been under attention and "may note that the scanned item is a label that matches  some key word in the task" .
The model considers that a direct match costs less than an indirect match .
Thus, it tries those items that have a direct match with the experimenter-supplied task description before trying anything else.
All the knowledge about what words are synonyms has to be explicitly given to the model.
Some models that are not based on cognitive architectures have been developed to explain learning by exploration.
One of the most interesting is the Ayn model  that simulates a user learning how to use MicrosoftOs Word menu system through exploration.
Ayn makes decisions about one menu at a time using four types of knowledge:  semantic knowledge to avoid irrelevant menu labels;  failure detection to avoid dead ends;  recognition to avoid exploring the same path more than once; and  task-control knowledge to remember the outcome of previous trials.
In the Ayn model, semantic knowledge is explicitly given to the model as lists of words that may be related to particular tasks.
In summary, all the available models of learning by exploration share the same intuition about the role of semantic distance: users tend to act on objects with labels that "seem" to be semantically close to their task goals.
Additionally, some of the models explain how the labelfollowing strategy is frequently combined with other exploratory mechanisms.
Although all the simulations confirm the reliability of the label-following strategy, they do not include an objective measure of semantic distance.
This paper proposes that a mathematical model of semantics, such as Latent Semantic Analysis, is a good candidate for computing semantic distance.
The LICAI+ model  simulates the userOs comprehension of task instructions and hints, the generation of goals, and the use of these goals to discover correct actions by exploration.
This model is based on the CI architecture , that was originally developed as a model for text comprehension and extended to action planning by Mannes and Kintsch .
LICAI+ predicts that successful exploration and recall requires semantic matching between the goal representation and the labels on the display objects.
The CI architecture combines propositional knowledge with connectionist spreading activation mechanisms.
CI assumes that two propositions are related if they share one or more arguments.
For LICAI+, this means that a menu label and the description of a task  are related if there is concept overlap between them.
The semantic distance notion of these models is very crude.
LSA is both a model and a technique to extract semantic information from large bodies of text.
LSA was originally conceived as an information retrieval technique  that makes use of statistical procedures to capture the similarity of words and documents in a high-dimensional space .
Once LSA is trained on a corpus of data , it is able to compute similarity estimates that go beyond simple cooccurrence or contiguity frequencies.
Although LSA does not have any knowledge about grammar, morphology, or syntax, it mimics humansO use of language in several areas ranging from the rates of vocabulary acquisition by schoolchildren, to word and passage priming effects, to evaluating the performance of students on essay tests.
The collection of documents used to train LSA is arranged in a matrix where the columns correspond to the documents, and the rows correspond to unique word types.
An entry in the matrix indicates the number of times the word appears in the document.
Using a linear algebra technique called singular value decomposition it is possible to represent each document and each word as a vector of high dimensionality  that captures the underlying relations of the words and their contexts.
To determine how similar two words are, LSA computes the cosine between the vectors that represent the words.
A cosine, like a correlation coefficient, ranges between 1 and 1, where 1 represents a perfect match , and 0 represents no relationship between the words.
The available evidence suggests that LSA is a plausible theory of learning, memory, and knowledge .
All the tests that LSA has performed successfully have been solved using semantic similarity as the main predictor of fitness.
For instance, LSA does well on the synonym portion of the Test of English as a Foreign Language  .
It computes the semantic distance between the stem word in each item and each of the four alternatives, choosing the one with the highest cosine.
Using this method, LSA performed virtually identically to the average of a large sample of non-English speaking students.
Since the HCI literature stresses the role of semantics as a measure of distance during hill-climbing-like strategies, LSA should be able to account for the Ogoodlabels effectO observed during exploration.
In other words, LSA could be extended to action planning.
Since LSA learns about language exclusively from the training texts, it is very important to choose the right corpus for the specific situation to be modeled.
Several corpora have been used to train LSA.
One of the most versatile is the TASA  corpus.
This group of documents uses a variety of text sources, such as newspaper articles and novels that represent the kind of material students read during the school years.
The TASA corpus is broken into grade levels, from third grade to college, using a readability score  that is assigned to each of its 37,651 documents.
TASA is a good training corpus to model na*ve or inexperienced users, especially because it is assumed that these kinds of users are forced to adapt their "everyday" knowledge about common words to the new context that is imposed by a computer application .
Previous studies have examined different configurations of matching and non-matching steps in an action sequence.
For instance, it has been found that the probability of discovering a correct action sequence that contains two nonmatching steps, one after the other, is very low.
Whereas the probability of discovering an action sequence that contains two matching steps, one after the other, is very high.
Finally, the probability of discovering a sequence with a non-matching step followed by a matching step is somewhere in the middle between the previous two probabilities .
Examination of commercial graphics packages reveals that it is not common to find tasks with a sequence of actions where a matching step is followed by a non-matching step.
In the study described here, four possible combinations of matching and non-matching pairs of steps are manipulated.
It is predicted that tasks where one matching step follows another matching step are the fastest to perform, whereas tasks where a non-matching step follows another nonmatching step are the slowest.
The other two cases  are in the middle, and there are no significant differences between them.
The main limitation of both the theoretical and the empirical work on the label-following strategy is the lack of a well-defined measure for semantic distance.
In most cases, semantic relationships are established exclusively via some form of literal word overlap.
Hence, the only well-defined distance metric is, in LSA terms, a cosine of 1 .
Likewise, "distance" is defined by the number of shared words or by the number of links between handcoded OpropositionsO that describe the object labels, the task descriptions, and any other knowledge related to them.
Unless informal intuitive estimates of semantic distance are included in the models, it is difficult to study situations where there are intermediate degrees of semantic matching.
For the present study, an add-in was written for Microsoft Excel to change the applicationOs interface and to experimentally manipulate the semantic distance between the screen labels and the task descriptions.
It is predicted that the probability of discovering and recalling a correct action sequence increases as the semantic distance between the labels on the menus and the task description decreases.
LSA cosines are used to estimate the semantic distance.
In a pilot experiment for this study, subjects learned direct manipulation tasks  either by exploration or by explicit instruction.
During exploration, subjects received hints if they could not figure out the solution of the task after 60 s. The surprising finding was that these tasks were recalled much better if they were explicitly instructed as opposed to learned by exploration.
A possible explanation for this result is that the memories of unsuccessful trials interfere with the storage of the correct solution .
In the study described here, subjects learned one set of tasks by exploration, and another set by explicit instruction.
Although none of the tasks involved direct manipulation, it is predicted that there will be, at most, no significant recall differences between the groups.
If the tasks are recalled better when they are explicitly instructed, it could be argued that the unsuccessful exploration might be interfering with the storage of a hint.
If not, it could be argued that this phenomenon, for some reason, only applies to tasks where no label is involved, and that unsuccessful exploration does not interfere with the storage of hints.
Fifty-five undergraduate students participated in the experiment.
Twenty-eight received class credit and twentyseven received $10 for their participation.
The data from seven participants, four from the group that received class credit and three from the group that received $10 were discarded: two of them were not able to follow the instructions correctly, and in the five other cases, technical errors invalidated the results.
However, the groups did not differ significantly in their years of experience with Microsoft Word, Microsoft Excel , Mac Draw, and WWW Browsers.
Likewise, they did not differ significantly in the number of graphs they had created by hand in their life.
None of the participants had experience with graphics applications such as Cricket Graph or with the graphics capabilities of Microsoft Excel.
Description of the experimental tasks Four levels of matching were used for the labels in the second and third steps : good match , and three degrees of bad match .
LSA cosines were computed between the description of the tasks and the menu labels using the TASA space.
To choose the menu labels, the closest 1000 terms to the description of the tasks were computed.
From this pool, words were selected and two-word phrases were created for each menu item.
In all cases cosines between the two-word phrases and the task descriptions followed the relationship G > B1 > B2 > B3.
G exhibited the best semantic match, and B3 the worst.
For all the experimental tasks, the fourth step had a label matching in the range of the G level, whereas the first step had a label matching in a range between B1 and B2.
Twelve computer tasks were designed manipulating the semantic match between the labels of the menu system and the description of the tasks.
Microsoft Excel was used to administer the tasks, running in a Macintosh Centris 650 with 16 MB in RAM, 500 MB in hard disk, and a page-size grayscale monitor.
An Excel add-in was developed to reconfigure Excel's interface.
This made it possible to have a fully-functional graphics application in which the tasks had the features required by the experiment, and which guaranteed that the application was novel for the participants.
An S-VHS camera and a clip-on microphone were used to record the computer screen and the participants' voice.
Each participant received a package containing an informed consent form, a blue pen, and a notebook with the instructions and the description of tasks.
There were four warm-up and eight experimental tasks.
All consisted of editing a bar graph, using a graphics application .
Participants received detailed descriptions of the tasks, but no information about how to perform them .
The eight experimental tasks followed the same structure of five steps.
Each of the eight experimental tasks could be presented in one out of four configurations, depending on the semantic matching for the second and third steps: C 1 = ; C 2 = ; C3 = ; C4 = , where Bi represents one of the three degrees of bad semantic matching, and G represents a good semantic match.
For instance, a task with a configuration C3 had a bad semantic match in the second step, and a good semantic match in the third step.
The tasks were divided in two sets of four tasks: set A and set B.
Each set was equivalent to the other in number of steps and in the degree of semantic match between the task descriptions and the labels of the menus.
Half of the participants explored the tasks in set A, and they were explicitly instructed in the tasks in set B.
The other half explored the tasks in set B, and they were explicitly instructed in the tasks in set A.
The presentation of the tasks were counterbalanced using a greco-latin square design for the eight tasks and the four configurations.
The experiment consisted of two 30-minute sessions: a training session followed by recall 7 days later.
Participants were interviewed individually, their responses were recorded, and the computer screen was videotaped.
In the training session, participants read and signed a consent form and received a written version of the instructions.
During the first 3 minutes, a verbal protocol practice task was administered consisting of a "think aloud" description of the participant parent's house, as recommended by .
During both sessions, participants had to think aloud while performing the experiment.
The experimenter reminded the participants that they had to think aloud if they remained silent for more than 15 s.
After signing the consent form, participants opened the notebook and read the instructions.
At this point, the experimenter answered any question the participants had.
Participants were informed that they would be explicitly instructed in 4 of the 12 tasks, and that they would have to pay close attention to what they did because they had to repeat it in one week.
When a task was explicitly instructed, the experimenter gave step-by-step instructions on how to perform the task.
When the task was not explicitly instructed, the participant could explore the interface to "figure out" how to perform the task.
During this process, users could undo or cancel any incorrect action.
If after 60 s the participant did not show progress, the experimenter gave a hint that consisted in revealing the corresponding step of the sequence.
The hints were the same as the ones used in the explicitly instructed version.
The experimenter gave as many hints as needed in order from the first step to the last step, and allowed 60 s for exploration at each step.
For the recall session, participants had to perform the same training tasks and in the same order.
During the recall session, none of the tasks was explicitly instructed, but hints were given if necessary following the same procedure used in the training session.
At the end of the recall session, a survey was administered to obtain information about the participants' computer experience.
After the questionnaire, the experimenter turned off the computer screen and handed them a piece of paper with the description of the tasks used during the experiment.
Participants were asked to write down as many labels as they could recall from the menus and other screen objects that had to be manipulated to perform each of the tasks.
During the explored part of the training session and during the whole recall session two measures were recorded for each task step: elapsed time, and number of hints.
The experimenter recorded the number of hints whereas the VCR's counter was used to measure the time per step from the videotapes of the sessions.
ANOVA tests were conducted for both dependent variables  to determine the effect of the "design" factors.
On average, no significant differences in performance were found between the group that received $10 for the experiment and the group that did not receive any payment.
Likewise, no overall difference was found between the group that explored set A during training compared to the group that explored set B. Additionally, there was no effect of task and configuration order.
None of these factors was included in further analyses.
Since the semantic distance of only the second and third steps in each task was manipulated, only the data from those two steps were used to analyze the effects of practice, semantic distance, degree of badness, and type of training.
Collapsing over task configuration, sessions, and step number , the closer in the semantic space the label of the step was to the task description, the faster it was performed and the fewer hints were needed.
Effects of type of training and degree of badness on task performance time.
Figure 4 shows that, on average, steps from explored tasks were performed faster than steps from instructed tasks.
The difference in time was not significant, but the difference in the number of hints was significant, F = 6.29, p < .05.
The interaction between type of training and degree of badness was not significant for step performance time, nor for the number of hints per step.
Only the data for the recall session was analyzed to determine whether the type of training  made a difference.
As shown in Figure 3, explored tasks were performed faster than the instructed tasks.
As expected, the time difference was not significant.
None of the subjects was able to recall a complete sequence of steps, and although 46 out of the 48 participants were able to recall at least one label, on average, only .11 labels were correctly recalled for each task.
Semantic distance between task descriptions and menu labels reliably predicted the ease of discovering and recalling the experimental tasks.
The semantic distance, computed as a LSA cosine, predicted users' performance not only at the task level, but also at the individual step level.
Additionally, there was no significant difference between training methods , and it was shown that subjects had very poor recall of the correct action sequences when they were away from the application interface.
Models of learning by exploration have been reviewed.
All of them describe an attention mechanism that is driven by semantics.
Regardless of the details of the processes assumed by each model, there is consensus that users select actions based on the semantic distance between the goal and the labels of the objects on the screen.
It is assumed that the display can be represented as a collection of objects and labels, and that other information about the objects  is stored in longterm memory.
Therefore, deciding what object to act on is a matter of matching object labels, task descriptions, and long-term memory knowledge.
LSA can then be applied to any of these models to estimate semantic distance.
It can be speculated that memories from successful trials, especially those from well-labeled tasks, do not need to be stored in users' memory at all.
Every time users face the task, they can reconstruct the whole action sequence by following the best matching labels.
In other words, the external memory supplied by the application interface provides the necessary cues to re-discover the action sequence.
Related to this, previous research  has found that subjects do not improve performance in well-labeled tasks from the training to the recall session.
The putative lack of memory required for well-labeled tasks may be the reason why subjects have poor recall of the action sequences when they are away from the display.
Perhaps it is impossible to design an application in which, for each task, the best matching labels can be always included in the action sequence.
However, a good design should guarantee that the correct label is always the best match among the available labels.
In order to evaluate the differences in semantic distance between labels and task descriptions, an objective method, such as LSA, could be desirable.
So far, theorist and designer have used very informal estimates of semantic distance.
This study suggests that this may not be necessary.
LSA could be used as an "automated" cognitive walkthrough .
This is a method for assessing the usability of a system, focusing on ease of learning.
It involves hand simulation of the cognitive processes of how users, with no formal instruction, learn an application by exploration.
The method takes into account users' elaboration of goals and users' interpretation of the application's feedback.
The cognitive walkthrough is very labor intensive, and for this reason it is impractical for large modern applications.
However, with LSA it would be possible to construct an automated system to evaluate large applications.
Given a set of task descriptions and the labels of the objects that have to be acted on to perform these tasks, it is possible to evaluate the learnability of the system.
As stated above, LSA can be trained in any written language and with different corpora of texts.
This makes it possible to model users with different backgrounds and skill levels.
In the present study, a corpus of very broad and general knowledge was used to train LSA because the participants were mostly college freshmen, and there was no reason to believe they had any advanced technical knowledge.
During the construction of the stimuli, it was found that one of the closest words to the phrase "hide the legend"  was "dragons", with a cosine value of .41.
No differences were found between the tasks that were learned by exploration and the tasks learned by explicit instruction.
This manipulation was included to explore whether the memories for unsuccessful trails interfered with the storage of the experimenter's hints.
In this experiment, every subject required at least one hint in both the training and the recall sessions.
As Figure 3 shows, for the 3rd degree of badness , the difference in recall between the explored and the instructed tasks is virtually zero.
During training, each 3rd degree task received, on average .84 hints , which means that almost every subject, after 60 s of exploration, received a hint.
These results show that memories from unsuccessful exploration do not interfere with the recall of the hint.
In practical terms, it is worthwhile to explore the application for a while, and then ask for help from a peer, the manual, or the help system.
This is just as good as receiving step-by-step instruction.
Hopefully, new techniques can be developed with the help of LSA to minimize the number of tasks that cannot be discovered by exploration, so that users will be able to learn new applications without needing explicit instruction.
LSA has no way of knowing that "legend" also refers to part of a graph.
The word "legend" was not used in the present experiment because "legend" does not seem to be a good way to describe this object to a novice user.
Using computer manuals to train LSA could bring more information about how more advanced users exercise the label-following strategy.
This study showed that users rely on semantic similarity to discover the correct action sequence necessary to perform tasks using a novel application.
The degree of closeness in a semantic space between the labels of the objects to be acted on and the description of the tasks determines how easy is for users to discover the correct action sequences and later to recall them.
Latent semantic analysis proved to be a reliable way to measure and explain the users' actionplanning processes.
The cognitive phenomena involved in the discovery and recall by exploration of computer tasks could be described as hill climbing driven by semantics or, in other words, as a process led by the label-following strategy.
LSA can be also applied to any of the cognitive models that has been developed to explain users' performance of rarely used applications.
Regardless of the particular mechanisms that the models propose to explain users' behavior, they all rely on semantic issues that can be modeled with LSA.
Eventually, LSA could be used in conjunction with other already available techniques  to automatically test the usability of computer applications.
Additionally, it was shown that users have very poor recall of correct action sequences when they are away from the display.
It was also shown that the type of training  has no effect on recall.
Partial support was provided by NASA Grant NCC 2-904.
This paper is based on the author's master thesis .
The author thanks his thesis committee members, Professor Peter G. Polson , Professor Tomas K. Landauer, and Professor Walter Kintsch, for their help and support in developing this project.
Dr. Eileen Kintsch provided very useful comments on an earlier version of this manuscript.
Proceedings of CHI'91 Conference on Human Factors in Computer Systems, pp.
Cognitive walkthroughs: A method for theory-based evaluation of user interfaces.
Turning research into practice: Characteristics of display-based interaction.
Theory-based design for easily learned interfaces.
Exceptions to generalizations: implications for formal models of human-computer interaction.
Unpublished masters thesis, University of Colorado, Boulder, CO. Muncher, E. .
The acquisition of spreadsheet skills.
Unpublished masters thesis, University of Colorado, Boulder, CO. Kitajima, M. and Polson, P.G.
LICAI+: A Comprehension-Based Model of Learning for DisplayBased HumanComputer Interaction.
Proceedings of CHI'97 Conference on Human Factors in Computing Systems, pp.
A dual-space model of iteratively deepening exploratory learning.
Learning consistent, interactive and meaningful device methods: A computational model.
Kitajima, M. and Polson, P.G.
A Comprehension-Based Model of Exploration.
LICAI+: A Comprehension-Based Model of The Recall of Action Sequences.
In F. Ritter and R.M.
Nottingham, UK: Nottingham University Press.
Comprehension: A paradigm for cognition.
New York, NY: Cambridge University Press.
Routine Computing Tasks: Planning as Understanding.
A model of the acquisition of menu knowledge by exploration.
Proceedings of CHI'94 Conference on Human Factors in Computing Systems, pp.
Indexing by Latent Semantic Analysis.
An Introduction to Latent Semantic Analysis.
How come you know so much?
From practical problem to theory.
In D. Hermann, et al.
Latent semantic analysis and the measurement of knowledge.
Princeton, N.J.: Educational Testing Service.
