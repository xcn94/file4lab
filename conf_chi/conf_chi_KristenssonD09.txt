We report on a longitudinal study of unconstrained handwriting recognition performance.
After 250 minutes of practice, participants had a mean text entry rate of 24.1 wpm.
For the first four hours of usage, entry and error rates of handwriting recognition are about the same as for a baseline QWERTY software keyboard.
Our results reveal that unconstrained handwriting is faster than what was previously assumed in the text entry literature.
Handwriting, handwriting recognition, software keyboard H5.2.
User interfaces: Input devices and strategies.
Unconstrained handwriting recognition means that the recognizer simultaneously accepts hand-printed characters, cursive script, and a combination of both.
It is not until recently that unconstrained handwriting recognition has become accurate enough to be practical.
This goal has been unattainable for unconstrained handwriting recognition until very recently.
Interestingly, ignoring the problem of recognition errors, the text entry rate for unconstrained handwriting recognition has remained an open issue.
Many sources claim that unconstrained handwriting is fundamentally slow, citing e.g.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Instead, they in turn cite Devoe .
What Devoe  actually did was to let three participants copy a 100 word long text message using four different text entry methods over ten sessions.
The unconstrained handwriting method was only used at the first two sessions and at the last one .
Devoe  found that his three participants had a grand mean text entry rate of 16.4 wpm for unconstrained handwriting .
No handwriting recognizer was used in  so this data point only covers articulation time and ignores any error correction time .
Occasionally Bailey  is cited as giving a 25 wpm upper bound of unconstrained handwriting .
However, the only plausible source for this number in  is estimation from a log-scale diagram of text entry measurements in , which again refers back to Devoe .
In the recent text entry surveys it has been either implicitly  or explicitly  assumed that studies of unrecognized handwriting speed represent an upper bound for handwriting recognition text entry performance.
The argument is centered on the fact that unrecognized handwriting lacks a verification and correction phase .
Therefore, it is assumed participants are writing as fast as they could if they were using a 100% accurate handwriting recognizer.
Under this hypothesis, the average handwriting entry rate of 16.4 wpm in Devoe's  study would classify handwriting recognition as a relatively slow text input method, well below the QWERTY software keyboard baseline .
However, without empirical measurements this is just an educated guess based on a small-scale study.
In this paper we answer two research questions.
First, what is the actual text entry performance of state of the art unconstrained handwriting recognition?
Second, how does handwriting recognition compare as a text entry method to the well-understood status-quo QWERTY software keyboard?
Six were men and six were women.
Participants were screened for dyslexia and repetitive strain injury .
Seven participants were native English speakers and five participants had English as their second language.
No participant had used a handwriting recognition interface before.
One participant had used a software keyboard before.
No participant had regularly used a software keyboard before.
Participants were compensated 10 per session.
We used a Dell Latitude XT Tablet PC running Windows Vista Service Pack 1.
Participants used a capacitance-based pen to write directly onto the screen in both conditions.
The dimensions of the handwriting recognizer writing area measured 1266 x 264 pixels and 257 x 55 mm.
The dimensions of both text entry methods' areas were held constant during the experiment.
A letter key on the keyboard measured 14 x 11 mm, SPACEBAR measured 72 x 10 mm and BACKSPACE measured 23 x 11 mm.
The experiment consisted of one introductory session and ten testing sessions.
In the introductory session the experimental procedure was explained to the participants.
Participants were shown how to use the software keyboard and the handwriting recognizer, including demonstrations of how to correct errors.
They also completed the built-in Tablet PC tutorial.
Each testing session lasted slightly less than one hour.
Testing sessions were spaced at least 4 hours from each other and subsequent testing sessions were maximally separated by two days.
In each testing session participants did both conditions .
The order of the conditions alternated between sessions and the starting condition was balanced across participants.
Each condition lasted 25 minutes.
Between conditions there was a brief break.
Participants were also instructed that they could rest at any time after completing an individual phrase.
In each condition participants were shown a phrase drawn from the phrase set provided by MacKenzie and Soukoreff .
Each participant had their own randomized copy of the phrase set.
Participants were instructed to quickly and accurately write the presented phrase using either the software keyboard or the handwriting recognizer.
Participants were instructed to correct any mistakes they spotted in their text.
After they had written the phrase they pressed a SUBMIT button and the next phrase was displayed.
The SUBMIT button was a rectangular button measuring 248 x 16 mm.
It was placed 9 mm above the keyboard and handwriting recognizer writing area.
In total we collected 100 hours of data.
In each session participants inputted on average 83.2  phrases in the software keyboard condition and 81.5  phrases in the handwriting recognition condition.
All statistical analyses were conducted using repeated-measures analysis of variance  at significance level  = 0.05.
The handwriting recognizer  was configured to learn and adapt to participants' handwriting style .
Each participant performed the experiment in a separate user account on the machine to ensure handwriting adaptation was carried out on an individual basis.
There was a potential confound in enabling handwriting adaptation since it caused the system, as well as the user, to learn as a function of usage.
In the interest of external validity we enabled adaptation since in actual use users would most likely have adaptation turned on.
The handwriting recognizer had two basic modes for correction.
One was to strike out one or more words by crossing them and then rewriting the misrecognized text .
Another method was to click on the button for a recognized word.
This action brought up a correction interface that enabled the user to edit the word character by character using a letter recognizer .
For the software keyboard condition we used the default QWERTY software keyboard on Windows Vista.
Both the handwriting recognizer and the software keyboard were docked to the lower part of the screen.
In the first session, the mean entry rate was 19.6 wpm  for software keyboard and 21.5 wpm  for handwriting recognition.
In the last session, the mean entry rate was 24.9 wpm  for software keyboard and 24.1 wpm  for handwriting recognition.
As is evident in Figure 3, the mean entry rate increased faster for the software keyboard than for the handwriting recognizer.
At session six the software keyboard became faster than the handwriting recognizer.
The mean entry rate difference between software keyboard and handwriting recognition was not significant .
This gave us an estimate of the proportion of time users spent correcting text because superfluous keystrokes  must have been either BACKSPACE presses or additional letters inserted by the participant to make up for previous BACKSPACE presses.
We assumed the time taken to hit any key was uniformly distributed.
In addition to simply correcting errors by pressing BACKSPACE multiple times, the participant could also select an entire block of text and delete it by pressing BACKSPACE once.
These cases were automatically detected and in these instances we retrieved the correction time by manual inspection of the log files.
Error rate was calculated as the minimum edit-distance between the phrase shown  and the phrase actually entered by the participant , divided by the number of characters in the stimuli phrase.
This means that if a participant entered the phrase completely correct the error rate was minimized to zero.
If a participant entered a phrase completely incorrect the error rate was maximized to unity.
Note that this was the corrected error rate, and it was not a measure of recognition accuracy.
The mean error rate difference between software keyboard and handwriting recognition was not significant .
The handwriting recognizer we used did not enable us to measure recognition accuracy directly.
Instead we opted for estimating participants' error correction time: the proportion of their writing time spent correcting errors.
In the handwriting recognition condition we stored participants' pen traces.
These were later played back so we could detect when users were crossing out words or invoking the word-edit box.
We built a software tool to automatically detect participants' error correction actions using model fitting techniques.
Thereafter we semiautomatically marked up the log file segments when participants were correcting the text.
Using this data we then calculated the proportion of participants' writing time that was spent correcting errors.
In the keyboard condition the procedure went as follows.
Further, the 95% confidence intervals of both methods' mean entry rates were very close .
Taken together, text entry performance of state of the art unconstrained handwriting recognition appears to have been underestimated in the literature.
In addition, our participants also thought handwriting recognition was more fun to use than the software keyboard.
We hope our findings inspire designers, developers and researchers to re-consider the role of handwriting recognition in interactive systems.
We express our gratitude towards the participants.
We also thank Keith Vertanen for his assistance.
The following applies to P.O.K.
As alluded in the introduction, text entry performance of unconstrained handwriting recognition has not been well studied before.
In comparison to Devoe  our participants were much faster, even after the first session .
At the last session, our participants had a mean entry rate of 24.1 wpm.
Last, our participants' final text had a character-level mean error rate of 1%.
This error rate level has been previously described by users as the acceptable error level for communicating with superiors .
We believe the difference in relation to Devoe  can be attributed due to primarily two factors.
First, Devoe  studied fewer participants  and let them write less text.
Second, we hypothesize that the presence of recognition feedback resulted in a positive feedback-loop between the recognizer's output and participants' behavior.
This pushed our participants to write faster and less precise as they observed how much the recognizer could tolerate.
In comparison, in  participants received no such feedback and most likely felt they had to write legibly enough for another person to be able to transcribe their handwriting.
In the recent text entry surveys  handwriting recognition is hypothesized to be relatively slow at around 16 wpm  and the QWERTY software keyboard to be faster at 25-40 wpm.
However, we found that handwriting recognition performs almost identical to a QWERTY software keyboard, at least for the first hours of usage.
During the first 25 minutes of use, participants wrote text using the handwriting recognizer at 21.5 wpm.
This was 2 wpm faster than the software keyboard.
After 250 minutes of writing, participants wrote text at 24.1 wpm with the handwriting recognizer and 24.9 wpm with the software keyboard.
