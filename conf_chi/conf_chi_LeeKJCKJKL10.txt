This study is aimed at understanding deformation-based user gestures by observing users interacting with artificial deformable displays with various levels of flexibility.
We gained user-defined gestures that would help with the design and implementation of deformation-based interface, without considering current technical limitations.
We found that when a display material gave more freedom from deformation, the level of consensus of gestures among the users as well as the intuitiveness and preferences were all enhanced.
This study offers implications for deformationbased interaction which will be helpful for both designers and engineers who are trying to set the direction for future interface and technology development.
However, emerging technologies in flexible electronics , including deformable batteries and bendable circuit boards, are helping researchers overcome such technical barriers.
Advances of technologies, such as light-emitting polymers and E-ink displays, have resulted in displays thin and flexible , interaction technology based on the flexibility of the display material is now regarded as one of the key technologies that will shape the future of HCI .
Up to now, the feasibility of employing deformation mechanism as a new interaction method, such as bending and twisting, has been explored.
However, most existing studies were mainly focused on technological aspects, and the only part opened to the user was the evaluation process which is for convincing validation of the new techniques .
Although several attempts have aimed to involve users during the initial stages of the design process , these just concentrated on gaining new ideas not moving further.
Consequentially, we believe that a user-centered approach to this area of study might prove to be valuable.
Despite the technical barriers, we are convinced that preliminary explorations are possible before implementing any real interface device or having real feedback.
In this study, we observed the gestures and behaviors of participants by using artificial deformable displays to understand how users manipulate deformable displays as input devices .
We used paper prototyping  to determine what gestures might be perceived as useful in deformable user interfaces.
Many researchers have been seeking new interface opportunities to move beyond traditional input methods, such as WIMP .
Interfaces based on deformable displays are one of the many attempts that have been made to create new interaction modes.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We also employed a guessability study methodology  that requires the participants to devise appropriate gestures for 11 fundamental commands while using each of the three devices.
As a result, 1,023 deformation-based gestures from 31 participants were collected.
Our results include deformation-based gesture sets grouped by command and the material properties.
In addition, significant findings emerged from observation of these user behaviors.
There are a number of theoretical approaches regarding human gestures , and various studies have been conducted in the field of HCI aiming to develop user interfaces suited for gesture interaction.
However, this study is the first to explore how deformation-based interactions with various materials and commands may affect users.
This study contributes the following to deformation-based interaction research: We have presented gesture sets that are paired with data related to different materials and commands.
These sets can be utilized for interface design of deformable displays directly.
Unlike existing studies, we explored how the physical properties of the interface material influence complex commands in deformationbased interaction.
Our results will be helpful guidelines for both designers and engineers who are trying to set a new direction for technology development in this area.
Moreover, implications of this study can be applied to new interface paradigm, such as an organic user interface .
The authors developed a bendable device and explored various methods of menu selection, and zooming on a map.
Holman introduced PaperWindow , a prototype windowing environment that simulates digital paper displays by projecting onto a physical sheet of paper.
This allows the user to capture the physical affordances of paper in the digital world.
Gallant  presents foldable user interfaces, a combination of a 3D GUI with windows possessing physical properties of the paper, and a foldable input schema.
Applying these concepts, users could easily and interestingly control digital contents.
All these studies, which have attempted to let users have intuitive control, could be implemented with practical applications by finding the metaphors in our lives, and by applying them to the design of interfaces.
However, these trials have all focused on adapting specific metaphors that the developers themselves thought to be suitable and users were only included in the studies to evaluate the researchers' ideas.
They proposed an intuitive page turning and browsing interface for flexible e-paper.
This research primarily tried to understand users' behavior in daily life, and as a result, defined a design guideline for the new interface.
This work has a similar intention to ours, however their study was limited to the page-turning command, and it mainly focused on obtaining an algorithm for implementation.
There has been an attempt to make a universal design guideline in which user studies are prior to the implementation of ideas.
Herkenrath's research  is similar to ours in this way.
The goal of his research is to investigate the effects of using the deformation concept as an input in mobile devices, and to provide a design guideline for deformable user interfaces.
However, it was hard to observe the various gestures from similar perspective, since this research limited the actions of its participants to 18 gestures based on twisting and bending, with a hardware prototype made of plastic.
In our study, however, we intended to draw an implication that could be more generally applied to interactions based on deformation by observing users' behaviors with various materials and commands by using extensive user studies.
Research that intends to make an interface guideline through the observation of user behaviors is not novel.
There have been studies attempting to give users intuitive control by deforming 3D shapes.
He also presented Slurp , a tangible interface for locative media interactions based on the logical affordance of an eyedropper.
One key finding from these studies was that simple interactions are generally the best.
These studies show that transformation interaction can arouse a positive response, however; only simple interactions related to a specific task designed by each developer were attempted.
Research regarding deformation-based interaction that could be applied to actual information devices was conducted on various occasions.
These studies look for possibilities of user interactions through an actual physical deformation by using a flat device with a screen that can present the information.
Although these studies do not provide any insight about interactions based on deformation in particular, their approach gave us a study guideline for our research.
While the plastic sheet has the highest elasticity among the 3 given materials, it has the lowest degree of freedom for gestures because of its thickness so it can only be used for bending or twisting.
However, plastic sheet is the most similar type of material to current flexible display.
With paper, it is possible to perform gestures including folding, crumpling, and tearing as it is much thinner and more flexible than the plastic.
Stretchable cloth allows for organic deformation since it is even more flexible and it can be stretched in any direction with no difficulty.
We emulated the future information devices in these three materials measuring 210 x 297 mm, the same size as a sheet of A4 paper.
We assumed that both sides of the devices are capable of displaying information.
The blank sheets of materials that we used for this study could seem perceptually removed from technological devices.
Due to the specification of the devices, the size, A4, and materials, either paper, cloth, or plastic, they may not actually appear like technological devices.
But this is the way that the deformable displays mainly aim to be as a user-friendly apparatus .
No graphic elements for a display screen were presented to avoid any bias induced by graphic information on the screen.
As this user research was upon an imaginary future device, we used paper prototyping  to determine what gestures might be perceived useful in deformable user interfaces.
We offered participants 3 types of A4-sized artificial devices of different materials; paper, elastic cloth, and plastic, along with several commands, and asked them to deform the devices with gestures they thought were appropriate.
This method carries the risk of omitting feedback, which is important for interaction .
However, by excluding a predetermined feedback, we were able to ignore the gulf of execution  by letting users think that any behavior was acceptable.
This led us to observe users' unrevised behaviors, and drive system designs to accommodate them .
In addition, we could explore the gestures that are difficult to implement due to technology limitations, such as stretching or folding, thus we could look for future directions of deformation-based interaction without considering current technical limits.
Figure 2 shows how we conducted study with different materials.
In general, the method of this study was based on prior related studies .
Detailed descriptions will follow later in this section.
We wanted to widen the deformation possibilities to include more than just simple gestures such as bending and twisting.
In view of this, we initially collected possible deformations of flat forms from previous studies  as well as from our daily life.
With this knowledgebase, we categorized 7 types of deformation: Bending, twisting, folding, rolling, crumpling, tearing, and stretching.
Our search for materials that could sustain all 7 types of deformation resulted in thin plastic sheet, paper, and elastic cloth.
To resolve this problem, we provided detailed descriptions of each situation to contextualize the task and help the participants concentrate more on object manipulation than on imagination about the situations and devices given to them.
In this study, as E-book readers could be one of the most likely application areas , we targeted the task of reading pages with both texts and images.
We selected basic commands from existing deformation-based interaction researches : zoom-in, zoom-out, next, previous, and scrap  and common features such as on, off, delete, open, close, and copy to other device, 11 commands in total were selected for the study .
More complex commands, such as typing texts, were excluded due to the difficulty of performing them with a deformation-based gesture.
Additionally, we tried to continue a conversation with the users, so that they could openly, verbally express their feelings or opinions about their actions.
Once the participants came up with gestures for each command, they were asked to record their preferences for that gesture with 7-point likert scale.
We measured the time from the moment the user understood the situation to the moment right before they finished the definition, which we refer to as the planning time.
The entire process of the experiment was video-recorded.
The average time for one session was 10.7 minutes, and total elapsed time, including the introductory part and the breaks, was approximately an hour for each participant.
All of the participants were right-handed college students with no previous experience with flexible interfaces, which is critical because their ignorance about the concept means they were handling the device the way any potential user would do.
Three different materials and 11 commands were used as independent variables, meaning that each participant was supposed to define gestures for 33 different situations.
Given 31 participants, a total of 1,023 individual motions were observed during the study.
The experiments contained three sessions for each participant.
At each session, the participants were asked to articulate gestures for 11 commands that were randomly presented for one material.
The sequence of the sessions was randomized in order to minimize the effects of presenting order.
The reason we divided into three parts and assigned one material to each session was to minimize the possibility that a participant would retain a gesture for the same command from a previous material.
After each session, we gave the users a break to help refresh their minds.
Prior to the start of the experiment, we gave the participants a detailed introduction on the commands.
We also asked the users not to think about how unique their gestures were, but to focus on devising the most natural and appropriate gesture for the given command.
It was particularly important to give them accurate descriptions of the situations since the users are supposed to deal with a device that does not exist at this time.
Thus, the moderator fully explained the context and each command, by following a standard script.
Through this contextualizing process, the participants were encouraged to recognize the artificial displays as futuristic devices.
Also included were combined gestures such as crumpling and tearing, as well as non-deformational motions, including rubbing, reversing and tapping.
As a result, the gestures were narrowed down to 76 unique motions, 15 of which appeared more than 10 times.
Figure 3 shows the relations among the 15 representative gestures, materials and commands with quantitative results.
Immediately after performing each gesture, the users were asked to record their preference for that motion with 7point likert scale .
The zoom-in command with the cloth showed the highest average preference at 4.83, followed by the copy-to-another-device and delete commands with the same material.
The scrap command with the plastic showed the lowest at 2.44 .
Statistically, the elastic cloth showed a significantly high preference, particularly compared with the plastic .
Average preference by command, - not considering the material - had a maximum value for the delete command at 4.44 , while the open command showed the lowest preference of 3.0 .
Average preference for all gestures was positively recorded at 3.82 .
The planning time, defined as the time between the moment the user understood a command to the moment the gesture was articulated.
User gestures for deformable displays with an average preference .
Gestures are arranged in the order of observed frequency.
Some gestures are not directly related with deformation.
The average planning time of the zoom-in command with the cloth was the fastest at 1.73 seconds , followed by the previous command with the cloth and the zoom-out commands with plastic.
The scrap command with plastic had the lowest average preference and its average planning time was also the longest at 14.28 seconds .
There was a significant difference between the average planning time of the cloth and the plastic .
Thus the elastic cloth showed the highest preference and the shortest planning time, while the plastic had the lowest preference and the longest planning time.
In order to understand the participants' consensus level towards a given gestures, we used the agreement score, which Wobbrock et al.
This score is calculated with the formula below:
When the agreement score is equal to 1, which means that all users performed same gesture for certain command, while as the score is close to 0, this means more different gestures were performed for one command.
For instance, there were four identical gesture groups for the plastic's zoom-out command.
The zoom-in  and zoom-out  were the commands that showed the highest consensus levels, while the open command was the lowest .
Regarding the materials, the agreement was the highest with the elastic cloth, except for the zoom-in, zoom-out and scrap commands.
Table 3 shows the detailed quantitative results.
More meaningful discussions and implications of these results will be covered in a later section of this paper.
For instance, some of the volunteers conducted commands, saying "give it some shock like this" or "stimulate this  by stretching it" This kind of application force to the device without deformation was seen more often with less-flexible materials, such as the plastic.
This indicates that force-sensing interaction  could be useful as an alternative to a deformation-based interface.
Figure 6 shows that which parts of the display were used frequently.
In many cases , the participants used their left hands to grab the left-central area in order to support the device.
Their right hands were used to mostly manipulate the right-central and the right-upper parts, while the left hands hardly handle the top area.
In a few cases, the right hands were used to manipulate the left part of the device.
Meanwhile, most gestures were made as users clutched the device horizontally.
Only 11% of the gestures were made as the device was held vertically.
The cloth, paper, and plastic showed 6%, 12% and 16% respectively for being held in a vertical orientation.
Since we only used a paper prototype, the users were given no feedback from their input, unlike the interaction with a real device.
Even though paper and cloth are hugely familiar, the participants were intrigued by the concept and, after the experiment was completed, they left comments such as "it was interesting experience" or "it would be a great fun if the device really works this way."
It was presumed that the deformation-based interaction provided fun factors, as evidenced by other studies .
The participants were asked to sit on a chair and define gestures with the device placed on a table.
However, except for 12% of the gestures, the rest of them were performed in the air with no contact with the table as they grabbed the device.
With the exception of one-handed motions, such as shaking, 73% of the gestures were performed while the users grabbed the device with two hands.
Participants hesitated to perform gestures related to folding or crumpling for fear of damaging the device.
Although we emphasized that they should not worry about this issue, the subjects seemed to be wary of it.
Interestingly, however, the more relatively aggressive actions such as folding , crumpling  and tearing  all ended up showing high preferences.
Even though participants handled the device very carefully at the beginning of the experiment, they grew highly interested in taking these aggressive motions.
It seemed that they became intrigued by the intuitive and analogue interface which had never been experienced before, although this should be subjected to further discussion.
The participants tended to apply common daily actions to define gestures as a metaphor.
In this way, the gestures were naturally intuitive and resulted in higher preferences.
Of the previous, next and scrap commands, for which the origins are easy to observe in real life , and the on/off, open and close commands, which are represented with button or WIMP interface actions, the former - or what we call the metaphoric commands - showed shorter planning times and higher preference levels on average.
Average planning time for those metaphoric commands was 5.79 seconds, and the preference was 3.84, while the nonmetaphoric commands' figures were 8.41 seconds and 3.5, respectively.
The close command produced an interesting result because of the lingering digital metaphor.
Many of the participants were stuck with the close commands, because WIMP-based close window tasks prevailed in their intentions.
However, some participants performed this as they would close a book by folding the left part against the right, which had a significantly high level of preference.
In some cases, participants verbally explained where they gained the ideas on those gestures by saying, for example, "as if you were closing a book..." or "as if you turn a page in a book..." This verbal expression led to a meaningfully high preference level of 4.9.
These examples showed high preferences and user satisfaction.
The aforementioned outcome signifies that such metaphoric gestures adopted from daily behaviors could play a critical role in the deformation-based interactions as they have done in manipulative user interface  and other conventional interface design.
The findings above indicate that users first perceive the relevance of a gesture to the command in lieu of the nature of the material.
Therefore, when they find it easy to implement the metaphoric movements onto the material, it can help users have more preferred and intuitive interactions with the device.
A common statement agreed upon regarding gesture interaction is `Simple is the best.'
Making too many possibilities for gestures complicates the use, causing confusion .
In the case of Gummi, a host of physical actions such as twisting, folding and stretching had been taken into consideration, but eventually excluded to allow only bending .
We predicted that the plastic would save the participants' planning time when considering the material and enable them to devise gestures more quickly and easily, since the material had limited possibilities of deformation to bending and twisting.
Similarly, we assumed that high flexibility would lower gesture agreement, because users were open to a wide range of reactions.
To our surprise, however, the results showed that gesture agreement was higher among flexible materials than among relatively less pliable ones, and these materials allowed the participants to instantly determine their favorite gestures.
This can be explained in the same context with the importance of applying ordinary actions to gestures creation.
Given that many of the users tried to get the ideas from ordinary behaviors as metaphors, the lack of flexibility meant a limited scope of the gesture pool.
So, even if they found an intuitive metaphoric gesture to match it with a certain command, they had to find alternatives, if the gesture could not be applied to the material.
As a matter of fact, for the plastic, it took an average of 7.66 seconds from receiving the commands to defining the gestures, while 7.12 seconds and 5.8 seconds were required for the paper and cloth, respectively - this constitutes a 24 percent gap between the planning time of the cloth and the plastic.
Moreover, average user preferences were at their lowest 3.6 with plastic, 3.91 with paper, and the highest 3.95 with cloth.
The tendency that higher device flexibility showed higher gesture agreement was consistently found over most commands, particularly for the on command.
14 different gestures were represented for the on command with the plastic from the 31 users, but this number of unique gestures reduced to merely five for the cloth.
Accordingly, the agreement score increased from the plastic's low 0.09 to the cloth's high 0.45.
This suggests that replacing the material could affect the acceptance of a given interface metaphor.
The users felt comfortable when gestures of the two opposite-but-closely related commands were paired.
Though we gave command missions randomly, the participants tended to pair related commands: the previous and next; and zoom-in and zoom-out.
Mostly these related commands had basically the same gestures but with different directions.
For example, the users bent the plastic to the left in order to move to the previous, and bent it back to the right to move to the next.
The same sense of direction was commonly witnessed among users.
Meanwhile, the participants became confused when they could not find either gesture to complete the pairing.
For instance, when cloth was given to participants, the most frequent action for zoom-in command was stretching, which showed the highest preferences of all gestures .
However, because they could not find a proper opposite gesture for zoom-out, the preference and agreement dramatically dropped for zoom-out command .
We intentionally placed the device on the table in a diagonal position to giving the users an ambiguous initial sense of orientation.
However, 89% of the gestures were performed while the device was held in horizontally, which is contrary to the way PaperWindow  was implemented and contrary to the general perception that portrait would be more dominant than landscape because it is the more common use of paper in general.
The results shows there could be more than one preferred gesture for a single command.
For example, concerning the previous/next commands, bending either corner , and turning over the whole device  were equally preferred, with no single dominant gesture observed.
This represents that different actions can be conceived for the same command.
Therefore, providing the possibility of multiple gestures per command may be a suggested design parameter.
When designing the gesture-based interface, we should recall the results of McNeill's work; "The important thing about gestures is that they are not fixed."
We used artificial deformable displays without any feedback.
Due to this lack of interacting capability, we were able to observe users' initial behavior to the device.
If the device is able to offer feedback with an actual display and contents, more various reactions and gestures will be shown.
Thus, further studies will be important when technology supports creating displays of this type.
Since each of the gestures was started while the device was flat, a gesture was not influenced by the previous one.
This means, when the deformable display lacks restorative capability, users are required to conduct a command while the device remained affected by the preceding one.
For instance, if either of the corners of the display is left folded, a user has to make a gesture while the display remains folded.
Likewise, each gesture would be dependent on the previous one.
Therefore, studies on the restorative capability, as well as how one motion would influence the next need to be conducted in the future.
To minimize variables that could affect the study results, we did not vary the context and gave users equal conditions.
However, hand-held devices are particularly dependent on the context in which they are used.
Therefore, different contexts and other variables should be covered in a future study.
In this study, we aimed to understand deformation-based user gestures by observing how users interacted with an object with a high degree of flexibility.
By doing this, we could gain user-defined gestures that would help with the design and implementation of deformation-based interfaces, and could produce useful implication for future research.
Particularly, we understood through the study that users tend to acquire ideas from day-to-day behaviors in order to define gestures for commands, and that the user preference, intuitiveness and the agreement were increased as the limitations of manipulating the material decrease.
It is true that it might be premature to discuss deformable displays and their usage, especially when the technology is still largely uncertain.
However, we hope that this study will provide both designers and engineers helpful implications and reference for shaping the future design and technology development based on users need, rather than making users adapt to technology.
We first like to thank all the participants of our study.
We would like to thank prof. Youn-kyung Lim, Heewon Lee, Byungha Kim, Mark Whiting, Kwanmyung Kim, Yong-ki Lee, Jinha Seong, and many professors and students in ID KAIST for their inputs and thorough comments on this research.
We also thank to Sungkwan Jung, Wonkyum Lee and other researchers of Information Technology Convergence team in KAIST Institute for their comments and support.
We would like to thank our colleagues in the HCIDL 
We also wish to thank the CHI reviewers for their helpful comments and suggestions.
We finally thank the Korea Ministry of Knowledge Economy and KAIST for their financial support on this research.
