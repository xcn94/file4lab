Indoor navigation systems for users who are visually impaired typically rely upon expensive physical augmentation of the environment or expensive sensing equipment; consequently few systems have been implemented.
We present an indoor navigation system called Navatar that allows for localization and navigation by exploiting the physical characteristics of indoor environments, taking advantage of the unique sensing abilities of users with visual impairments, and minimalistic sensing achievable with low cost accelerometers available in smartphones.
Particle filters are used to estimate the user's location based on the accelerometer data as well as the user confirming the presence of anticipated tactile landmarks along the provided path.
Navatar has a high possibility of large-scale deployment, as it only requires an annotated virtual representation of an indoor environment.
A user study with six blind users determines the accuracy of the approach, collects qualitative experiences and identifies areas for improvement.
Studies show small differences in path integration ability between sighted and individuals with visual impairments , but cognitive mapping is significantly slower for people with visual impairments  often leading to reduced mobility .
Various indoor and outdoor navigation systems for users with visual impairments have been developed.
Whereas outdoor navigation systems rely on GPS for locating the user, indoor navigation systems use different techniques, described in the following section, as GPS signals cannot be received inside buildings.
Indoor systems have not been implemented at a large scale as they often require expensive physical augmentation of the environment or they require the user to carry expensive sensing and computing equipment, which may further impede their mobility.
This work presents an inexpensive navigation system called Navatar that does not augment physical infrastructure and that depends only on lightweight sensors available in popular devices such as smartphones.
Navatar exploits the unique tactile sensing capabilities of users with visual impairments  by having its users confirm the presence of anticipated landmarks, such as doors and hallway intersections along the provided path that are extracted from a virtual representation of the environment .
This type of interaction seamlessly integrates with how users with visual impairments navigate familiar spaces as this includes the identification of known tactile landmarks .
In prior research a feasibility study with blindfolded users was performed .
This paper evaluates Navatar with blind users.
Navigation relies on a combination of mobility and orientation skills .
People typically employ path integration, where they orient themselves relative to a starting position using proprioceptive data, or landmark-based navigation, where they rely upon perceptual cues together with an external or cognitive map .
Path integration allows for exploring unfamiliar environments in which users may build a cognitive map by observing landmarks .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Certain navigation devices have been developed that provide local obstacle-avoidance capabilities to users with visual impairments .
Most navigation systems, however, are able to locate the user and provide directions to a specified destination.
Outdoor navigation systems  mainly use GPS for localization purposes, but indoor systems cannot use GPS signals, as buildings block them.
To surpass this issue, alternative localization techniques have been developed that can be distinguished into three different categories.
Dead-Reckoning techniques estimate the user's current location based on a previously estimated or known location.
While the user is moving, dead reckoning estimates the user's location by interpreting readings from one or more sensors that the user carries such as accelerometers, magnetometers, and gyroscopes .
The initial location is typically determined using GPS , RFID tags , or cellular phone positioning .
The main drawback of this technique is the error accumulates over time as dead reckoning is a recursive process and each sensor has some inaccuracy.
One significant benefit of this approach is the low installation cost, as it does not require physical infrastructure.
Beacon-based approaches augment the physical space with identifiers.
Such beacons can be retro-reflective digital signs detected by a camera , infrared  or ultrasound identifiers .
Beacons can be integrated with deadreckoning to correct the accumulated error through environmental knowledge , RFID tags , ultrasound beacons , and map-matching .
Especially, Radio Frequency Identifier  tags have attracted a lot of attention in this direction .
Nevertheless, locating identifiers may be hard, as beacons may require line of sight or close proximity to the human.
Other beacon based techniques use triangulation to locate the user.
For example, wireless network positioning systems  may triangulate the location of base stations using the provided signal strength or could be building signal strength maps.
Wireless nodes often suffer from multi-path effects or interference.
Another drawback is the significant time and cost spent installing and calibrating beacons.
Though RFID tags are cheap, a large amount of them is required to cover a whole building.
Installing them in non-carpeted environments, such as concrete floors or tiles is often prohibitively expensive.
Sensor-based solutions employ sensors, such as cameras that can detect pre-existing features of indoor spaces, such as walls or doors.
For instance, a camera system matches physical objects with objects in a virtual representation of the space  to locate the user.
However, cameras require good lighting conditions, and may impose a computational cost prohibitive for portable devices.
An alternative makes use of a 2D laser scanner .
This method achieves 3D pose estimation by integrating data from inertial sensors, the laser scanner, and knowledge of the 3D structure of the space.
While this method achieves accurate localization, it relies upon expensive and heavy sensors, which may further impede the mobility of users with visual impairments as they already carry devices such as a cane or a Braille display.
Localization is required in any navigation system to be able to detect veering, e.g., when the user is deviating from the intended path.
Precise localization in indoor environments comes at a significant cost; for example, large scale augmentation of indoor environments with RFID tags is often prohibitively expensive.
Veering is likelier to occur in outdoor environments than in indoor environments as navigation is typically constrained by physical infrastructure, such as walls .
To facilitate large-scale deployment of an indoor navigation system, we argue that less precise but less expensive localization solutions may need to be explored.
Of the three techniques discussed in the related work section, dead reckoning does not require expensive sensing or augmentation of the environment.
Dead reckoning can be achieved with commodity portable devices such as a smartphone, which have integrated accelerometers and magnetometers.
To achieve a better level of localization accuracy, the proposed approach integrates inexpensive sources of dead-reckoning data with sensing information that comes from a human user.
Dead reckoning is relatively accurate for short distances where error can be mitigated by periodic sensing of beacons with a known location  .
The novelty in this approach is the utilization of a human as a sensor rather than just an actuator.
The cognitive mapping of indoor spaces by people with visual impairments has been extensively studied .
Tactile landmarks that are easily sensed through touch, such as doors, hallway intersections and floor transitions, play an important role in the cognitive mapping of indoor spaces by users with visual impairments .
Sounds or even smells may also be used as landmarks .
Users who are blind have better tactile sensing capabilities than sighted people .
To localize the user, this work followed a Bayesian filtering approach called particle filters .
Each particle contains  an estimation of where the user might be located and  a weight of how probable this estimate is.
As the user moves in the environment each particle location is updated based on a distribution of error in steps detected .
The weight of the particle is updated based on the user's confirmations of anticipated landmarks.
Weights are also affected by map information.
For instance, when a particle is inside a wall, its weight is zero.
The particles that have low weight, such as particles inside a room, while the user has not confirmed a door, are replaced with new particles to achieve more accurate localization.
In some cases the user might fail to confirm the presence of a landmark, which can be problematic for localization.
To solve this problem particles are monitored.
If all the particles have progressed beyond the landmark, which the system expects to be recognized, the failure is detected.
Navatar will be able to self-correct by providing new directions.
Specific implementation details of the particle filters process can be found in related work  as their discussion is outside the scope of this paper.
Because the identification of tactile landmarks already plays a significant role in how users with visual impairments navigate familiar spaces, our navigation system combines dead reckoning with sensor based localization by incorporating a role for the user as an "intelligent sensor" that confirms the presence of a landmark.
Landmarks along the provided path are embedded in directions .
The successful execution of the direction  is confirmed by the user.
Navatar aims to augment navigation capabilities of users with visual impairments, but it is not a local obstacle avoidance system, as this requires being able to track movable objects, such as trashcans, chairs and tables.
The location of such objects may change and it is difficult to keep track of them.
Instead the focus is on immutable landmarks, such as, hallway intersections, staircases and doors.
Figure 4 lists a high-level overview of the four different components of Navatar, which are:  a virtual representation component that stores annotated models of indoor environments;  the localization component that provides a location estimate of the user;  the direction provision component that provides directions to a user specified location; and  the interface component that interacts with the user.
Given a map, Navatar computes the shortest path using the A* algorithm  and identifies landmarks along the path upon which directions are generated.
Directions are provided using synthetic speech and are of the following form: * Moving to a landmark, i.e., "Follow the wall to your left until you reach a hallway intersection".
For moving to a landmark, Navatar uses directions that include wall following and door counting rather than metric information based on the results of preliminary studies .
Users confirm the successful execution of each direction  by tapping the screen.
Instead of 2D maps, 3D models are used to more accurately represent indoor environments with multiple levels and features like low ceilings, ramps, uneven floors and rails, which are often impediments to navigation for users with visual impairments.
In order to exploit crowd sourcing it may be easier for sighted users to annotate 3D models with tactile landmarks.
The use of virtual worlds is further motivated by the observation that thousands of 3D models of the exteriors of public buildings have been successfully created through crowd-sourcing efforts.
They can be found on virtual globe applications, such as Google Earth.
Google recently announced the availability of indoor maps for several airports and malls in the US .
Furthermore, 3D models of indoor spaces are becoming available  and can be created with little effort using tools such as Google Sketchup.
The number of landmarks that need to be confirmed and the number of directions provided can significantly affect accuracy and performance.
A feasibility study with 10 blindfolded users  focused on determining the effectiveness of different types of directions.
Two versions of providing directions were tested:  metric-based directions ; and  landmarkbased directions .
For each type of direction the distance to be navigated varied, as well as the number of landmarks to be confirmed.
The user had to confirm the successful execution of a direction before receiving the next one.
The highest rate of success was achieved for landmark-based directions involving few reliable landmarks, such as hallway intersections.
Especially when landmarks, such as doors, are close to each other, they become harder to distinguish.
A second study with eight blindfolded users focused on integrating localization and path planning .
It investigated whether localization was accurate enough to allow directions to be computed on at run-time on the phone.
Experiments show that the path can be adjusted when users fail to correctly follow directions by depending upon localization estimates.
This work also explores the use of multiple particle filters, where each filter uses a different assumption for the user's average step length, which helps to adaptively estimate the value of this parameter on the fly.
For localization, multiple particle filters are executed in parallel.
Each one of them is employing a different set of assumptions regarding the capabilities of the user depending on pedometer and compass data from the smartphone and landmark confirmations by the user.
For a small but sufficient number of particles per filter , as well as number of filters , each iteration of the algorithm required on average less than 200ms, sufficient for real-time tracking.
Ground truth was collected by a visionbased system carried by the user, which is described in the following section.
Navatar has been implemented in Java for the Android mobile platform.
A 3D model of the Engineering building of the University of Nevada, Reno was created using Google Sketchup using the Keyhole Markup Language .
Sketchup allows for creating named components and adding such a component to a location in the model.
This allows for augmenting this model with addressing information and landmarks such as doors, hallway intersections, ramps and staircases.
Though it is possible to plan paths in 3D models, this was anticipated to be too computationally intensive to be performed on the phone and hence a parser was created that extracts intermediate 2D maps that are more suitable for path planning, since the user is grounded anyway.
Figures 6 and 7 show representations of extracted maps for the first and the second floor.
For the estimation of the user's location 10 particle filters were used in parallel, with each filter having between 50-100 particles.
Preliminary studies found the phone's compass to occasionally interfere with metal studs in the wall leading to highly noisy data.
To avoid this, consecutive compass readings were averaged over a time period and then the measured value was discretized into one of eight possible directions.
A commercial beacon based localization system called Hagisonic StarGazer was used to capture the ground truth.
Passive landmarks with unique identifiers were installed on the ceiling with a resolution of one identifier per three feet.
An infrared camera worn by the user provides ID, distance and angle to the closest landmark.
An open source library is used to access the camera, which allows the creation of a map of landmarks.
To use the system for human localization, the camera and a battery are installed on a belt that the user wears.
The camera is worn on the back to avoid occlusion.
The accuracy of the camera is 2 cm when installed on a flat surface; but the camera is sensitive to tilt and might result in incorrect readings when installed on a belt.
To reduce the noise the ground truth is smoothed using an outlier detection algorithm.
Participants were recruited through the local National Federation of the Blind chapter.
All participants used a cane for navigation.
None had any self-reported impairments in mobility or hearing.
One user did not own a phone, four owned a cell phone and one owned a smartphone.
None of the participants had been in the Engineering building prior to the user studies.
For the user study, 11 paths were tested; and these paths were created so as to have approximate equal lengths with the smallest amount of overlap between paths.
Two paths involve transitions between floors.
Specific characteristics of each path such as its length and number of landmarks present can be found in Table 1.
Initially 10 paths were created but in a previous study  one path  was found to lead to large errors in location estimation as it involves traveling a relatively large distance with only doors as landmarks .
To evaluate the effectiveness of planning longer but more accurate paths that lead along more distinguishable landmarks an alternative version of this path was created  that includes 3 hallway intersections.
The application provides directions through text to speech using the smartphone's speaker and the user confirms executing each direction by tapping the screen.
Because touch screens are notoriously inaccessible to users with visual impairments a successful tap was indicated using a vibrotactile cue.
All users were right handed and navigated with the cane in their right hand.
Participants were equipped with the StarGazer camera and backpack containing the tablet.
None of the participants felt the belt and backpack impeded their mobility in any way.
User studies were conducted over a weekend with few students present in the building to avoid interference.
Hallways were cleared of any obstacles and all doors to offices were closed.
Prior to the user study the user followed one path to have them become familiar with the phone and the direction provision and to initialize the pedometer .
For each path participants were led to the start location upon which the observer would select the right path and activate the direction provision.
Navatar can recover from users missing landmarks or confirming them too early or too late.
If the user could not complete a provided direction, the observer would intervene and cancel the current path, mark it as a failure and guide the user to the next start location.
After the user study, participants were interviewed using a questionnaire to collect qualitative experiences.
Table 1 shows the success rate of each path, the average time and standard deviation it took to execute each one and the average number of steps and the standard deviation for each path.
Though the use of multiple particles in a previous study with blindfolded users  was able to mitigate individual differences between step lengths, for some users with visual impairments this turned out to be difficult to achieve.
Table 2 lists the distance between the target destination and the user's position upon completion of the path.
There was no observation of users unable to confirm a landmark, but system failures occur when the system thinks the user has arrived at their destination when they actually have not arrived yet.
Paths were considered a success when the user was able to navigate within 2.0 meter of the target location.
A length of 2.0 meter was chosen, as this is approximately the range that can be sensed using a cane held in the hand and to correct for the actual user location as the user was wearing the camera on their back.
Failures are indicated using an F in table 2.
For all paths and users an average error of 1.85 meter  was found.
Paths that involved counting many doors,  had the lowest success rates and users were often observed to scuttle from door to door, which made it harder for the system to pick up steps.
The alternative path for  on average took longer to complete  than the original path  but has a higher success rate  and a lower average error .
This may indicate that planning longer paths that include more distinguishable landmarks may increase accuracy.
The completion rate for all paths improves from 85% to 88%, and the average error reduces to 1.42  if results for the original path are excluded.
They also agreed this system could minimize the chance that they would get lost in unfamiliar environments .
Despite Navatar failing to guide user 5 to the target destination for four paths, this user remained positive about the system.
Upon further inquiry this user stated: "I have never used such a system before and I think it could be really helpful, despite its current shortcomings".
Suggestions for improvement included:  improving the accuracy;  being able to repeat directions;  offer Braille output;  not holding the phone in the hand but in a pocket and  for directions involving stairs indicating whether you need to go up or down the stairs.
This last suggestion was caused by the system providing directions dynamically so depending on the estimate of the user location the system would direct "go down the stairs" but if the system's estimate were beyond or on the stairs this direction would be omitted.
One user preferred to not hold the phone in the hand as this user prefers to use this hand to protect their face while navigating.
Repeating directions was already implemented as directions are calculated dynamically based on the system's current estimate of the location of the user.
This functionality was not made available to the participants to keep the study simple and avoid any false confirmations.
Overall the system was able to successfully guide users to their destination in 85% of the paths.
The experiences and results collected were encouraging.
The following issues need to be addressed in order to improve the accuracy of Navatar.
The sensors used are inexpensive but also erroneous and to improve step detection, rather than holding the phone in the hand better results may be achieved by placing at a different part of the body such as the legs.
Detecting steps was more difficult as some users would scuttle when counting doors and alternative direction provision strategies may need to be employed.
When following walls users typically have a firmer stride, so wall following in direction provision may be preferred over door counting.
Scuttling may not be avoided completely as users with visual impairments often have inefficient gait due to predominantly sedentary behaviors during the developmental years .
Alternatively, a pressure sensor could be embedded in the user's shoes to detect steps, which may be more precise than detecting steps using an accelerometer, but this may add to the cost of the system.
Future user studies will include a larger number of users who are visually impaired and will also involve navigating in more complex environments, such as open spaces, and involving a larger variety of landmarks, or sounds and smells.
The intention is also to explore planning safer paths that lead along a larger number of landmarks or include landmarks that are more distinguishable.
In the current system the next direction is provided manually based on the user's confirmation of successfully executing the direction.
The studies demonstrate that sufficiently accurate localization is achievable, which may allow for automatic direction provision similar to how car navigation systems provide directions.
Non-directed interviews with open-ended question were used to collect qualitative experiences.
None of the participants had used a navigation system before but all users had previously experienced getting lost in outdoor as well as indoor environments.
Depending on the space all users engage in cognitive mapping when this space is unfamiliar to them.
Primarily tactile landmarks, such as doors or windows are used in cognitive mapping, but one user noted using sounds as well, such as the sound of a water cooler or an air conditioning unit.
Cognitive mapping was typically limited to smaller spaces such as rooms.
None of the participants had freely explored large indoor environments out of fear of getting lost.
Participants typically used a sighted guide when navigating in unfamiliar environments, which would help with working out a route.
Three users preferred routes with many landmarks but would avoid ones with too many turns as that was found to be confusing.
A 5-point Likert scale was used to measure the usability of the system.
Confirmations of landmarks will still be incorporated periodically as to mitigate the error of dead reckoning localization.
A significant benefit of Navatar is that it can be installed at a much lower cost than alternative indoor localization systems.
The StarGazer system used for measuring ground truth cost over $2,000 and three days to install for two floors of the building where the user study was conducted.
It also requires the user to carry a heavy, awkward sensor.
For comparison creating the 3D model and annotating it with landmarks only took 3 hours using Google Sketchup, with the use of an existing 2D floor plan, which is available for most buildings.
Future research will study how to automatically extract landmarks, such as doors or staircases, from the geometry of a 3D model rather than annotating them manually.
This would allow for using models that are created using robotic mapping .
As some annotations such as room numbers need to be annotated by sighted humans, to facilitate crowd-sourcing efforts, the intention is to explore how models can be most easily annotated and how annotations can be verified.
To improve obstacle avoidance and successful landmark recognition, it is desirable to leave users' hands free.
Users could wear a wireless headset with a microphone worn on one ear leaving the other ear free to listen to any sounds in the immediate environment.
Speech recognition could be used that allows users to verbally provide a destination, e.g., "lead me to room 334".
Further down the road, Navatar could facilitate answering spatial queries such as "where is the nearest fire escape?
Amemiya, T., Yamashita, J., Hirota, K., and Hirose, M. Virtual leading blocks for the deaf-blind: A real-time way-finder by verbal-nonverbal hybrid interface and high-density rfid tag space.
Apostolopoulos, E., Fallah, N., Folmer, E., and Bekris, K. E. Feasibility of interactive localization and navigation of people with visual impairments.
Apostolopoulos, E., Fallah, N., Folmer, E., and Bekris, K. E. Integrated localization and navigation of people with visual impairments.
B Tsuji, G Lindgaard, A. P. Landmarks for navigators who are visually impaired.
In Proceedings International Cartography Conference .
Bessho, M., Kobayashi, S., Koshizuka, N., and Sakamura, K. Assisting mobility of the disabled using space-identifying ubiquitous infrastructure.
Bhattacharjee, A., Ye, A., Lisak, J., Vargas, M., and Goldreich, D. Vibrotactile masking experiments reveal accelerated somatosensory processing in congenitally blind braille readers.
Foulke, E. Spatial abilities: Development and physiological foundations, New York, 1982, ch.
Perception, cognition, and mobility of blind pedestrians., 55-76.
Fox, D., Burgard, W., and Thrun, S. Markov localization for mobile robots in dynamic environments.
Z., and Legge, G. E. Wayfinding with words: spatial learning and navigation using dynamically updated verbal descriptions.
Golledge, R. Geography and the disabled: A survey with special reference to vision impaired and blind populations.
A 3d pose estimator for the visually impaired.
An indoor localization aid for the visually impaired.
This paper presents an indoor navigation system called Navatar that allows for navigating a user with visual impairments by exploiting the physical characteristics of indoor environments, taking advantage of the unique sensing abilities of users with visual impairments, and minimalistic sensing achievable with a smartphone.
A user study with six visually impaired users evaluated the accuracy of Navatar and found that users could successfully complete 85% of the paths.
To improve its accuracy more sophisticated techniques need to be developed that allow for detecting the unique steps of users with visual impairments using low cost sensing.
Hollerer, T., Hallaway, D., Tinna, N., and Feiner, S. Steps toward accommodating variable position tracking accuracy in a mobile augmented reality system .
Horvat, M., Ray, C., Ramsey, V., Miszko, T., Keeney, R., and Blasch, B. Compensatory analysis and strategies for balance in individuals with visual impairments.
Hub, A., Diepstraten, J., and Ertl, T. Design and development of an indoor navigation and object identification system for the blind.
Jensen, B., Weingarten, J., Kolski, S., and Siegwart, R. Laser Range Imaging using Mobile Robots: From Pose Estimation to 3d-Models.
Learning building layouts with non-geometric visual information: the effects of visual impairment and age.
Kleeman, L. Optimal estimation of position and heading for mobile robots using ultrasonic beacons and dead-reckoning.
Koide, S., and Kato, M. 3-d human navigation system considering various transition preferences.
In Systems, Man and Cybernetics, 2005 IEEE International Conference on, vol.
Kulyukin, V., Gharpure, C., Nicholson, J., and Pavithran, S. Rfid in robot-assisted indoor navigation for the visually impaired.
Ladd, A. M., Bekris, K. E., Rudys, A., Kavraki, L. E., and Wallach, D. S. On the feasibility of using wireless ethernet for indoor localization.
Ladd, A. M., Bekris, K. E., Rudys, A., Marceau, G., Kavraki, L. E., and Wallach, D. S. Robotics-based location sensing using wireless ethernet.
In Eight ACM International Conference on Mobile Computing and Networking , ACM Press , 227-238.
Lahav, O., and Mioduser, D. Multisensory virtual environment for supporting blind persons' acquisition of spatial cognitive mapping - a case study.
In Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications 2001, C. Montgomerie and J. Viteli, Eds., AACE , 1046-1051.
Loomis, J. M., Golledge, R. G., and Klatzky, R. L. Navigation system for the blind: Auditory display modes and guidance.
Loomis, J. M., Klatzky, R. L., and Golledge, R. G. Navigating without vision: basic and applied research.
Lynch, K. The Image of the city.
A walking navigation system for the blind.
Ran, L., Helal, S., and Moore, S. Drishti: An integrated indoor/outdoor blind navigation system and service.
IEEE International Conference on Pervasive Computing and Communications , 23-30.
Retscher, G. Pedestrian navigation systems and location-based services.
In 3G Mobile Communication Technologies, 2004.
An indoor navigation system to support the visually impaired.
In 30th Annual International Conference of the IEEE Conference on Engineering in Medicine and Biology Society  , 4435-4438.
Rina, D., and Pearl, J. Generalized best-first search strategies and the optimality of a*.
Development of a wearable computer orientation system.
Semwal, S. K. Wayfinding and navigation in haptic virtual environments.
Shoval, S., Borenstein, J., and Koren, Y. Auditory guidance with the navbelt - a computerized travel aid for the blind.
An indoor navigation system for blind individuals.
In Proceedings of the 13th annual Conference on Technology and Persons with Disabilities  .
Acquisition of structural versus object landmark knowledge.
Tjan, B., Beckmann, P., Giudice, N., and Legge, G. Digital sign system for indoor wayfinding for the visually impaired.
Yuan, D., and Manduchi, R. Dynamic environment exploration using a virtual white cane.
