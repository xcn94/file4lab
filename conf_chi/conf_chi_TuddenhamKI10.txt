We present an experimental comparison of multi-touch and tangible user interfaces for basic interface actions.
Twelve participants completed manipulation and acquisition tasks on an interactive surface in each of three conditions: tangible user interface; multi-touch; and mouse and puck.
We found that interface control objects in the tangible condition were easiest to acquire and, once acquired, were easier/more accurate to manipulate.
Further qualitative analysis suggested that in the evaluated tasks tangibles offer greater adaptability of control and specifically highlighted a problem of exit error that can undermine fine-grained control in multi-touch interactions.
We discuss the implications of these findings for interface design.
These multi-touch surfaces invite us to question the use of TUIs.
The oft-cited benefits of TUIs, such as arguments for two-handed interaction, parallel input and collaborative use , could all arguably apply to multi-touch surfaces.
This is particularly so when the tangible objects are necessarily tethered to an interactive surface .
In these instances, conceptual frameworks created to underpin tangible interaction approaches, such as Fishkin's  exploration of metaphor, Hornecker's  exploration of embodiment and Ullmer and Ishii's MCRpd model  would, arguably, apply to graphically rendered objects just as much as to physical objects, and the ways in which we might bodily, spatially and cognitively interact with them.
Additionally, multi-touch systems offer various potential benefits that TUIs do not, such as providing dynamic content and controls that can be generated, reproduced, replayed, merged and deleted at will, and also  offer near-field interactions that don't actually require physical contact for interaction.
If designers are to make informed decisions between using TUIs and using multi-touch on interactive surfaces, there is a need to better understand the tradeoffs involved.
When should one use a multi-touch surface instead of a TUI?
And what value is the tangible element really delivering to the user?
In this paper we explore these issues in the context of simple, but common, interface actions found in many multitouch and tangible surface interfaces.
We explicitly compare TUIs and multi-touch input, using controlled experiments as a forcing function to develop a deeper understanding of user response to the technologies.
This highlights interactional problems and potential benefits.
Fitzmaurice and Buxton adopted such an approach in their seminal graspable user interfaces work .
They experimentally compared spatially-multiplexed TUIs and time-multiplexed input to reveal the benefits of TUIs.
However, there has been little comparable research exploring the benefits of TUIs versus multi-touch input .
In this paper we address this issue.
Since the inception of Tangible User Interfaces , researchers have discussed at length how they might be categorized and conceptualized .
In particular, designers of TUIs have increasingly tried to understand the interactional benefits of TUIs over other kinds of interface .
Proposed benefits of TUIs have included spatial multiplexing and bimanualism , natural affordances of tangible objects , various aspects of embodiment , and also the potential for experiential learning through action .
However, in recent years, multi-touch surfaces  have also become prevalent.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We offer an experimental comparison of tangible and multitouch input methods, which observes a benefit of tangibles in simple control tasks in terms of both manipulation and acquisition time.
These results suggest that in control tasks, tangibles are quicker to acquire on a table and easier/more accurate to manipulate once you've acquired them.
We also provide an accompanying qualitative analysis of user behaviour which highlights how tangibles can offer greater adaptability of control and heterogeneity of user interaction.
We also specifically highlight an under-reported problem with multi-touch systems, which we call "exit error", which can undermine users' attempts at fine-grained manipulation.
Of course this discussion does not cover all possible interactions with either multi-touch or tangible interfaces, and there may be benefits under particular circumstances of either approach.
What we provide herein however, as a discussion point within a broader debate on the role of tangibility in interfaces, is an exploration of the impact of tangible or multi-touch input on one sub-set of common user interface actions, namely target acquisition and manipulation.
Many of the theoretical benefits they discussed would appear to apply also to multi-touch input, but the data from the original experiment does not help designers decide which technology, multi-touch or tangibles, might be preferable.
Consequently, we have adapted the experimental methods of Fitzmaurice and Buxton  to enable an explicit comparison of TUIs and multi-touch input.
Since prior studies had shown clear support for TUIs over other input means, and that this had been explained in a variety of ways, such as eyes-free manipulation and haptic feedback , we hypothesised that TUIs would still outperform multi-touch input in a control-based task.
We hypothesised that TUI controls would be quicker to acquire and would lead to less error in manipulation than observed in a multi-touch condition.
A recent study by Forlines et al  explored benefits of multi-touch input over mouse input.
They illustrated how multi-touch input can be suitable for bimanual tasks but unnecessary in other tasks.
To further develop this understanding of multi-touch input, we sought to compare multi-touch against more complex tangible input devices.
However, whilst studies of TUIs proliferate, few have experimentally compared tangibles versus alternative forms of input .
Where studies have directly compared multi-touch with TUIs, the results have tended to either lack generalisability due to an overlyfocused user group  or have been hampered by the simplicity of their user task .
The seminal graspable user interfaces work of Fitzmaurice and Buxton explored user response to more complex TUIs .
They experimentally compared spatial vs. timemultiplexed input.
This distinction was critical in understanding the benefits of TUIs.
Time-multiplexed input is characteristic of GUI use with a mouse.
A single input device  is used to access different GUI controls sequentially over time.
The input device is non-specialised and operates all controls.
Space-multiplexed input enables the user to interact with all controls concurrently.
Their work empirically demonstrated that spatially-multiplexed input led to more accurate use of interface controls and reduced acquisition time.
This was most evident with input delivered through specialised devices such as TUIs .
Fitzmaurice and Buxton  discussed two phases of interaction during Graspable interface use, namely acquisition and manipulation.
Consequently we designed two experiments to compare multi-touch and tangible interfaces.
The first experiment considered the manipulation phase of interaction, and was based on elements of the study design used by Fitzmaurice .
Assuming users have already acquired a control widget, the experiment asks participants to manipulate, as quickly as possible, an on-screen object  so that it matches the position and orientation of a target object  .
This tests participants' abilities to perform rapid and fine-grained manipulations of a control widget.
As an object, we chose a virtual ruler, similar in principle to that used by Fitzmaurice .
We introduce each of our three experimental conditions below.
Participants could equally use either fingers of the same hand, or of different hands.
A tangible ruler was constructed from foam-board and tracked using the multi-touch surface .
The user could move the object on the surface, and also move the sliding indicator independently or simultaneously.
An on-screen ruler  followed the tangible ruler.
This was intended as a control condition to act as a reference point.
A single mouse is inadequate because it provides only two degrees-offreedom , and so would require a user interface technique such as handles to enable the user to select whether to rotate or translate.
This would require regular reacquisition of handles, effectively biasing against the technology.
This would also represent the `timemultiplexed' condition in the Fitzmaurice  study, which we were choosing not to examine.
Consequently we chose to use a mouse and puck to provide 4-DoF, rather like two contact points in the multi-touch condition.
We chose a puck and mouse instead of two mice to support a level of asymmetry in the interaction and aid ergonomics.
We sensed each absolutely using the same multi-touch surface .
This not only allowed us to reuse the same sensing technology across conditions, but also allowed us to move away from using relative pointing mice, which in our pilots we found remarkably difficult to use because the pointers would cross over on the screen while the mice did not cross.
The mouse and puck controlled two differently-coloured pointers on the surface.
When the mouse pointer was pressed, both pointers would act effectively as touch points in the multi-touch case.
The mouse and puck were used in part of the surface shown with a red rectangle, which was kept clear of target shapes.
The red rectangle was displayed in all conditions to avoid biasing.
Right-handed participants held the mouse in the right hand and the puck in the left.
Left-handed participants were generally used to holding a mouse in the right hand, and in this experiment held the mouse in whichever hand seemed most comfortable for them.
Twelve paid participants  were recruited from a computer science research lab and a university computer science department.
All had normal or corrected-to-normal vision.
Four had limited prior experience using tangible input, multi-touch or two mice; others had none.
Participants were not aware of the hypotheses.
The system was implemented on a Microsoft Surface, using the Microsoft Surface SDK for input and the XNA graphics library for rendering.
The Surface uses a rear-projected display of resolution 1024x768 and is approximately 24"x 18" in size.
The foam-board ruler had these markers stuck underneath one end and also underneath the indicator.
The mouse and puck instead used white stickers stuck to their undersides - so they appeared like fingertips to the underlying sensor, as we did not require the additional orientation data to track them.
The ruler was approximately 22cm x 3cm x 1cm in physical size.
The system ran at approximately 60fps.
In the experimental software, both the interactive yellow ruler and the blue target ruler are surrounded by a semitransparent coloured halo so that the target position can be inferred from the halos even if the tangible ruler obscures the target itself .
Surrounding the halo of each ruler are 6 coloured markers: 4 at the corners; and two on the indicator.
When a marker of the interactive yellow ruler comes within 5 pixels of the corresponding marker of the blue target ruler, both markers turn red, indicating a sufficient match.
When all 6 markers match for 0.75 seconds, the target is considered matched.
Progression to the next test stimulus depends on successful completion of each stimulus in turn.
The experiment used a within-subjects repeated-measures design.
Each of the 12 subjects matched each of 9 target stimuli using each of the three input technologies: multitouch, tangible, and mouse and puck .
Time to complete each matching task was measured and subjective reports of comfort and ease of use were taken for each technology .
Stimulus presentation order was randomised for each technology and participant, and the technology presentation order was counterbalanced using a Latin square design to avoid order effects.
Each participant began with an introductory session to familiarise themselves with the input method used in each of the conditions.
Once the participants were comfortable with all of the technologies, the study began.
For each condition, the experimenter first demonstrated how to match four example stimuli.
Participants were reminded that they could use any combination of touch points they wished to manipulate objects.
Participants then matched 9 randomly-ordered stimuli themselves as a practice before matching 9 further randomly-ordered stimuli.
This was done for each condition.
After each condition, participants completed a questionnaire that asked them to rate their perceived comfort and ease of use  on a continuous scale.
After all conditions were completed, participants completed a post-study questionnaire that asked them to rank the input technologies in order of preference.
The study concluded with a semi-structured interview.
Figure 2 shows the results of Experiment 1.
A three-way  repeated-measures ANOVA demonstrated a significant effect of experimental condition on target matching manipulation time  = 45.529, p<0.001.
Post-hoc pairwise comparisons using t-tests  showed that all three conditions were significantly different .
The mean scores show that the tangible condition had the lowest manipulation time, followed by the multi-touch condition and finally the mouse and puck condition.
Overall our results from Experiment 1 suggested that tangible widgets are significantly easier to manipulate  than multi-touch widgets.
That the mouse and puck condition showed significantly poorer performance is somewhat to be expected: this is consistent with the study of bimanual input by Forlines et al.
Our result builds on this finding by showing a further performance advantage when tangibles are used.
The results of Experiment 1 differ slightly from Fitzmaurice's earlier study : he found no differences between generic space-multiplexed devices and specialized space-multiplexed devices; whereas we did find a difference between our comparable specialized devices  and our approximation of space-multiplexed generic input .
Broadly, however, the times for manipulation were similar to those found in the earlier study, and it is hard to draw conclusions based on these stated differences.
There were also physical differences between Fitzmaurice's  generic bricks and our mouse and puck; and the visualization of the control widget and the tracked object were indirectly displayed in the earlier work and were co-located with the control widgets in this work.
Post-experiment interviews enabled us to further probe the user experience.
The mouse and puck were praised for being "more precise" but this contrasted with experiences of the difficulty of actually using them when complex manoeuvres were required, such as crossing the controls over, in which case the mouse and puck would often collide.
However, the lack of requirement to reach across the table when using such devices  arguably led to less perceived arm fatigue.
Ultimately, however, users frequently praised the "better degree of control with tangibles, especially when rotating".
Participants' ratings of comfort for each of the three input conditions were also entered into a three-way  repeated-measures ANOVA.
This showed no significant difference between the conditions .
Analysis of participant's ratings of ease of use however, did show a significant effect of input technology, using a threeway  repeated-measures ANOVA =5.471, p=0.012.
Post-hoc  pairwise t-tests failed to show a significant difference between the conditions but the trend suggested that participants considered tangibles easiest to use , followed by multi-touch  and finally mouse and puck .
Overall preference rankings were analysed using a Friedman  test which demonstrated a significant difference between input technologies =8.167, p=0.017.
The trend was consistent with performance results, and pairwise comparisons  showed mouse and puck significantly less preferred than both tangibles  and multi-touch .
Other pairwise comparisons were non-significant.
Again participants were presented with three technologies - tangibles, multi-touch, and mouse and puck.
In this study participants were presented with four yellow on-screen shapes that initially sat atop corresponding blue targets.
After a short countdown, the blue targets moved slowly away from the yellow shapes, following pseudo-random paths for 90 seconds .
Like Fitzmaurice and Buxton , the mouse and puck were absolutely positioned; unlike prior work they were active only in part of the surface  near the user, which was kept clear of target shapes .
This rectangle was displayed in all conditions to avoid bias.
Participants were asked to keep the four yellow on-screen shapes continuously over the four blue targets and were told that for the best performance they would need "all the shapes to match the targets as closely as possible for as much of the trial time as possible".
Participants were motivated by the prospect of an extra gift voucher for the person with best performance across all three conditions.
This required participants therefore to move rapidly between the four yellow shapes, i.e.
The four targets comprised: a brick  that could be rotated and translated; a rotor  that could be rotated and translated; a ruler with an adjustable position indicator ; and an adjustable square that could be rotated, translated and stretched.
The orientation and position of each shape followed a pseudo-random path  and size  as appropriate.
We again tested three different conditions :
The experiment used a within-subjects repeated-measures design.
Each of the 12 subjects matched each of 4 stimulus paths using each of the 3 input technologies: multi-touch, tangibles, and mouse and puck .
The stimulus presentation order was randomised for each technology and participant, and the technology presentation order was counterbalanced using a Latin square.
We continuously measured the error between the yellow shapes and the blue targets, using a root-mean-square metric described below.
We also captured overall preference, and subjective measures of comfort and ease of use for each technology.
We later performed qualitative observational analysis of video records of the study tasks to determine relative handedness and predominant forms of interaction.
The system was again implemented using a Microsoft Surface.
Each foam-board shape had two trackable markers stuck to its underside, allowing sensing of position, orientation and, in the case of the stretchable square and ruler, size and indicator position.
Again, the system ran at approximately 60fps.
This was implemented as in the previous study.
The square, similarly to the ruler, was adjusted by creating touch points on each substantive segment of the square and then moving the segments closer or further apart .
Multiple fingers and hands could be used as required.
Foam-board tangibles were again tracked using visual markers attached to their underside.
The on-screen shapes followed the tangible shapes.
Participants could manipulate any number of shapes simultaneously.
The experimenter first showed the participants a 90 second demonstration of the study task.
Participants then attempted each technology in turn.
For each technology, the participant completed one 90s practice stimulus, followed by a further four randomly-ordered 90s stimuli.
After each technology, participants completed a questionnaire that asked them to rate the perceived comfort and ease of use of the technology on a continuous scale.
Once all technologies were complete, participants completed a poststudy questionnaire that asked them to rank the technologies in order of preference.
The study concluded with a semi-structured interview.
During each trial, we logged the error between the yellow shapes and the blue targets at 50ms intervals.
We then calculate a single tracking error score for the trial, using a method similar to Fitzmaurice and Buxton .
The tracking error score is determined as the root-mean-square  Euclidean distance off target for all four target shapes and along all three dimensions :
Post-hoc pairwise comparisons  showed significant differences between the three input technologies overall and at the level of each individual control widget , with the exception of a comparison between the mouse and puck and multi-touch rulers.
Trends for each of the separate control widgets were consistent, suggesting reliability of results, and demonstrating that RMS tracking error was consistently lowest for tangibles, followed by multi-touch and with largest error in the mouse and puck condition.
At any instant k during the experiment, errorTransk is defined as the Euclidean distance between the centre of the yellow shape and the blue target.
Lower scores therefore indicate more accurate tracking.
Figure 6 shows the results.
A three-way  repeated-measures ANOVA demonstrated a significant effect of experimental condition on the amount of time spent not moving any shape   = 520.093, p<0.001.
All post-hoc pairwise comparisons  showed significant differences .
This suggests that in the tangible condition, a greater proportion of time was spent actively manipulating control widgets.
A three-way  repeated-measures ANOVA demonstrated a significant effect of experimental condition on the amount of time with two or more objects moving simultaneously  = 84.922, p<0.001.
Again, all post-hoc pairwise comparisons  showed significant differences between conditions .
Note that whilst there was a significant increase in the amount of time spent manipulating multiple objects concurrently in the tangible condition, this represents an extremely small proportion of the total time in which objects were being manipulated.
As Experiment 1, participants' subjective ratings of comfort and ease of use for each of the three input technologies were also analysed using a three-way  repeatedmeasures ANOVA.
For measures of comfort this showed no significant difference between the conditions.
Only one post-hoc pairwise t-test was significant .
The trends suggested that tangibles were easiest to use followed by multi-touch and then mouse and puck.
As Experiment 1, overall preference rankings were analysed using a Friedman  test.
The trend was again consistent with performance results.
Other pairwise comparisons were non-significant.
Where others have suggested that bimanualism is promoted by tangibility, we saw a much more complex relationship.
Bimanualism conventionally refers to two hands interacting with one object .
While we observed this conventional bimanualism , we observed two further distinct interaction patterns.
Firstly, we observed both hands interacting concurrently but each hand operating an independent control widget .
For the purposes of this paper, we shall refer to the interaction pattern of two-handed one-object interaction as bimanualism; and refer to this other pattern of two-handed two-object interaction as concurrent unimanualism.
In order to further interpret the performance data and gain a deeper understanding of participants' interactions, we analysed the video recordings of the study sessions.
We coded the video data to discern predominant patterns of interaction; we then selected instances of interest for more focused analysis .
Our findings highlight behavioural differences between the multi-touch and tangibles conditions .
Although the manipulation experiment allowed us to observe the fine-grained manipulation of controls, the conclusions that could be drawn seemed limited given the highly constrained nature of the task.
By contrast, the acquisition experiment provided participants with much more scope to explore the interaction space and determine the most productive and comfortable forms of interaction, because the acquisition task required both fine-grained manipulation and regular reacquisition of objects.
Our analysis therefore focuses on the acquisition study, but the findings are equally applicable to the manipulation study.
Our observations identify four major areas in which tangibles and multi-touch differ.
We consider each in turn.
Secondly, we observed a further interaction pattern, which we refer to as lateral sequential unimanualism .
In this pattern, each hand worked a separate  area of the interface.
The widgets on the left were controlled by the left hand and the widgets on the right by the corresponding right hand.
Commonly, the dominant side alternated from left to right.
Often a non-dominant hand would be pre-moved to hover near  the next intended object for active control prior to acquisition.
Sometimes these actions overlapped, effectively producing fleeting moments of concurrent unimanualism.
Or the actions might be chained such that after manipulation for example, the right -hand did not entirely disengage from the control widget but rested against it  whilst the left hand was used on another control, only to return to manipulating the right-hand object as soon as this was complete, and therefore reducing the time required for reacquisition.
We found no causal link between tangible or multi-touch interaction and participants' overall choice to use a unimanual or a bimanual interaction pattern.
This questions arguments that tangibility promotes bimanualism.
We did, however, observe that participants in tangible cases were much more likely to use different interaction patterns for the simple brick and rotor controls than for the more complex ruler and stretchable square controls.
In tangible cases, 10/12 participants used different patterns for simple and complex controls: 7/12 participants used predominantly unimanual interaction for the simple shapes and bimanual interaction for the complex shapes .
This contrasts markedly with the multi-touch condition, in which 11/12 participants used predominantly a single pattern for their interaction with both the simple and complex objects.
In the multi-touch condition there was an even split of participants using bimanual or unimanual patterns; Figure 10 shows examples from one participant.
Rotations were achieved by pivoting one finger around the other.
Translations were achieved by movement of the whole hand .
When bimanualism occurred in the multi-touch condition, participants again almost exclusively used two contact points.
This is similar to the multi-touch unimanual behaviour, but instead of using forefinger and middle finger, participants used the index finger of each hand.
In the multi-touch condition, we also observed the coping strategies used by participants when experiencing interaction difficulties, such as in early stages where they experimented with more complex gestural forms and more fingers.
In these cases, users quickly realised a common coping strategy: if they required more control over a multitouch object, they reduced the number of contact points that they had with it, as this invariably improved performance .
This coping strategy contrasts with the way we naturally interact with tangible objects: if we wish to exert more control over a tangible object, we routinely increase the number of contact points.
This was evidenced by the way the majority of our participants used an increased number of contact points for complex objects than for simple objects in the tangible condition.
Beyond broad patterns of handedness, the nature of contact with controls differed by input condition.
This was evident in two ways: the number of contact points with objects; and the use of object edges.
We firstly consider contact points.
In the tangible condition, participants commonly used at least three contact points, even when interaction was unimanual, and often used more.
A typical gesture can be seen in Figure 10c.
Participants commonly used either the first three fingers of one hand, or a combination of thumb and first two fingers in a more grip-like gesture.
Where three fingers and no thumb were used, participants commonly rotated objects by applying frictional forces to the top of the object  accompanied by an ulnar deviation at the wrist.
When bimanual interaction occurred in the tangible condition , the number of contact points was frequently as high as 7 or 8 .
In these cases, the left hand was commonly used as an anchor point using either a top-down friction-based pressure or a form of thumb forefinger pinch grip, with a thumb-forefinger pinch grip used for fine manipulation of the moveable part of an object by the right-hand.
Participants sometimes anchored their movements of the tangibles against the surface itself, using either fingers or parts of hands.
In the multi-touch condition, contact behaviour was very different.
11 of 12 participants used predominantly two contact points .
In unimanual interaction this strategy almost exclusively utilised forefinger and middle finger manipulation.
The second major difference in making contact was the use of edge information.
In the multi-touch condition, participants sited their two contact points on top of the control widget clearly within the edge boundaries of the active areas of the objects.
For the tangibles, however, much of the interaction relied on grip gestures in which fingers rested against salient upper edges of the shape.
It is fairly evident that this provides a richer level of control, as more complex forces can be applied.
One of the most commonly observed interactional differences between the two conditions is a factor we refer to as exit error.
This seems in part similar to observations made by Forlines et al.
They describe how reaching for interactive surface objects at a further distance leads to larger touch points because of the relative angle of the finger to the surface as it makes contact.
Herein, however, the problem is slightly different.
In the multi-touch condition, participants had difficulty disengaging from the object without causing some form of unintended extra movement.
This seemed to occur regardless of the distance of the object from the participant.
This exit error occurred repeatedly  for 9 of our 12 participants in the multi-touch condition and for none of our participants in the tangible condition.
For one participant it evidently became so frustrating that he, presumably unconsciously, began adopting an exaggerated gestural response when he was removing contact from the surface .
Such maladaptive strategies to cope with exit error were significantly problematic.
The results of Experiment 2 demonstrated an advantage of tangibles over multi-touch.
This could be due to either the ability of participants to more accurately manipulate controls in the tangible condition once acquired ; or because participants were able to spend proportionally less time acquiring controls in the tangible condition ; or a compound effect of both.
Why might participants be able to spend proportionally less time acquiring controls in the tangible condition than multitouch?
Whilst a factor such as `un-in-handedness'  might explain the better performance of the tangible conditions over the mouse and puck control it does little to explain the differences between tangible and multi-touch conditions.
The idle time analysis results suggest that either it is faster to acquire tangible controls compared to multi-touch or that the need to reacquire is reduced because of increased concurrent manipulation of controls.
However, though we observed a significant difference in the extent to which controls were concurrently manipulated, it is proportionally so small  that it cannot be responsible for differences between conditions.
This suggests that it may be faster to acquire tangible controls compared to multi-touch.
This is supported by the video analysis, which showed that there were more adaptive, differentiated and elaborate patterns of hand use in the tangible condition, with a subtly increased propensity for hover and hold actions in that condition.
The reduced acquisition time may well then be largely due to participants' abilities to use the tactile feedback available in the tangible condition and to exploit their familiarity with grasping gestures, to foster not just better more accurate fine grained control but to be able to do this in a faster way than might be possible in the multi-touch conditions.
When comparing tracking error between tangibles and multi-touch, the video analysis would suggest that the most important factor was the exit error during the multi-touch condition.
This factor alone clearly impacted the results.
It has significant implications for future technology design and, as discussed above, has not previously been reported.
It seems that the natural movements of hands when leaving contact with an interactive surface naturally change the shape profile of a contact point, which in turn shifts the contact's centroid location.
This can then cause  last millisecond adjustment of on-screen controls.
This is a significant problem that must be addressed.
This problem of exit error was likely significant enough to single-handedly explain the differences in RMS tracking error between the tangible and multi-touch conditions.
It was clear from looking at the video data that the exit errors would have inevitably increased RMS tracking error significantly in the multi-touch condition.
Nevertheless, there remains a clear difference in performance characteristics between tangible and multi-touch that cannot be explained by exit error and its impact on tracking error, namely the differences in acquisition time.
A final point concerns the extent to which participants could adapt and shape their responses to changing needs in the tangible but not the multi-touch condition.
In the multi-touch condition, we clearly saw that participants reduced the number of contact points to increase control.
This seemingly led to a relatively homogenous pattern of control action .
Participants found one comfortable style of interaction  and then stuck to it, treating all control widgets similarly.
In the tangible condition, there was greater heterogeneity of handed interaction with control objects, not just between participants but within participants as well.
It was common for participants to adapt their gestural response, such as adding more contact points and control for more complex objects.
Furthermore, this was accompanied by a tendency towards heavy use of the cueing by leaving hands in peripheral contact with controls or hovering over them after or ahead of action, thus leaving open a rich possibility of rapid correction gestures and fleeting returns to control action as attention was shifted from control to control.
This often led to complex patterns of interaction.
We postulate that one possible solution to this problem is "resetting" the centroid exit location to counteract the potential exit error.
Specifically, this could be implemented by routinely setting the centroid exit location to be equal to the location of the centroid 100ms prior.
This straightforward solution may go some way to addressing the exit error issue and would not affect input responsiveness during contact.
Our experiments broadly replicate the work of Fitzmaurice and Buxton  with comparable findings.
Like them, we have demonstrated significant advantages for tangible controls over other inputs.
We have extended their results to differentiate between tangible and multi-touch controls, and updated the results to include comparisons when the output and inputs for the interaction are co-located.
There are clear limitations to our work.
The experimental control task we have chosen is somewhat limited in scope.
One might argue that these fine-grained interactions are not indicative of the types of interactions that might dominate TUIs or multi-touch surfaces in the future.
We do not, for instance, consider complex grouping gestures  or simple selection gestures like nudging.
Moreover, the comparison of multi-touch and tangibles focuses on interaction with defined objects at the surface, whereas designers will also need to consider additional factors.
For instance, tangibles can function as cognitive artefacts outside the interaction surface .
Therefore, although we have articulated our results in terms of one technology being `better' than another, this finding is limited to the context of an experimental exploration of simple control widget tasks.
Ultimately, TUIs and multitouch technologies might lend themselves to different types of task, and so it is sensible to move to considering the differential affordances that such technologies offer the user, and away from the rhetoric of which might be `better'.
Nevertheless, in framing this experiment we hope to have made some small inroads to this process.
By exploring experimentally inspired manipulations, we have begun to highlight interactional implications for multi-touch technologies, such as exit errors, limitations in contact point selection, and the reduction in adaptability and heterogeneity of action.
These issues are, we believe, of importance at a much broader scale than might at first be assumed from a generic control-based task.
