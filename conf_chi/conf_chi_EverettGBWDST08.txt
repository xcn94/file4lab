In the 2006 U.S. election, it was estimated that over 66 million people would be voting on direct recording electronic  systems in 34% of the nation's counties .
Although these computer-based voting systems have been widely adopted, they have not been empirically proven to be more usable than their predecessors.
The series of studies reported here compares usability data from a DRE with those from more traditional voting technologies .
Results indicate that there were little differences between the DRE and these older methods in efficiency or effectiveness.
However, in terms of user satisfaction, the DRE was significantly better than the older methods.
Paper ballots also perform well, but participants were much more satisfied with their experiences voting on the DRE.
The disconnect between subjective and objective usability has potential policy ramifications.
The ballots and voting methods can take many forms from paper-based to electronic.
The variety of voting methods and large numbers of candidates and races can lead to confusion and error.
The problems in the 2000 U. S. Presidential election in Florida focused national attention on the need for usable voting systems.
As the country became familiar with terms such as "butterfly ballot" and "hanging chads," many states decided to replace these systems to avoid such problems in future elections.
The Help America Vote Act  of 2002 provided funding for updating voting equipment and intended for states to replace their outdated voting methods with newer, more reliable systems.
Because of this legislation and its requirement that election equipment be replaced by 2006, millions of dollars have been spent purchasing direct recording electronic  systems to replace older technologies.
In U.S. elections, voters are often presented with a multitude of races and candidates to consider.
There can be national, state, and local races on the same ballot, and there may be one or many candidates running in each of these races.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To ensure the integrity of elections, voters must be able to cast their votes as intended.
Unintentional undervotes , overvotes , or votes for the wrong candidates can substantially impact elections, as evidenced in the 2000 election upset in Florida.
In another analysis of this election, Mebane  used ballot-level data to show that 50,000 votes intended for Bush or Gore were lost to overvotes.
He claims that had technology been available to warn voters of overvotes, Gore would have won by more than 30,000 votes.
Voter confusion is especially problematic in elections because it is the voters themselves who must consider their voting experience to be a success.
To have confidence in the outcome of an election, voters must believe that the record of their votes accurately captures their intent and that the final tally of the votes accurately includes all cast votes.
Usability problems can also cause long lines at the polls.
Finally, usability can affect future voter turnout.
If voters have had a bad experience, believe their vote will not count, have to wait an extremely long time to vote, or worry about figuring out how to use a voting system, they may choose to abstain from voting in the future.
The consequences of poor usability are magnified in the United States with its tendency to have a large number of issues presented to the voter at once, versus other countries where voters may only be asked to vote on a single issue.
It is a DRE voting system that accepts user input via mouse; we are in the process of adding support for other input methods, including touch screens and hardware buttons.
The interactive ballot presented to the voter  is similar to other DREs in common use: it presents a number of informational screens, followed by a number of contests, a review screen, a final confirmation screen, and a terminal screen.
Contests consist of a number of choices that may be directly selected by the voter; the interface permits undervotes but prevents overvotes.
The voter may move backward and forward among all but the last of these screens using on-screen controls.
In addition to providing these basic voting functions, this version of VoteBox has been specifically modified to capture timing and other data necessary to support our studies.
To assess the usability of voting systems, the National Institute of Standards and Technology  recommends using the International Standards and Organization's   metrics of efficiency, effectiveness, and satisfaction .
As NIST is responsible for setting voting system testing standards, it is important for research on the usability of such systems to use these metrics.
Efficiency is an objective metric defined as the relationship between the level of effectiveness achieved and the amount of resources expended, and is usually measured by time on task.
Effectiveness, also an objective metric, refers to the relationship between the goal of using the system and the accuracy and completeness with which this goal is achieved.
Effectiveness is usually measured by collecting error rates, but may also be measured by completion rates and number of assists.
The third usability metric recommended by NIST is satisfaction, defined as the user's subjective response to working with the system.
Satisfaction can be measured via an external, standardized instrument, such as the System Usability Scale  , which has been used and verified in multiple domains .
While commercial DREs are what voters actually use to cast votes, they have certain limitations as a usability testing platform.
They do not allow for easy modification by researchers so that the design space can be explored.
Detailed information about user actions  can be highly useful in usability assessment, but such information is not collected by commercial DREs, nor should it be, for privacy reasons.
For these and other reasons, we chose to develop our own DRE, VoteBox.
Our VoteBox software platform supports a broad array of electronic voting research efforts; for example, it uses a prerendered user interface to minimize the complexity of the software stack that must be trusted to faithfully record the voter's intentions .
Additionally, it can be deployed in a networked configuration to provide fault-tolerance, tamperevident audit logs, and an administrative interface that assists poll workers in correct operation of the voting equipment .
Traditional measures of residual vote rate  are not hard to determine in real elections, but this does not include errors where the voter casts a vote for the wrong candidate, and not all undervotes are errors.
In order to more directly measure error, researchers must conduct simulated elections where voter intent is clear and unambiguous.
Previous work in this vein has examined baseline usability data for several traditional voting systems such as paper ballots, punch cards, and lever machines .
Paper ballots take many forms, but usually require the voter to use a pencil to make a mark next to the candidate of their choice on a pre-printed form containing many races.
With the punch card system, the voter slips the punch card behind a booklet containing the races and candidates and uses a stylus to punch out the chads corresponding to the candidates of their choice.
On a lever machine, voters are presented with all the races at one time with levers beneath each candidate.
Voters indicate their choice by pulling the levers corresponding to the candidates for whom they would like to vote.
Overall, paper ballots, especially bubble ballots , seem to be the voting method that is the most usable for the greatest range of users.
This is due in part to their error rate of about 1.5% .
While this number is certainly higher than we would like to accept in a task this important, it is lower than the error rates for both the punch cards and lever machines.
In the past few years, researchers have begun to look at the usability of DREs.
Although they have not directly compared their findings on DREs to traditional forms of voting, Herrnson and colleagues  have examined voters using commercially-available DREs.
In more recent work , they found evidence of serious usability problems with DREs.
In a large-scale field study, error rates  on the presidential race were as high as 4.2% on one electronic voting system.
Herrnson and colleagues point out that this is an especially serious type of error because not only does the preferred candidate lose a vote, but an opponent also gains a vote.
This study also showed evidence that error rates were affected by voter demographics such as age, education, computer experience, race, and English as primary language.
These studies are important as they compare different types of DREs, and their results begin to shed light on the usability of electronic voting systems.
What these studies do not tell us is how DREs compare directly to previous voting methods.
The following studies attempt to answer this question.
Experiment 1 was comprised of two nearly identical experiments, 1A and 1B.
For both of these experiments, VoteBox was configured so that the basic model for navigating through the system was similar to most commercially-available DREs.
Users  were first presented with an instruction screen, followed by a series of screens that presented contests.
Navigation between these screens was through "Next Page" and "Previous Page" buttons.
At the end of the sequence, users saw a review screen which allowed them to double-check their choices, after which they could cast their ballot.
It should be noted that these experiments were part of a larger project that examined the effects of tampering with the content of the review screens.
That issue is beyond the scope of the current paper; for additional information on that and more complete details on demographics, see .
Experiment 1A was a 2 x 3 between-subjects design with random assignment to the six conditions.
The 2-level variable was information condition: half the participants were in an undirected condition in which they were offered a voter's guide and allowed to make their own selections.
The second condition was a directed condition in which participants were given a piece of paper  that specified which candidates they should choose.
Since a choice was indicated for every race on the ballot, this condition will be referred to as "directed with no roll-off."
The second between-subjects experimental factor was the type of additional voting method used.
Each participant was randomly assigned to vote with one of a paper ballot, a punch card, or a lever machine along with the DRE.
Experiment 1B was a 3 x 3 where a third level of information condition was added: some participants were given a paper instructing for whom they should vote, but this slate directed them to not select a candidate in some races.
This was done to simulate real roll-off.
The locations of skipped races on the slate were selected such that they represented real roll-off rates .
Thus, in 1B, the information condition variable had 3 levels: undirected, directed with no roll-off, and directed with moderate roll-off.
All participants were recruited through advertisements in local newspapers, and were paid $25 for their participation in either of the one-hour studies.
In both studies, participants were fairly diverse in terms of education, ethnicity, and income.
A 10-point Likert scale was used to assess self-rated computer expertise, with 1 representing a computer novice and 10 an expert.
31 were male and 35 were female.
88% of the participants had previous voting experience.
Participants were fairly comfortable with computers, rating themselves on average at 5.7 out of 10 .
There were 101 participants in Experiment 1B.
There was an even gender split in the study with 51 male and 50 female participants.
Candidate names and voter guides were fictitious, taken from prior work  .
The SUS  was used to assess subjective user experience with each voting method.
Additionally, a study-specific survey packet was created, containing questions about general demographics, previous voting experience, proficiency with computers, etc.
Instructions given to participants differed depending on information condition.
Participants in the directed conditions were told to vote as their slate instructed.
Participants in the undirected condition were given a chance to study a voters' guide which described the candidates and their positions and were instructed to vote consistently, i.e.
Participants cast two ballots, one with each of voting technologies to which they were assigned .
The SUS was administered directly after each ballot was completed.
If the participants were in the undirected condition, a brief exit interview was conducted after voting to obtain the voter's intent.
All participants completed a survey packet.
Although punch card ballots and optical-scan ballots like our paper ballot are typically scored by machine, they were hand-counted in this study.
Scorers judged the intent of the voter, counting marks that may not have been counted if the ballots were run through a machine.
Because of this, the error rates reported here may represent a best-case scenario; even if participants did not mark ballots as instructed, their marks were counted if their intent could be inferred.
This system is similar to manual recounts that are occasionally mandated for auditing purposes or in extremely close races.
This effect was not replicated in Experiment 1B; the overall average ballot completion time for 1B was 281 seconds  but there were no reliable differences between voting methods.
One other effect that was replicated across both 1A and 1B was that of computer experience.
Participants with more self-reported computer experience took less time to vote on the DRE than those with less experience.
Data from Experiment 1A is presented in Figure 3 .
For all four of the voting methods, ballot completion time was measured beginning when the participant entered the voting room and ending when they exited the room.
The overall average ballot completion time for Experiment 1A was 377 seconds .
Completion times by paired voting methods can be seen in Figure 2.
Each pair represents scores from one group of participants.
For example, in the two columns labeled "Bubble," the white column represents the ballot completion times on the bubble ballot, and the gray column represents time on the DRE for those same people.
Paired t-tests revealed that the difference between the DRE and the lever machine was statistically reliable, t = 2.87, p = .01, but the other two methods were not.
In both experiments, there were no reliable effects or interactions involving the information condition variable.
There were two other effects that appeared in only one of the two experiments.
Replicating our findings in , more educated voters took somewhat less time to vote in Experiment 1A, F = 4.54, p = .04, but this effect was not statistically reliable in 1B.
In the directed condition, errors were counted when the voter made an incorrect selection .
In the undirected condition, errors were counted when a voter's selection was inconsistent with their other vote cast and their intent as given in the exit interview.
For example, an error in the undirected condition would be if the participant voted for the Republican candidate twice and the Democratic candidate once.
The Democratic vote would be counted as an error in this case.
There are two ways to consider errors: by race and by ballot.
On every ballot, there were 27 races and each of these represented a potential for error.
Error rates were computed by simply summing the errors committed, and dividing by the total possibilities for errors.
The other way to consider errors is by ballot: each ballot can be error-free  or have one or more errors.
However, there were no differences as a function of experimental condition, nor were there effects of age, education, computer experience, or previous voting experience.
Overall, 27% of the ballots collected in Experiment 1  contained at least one error.
However, there was no evidence that any of the four voting methods was more likely to produce a ballot containing an error.
Finally, DREs allow a particularly insidious type of error, termed a postcompletion error .
These errors occur when someone leaves out the final subgoal or step in a procedure, and such errors are highly resistant to mitigation efforts .
In voting on a DRE, this error occurs when a voter completes all his/her selections, then walks away from the voting booth having failed to activate the final "cast ballot" button.
While this error is possible with the non-DREs, we had never observed this error until introducing DREs in 1A and so did not systematically record it until 1B.1 In Experiment 1B, 6% of the participants made this error with the DRE.
Average SUS scores and standard deviations for the four voting methods used in Experiment 1A are presented in Table 1.
This was replicated and extended in Experiment 1B; satisfaction levels in for each of the four voting methods can be seen in Figure 4.
Satisfaction levels for each voting method were unrelated to their previous voting experience with the method.
Satisfaction levels on the DRE were also unrelated to computer expertise.
However, the main effect of age also approached significance  = 3.9, p = .054, with older adults giving higher SUS scores for both of the methods on which they voted.
There were no other significant main effects or interactions on SUS scores.
Because of these high satisfaction ratings of the DRE, it is likely that, given experience with the system, voters will not be deterred by the change in technology.
These results also suggest that once DREs are adopted, voters may resist any transition back to non-electronic technologies, despite the DREs lack of superiority on objective performance measures.
While voters were most satisfied with their voting experiences on the DRE, this was not due to improvements in error rates or ballot completion times, and the DRE introduced a serious postcompletion error.
None of the methods helps solve the problem of voter error, which this and previous studies  have shown is a significant concern.
In both experiments, more than a quarter of all of ballots contained at least one error, a figure that is much higher than would be desirable in situations like elections with such important outcomes.
Simply changing technologies does not necessarily solve problems with user error.
Furthermore, voters with less computer expertise took more time voting on the DRE, though they were not any more or less satisfied with the experience.
Voters with more education also took less time voting, an effect previously reported by Byrne et al.
Results such as these can help inform machine allocation decisions.
Precincts in which voters are more highly educated or which have more industry requiring computer skills need fewer voting machines since their voters will be able to cast their ballots more quickly.
As shown by Mebane , in Ohio in 2004, improper allocation of voting equipment can lead to long lines at the polls, which can even affect the outcome of elections.
The obvious question is whether or not this navigation model affects the usability of the system.
It is not hard to imagine how this might reduce the time taken to vote, but there may be a cost in terms of votes cast or possibly errors.
In Experiment 2, we put this to the test.
Again, this experiment was part of a larger project.
Additional information relating to this project is available elsewhere .
The sample was quite diverse in terms of education, ethnicity, and income.
Only 8 participants had never voted in a national election before; on average, people had voted in 9.34 national elections.
On a Likert scale ranging from 1 to 10 , the mean self-rating of computer expertise was 5.7 .
A new between-subjects variable was added in this experiment--a second VoteBox interface--and a fourth information condition was also added, resulting in a 2  x 4  x 3  design.
Random assignment was used in all cases.
Half of the participants used a version of VoteBox with a sequential navigation model , while the other half used a direct access version.
The sequential DRE interface forced users to page serially through each of the 27 races before casting their vote.
Conversely, the direct access interface permitted users to navigate directly to races of their choice, skip undesired races, and move directly to the review screen to cast their ballot at any time.
The three information conditions used in Experiment 1B were undirected, directed with no roll-off, and directed with moderate roll-off.
A fourth condition, "directed with additional roll-off," was added in Experiment 2.
Motivation for this was in part due to data from a prior study  in which roll-off rates in an undirected condition were as high as 78%.
The average of the top third of that distribution was .52, which was the number that was used to calculate rolloff rates for the new "directed with additional roll-off" manipulation in Experiment 2.
The choice to configure VoteBox to mirror the navigation model used in commercial DREs is useful in that it provides a baseline performance against which alternative interfaces can be compared.
In some sense, this also mirrors the presentation in media like punch cards, where the voter must at least page through every intervening race on the ballot to reach a later race.
However, real voters rarely vote in every single available race; the further down the ballot, the less likely it is that a voter will cast a vote in a given race.
Why, then, does the typical navigation model require voters to be presented with every race?
It is hard to imagine how else one could do this with a paper ballot, but computer interfaces are not so restricted.
The primary purpose of Experiment 2 was to compare the "standard" sequential navigation model with a system where users started on an overview screen and could directly jump to individual races, then return to the overview.
This is similar to the "hub and spoke" navigation model found on many Web sites.
Whereas participants in Experiment 1 voted only twice, Experiment 2 participants voted three times: first with a DRE , then with one of the "other" voting methods , and then again with the same DRE.
No exit interviews were conducted in Experiment 2.
Since participants now voted three times, errors could be determined by majority rule without the exit interviews that were previously conducted with participants in the undirected information condition.
This is most likely due to the fact that participants using the direct DRE voted in far fewer contests; as presented in Table 3, those in the sequential DRE condition almost never abstained from a race, but those in the direct condition abstained from nearly half of them.
Differences between the DRE conditions and the other voting methods were not statistically reliable, nor were the differences between the other voting methods.
Finally, self-reported computer expertise was again negatively correlated with time taken to vote, but only for the direct condition, r = -0.47, p = .014.
Observations more than three interquartile ranges  below the 25th or above the 75th percentiles of the distribution were considered outliers and excluded from the analysis; five participants were excluded this way.
The overall average ballot completion time was 290 seconds .
Mean ballot completion times for each voting method are displayed in Table 4.
The difference between sequential and direct DREs was statistically reliable, F = 4.86, p = .03 .
The mean ballot completion time for the sequential DRE was 442 seconds .
Participants were markedly faster with the direct DRE, which had a mean ballot completion time of 270 seconds .
No differences in ballot completion times were found between any other pairs of means.
We differentiated between three types of errors: overvote errors, undervote errors, and wrong choice errors.
An overvote error occurs if a voter chooses two candidates for a race in which only a single selection is allowed.
This type of overvote error is part of the standard "residual vote" rate and is available in actual elections.
The lever machines and DREs  prevented participants from making this particular type of overvote error.
Thus, only some of our participants even had the opportunity to make this error; none actually did so.
However, a different type of overvote error occurs if a voter makes a selection for a race s/he had originally intended to skip .
The distinction between these two types of overvote errors is an important one; the overvote error rates we report refer to this second definition only and we will refer to these as "extra" votes.
A distinction is also drawn between two types of undervotes: undervote errors and intentional undervotes.
An undervote error occurs if a voter fails to choose a candidate for a race in which s/he had intended to vote.
An intentional undervote occurs when a voter omits a race on purpose; this is not actually an error.
Finally, a wrong choice error occurs when a voter makes a selection other than the one intended.
In cases where the exact nature of an error is irrelevant, the three error types have been combined into a single "any error" error rate.
When considering each voting method separately, no differences in undervotes, overvotes, or wrong choice errors were found for bubble ballots, lever machines, punch cards, or the sequential DRE.
For each of the four information conditions, differences in ballot completion times between voting methods were examined.
No timing differences were found within any of the three directed information conditions.
The use of a slate  did not have differential effects on ballot completion times between any of the voting methods.
However, in the undirected information condition, ballot completion times differed reliably between sequential and direct DREs, F = 10.47, p = .01.
The mean ballot completion time for the sequential DRE was 910 seconds .
Error rates for each voting method are shown in Table 4.
For the direct DRE, the undervote error rate was significantly higher than the extra vote rate, t = 2.92, p = .007.
Undervote error rates were also significantly higher than wrong choice error rates, t = 2.78, p = .01.
Neither previous voting experience nor computer expertise were correlated with any of the various error rates.
However, none of the differences between the other methods were significant.
In general, previous voting experience with a particular technology was not correlated with SUS scores for that technology.
The only exception was a significant positive correlation between prior punch card experience and punch card SUS scores, r = 0.47, p = .04.
There were no reliable effects of variables like age and education on SUS scores.
When examining error rates across voting methods, large differences were seen between the sequential and direct DREs.
The direct DRE undervote error rate was reliably greater than that of the sequential DRE, t = 2.92, p = .007.
No reliable differences in error rates were found between paper ballots, punch cards, and lever machines.
As noted in Experiment 1, DREs are additionally plagued by two errors with the unique potential to disenfranchise voters.
In addition to the postcompletion error, users of DREs can also cast their vote prematurely by hitting the "cast vote" button before they intended.
We did not see this error in Experiment 1, however, in Experiment 2, 8 of 32 people  cast their vote prematurely with the direct DRE, while none did with the sequential DRE.
These cases varied in terms of how many choices--if any--had been indicated at the time of accidental casting.
There were no differences in "failed to cast" error rates between the two DREs in this study, though the overall rate of this error was surprisingly high at 9.4%.
As with Experiment 1 and prior work , examination of the per-ballot error rate revealed a distressingly high proportion of ballots  contained at least one error.
However, no differences between voting methods for number of ballots with at least one error were found.
Unlike with Experiment 1, the voting method had a marked impact on objective measures; in particular, the direct DRE was both faster and more error-prone than the sequential DRE.
However, even those numbers do not tell the entire story, as the rate of non-erroneous abstention  was also markedly higher for the direct DREs.
These differences are certainly large enough that they would likely influence the outcomes of elections if they scaled up to real contests.
Also like Experiment 1, the sequential DRE clearly produced the best subjective usability ratings.
This is, in some sense, counterintuitive; UI designers often clearly believe that faster interfaces will be preferred.
However, it appears that the inaccuracy of the direct DRE overwhelmed its speed advantage and that became the main driver of subjective scores.
Alternately, it may simply be the case that subjective and objective usability measures have little to do with one another.
Although the direct DRE was significantly faster than the sequential, its higher error rate and lower subjective usability ratings are cause for substantial concern.
Is it more important that people vote quickly, or that they correctly indicate their choices?
How important is voter satisfaction?
These results present another cautionary note; at least in the voting domain, be wary of changes enabled by more flexible technologies.
Simply because we now have the ability to support non-paper-like navigation models, and such alternate designs may indeed save users time, it does not mean that such changes are necessarily improvements.
We suspect that these effects may be driven by people's expectations about DREs, whereas our study measured only people who had actually experienced the DRE.
It is also possible that VoteBox provides a particularly good subjective experience relative to other DREs despite the lack of objective advantages.
It is interesting that voters strongly prefer using the DREs even though their performance is not any better on them.
This type of preference versus performance disassociation is not an uncommon finding, but has important implications in elections.
Because of the controversy currently surrounding the use of DREs in elections, some groups are calling for a ban on DREs and at least one state  has reverted from DREs to paper ballots.
Although participants do not like paper ballots as much as DREs, they are still generally more satisfied with paper ballots than with lever machines and punch cards.
As previous research has reported , old-fashioned paper ballots actually work quite well.
Most voters can perform at reasonable efficiency and effectiveness levels with paper ballots and are satisfied with the experience.
However, paper ballots are seen as inaccessible for many groups of people.
For example, paper ballots do not obviously provide voters with disabilities such as low vision or manual dexterity impairments with the ability to vote an independent secret ballot.
Overall, the usability findings from the current studies show that the use of DREs does not lead to more efficient or effective voting performance, although voters are very satisfied with these electronic systems.
The high satisfaction participants feel with the DREs means that citizens may be unhappy about abandoning the new voting systems in the face of security concerns.
Even though their performance on the DREs is not any better, voters may fight to keep the systems that they so strongly prefer.
The goal of these studies was to address the usability of DREs compared to older, more traditional voting methods and to begin to explore the space of what is possible with DREs.
It has clearly been assumed by policy makers and others that DREs would be better than the paper ballots, lever machines, and punch cards they have been replacing.
Our results indicate that performance on DREs in terms of efficiency and effectiveness is not better than with more traditional methods, and due to the high rate of postcompletion errors it may actually be notably worse.
This finding replicated across all experiments and strongly shows that DREs do not necessarily lead to better voting performance as had been assumed.
This was a robust effect and although there were some effects of age and education on ballot completion times, in general only computer expertise affected voters' levels of efficiency on the DRE.
No differences in error rates were seen between the "standard" DRE and the older voting methods, but the high frequency of ballots containing errors is cause for concern.
The high per-ballot error rates seen here are consistent with previous work on older voting methods .
Because the outcomes of elections have such important and widespread impacts, this is clearly problematic.
Although there were no differences in ballot completion times or error rates between the DRE and the other three methods, participants were most satisfied with their voting experience on the electronic voting system.
This was indicated by an average SUS score over 90 for this measure in the second experiment.
According to suggestions made by Bangor, Kortum, and Miller  from their work evaluating a range of technologies, scores above 90 indicate "truly superior products."
Using this scale to interpret the SUS scores for the other voting methods shows that the bubble ballot has acceptable SUS scores, while the lever machine and punch card are marginal.
Even though participants with less computer expertise voted more slowly on the DRE, their satisfaction with it was not any less than those users with more computer expertise.
The performance on and preference for DREs was not generally affected by age or education levels, though there is some evidence of a digital divide with respect to efficiency.
The primary limitation of this research is that the DRE used in these studies, VoteBox, is a prototype of an electronic voting system and may not be representative of all DREs.
Some systems may be better or worse than the VoteBox system and may return different usability measurements.
However, there is little room for improvement in terms of subjective usability, and field studies with commercial DREs are not encouraging in terms of the error rates achieved.
Efficiency of commercial DREs is as yet unknown but so far across multiple studies  we have not seen any evidence for strong differences between voting methods on efficiency.
Future studies could include different types of DREs or add features such as touchscreen input or multiple language and font size support to the existing VoteBox system.
Usability of Electronic Voting Interfaces: Sequential Versus Direct Access.
Masters thesis, Rice University, Houston, TX.
A comparison of usability between voting methods.
In Proceedings of the 2006 USENIX/ACCURATE Electronic Voting Technology Workshop.
Voters' abilities to cast their votes as intended.
Paper presented at the Workshop on the Usability and Security of Electronic Voting System.
14. International Committee for Information Technology Standards.
Ergonomic requirements for office work with visual display terminals s - Part 11.
Improving the usability and accessibility of voting systems and products.
The wrong man is president!
Overvotes in the 2000 presidential election in Florida.
Voting machine allocation in Franklin County, Ohio, 2004: Response to U.S. Department of Justice letter of June 29, 2005.
Electronic voting machines and ballot roll-off.
The effect of voting technology on voter turnout: Do computers scare the elderly?
Improving access to voting: A report on the technology for accessible voting systems.
Casting votes in the Auditorium.
In Proceedings of the 2nd USENIX/ACCURATE Electronic Voting Technology Workshop.
The impact of voting systems on residual votes, incomplete ballots, and other measures of voting behavior.
Paper presented at the annual conference of the Midwest Political Science Association, Chicago, IL, April 7-10.
The butterfly did it: The aberrant vote for Buchanan in Palm Beach County, Florida.
American Political Science Review, 95.
Prerendered user interfaces for higher-assurance electronic voting.
In Proceedings of the USENIX/ACCURATE Electronic Voting Technology Workshop, Vancouver, B.C.
We would like to thank Phil Kortum, Margaret Beier, Amanda Flato, and Michael O'Conner for comments on and assistance with this research.
This research was supported by the National Science Foundation under grant #CNS0524211 .
The views and conclusions expressed are those of the authors and should not be interpreted as representing the official policies or endorsements, either expressed or implied, of the NSF, the U.S. government, or any other organization.
An empirical evaluation of the System Usability Scale .
To appear in International Journal of Human-Computer Interaction.
Electronic voting system usability issues.
A working memory model of a common procedural error.
Usability of voting systems: Baseline data for paper, punch cards, and lever machines.
The usability of electronic voting systems: Results from a laboratory study.
Paper presented at the Midwest Political Science Association, Chicago, IL.
Cue effectiveness in mitigating postcompletion errors in a routine procedural task.
To appear in International Journal of Human-Computer Studies.
The Usability of Electronic Voting Machines and How Votes Can Be Changed Without Detection.
Doctoral dissertation, Rice University, Houston, TX.
Measuring the usability of paper ballots: Efficiency, effectiveness, and satisfaction.
In Proceedings of the Human Factors and Ergonomics Society 50th Annual Meeting.
Santa Monica, CA: Human Factors and Ergonomics Society.
