We present a multi-site field study to evaluate LemonAid, a crowdsourced contextual help approach that allows users to retrieve relevant questions and answers by making selections within the interface.
We deployed LemonAid on 4 different web sites used by thousands of users and collected data over several weeks, gathering over 1,200 usage logs, 168 exit surveys, and 36 one-on-one interviews.
Our results indicate that over 70% of users found LemonAid to be helpful, intuitive, and desirable for reuse.
Software teams found LemonAid easy to integrate with their sites and found the analytics data aggregated by LemonAid a novel way of learning about users' popular questions.
Our work provides the first holistic picture of the adoption and use of a crowdsourced contextual help system and offers several insights into the social and organizational dimensions of implementing such help systems for real-world applications.
To address these problems of discoverability, recent approaches have explored integrating crowdsourced help within the application's user interface .
For example, the CHIC framework  for Eclipse adds links from each UI control to a wiki where users can author help.
TurboTax help  and IP-QAT  display help discussions relevant to the current view in a sidebar within the application.
The LemonAid help system  lets users retrieve Q&A at an even finer granularity by selecting a label, widget, link, image or another UI element relevant to a given help need.
Although these crowdsourced contextual help systems are quite promising, many questions remain about their effectiveness at providing help during real tasks.
For example, while IP-QAT's initial evaluation  showed that it was more useful and easier to use than a basic discussion board, the study involved only one instrumented application and was carried out with paid volunteers who were required to contribute 3 items per day.
LemonAid's initial evaluation  showed that LemonAid could retrieve relevant help in the top 2 results for over half of help seeking scenarios, but this finding was based only a simulated community of users and pre-defined tasks.
As social systems often interact with the social and organizational contexts in which they are implemented , it is difficult to know if and when these lab study results apply to real users and their real situations and tasks .
To understand how crowdsourced contextual help is perceived in real settings and to increase the ecological validity of LemonAid's design , we partnered with multiple software teams to deploy and evaluate LemonAid in the field.
After extending the original LemonAid prototype with community Q&A, answer notification, and back-end analytics features, we deployed it on a set of four diverse sites for periods ranging from 7 to 15 weeks, reaching user populations of 150 to over 40,000 users in size.
From over 1,200 logs, 168 exit surveys, and 36 interviews with end users, we report on end users' perspectives on LemonAid's helpfulness, usability, and desirability for reuse, and how it compared to alternative forms of help seeking.
We also present software teams' perspectives on issues related to LemonAid's adoption and integration and the utility of LemonAid's analytics data.
Most software help is simply not helpful.
One-on-one technical support is time-consuming, frustrating, and rarely puts users in touch with the experts who have answers .
Documentation, tutorials, and tooltips rarely provide the taskspecific help that people need  because software designers cannot anticipate at design time the full range of how, why, what, and where am I questions that people ask .
Even crowdsourced solutions such as technical support discussion forums require users to browse lengthy, scattered, and often irrelevant conversations in search for even one helpful reply .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The result of these deployments is the first holistic picture of the adoption and use of a crowdsourced contextual help system, contributing: * Extensions to LemonAid's crowdsourced contextual help  that support participation by end users and software teams, moderation of help activity, and improved discoverability of help content; * Empirical findings that highlight the unique strengths of crowdsourced contextual help and selection-based help in particular, but also tradeoffs in integrating help into an application user interface; * Results that illustrate the factors affecting software teams' adoption of a crowdsourced contextual help tool and teams' perceptions of the value of crowdsourced contextual help analytics data; * Insights into the process of developing and integrating help systems into existing applications, and how help systems fit into larger social and organizational contexts.
The advent of searchable, online help documentation offered an improvement, but the static nature of the help content remained to be a problem .
Recently, online forums have become a new platform for help, with millions of users seeking help, and a smaller but still large population of people asking and answering questions .
Although forums offer a rich repository of application-specific help, they are often not easily navigable or searchable .
While crowdsourced contextual help approaches can bring more relevant help to users within the user interface, our study sheds light on several social, organizational, and technical factors that have to be aligned to deliver such help.
Another body of work evaluates software product support from the perspective of software teams.
Many of these studies have focused on organizational, procedural, and management problems in the provision of support  and knowledge and management of support issue repositories .
Other efforts have focused on diagnosis and web-based one-on-one support practices .
A large body of work has also investigated peer-to-peer help in open source software  communities, where users can directly communicate with one another and with developers about support issues and bugs .
Still, we know little about how organizations integrate new provisions for help and support or how they may use data from these systems to inform design.
To our knowledge, we are the first to contribute insights into how teams adopt a new crowdsourced contextual help tool and how teams perceive the value of the analytics data it generates.
Although "in-the-wild" field studies are increasingly common in HCI , field studies of software help have been rare.
Although not directly in the domain of software help, Ackerman's  evaluation of AnswerGarden, a questionanswering tool, comes closest to our study design.
The study used multiple data collection procedures including questionnaires, usage data, and interviews and the findings provided rich insights into whether augmenting organizational memory was possible, and also contributed a number of lessons for designing organizational memory systems.
The field study component helped them gain valuable insights into how participants integrated Blueprint into their everyday programming workflow.
Although not a field evaluation, the study of IPQAT is one of the first to evaluate crowdsourced contextual help with users .
While the study ran for 2 weeks and there were 36 participants, all of these participants were using the same application and most of them were already users of product discussion forums.
Furthermore, these participants were required to contribute 3 items per day and received additional monetary incentives to contribute more.
In contrast to these works, our field study investigates the use of crowdsourced contextual help by hundreds of users for their real tasks across multiple sites.
Early studies of help seeking showed that when people experience breakdowns in using software, they begin a process of trial-and-error to find a resolution .
Carroll calls this the "paradox of the active user" , because even though it would be more efficient for the user to get help, the user opts to tackle the task on her own first .
The key idea behind LemonAid  is that users enter a semi-transparent help mode overlaid onto the application's user interface and find help by selecting a label or image they believe is relevant to their problem .
Upon a selection, LemonAid retrieves questions and answers related to the selection  based on an algorithm that retrieves help based on the text, tag type, and location of the selection within the user interface .
Users can then select a retrieved question to view its answers and indicate their helpfulness, or if they do not find a relevant question, they can submit one.
Support staff and other users can then answer them.
Users can also use keywords to search through existing questions, allowing LemonAid to behave like a site-specific Q&A search.
The prototype described in  included only a retrieval algorithm and a selection user interface.
For deployment, we added several critical features: Improving discovery of existing help content.
One of our design goals was to facilitate users' navigation of the existing help content upon entry into the help mode.
To facilitate this, LemonAid adds yellow question marks next to elements that have at least one question attached .
Main components of the LemonAid interface in the help mode:  a user selects an on-screen label or image highlighted in yellow;  the user's selection triggers the retrieval of relevant questions;  the user can click on a question to see the answer for that question and indicate whether the answer was helpful or not helpful.
To help users remember which UI elements they have already viewed, the system modifies the color of the visited question marks.
Another goal was to encourage users to contribute questions and answers.
When users add a new question in LemonAid, they can provide their email address and be notified of new answers without having to return to the site.
LemonAid also allows users to report potential spam and offer "me too" votes on questions .
When users view answers, they also contribute data on whether the answer was helpful or not helpful in order to dismiss the answer window .
Many of the software teams that we approached were concerned about users' ability to deface their site with unwanted content.
Therefore, we added basic a moderation feature, allowing support staff to receive e-mail notifications of new questions and answers so that they can both approve content, answer new questions, and improve user-contributed answers.
The moderators also get notified when a user flags a post as inappropriate.
Analytics dashboard for monitoring help activity.
We added a web-based administrator "dashboard" that shows an aggregate view of users' activities in LemonAid, specific to the host deployment site.
Moderators can get a real-time snapshot of the elements where users are looking for help.
They can also see rankings of popular questions based on views and votes, and the helpfulness of answers.
Implementation and setup: To set up LemonAid for each site, we needed:  a whitelist of the UI literals where Q&A would be attached in the interface, to prevent users from selecting privacy-sensitive content; and,  access to the team's source to include the LemonAid functionality on the site.
We hosted all of the Q&A data, logs, and scripts related to LemonAid on our own servers.
For , the previous version of LemonAid  offered a mechanism for extracting UI literals from the source code; teams desired more control over what part of the interface would actually become selectable on the screen, so we created a plug-in that would help us interactively choose selectable labels and images on specific pages.
One team decided to embed this link at the root level so that LemonAid would appear on all pages in the site; the other three teams placed LemonAid on their home page and selectively on other frequently accessed pages.
We also created a client-side browser plug-in that allowed us to test LemonAid on any site.
Except for mobile devices, we were able to support LemonAid on most major browsers and platforms .
We also had to resolve some minor JavaScript namespace conflicts on sites that used different versions of common JavaScript libraries, such as jQuery.
The plug-in allowed us to sort out most of the compatibility issues independently of the host software teams, before deployment.
The context for all of our deployment sites was a large university and its various software teams developing web applications to support teaching and research.
To recruit field deployment sites, we met with software developers and site administrators across campus.
Prior to making a commitment, the teams had a chance to see LemonAid demos, learn details about the field study, and discuss time and effort that they would be willing to commit.
Six teams were enthusiastic about participating; the first site served as a pilot deployment and one of the remaining five had to leave the study due to conflicting schedules.
The pilot site was a local intranet used by about 20 staff and students for internal project management.
The pilot site allowed us to assess the usability of the interface, uncover implementation issues that had to be addressed for larger cross-platform deployments, and collect pilot data for improving our set-up and logging.
We describe the four web sites augmented with LemonAid in Table 1.
Before making LemonAid live, we asked each software team to seed the help database with FAQs or other help content relevant to each page.
In two cases, additional staff and the first author were also involved in seeding the help content.
We treated each deployment as an individual case , but used the same data collection procedure for each site.
We used a mixed-method approach to better capture the plurality of experiences and perspectives of users .
We collected data from three sources:  usage logs, 
The data collection occurred during the spring and summer of 2012.
We instrumented LemonAid to gather a time-stamped log including: entry into help mode, exit from help mode , selections of elements on the screen, selections of questions, helpfulness of answers viewed, votes on questions, flags used to mark questions as spam, content of submitted questions and/or answers, content of search keywords, and lists of users subscribed to receive answers to questions.
LemonAid gathered all of the foregoing data anonymously, meaning that we could not ascribe activity in the help mode to individual users, only users in aggregate.
When users exited the help mode for the first time , they were presented with a brief exit survey.
Users had the option of being included in a $50 gift card drawing as an incentive to participate and could win additional prizes if they wanted to participate in a follow-up interview.
Survey responses and identities of the respondents were not linked.
The survey asked users three questions about their experience, each scored on a 7-point Likert scale, ranging from "strongly disagree" to "strongly agree."
These questions were:  "I found something helpful;"  "I would use this help feature again," and  "the help feature was intuitive."
We required users to provide a response for each of these questions to maintain consistency in the responses.
Using the list of survey respondents who wished to be interviewed , we recruited interviewees from each deployment site.
We conducted all interviews on campus, except four were on the phone.
The interviews were semi-structured and lasted around 30 minutes on average.
Interviewees had access to LemonAid during the interview to facilitate recall.
We also probed into interviewees' perceptions of LemonAid's usability and usefulness, particularly compared to other help formats.
We also probed into aspects of LemonAid that the interviewees believed were confusing or could be improved.
Near the end of each deployment period, we also interviewed the software team members involved in the deployments.
We began the interviews by probing into teams' perspectives on the experience of integrating LemonAid compared to other forms of help.
We also asked interviewees to describe how they currently managed user feedback and whether the feedback was used to inform design decisions.
We also showed them usage data captured by LemonAid's Administrative Dashboard and asked them to explore and comment on the utility of the aggregated data, if any.
These interviews lasted about 45 minutes.
To assess how LemonAid was used in practice and whether LemonAid was helpful, usable, and desirable for reuse, we used the concept of data triangulation  and looked for convergence in the logs, survey data, and interview data.
We began the analysis by parsing the usage logs to get an overview of usage patterns and help activity in LemonAid .
For the survey responses, we investigated associations between our key outcome variables and user demographics.
To analyze the qualitative data, we audiotaped and transcribed each interview, then organizing, coding, and analyzing with qualitative data analysis software.
In the first pass, we coded for data related to questions about LemonAid's usage, and end users' perceptions of utility and usability.
In the next pass, we examined interviewees' descriptions of help seeking strategies using an inductive analysis approach .
This inductive analysis approach was useful for seeing patterns in the data and in identifying recurring themes across the different deployments.
A similar inductive approach was also used for analyzing interviews with software team members.
To describe the usage activity in LemonAid, we use an individual help session as the unit of analysis since we were unable to track individual users.
We define a help session as an episode consisting of all the activities that occurred when a user entered the help mode and pressed exit or left the page.
In our analysis, we ignored all of the help sessions where entry into the help mode was immediately followed by an exit out of the help mode or the current page.
About 20% of the logs followed this pattern.
LemonAid's deployments across the four different sites resulted in over 1,200 help sessions.
Table 2 summarizes this activity for each deployment site.
Since the LIBRARY site was the longest deployment and had the largest user base, it yielded the highest usage of LemonAid .
The LIBRARY logs showed that 16 new questions were added during the deployment, constituting about 1.6% of the total help sessions.
We also found that no end users answered a question; library staff answered all new questions.
We did find that the 16 new questions asked by users received 121 views, accounting for about 21.5% of all question views and 74.3% of the corresponding answers were marked as helpful.
Prior work has shown that this level of activity is typical of what occurs in technical forums  and more broadly on the Internet  where most users are consumers of online content rather than contributors.
A breakdown of the survey respondents based on their role is shown in Table 3.
Note that the EDC and RDB sites required authentication and had respondents who were exclusively staff members at the university or an affiliated institution.
The LIBRARY and the DEPT sites were largely public sites  and garnered responses more broadly from the university's graduate and undergraduate students, staff, faculty, and external visitors.
We interviewed 36 users across the four deployments, with the majority of interviewees being users of the LIBRARY site , followed by DEPT , EDC , and RDB .
Table 4 shows the breakdown of the interviewees based on their role.
Our survey data for the LIBRARY site showed that the majority of respondents were regular site users, with 70% reporting visiting site the daily or few times a week .
To get a sense of how users normally found help on web sites, we asked respondents to indicate their preferred method of finding help.
To assess the helpfulness of LemonAid, we sought convergence in our three sources of data:  helpfulness of Q&A selections captured during actual use ;  data collected from an exit survey which explicitly asked whether users found something helpful; and,  interview data that shed light on why or why not something was helpful.
There is some noise in this log data--some of our interviewees indicated that they were simply browsing questions and not looking for help on a particular topic, but were forced to assess the helpfulness of the answer set anyway.
Still, our exit survey data largely corroborates the log data: 70.6% of respondents on average indicated having found something helpful during the use of LemonAid .
The distribution of the survey responses from the LIBRARY site is shown in Figure 3a.
Only 14.9% of respondents disagreed at some level that LemonAid was helpful, whereas 71.1% agreed at some level.
We also analyzed whether certain types of users were more likely to find LemonAid helpful.
For the LIBRARY data, we found that respondents' frequency of site use was correlated significantly and negatively with whether they felt LemonAid was helpful , meaning users who indicated visiting the site more frequently were less likely to find something helpful through LemonAid.
Our interview data corroborates this finding as the more frequent users indicated that they were already familiar with the site's main features and many of the existing help questions were "too basic" for them.
In contrast, the interviews consistently revealed that new or less frequent site users found LemonAid to be helpful for learning about unfamiliar parts of the interface:
Well, what I liked about it is that...you might find a question that you didn't even know you had, so I like that...I was just hovering and looking at stuff and somebody asked a question about audio books, and it reminded me, "oh yeah, I need to get an audio book."
So I like that about it, so I think it's good in that when you don't have a specific question or you are just looking at what other people are asking, then it's really helpful.
Many of our interviewees felt that there was often a social cost to asking someone for help.
LemonAid, in contrast, allowed users to learn from other people's experiences without having to bother anyone or lose face:
I think that students nowadays like to help themselves a lot more.
In terms of figuring out a website, it can be kind of embarrassing to not know how to get around a website.
So I think the nice thing about this  is that it's 24/7 and it doesn't require that stepping out of what maybe your comfort zone, you know.
If English is your second language, or you're really new...and you're nervous walking up to the librarian at the library, or someone's just not available when you want a question answered, I think it  can be a real positive experience...
Still, many of our interviewees noted that in-person explanations were richer and allowed opportunity for instant back-and-forth than textual, asynchronous Q&A.
Some interviewees also revealed that even if they did not ask others for help, they were sometimes on the other side answering technical questions for less tech-savvy family members or friends.
In these cases, the interviewees felt that hands-on help was the most preferred form of help :
We analyzed the relationship between users' preferred method for finding help and helpfulness of LemonAid for the LIBRARY data .
Although this association was not significant , we did learn from the interviews that users who normally preferred to find help using the site's existing help through FAQs or documentation found LemonAid's contextual Q&A approach to be more helpful.
The distribution of the responses from the library deployment is shown in Figure 3b.
Our interviews also revealed that most users found the help interface to be easy to understand even though it was their first time using this crowdsourced contextual help tool.
One consistent theme was that users appreciated having direct access to help content without having to visit a separate interface, unlike most other help tools:
I liked the visual overlay.
I think for a second I thought, "Whoa!"
I'm not searching through a separate site to get to the help screen like  where a different window pops up and you scroll through topics...I think that  is extremely intuitive.
Having the help topics spread out by the areas in which I would need them as opposed to having a separate window open... 
The majority of interviewees expressed frustration in using current forms of software help.
Although a third of the users said they consulted built-in help, the vast majority of users completely avoided consulting help because they feared seeing "long pages of text."
Although this behavior has been known for years , countless applications still offer only textual built-in help.
Users also felt that the built-in help content was often outdated, even though more updated content and tips were available through online channels:
This finding is consistent with other studies that show that switching modes between an application and help often causes users to forget why help was requested in the first place and it takes them longer to return to their task .
Users also liked that they could simply turn off the help mode when finished with their help-seeking task and could return back to the application.
But, some users mentioned that if they were working on a complex, multi-step task, going back and forth in the help mode could be distracting and having keyboard short-cuts perhaps would be useful.
As our survey and interviews primarily probed the first use of LemonAid, we also wanted to know if users were likely to use it again.
Whether users found LemonAid helpful correlated significantly and positively with whether they felt they would reuse LemonAid again .
These results are a strong contrast to prior work that has shown that users are frustrated with help systems and fear clicking on help .
In fact, about two-thirds of our interviewees said that they would like to see LemonAid even on other sites and desktop applications.
For example, one interviewee recounted frustration with a site and its internal help tools she used recently, wishing that she could instead access help via LemonAid on that site:
You search  and it gives you three very different things.
The only two hits have nothing to do with what I want.
If there was a LemonAid type help for each section it would help...
I could type in the keywords into LemonAid and see how many other administrators across campus have the same question or other questions...that would be helpful!
In contrast, users felt that the help content in LemonAid was more current and relevant since it could be added and edited at any point and could represent a wide range of actual problems rather than those anticipated by software designers.
Furthermore, several users mentioned that the questions and answers in LemonAid were easier to understand than official built-in FAQs that often used a lot of system-specific jargon and were presented in a long list:
FAQ's are a pain because, first of all, they don't use the same language that a normal user would most of the time but rather an internal jargon.
I tried LemonAid and I could relate to the questions if I was a student or whoever.
This pushes the popular questions out.
An FAQ is more like an artificial taxonomy giving a lot of useless information.
Although about 30% of our interviewees said they prefer to search when seeking help, they identified a number of problems with keyword-based searching.
One common reason that users were not able to find relevant content was due to the mismatch between how users described their questions versus how other users or system designers may have answered the same questions .
The issue of choosing relevant keywords was particularly acute for non-Native English speakers.
As seen in Table 2, only a small percentage of users ended up using the built-in LemonAid search feature .
Although it is possible that some users may not have noticed this feature, many interviewees pointed out that LemonAid reduced the need to search altogether because they could find relevant questions and answers by clicking around in the interface:
When you're looking at something in the forums, you have to know what it's called...people can call it different things.
With the kind of thing you're talking about, you have to know the terms then you may not find what you're looking for...With this , you're like, `Oh, I don't know what's happening here!
Although every software team that we worked with had different motivations for integrating LemonAid, a common theme was that with small teams, it was difficult to offer one-on-one support.
Although the sites already offered some forms of built-in help, their developers felt that LemonAid could improve the discoverability of existing help, especially at locations in the interface where users were more likely to need it.
Another motivation was curiosity about what kind of questions users would want to know answers to.
A more practical motivation was that the developers did not have to change the underlying code of their website:
I think  may save time if you are doing research or something like that so you have the ability to get input from other sources very quickly at your fingertips.
To me, it's almost like a gold mine in that sense because it leads me into a direction that I may have not been able to get just going through traditional methods.
Two teams were initially concerned about the possibility of spam, especially since the tool did not require a login.
Despite the initial concerns, we found that spam was not a major issue during the deployments--only 5 questions were marked as spam across the four deployments over several weeks.
Upon inspection, we found that 4 of these 5 questions were originally put in by staff and were not actually spam; 1 other question was an incomplete sentence that was marked by a staff member to flag it for removal.
Similar to the concerns of end users, administrators were also concerned about maintaining the quality and accuracy of answers.
Furthermore, administrators also had concerns about the "public defacing" that was possible with LemonAid's content overlaid on the site:
Although few users contributed questions and answers in our deployments, it is possible that problems that impede social learning on forums could affect LemonAid as well.
However, as some users pointed out, because users invoked LemonAid from within the application rather than from a separate forum, people were perhaps more cautious about posting content because the help seemed "official," as it was overlaid on the application itself.
LemonAid perhaps also succeeds in limiting the number of repetitive "I have the same question" comments because users tended to find relevant questions and could express the same sentiment by clicking on me-too, rather than posting a new redundant question.
In fact, none of the new 16 questions posted on the LIBRARY site were duplicate questions and users voted "me-too" 63 times.
It definitely takes the load off of the administrators to have users helping each other, right?
But you do have to monitor and make sure that they are giving the correct information.
I've already seen a few occasions where I had to clarify something on an email  that some user sent out.
This is consistent with studies that show the disconnect between software support and usability engineering .
After asking teams about these existing practices, we showed them the LemonAid data for their site in the form of a basic dashboard, like the one found in web-based analytics tools.
Most of the interviewees felt that they were not able to obtain this type of data from anywhere else and LemonAid would be a useful way to augment existing usage-based analytics services.
For example, one software developer pointed out that unlike LemonAid, data from tools such as Google Analytics did not reveal users' intentions in terms of why users were clicking on certain areas or spending more time on a particular page:
In the current set of deployments, the host teams were able to devote staff time and resources to monitor the questions and provide answers.
It may be that to sustain the same level of quality in answers, a long-term commitment from the host teams would be necessary.
Since many modern organizations have already opted to create peer-to-peer support forums , perhaps engaging with users through crowdsourced contextual help will be a natural extension.
In particular, the teams felt that investing in one-to-many support is more efficient and provides greater cost-savings in the long run compared to supporting users one-on-one.
Still, in many cases, the true value of crowdsourced help may be from users helping each other, particularly when staff moderators are not the power users of the application.
Future work in crowdsourced contextual help should explore strategies for encouraging the contributions of these power users, reducing the burden on staff while maintaining the authority and quality of answers.
For example, the badges, awards and leaderboards that help make sites such as Stack Overflow successful  might translate well to crowdsourced contextual help systems.
Although the web offers a plethora of resources for software help, as seen in our results, about two-thirds of users still preferred to find help by  trying on their own, or,  using the site's help.
This finding suggests that users overall are reluctant to leave the interface when seeking help.
Our results further suggest that the majority of users who benefit from modern forms of help  are more likely to be tech-savvy users.
Users who are less tech-savvy are more likely to need help, but they are also less likely to search for help on the web or on forums.
Thus, we believe that there is potential in further understanding users' software help-seeking behavior in different contexts and in exploring strategies for delivering more relevant help in the context of the users' tasks.
LemonAid and other crowdsourced contextual help systems offer only one step in this direction.
Our study has several limitations that should be taken into account when interpreting our findings.
First, the field study method inherently lacks the control and precision that could be attained in a controlled setting .
There is also a sampling bias given that all of our participants had some level of university education and our deployment sites were all hosted within one U.S. university.
Some of the initial usage at each site was possibly influenced by a novelty effect as announcements were sent out to users to advertise the feature.
Administrators for each site did point out, however, that the number of LemonAid-based help sessions overall were similar to the number of help requests they receive on average.
The way the interview data was collected and coded affects its interpretation and there may be alternative explanations.
I think from the useful perspective, just seeing what questions people have on what elements and what are being asked.
Because that kind of informs us where we might have some problems, lack of understanding, or where we can do some PR.
And that's a lot of where we need some help...how do we make  more accessible?
Although some team members were enthusiastic about using this data to argue for design and content changes, they also felt that it would be most useful over a longer period of deployment and with greater user participation.
The primary goal of this field study was to investigate how users perceive the usability, helpfulness, and reuse value of crowdsourced contextual help in real tasks.
Although we found that LemonAid was helpful, intuitive, and desirable for reuse for over 70% of end users across all the deployment sites, our findings point to several social aspects of generating and maintaining help content that should be addressed in the design of crowdsourced contextual help systems.
The majority of concerns raised by end users were about the timeliness, quality, and authority of answers in the long run.
As mentioned in our results, frequent site users were less likely to find LemonAid to be helpful than new users.
The frequent users often had more advanced questions but were not sure if and when their questions would be answered and who would be providing the answers.
Even though staff moderators were ready to promptly answer users' questions as they emerged, the availability and role of moderators in the help interaction could perhaps be conveyed more clearly to users in the interface.
This study shows that, while difficult, field evaluations can be quite valuable in demonstrating not only that an innovation is valuable, but why it is valuable in the context of social and organizational factors.
