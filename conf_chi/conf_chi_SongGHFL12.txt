Commercial 3D scene acquisition systems such as the Microsoft Kinect sensor can reduce the cost barrier of realizing mid-air interaction.
However, since it can only sense hand position but not hand orientation robustly, current mid-air interaction methods for 3D virtual object manipulation often require contextual and mode switching to perform translation, rotation, and scaling, thus preventing natural continuous gestural interactions.
A novel handle bar metaphor is proposed as an effective visual control metaphor between the user's hand gestures and the corresponding virtual object manipulation operations.
It mimics a familiar situation of handling objects that are skewered with a bimanual handle bar.
The use of relative 3D motion of the two hands to design the mid-air interaction allows us to provide precise controllability despite the Kinect sensor's low image resolution.
A comprehensive repertoire of 3D manipulation operations is proposed to manipulate single objects, perform fast constrained rotation, and pack/align multiple objects along a line.
Three user studies were devised to demonstrate the efficacy and intuitiveness of the proposed interaction techniques on different virtual manipulation scenarios.
There are basically two approaches to accommodate mid-air interactions in such a visual interactive setting.
The first employs a handheld controller device, such as the Nintendo Wiimote.
User inputs via button clicks and accelerometer-based motion sensing are integrated to form high-level gestures to support the interaction.
The second is a controller-free approach, where users can manipulate the graphical contents on the display with their bare hands.
Temporal information to support mid-air interaction is obtained by using an image and/or depth sensor  to continuously sense and analyze the user's body posture and hand gestures via realtime image processing techniques.
This paper studies mid-air interaction designs to support object manipulation in a 3D virtual environment in a controllerfree setting.
This approach is particularly useful for distant viewing and interaction in front of large displays since users can directly perform spatial gestures in their own physical space.
This physical space alone can be used to produce natural 3D manipulation inputs without cumbersome handheld peripherals.
With the wide availability of the Microsoft Kinect sensor , the cost barrier of realizing such mid-air interaction system has been significantly reduced.
However, due to the limitation of the Kinect sensor, which can robustly sense hand position but not hand orientation, current interaction methods often require mode switching to move between different operations such as rotation, translation and scaling.
As a result, it is difficult for users to recall and execute these operations.
This paper aims to address such shortcomings within a controller-free environment that supports natural and intuitive mid-air interactive gestures.
In recent years, mid-air interaction supported by 3D spatial gestural inputs has received increasing attention from both the research community  and the gaming industry, as evidenced by the popular gaming devices such as Nintendo Wii-mote and Microsoft Kinect, which allow us to perform natural physical interactions in our own physical space while moving freely in front of a large display.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
At the heart of this inquiry is the question of what suitable metaphors one can use to map the 3D gestural actions of a user to the manipulation operations on objects in a 3D virtual environment.
The metaphor we proposed for visual manipulation tasks is a bimanual handle bar shown in Figure 1.
We call this the handle bar metaphor.
Both hands from the users are employed to manipulate the virtual objects in a natural manner.
After the related work section, we give an overview of the interaction system, and then describe the handle-bar-
User evaluations were carried out on various visual manipulation tasks that involve translation and rotation, constrained rotation, and multiple object alignment.
Results show that all users can quickly improve their competency in performing the required tasks using our interaction design with only a short period of practice.
Benko and Wilson  proposed to interact with a large curved display by combining speech commands with freehand pinch gestures to provide immersive and interactive experience to multiple users.
More recently, Nancel et al.
To manipulate a 3D virtual object with a single hand, one typical metaphor is to grip and manipulate it with the thumb and forefinger, i.e., a pinch gesture.
Segen and Kumar  described the GestureVR system that used this metaphor to continuously manipulate 6DOF of an virtual object; the object can be translated by moving the hand and oriented by rotating the wrist.
Though this metaphor is very natural and intuitive for common users, it requires fine and robust detection of dynamic fingers poses, which is not achievable with the poor image resolution of low-cost depth sensing devices such as the Kinect sensor, or when the user stands too far away from the sensor as in the case of large display setting.
Closer to our approach are freehand interactive systems that employ two-handed gestures, i.e., bimanual interaction .
Benko and Wilson  enabled users to visualize and manipulate 3D virtual objects using bimanual gestures on an interactive surface and above it .
However, their approach is unable to support simultaneous multiple object manipulation operations.
Compared to these approaches, our use of a handle bar that can be positioned outside the selected object allows us to flexibly perform non-object centric manipulation.
Very recently, Wang et al.
Unlike , our handle bar metaphor design seeks to provide precision control for all R-T manipulations in a unified bimanual manner.
In addition, it does not combine mid-air and keyboard interaction since it is designed for use in a "in-front-of large display" setting with a low-resolution tracking system such as the Kinect sensor.
There are a wide range of methods to interact with 3D contents in virtual space .
Since this work focuses on interactions with freehand gestures, we review mainly two more relevant areas: virtual reality and freehand interfaces.
This approach immerses users in a virtual space for them to perform interaction via various sensors and input devices.
More recently, Ang et al.
Among the virtual reality interfaces, some employ data gloves for gestural mid-air interactions.
In particular, they proposed a grab-and-carry tool for a user to hold an object with two hands, as well as to "carry" it and turn it around.
Though VR interfaces provide highly immersive perception and interactive controls to users, they typically require users to wear instrumented gloves for gestural input, which could be uncomfortable and restrict the freedom of movement.
Freehand interfaces employ tracking systems to recognize mid-air hand or arm gestures as user input.
Luo and Kenyon  employed scalable computing methods for vision-based gesture interaction in a large display setting.
Since its launch, it had sold at an average volume of around 133 thousand units per day in its first sixty days.
Due to its low-cost and wide availability, it has not only gained popularity for gaming, but also employed in numerous research projects in various disciplines.
In particular, this recent innovation spawned many interesting mid-air interaction applications, which have made their rapid debut on the Internet.
For example, the manipulation of 2D and 3D objects , tracking of human motions, gesture control for robot systems, multi-touch-like interface for controlling GUI functions like those seen in Minorty report, see , and .
In this work, we explored the use of this low-cost device for object manipulation.
Our proposed handle bar design can support efficient and effective bimanual manipulation of 3D objects while accommodating the limitations posed by the Kinect sensor.
The different recognized hand gestures as seen by the 3D scene acquisition sensor and their respective visual icons used on the large display for visual feedback.
The  POINT finger and  OPEN palm gestures are less stable as their shapes will change based on the orientation of the hand,  unlike the CLOSE fist gesture.
Our system setup consists of an Alienware Aurora ALX desktop computer with QuadCore CPU 3.20GHz and 9GB memory, running Linux Ubuntu 10.10  with an NVIDIA 1.5GB GeForce GTX480 graphics board, a Kinect sensor, supporting an image resolution of 320x240 at 30 frames per second with both color and depth, and an LCD display of physical size 32 inches.
The Kinect sensor is placed below the large display and the user stands at a distance of around 2 meters from the display during the interaction .
If a hand is located below the center of the user's body, a DOWN gesture is assigned to the hand.
This allows the system to distinguish between one and two-handed interactions.
In addition, the two centroid points from each of the two hands  give the instantaneous length and 3D orientation of the handle bar.
Consider the task of manipulating a 3D virtual object on a wall display using only our two bare hands.
What would be the most effective and intuitive way to do this?
A survey of existing literature revealed a dearth of mid-air interactive designs to perform such a task, especially in environments where multiple objects can be independently manipulated.
We use PrimeSense's OpenNI  open source drivers and the NITE middleware to interface with the Kinect sensor; the depth generator in the OpenNI framework is first employed to obtain the depth image data from Kinect.
Then, we use the skeleton tracker in NITE to compute the user's joint positions from the depth image so that we can determine the 3D location of the user's hands.
At the same time, we use the perception point cloud library  from the Robot Operating System  framework  to generate point clouds from the depth image.
Lastly, based on the hand locations obtained from the 3D skeleton, we segment a point cloud set associated with each of user's hands.
Our experience suggests that the use of the 3D skeleton as a guide produces more accurate and robust segmentation.
Our system is able to recognize three basic single-handed gestures, namely POINT, OPEN, and CLOSE .
To differentiate among them, the extracted point cloud data of each hand is first low-pass filtered  to remove unintentional hand shaking.
These segmented clusters of unity-weighted points  permit two 3D centroid locations to be computed.
One possible approach is to project our physical hands into the virtual space using two iconic hands that represent the hand positions and gestures .
Using the iconic visual feedback, the user can move one's hands to grip the virtual object and then rotate-translate  it with further coordinated hand movements.
This two remote grippinghands metaphor has a direct representation in the virtual space and provides a good semantic mapping between the physical and virtual manipulation.
However, without haptic feedback, it demands substantial physical dexterity to maintain the gripping separation whilst performing the basic R-T manipulations.
Moreover, the hand icons can be easily occluded by the virtual object during the rotation.
Direct gripbased metaphors can also be problematic because the virtual object may not have stable flat contact surfaces for gripping.
To overcome these limitations, a novel handle bar metaphor is proposed.
The handle bar metaphor circumvents this deficiency by designing object manipulation controls that are based on the manipulation of an elongated bar that is specified by the two separated hands of the user.
With the object now attached to the handle bar, manipulation of the object is done by performing R-T manipulations on the handle bar instead .
Unlike the two remote gripping-hands approach, the handle bar icon  provides helpful visual feedback to the user, continuously presenting the relative orientation of the two hands in 3D space during our interactive manipulation.
A summary of the advantages of the proposed handle bar metaphor as suitable interaction paradigm for mid-air interaction is as follows: * Physical familiarity - Bimanual motion gestures required to manipulate the handle bar are intuitive for most users since holding and manipulating an elongated bar with our two hands is a familiar undertaking in common activities such as cycling and lawn mowing.
Interaction design for fast and precise constrained rotation can also be realized with a perpendicular extension to the virtual handle bar metaphor.
Speedy multi-object manipulation can also be supported by piercing the handle bar through more than one virtual object.
These pierced objects can be made to align or slide along the handle bar by using simple variations to the standard bimanual gestures.
Figure 5  shows an example that has a selected object  rotated about another .
This work is concerned with enabling a single user to interactively manipulate single or multiple 3D objects in a virtual environment.
Users execute different visual manipulation operations by moving one or two hands freely within the physical space defined by their frontal bimanual armreach envelope.
This section discusses the handle-bar-based interaction designs to perform the three basic categories of manipulation operations summarized in Figure 6.
One manipulates the handle bar .
Another involves the manipulation of both single and multiple virtual objects , and the last one manipulates the view of a virtual camera in the 3D environment .
Our system can recognize three basic hand gestures, namely POINT, OPEN, and CLOSE .
As highlighted in Figure 6, the interaction design employs a consistent interpretation of these hand gestures.
The POINT and CLOSE gestures are always associated with the handle bar and virtual object, respectively.
POINT or CLOSE gestures are used.
Combining POINT and OPEN gestures allows the handle bar to be modified for constrained rotation .
A combination of CLOSE and OPEN gestures allows multi-object alignment along the handle bar .
As shown in Figure 3 , both the POINT and OPEN hand gestures are sensitive to viewpoint changes, which often make their automatic recognition and classification less robust than the CLOSE gesture.
Hence, they are assigned to interactions that are gesturally less complicated and used less frequently, e.g., browsing and handle bar manipulation.
Since the centroid computation of the CLOSE fist gesture is orientation-invariant and thus more stable, it is used in the object manipulation interactions that often require the user to perform bimanual motion gestures with high degree-offreedom.
This assignment also fits well into the semantic mapping of how a physical handle bar can be manipulated.
Figure 7 shows the state transitions and the expected hand gestures at each state when a user manipulates a single object.
Details of various states are described next.
An active item will be deselected by performing a CLOSE hand gesture when the open hand icon overlaps with the selected object.
All the active items will be deselected if the user performs a CLOSE hand gesture when the open hand icon overlaps with an empty space.
In the Selected state, we can then proceed to other interaction modes such as the mode to manipulate the virtual handle bar.
Selectable objects also include the virtual camera icon located at the bottom of the screen.
The handle bar metaphor provides 7DOF manipulation  of virtual object and supports continuous transitions between operations.
Such manipulation involves the appropriate placement of the virtual handle bar and the subsequent manipulation of the selected object about the center of the positioned bar.
The modes associated with this process are described here.
Note that we use different handle bar colors as a visual feedback to indicate which mode is currently active.
This is the initial state when the system starts.
In this state, no object or camera is selected, and as such, no manipulation can take place.
The provision of a Neutral state is important as it helps overcome the immersion syndrome , where every hand gesture is captured and constantly interpreted by the system.
This can lead to undesirable operations due to misinterpretation of the user's unintended hand gestures.
When interaction is no longer desired, we can re-enter the Neutral state by selecting an empty screen region.
Employing the bimanual POINT gesture in Figure 6 , users can manipulate the virtual handle bar by changing the relative position and orientation of the invisible line that joins their two hands in the physical space .
The end points of the handle bar are determined by the centroid of the 3D point clouds associated with the two POINT hand gestures.
These were observed to be more stable end points than the more appropriate pointing finger tips, which result in handle bar jittering when switching between handle bar and object manipulation modes.
The users leave the Neutral state and enter the Browse mode by keeping one hand on their side  and waving the other raised OPEN palm.
A small open hand visual icon in the virtual space moves within a 2D plane in tandem with the movement of the raised OPEN palm .
When the open hand icon overlaps with a 3D object or the virtual camera icon, the user can perform a CLOSE hand gesture to select the item .
Upon selection, a virtual handle bar will protrude out of the object in the default orientation, namely through the object's centroid and along the x-axis.
This virtual handle bar icon indicates that the system is no longer in Neutral state and is currently in the Selected state .
The handle bar position is not restricted to the confines of the 3D object.
Large translation of the handle bar can be achieved by repeatedly releasing the bar with a bimanual OPEN gesture, retracting the open palm, and then translating the bimanual POINT gesture again in the same direction .
In other words, the 3D gestural workspace need not have an absolute mapping to the 3D virtual world.
The user's physical translational motion can move the handle bar relative to its current 3D virtual world position.
This convention is applied generically to the R-T interactions of the handle bar, selected 3D object, and the virtual camera.
The midpoint of the handle bar is the center for rotating the selected virtual object.
During the handle bar manipulation, the selected object is not affected so that we can change the rotation center by translating the handle bar.
Once the handle bar has been manoeuvred into the desired position, the user can manipulate the selected object by the next mode.
However, rotation about the x-axis can still be obtained in an incremental fashion by executing appropriate concurrent bimanual uni-directional rotation about the y and z axes simultaneously  , which is not immediately intuitive for uninitiated users.
In this case, the constrained rotation provision  right may be a better option as it provides faster and more precise manipulation albeit requiring a mode switch step.
The object scaling operation is done by moving the two CLOSE fists towards each other  or away from each other , along the invisible line that connects the centroids of the two hands in physical space .
The scaling factor S is given by L2 S= ,  t where L is the change in distance between the two centroids in the sampling time t determined by the Kinect sensor's frame rate.
In other words, the amount of scaling can be controlled by both the hand movement distance and speed.
A vigorous gesture gives a larger scaling factor.
The manipulate object mode is a bimanual interaction mode that employs two CLOSE fist gestures  left.
We can apply three basic manipulation operations to a selected object: rotation, translation, and scaling .
The selected object can be translated in the x, y, and z directions by simply translating the bimanual CLOSE fist gesture in the corresponding directions .
The translation of the object is based on the movement vector of the virtual handle bar mid-point, as defined by the centroids of the two CLOSE fists.
The rotation of the selected object about a specific axis is based on the relative angular displacement of the virtual handle bar along that corresponding axis .
No absolute angular mapping is needed since the virtual handle bar can be released in a similar fashion as described in Figure 9  .
Once released, the user may re-initiate a bimanual CLOSE fist gesture at a new position and perform a further rotation.
This manner of executing a rotation allows the user to make large angular changes to the 3D virtual object about the yaxis without getting into an undesirable situation where the front hand occludes the back hand , which may result in an indeterminable 3D pose of the virtual handle bar.
The ability to perform continuous translation and rotation can sometimes make it difficult to execute fast and precise rotation of an object about a specific straight line.
In such circumstance, a less flexible constrained rotation operation could be more preferable.
In our handle bar interaction design, constrained operations can be initiated with a combination of non-homogenous bimanual gestures.
From a two POINT finger handle bar manipulation gesture  left, the user can change one hand to an OPEN palm gesture and move the OPEN palm away from the handle bar axis to create a "cranking bar" with a perpendicular extension  right.
In detail, the horizontal line AB is defined by the standard handle bar when the palm OPENs.
After the user moves the right OPEN palm to define the vertical line BC  right, one can CLOSE both fists to enter the constrained rotation state.
The angular velocity can be controlled by the length of virtual line BC , which is drawn continuously on the display as a helpful visual feedback.
Albeit less useful, constrained translation of a single object along the handle bar can be performed with a non-homogenous combination of a CLOSE fist and an OPEN palm.
Sliding the OPEN palm towards the CLOSE fist moves the single object on the handle bar towards the CLOSE fist end.
This idea is more useful when applied to the manipulation of multiple objects.
Twelve participants  aged between 21 to 28 years were enrolled.
None of them has performed mid-air visual manipulation before but ten of them have played games with the Kinect sensor or Nintendo Wii-mote gaming system.
The physical setup used in the study is shown in Figure 2.
Before the start of each session, the required task was first explained to the user, and then, an expert user showed a demonstration of how the task could be done.
A group of objects can be manipulated together and/or aligned along the virtual handle bar once they are selected  and are all pierced by a handle bar.
First, standard RTS operations can be performed on all these selected objects in the same manner as with a single object  left.
All objects on the handle bar can translate and scale as an aggregation.
Rotation of all these objects will be centered about the mid-point of the handle bar.
Three basic alignment operations for aligning multiple objects on a handle bar are provided  right: * The first allows the user to "pack" objects by interactively sliding the selected objects towards each other using a gesture that moves the OPEN palm towards the CLOSE fist.
Objects stop sliding when boundary collision is detected.
Multiple objects can also be made to slide towards the CLOSE fist by "tilting" the virtual handle bar as shown in Figure 13.
The larger the tilt angle is, the faster the objects will slide.
This manner of packing multiple objects has a very close semantic mapping to the physical nature of object behavior along a handle bar  and may be preferred by some users.
One of the strengths of the interaction design using the proposed handle bar metaphor is the ability to execute continuous transitions between RTS manipulation operations without the need to switch modes.
We wanted to evaluate if naive users were able to perform simple R-T manipulations without any training and whether subsequent repeated attempts can quickly improve their performance.
Figure 14  shows the task of rotating and translating a randomly-oriented cube to its desired position as indicated by the wire-frame outline.
Before starting the task, a brief demonstration was given to show the user the required hand gestures to select an object, position a handle bar, and perform the R-T manipulation required to put the cube into its destination.
At each attempt, the user was given 2 minutes to put as many cubes as possible into the wire-frame envelope subject to a reasonable precision indicated by a wire-frame color change.
The user can shake both CLOSE and OPEN hands and objects on the handle bar will be distributed at equi-distance along the handle bar.
This operation is useful for "unpacking" objects that are too close to each other.
The separation distance can be controlled by the user by adjusting the length of the handle bar before the shaking.
Results in Table 1 show that the handle-bar-based R-T manipulation can be quickly learnt by just having on-the-task practice.
On average, the 12 participants were able to double the number of cube placements within 6 attempts.
However, the variance in user performance is high, indicating that some users are better in performing this type of interaction than others.
The best performer managed 10 cubes in attempt #1 and improved to 15 cubes by attempt #6, compared to the worst performer who managed only 1 cube in attempt #1 but did improve to 6 cubes by attempt #6.
The task evaluates the speed the user can rotate the disc clockwise or anti-clockwise to reach the desired angular position to "catch" the falling ball.
To achieve this, the ball dropping speed increases linearly in each successive drop.
This task also evaluates the angular precision the user can maintain in order to ensure the ball "drop" into the hole on the disc.
For this, the hole is made small and a red vertical line provides the user with the visual cue required to align the "catch."
The task is to "catch" as many falling balls as possible into the hole on the rotating disc within 60 seconds.
Like before, a demonstration on how the task is done was first given to each user and attempt #1 was done without any practice.
The subsequent two attempts were performed after giving the participants 2 minutes practice time before each attempt.
Table 2 shows the average number of balls caught by the 12 participants; after a short period of practice time.
The performance can increase from about 11  to about 17 balls .
The handle bar interaction design for rotating a virtual object about a fixed axis is able to provide fast angular speed, yet still offering good angular position controls since the speed of the 17th dropping ball is significantly faster than the 11th ball.
This conclusion is further supported by the fact that on average, the first error  made by the users were delayed from about the 3rd ball  to the 9th ball  after a short period of practice, again indicating the angular controllability and precision of the "cranking" bimanual CLOSE fist gesture despite the small room for angular error in catching the ball.
It is interesting to note the performance variability among the 12 participants after practice, as evident in the large variance increase in attempt #2 for both the number of balls caught and first ball missed.
Like the first user study, this suggests that some users found executing the correct bimanual mid-air gesture requires more practice than just 2 minutes.
The variance was observed to drop significantly after they were given a little more time  to practice the constrained rotation gesture.
Results in Table 3 show that for both Tasks 1 and 2, it was at least 2 to 3 times faster when using the multi-object manipulation and alignment techniques to do the required task than placing objects one at a time.
From the absolute average time taken and rate of improvement with each subsequent attempt, it is clear that the "pack" multi-objects alignment procedure of Task 1 is easier to execute than the "equidistribute" multi-object alignment procedure of Task 2.
A set of questionnaires were given to each user after each of the three user studies to gauge the subjective aspects of their experiences in the handle bar interaction.
Table 4 presents the mean response values of the 12 participants and the associated standard deviation bar for each survey question.
The bimanual R-T manipulation hand gesture designed using the handle bar metaphor was found by most users to be generally intuitive to use, ease to remember, and providing good controllability.
Consistently high ratings were received from the users for the ability to perform rotation and translation in one continuous motion.
The subjective evaluation of the constrained rotation interaction design fared a little worst, with mean ratings at values just below 4.0.
The user variability was far higher though, suggesting that performing fast and precise angular rotations with a cranking action is not universally straightforward for everyone.
The very high mean rating for question #8 suggests that most users find the ability to rotate, translate, and align multiple objects at the same time to be very useful and preferred when manipulating several objects with similar trajectory and orientation.
The final user study compares the time performance of manipulating three in-line objects such that we have to move them to some target positions.
In Task 1, the three objects are initially positioned at a distance apart and needs to be brought close to one another at the destination.
Task 2 does the reverse .
The users were asked to perform these manipulation tasks using the single object manipulation technique as well as the multiple object manipulation and alignment techniques .
The last application example  presents how the handle bar metaphor can be used to manipulate and assemble 3D mechanical parts.
Two different computer-aideddesign  models, double-range burner and launcher,  are employed here.
Using our interaction methods, we can efficiently assemble the models with bare hands .
The proposed interaction designs based on our handle bar metaphor were applied to three different applications to illustrate their potential.
The first application example shows how furniture can be arranged to a desired layout in a 3D virtual environment .
The multiple object manipulation technique was used to quickly arrange similar chairs.
The translate-rotate manipulation was used to "pick up" a toppled flower pot and place it on the table in one continuous bimanual hand movement.
Once on the table, constrained rotation was invoked to continuously rotate the pot till it was deemed to be at the desired orientation .
We propose the handle bar metaphor as an effective way to perform mid-air interactions that manipulate the pose and scale of 3D virtual objects, suitable for use with a low-cost depth sensing device like Kinect in a large-display setting.
The main strength of this metaphor is the physical familiarity it provides users with, as they mentally map their bimanual hand gestures to manipulation operations such as translation and rotation in the virtual 3D environment.
The provision of visual cues in the form of the instantaneous orientation of the protruding virtual handle bar that corresponds interactively to the ever-changing positions of the user's two hands was observed to be very effective in providing a strong sense of control to the user during interactive visual manipulation.
In addition, the flexibility and variety of interaction designs based on the handle bar metaphor have been demonstrated.
These include the constrained rotation operation based on a novel "cranking" bimanual gesture and speedy techniques to manipulate and align multiple objects along a straight line using a simple combination of CLOSE and OPEN hand gestures.
The virtual molecule exploration application example suggests that the same handle bar metaphor could be applied to manipulate a virtual camera to support an intuitive and flexible means of performing interactive visual navigation in a 3D virtual environment.
Observations from user studies suggest that the competency in using mid-air interaction techniques for visual manipulation is not universally innate to all users.
However, interaction based on the handle bar metaphor seems to provide an intuitive way for users to quickly learn how to map the action of their bimanual hand gestures to corresponding visual manipulation tasks in a 3D virtual environment.
Practice was observed to quickly improve everybody's performance and reduce the differences in skill levels among first time users.
However, the issue of fast fatigue onset is still a perennial problem when using mid-air interaction for precise control.
The second application example  illustrates how the handle bar interaction designs can be applied to manipulate the virtual camera  to facilitate visual exploration of a complex molecular structure.
The translate-rotate manipulation allows us to visually navigate within the virtual molecular structure.
The scaling gesture, when applied to a virtual camera, enables us to zoom in and out while navigating freely within the virtual 3D environment.
Ang, B. Horan, Z. Najdovski, and S. Nahavandi.
Grasping virtual objects with multi-point haptics.
M. Annett, T. Grossman, D. Wigdor, and G. Fitzmaurice.
Medusa: a proximity-aware multi-touch tabletop.
T. Baudel and M. Beaudouin-Lafon.
CHARADE: remote control of objects using free-hand gestures.
H. Benko and A. D. Wilson.
DepthTouch: Using depth-sensing camera to enable freehand interactions on and above the interactive surface.
H. Benko and A. D. Wilson.
Multi-point interactions with immersive omnidirectional visualizations in a dome.
F. Bettio, A. Giachetti, E. Gobbetti, F. Marton, and G. Pintore.
A practical vision based approach to unencumbered direct spatial manipulation in virtual worlds.
Bowman, E. Kruijff, J. J. LaViola, and I. Poupyrev.
3D User Interfaces: Theory and Practice.
Combining and measuring the benefits of bimanual pen and direct-touch interaction on horizontal interfaces.
L. D. Cutler, B. Fr ohlich, and P. Hanrahan.
Two-handed direct manipulation on the responsive workbench.
T. Duval, A. L ecuyer, and S. Thomas.
SkeweR: a 3D interaction technique for 2-user collaborative manipulation of objects in virtual environments.
W. Garage and the Stanford Artificial Intelligence Laboratory.
T. Grossman, D. Wigdor, and R. Balakrishnan.
Multi-finger gestural interaction with 3D volumetric displays.
Asymmetric division of labor in human skilled bimanual action: The kinematic chain as a model.
G. Hackenberg, R. McCall, and W. Broll.
Lightweight palm and finger tracking for real-time 3D gesture control.
Interactions in the air: Adding further depth to interactive tabletops.
