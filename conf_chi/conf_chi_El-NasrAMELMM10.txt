Cooperative design has been an integral part of many games.
With the success of games like Left4Dead, many game designers and producers are currently exploring the addition of cooperative patterns within their games.
Unfortunately, very little research investigated cooperative patterns or methods to evaluate them.
In this paper, we present a set of cooperative patterns identified based on analysis of fourteen cooperative games.
Additionally, we propose Cooperative Performance Metrics .
To evaluate the use of these CPMs, we ran a study with a total of 60 participants, grouped in 2-3 participants per session.
Participants were asked to play four cooperative games .
Videos of the play sessions were annotated using the CPMs, which were then mapped to cooperative patterns that caused them.
Results, validated through inter-rater agreement, identify several effective cooperative patterns and lessons for future cooperative game designs.
In the past year alone, several AAA titles, such as Resident Evil 5  and Left4Dead , included an optional cooperative mode.
While cooperative design patterns1 have been around since the inception of games, very few research studies discussed or documented them.
Methods for evaluating them are also in their infancy.
Most often, user testing groups within game companies evaluate cooperative games using the same methods used to evaluate single player games , which are inappropriate for investigating the cooperative aspect of a game.
Therefore, there is a need for  understanding current successful cooperative patterns and  creating methods to evaluate their effectiveness.
This paper aims to address these issues within the context of cooperative video games by discussing three contributions.
First, we present a set of cooperative design patterns developed based on analysis of fourteen cooperative games.
These patterns extend previous work and present a comprehensive framework for cooperative game analysis.
In addition, we outline a set of Cooperative Performance Metrics  used to analyze and evaluate cooperative play.
These CPMs were used to analyze data collected through a study of a total of 60 participants grouped in 25 sessions with 2-3 participants/session, playing four cooperative games: Rock Band 2 , Lego Star Wars , Kameo: The Elements of Power , and Little Big Planet .
The aim of the analysis was to investigate connections between the CPMs and the cooperative design patterns discussed in the paper.
Results from this study revealed several interesting design lessons for building better cooperative games.
We present these results as the second contribution of this paper.
The CPMs themselves constitute the third contribution of this paper;
Cooperative games encourage participation and collaboration; the goal is not to win as a player but as a team of players.
Discovering effective cooperative game patterns is an elusive and important problem .
Results of our background questionnaire with 60 6-16 year olds revealed that kids are split when it comes to cooperative games.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The paper is structured as follows.
After discussing previous work, we detail the cooperative design patterns we developed based on analysis of fourteen co-op games.
In discussing these patterns, we examine the process used to derive them, thus addressing their validity.
We then explain the CPMs; in discussing these metrics, we detail the process we used to validate them as instruments for analysis and evaluation of cooperative games.
We then outline the study we conducted on four cooperative games.
We conclude by discussing the findings and their implications, as well as future research.
They concentrated on challenge as a main aspect of engagement.
In addition, Lazarro et al.
They used observation notes, videotaped interaction, and questionnaires/interviews with friends and family.
Based on their study they identified four kinds of fun:  hard fun: motivated by achievement,  Easy Fun: motivated by exploration,  Altered states: motivated by visceral rewards, and  Social: motivated by competitive or cooperative play or just to be with friends.
Even though the sample was small and included many games, their work contributed data showing variations in play styles and motivations.
An alternative approach to playtesting and usability studies is the use game heuristics evaluation.
This method is based on the usability heuristic technique developed by Nielsen .
Game heuristic evaluation is accomplished through a systematic inspection of a game using a set of heuristics or guidelines.
The technique provides a very cheap and easy to administer testing method, which became very popular within software companies.
Several researchers  worked on developing a set of game design heuristics, expanding on the user interface heuristics develop by Nielson .
However, heuristics cannot completely replace playtesting and usability studies, as it does not provide attitudinal, behavior, or play data from actual users.
While previous works present excellent research that addressed the evaluation of games, the measurement and evaluation of cooperative games is still an untapped area.
The only work we found targeting this area was Pinelle et al.
While their work and criteria is close to ours, they focused on heuristics based techniques and developed methods to evaluate networked games rather than games where participants share the same physical space.
In this paper, we specifically propose a set of validated Cooperative Performance Metrics CPMs for analyzing and evaluating cooperative play occurring over the network or within the same space.
Usability testing is an integral part of any software development process.
Methodologies of usability and user testing have been addressed in many Human Computer Interaction works .
Game evaluation methods integrate many of these HCI methods, but extend them to include testing for playability and engagement .
The game industry realizes the importance of developing and conducting game evaluations; this is evident by the formation of groups such as Microsoft's User Experience group, Sony's usability and playtest, Ubisoft's playtest, and Eidos' user-research groups, and the emergence of several user research companies, such as Emsense and XEODesign.
Further, Microsoft developed an online tracking system called TRUE to collect and visualize gameplay telemetry data and synchronize them with attitudinal and observational data .
This enabled them to "detect issues and understand root causes in the same way usability testing does ."
They validated their system using two games: Halo 2 and Shadowrun.
In addition, Dracken et al.
They used Geographic Information Systems to visualize spatial gameplay metrics, developing the Heat Map, which enabled them to detect level design problems, such as places where a lot of players died .
Similar methods are used by Valve , Bungie and Electronic Arts.
A few studies concentrated on defining methods for evaluating engagement or enjoyment in games.
Sweetser and Wyeth developed a model called GameFlow  based on the Flow theory .
The model consisted of a set of qualitative criteria for measuring eight specific elements of a game: concentration, challenge, skills, control, clear goals, feedback, immersion, and social.
They validated the model by evaluating two commercial games, and comparing their results to that of expert reviews.
Some researchers analyzed a set of cooperative games to develop cooperative game design patterns.
Zagal et al., for example, explored cooperative patterns within board games .
Also, Bjork and Holopainen  presented a large number of game design patterns, which included cooperative and social interaction patterns.
In addition, Zagal et al.
In this paper, we follow in the footsteps of these efforts by extending Rocha et al.
We chose to extend Rocha et al.
Note that the patterns described by Rocha et al.
It implies that players play different character roles to complement each others' activities within the game.
For example, in World of Warcraft , a Shadow Priest can cause an enemy to become vulnerable to shadow damage, which also results in an increase in the damage that Warlocks  can cause.
For example, the achievement system developed for the Pyro and Medic character classes within Team Fortress 2 gives Pyros the goal of killing three enemies while ubercharged .
The Medic, on the other hand, has a different goal, which is to ubercharge a Pyro while he/she burns enemies.
For example, designers can encode rules to denote specific effects to actions within the game when performed on a friendly player.
The idea behind these differences is to promote and facilitate cooperation.
A good example is the rule in FPS  games that prevents damage when players accidently shoot other players on the same team, known as Friendly Fire modes.
The analysis process took two months to complete.
During this time, two researchers analyzed each game in detail using game design theory  and previous work on cooperative game design .
They identified distinct design techniques, including resource sharing, controls , shared goals and puzzles, and reward structures.
They also noted visual design characteristics, such as camera settings.
They developed a set of design patterns based on this analysis.
For validation, the patterns were reviewed by an independent researcher, who has over 10 years of game industry experience.
After his approval, we asked a team of two independent researchers to play all the identified games and develop their own cooperative game design patterns.
Although we didn't run a Kappa analysis on the rater agreement, we can report a very high agreement, as researchers identified the same patterns, but have used different terms to denote some of them.
At the end of this process, researchers met and discussed the patterns; a final set is discussed below.
We differentiate between cooperative games that support cooperative play through sharing a computer or screen vs. patterns designed for online or distributed collaboration.
This is important as we discuss cooperative patterns, since we identified the same distinction between cooperative games in the market.
All fourteen games reviewed in our study were designed for kids to play together through a shared screen.
In these situations, camera set up emerged as an important design component.
We identify the following additional patterns: * Camera Setting: there are three design choices for developing a successful camera in a shared screen co-op games--split screen horizontally or vertically, one character in focus, all characters are in focus .
In Beautiful Katamari, players share a ball.
Similarly, in Little Big Planet, both players can push or grab one object together.
This pattern was observed in games such as Lego Star Wars and Little Big Planet, where both players encounter a shared challenge or obstacle.
In parallel with our research on previous work, we conducted a study analyzing new and old cooperative games, specifically exploring their core mechanics and identifying the interaction models behind their co-op game play.
Our initial research resulted in a total of 215 PC and video games that had a multiplayer component.
However, most of them included competitive rather cooperative patterns.
After an initial review, we selected fourteen games for deeper analysis that included cooperative modes; these were: Left4Dead , Resident Evil 5 , Beautiful Katamari , Kameo: The Elements of Power, Lego Star Wars, Wall-E ,
This pattern can be seen in Lego Star Wars, where both players have the ability to assume a special character, but only one can.
This enables discussions among players concerning how to share the character.
In Left4Dead, the Hunter and Smoker are good examples of this pattern.
It, thus, encourages players to play close together and support each other.
Resident Evil 5 uses this technique; many examples of this pattern can be seen in board games .
Table 1 shows example patterns from two games.
Game Significant Design Pattern * Limited resources: the number of stars collected is a shared resource.
The selected games were Rock Band 2, Lego Star Wars, Kameo, and Little Big Planet.
We will use the following abbreviations to denote the games: RB, LSW, K, and LBP, respectively.
After each play session, participants were interviewed individually to gauge their perceptions on their play experience.
For further analysis we videotaped all the play sessions front and back as shown in Figure 1.
In order to analyze the cooperative nature of these games, we defined several metrics: Cooperative Performance Metrics .
These metrics are associated with observable events within a play session, and thus can be used as a basis for video annotation or structured observation of a cooperative play session.
We created these CPMs through an iterative process involving expert and team reviews.
The first initial set of metrics was defined based on several play sessions, where researchers played cooperative games and others observed.
These metrics were then reviewed and revised by the team of five researchers involved in this study.
The metrics were then used to observe and annotate two pilot cooperative play sessions.
The metrics were also sent in parallel to three industry game designers working at Electronic Arts and Square Enix.
Based on their feedback and the results observed from the two pilot sessions, we revised the metrics.
In a meeting conducted with the research team, three with previous game industry experience, we discussed the metrics and approved the final set, which was used to video annotate the 25 play sessions.
We later validated the metrics through an inter-rater agreement method discussed below.
The final set of CPMs developed is as follows: Laughter or excitement together, which we identified as events, where participants: * laughed at the same time due to a specific game event; * expressed verbally that they are enjoying the game, looking for utterances, such as "sweet", "it is a lot of fun", etc.
This behavior was coded by labeling each event in the video that led to laughter or excitement based on the criteria above.
Given the cooperative patterns discussed above, we ran a study to investigate how players experience cooperative games that embed these patterns.
Participants were recruited through bulletin boards, special contact lists, schools, and organizations, such as the Boys and Girls Club.
We invited participants to come in groups of 2-4 participants: friends or family for a 3 hour play session.
As they came in, they signed a consent form and were interviewed.
The first interview included questions about their background, playing habits, and previous gaming experience.
After this initial interview, we asked them to play four games in 10 minute-sessions.
We also imposed the constraint that researchers should label events happening in the same space only once per cause.
Another metric that is central to our work is an event that caused participants to Worked out strategies.
This was identified when participants: * talked aloud about solving a shared challenge; * divided a game zone to different parts in order to divide and conquer; * navigated the world while consulting with each other.
This is important as it refers to cases during gameplay where an obstacle encourages participants to consult with each other and make a local plan to resolve it.
For example, in Lego Star Wars, there were different platform puzzles that required players to jump over some specific platforms to open the path.
This challenge allowed players to consult with one another and make decisions together.
Another related metric is Helping each other.
This metric corresponds with helping events.
These events come in different varieties.
For example, we often found that some players help others by leading them through the game, or by pointing to specific buttons.
In Little Big Planet, we found many tangible instances of this metric, where participants helped one another by pointing to the controller or by handling the controller for the other player.
Thus, we define events that signify this metric as events where players: * talked about controllers, and how one can use the game mechanics; * told each other the correct way of passing a shared obstacle; * saved and rescued the other player while he or she was failing; In our inter-rater agreement experiments we found that researchers can confuse this tactic with the Worked out strategies tactic, especially if participants are helping each other.
Thus, we imposed the constraint that researchers should label events under the Helping CPM when one player is helping the other and not when both are helping each other.
Global Strategies is a metric we created to refer to events where players take different roles during gameplay that complement each others' responsibilities and abilities.
A tangible example of this parameter was observed in Lego Star Wars, where one player played the role of Jar-Jar  and the other one tried to support Jar-Jar while facing enemies.
One important problem with cooperative games is the gap between skills which causes players to wait for one another.
Most of the time this builds frustration, and thus we developed a metric called Waited for each other to label events, where one player waits for the other to catch up.
We used the CPMs to annotate all game play sessions.
A total of 3000 minutes of video data were reviewed and annotated .
One researcher took on this task.
He went through all videos and labelled each CPM occurrence.
For example, when a laughter event as described above is observed, he marked the video and annotated it by labelling the instance as Laughter and Excitement Together CPM.
In this section, we discuss the totals, averages, standard deviation, 95% confidence intervals for all CPMs per game.
We also discuss paired ttests evaluating statistical significance of the results.
Furthermore, for each CPM label within the video analysis, the researcher identified a cause based on the cooperative design patterns, specifically: complementarity, synergies between abilities, shared goals, synergies between goals, special rules, camera styles, Interacting with the Same Object , Shared Puzzle , Shared Character , and Miscellaneous .
PM is a miscellaneous category that includes animations, cut scenes, or special elements that are specific to one game.
For example, the dance animation in Little Big Planet caused much laughter.
The mapping between CPMs and cooperative patterns were performed through a qualitative interpretive exercise.
Before discussing the results, we will discuss the validation process we performed to evaluate the reliability of the results.
First, to establish face validity, patterns and CPMs were developed through an intensive review process as discussed above.
To establish scientific validity, we performed a formal validation process.
We asked two independent researchers to rate two sessions given the CPMs and the cooperative patterns identified.
All researchers were given an introduction to the CPMs and cooperative patterns and were shown an example of how to apply them using a video-taped gameplay session.
Afterwards, they were given two videos of play sessions of Kameo and Lego Star Wars to analyze.
We then performed inter-rater agreement and calculated kappa values .
Table 2 shows the results of this process.
Figure 2 shows totals of events for all sessions labeled as Laughter and Excitment Togehter.
Table 3 shows averages per session, standard deviation, and confidence intervals.
As it can be seen, Lego Star Wars is in the lead with a lot more laughter and excitment events than the rest of the games.
Little Big Planet follows, then Kameo and Rock Band 2 .
We ran t-tests to check for signficance of the differences between the games.
Figure 4 shows totals for Worked Out Strategies events for all sessions and table 4 shows averages, standard deviation, and confidence intervals.
As it can be seen, Lego Star Wars is significantly in the lead and Rock Band 2 is far behind all others with significance.
We ran t-test between each pair.
Further analysis of the causes of these events reveals that, interestingly, PM is the main cause .
PM includes a variety of different visual and audio patterns such as character design, character animations, interactive objects, and cut scenes.
For example, the falling down animation in Lego Star Wars had a great impact on players' excitement.
Little Big Planet's character designs also had many exciting features such as dancing, shaking hands, etc..
Figure 5 shows analysis of patterns that caused these Worked Out Strategies events.
As players tried to solve puzzles cooperatively, they talked aloud and made plans.
Additionally, the complementarity of roles in Kameo made this game very challenging, as players switch to different characters to solve puzzles and divide tasks.
In one observation, two players worked out their strategies so that one player explored the map while the other fought.
Figure 7 shows a strong relation between Helping events and the shared puzzles and goals patterns.
These two patterns cover 70% of the Helping metric.
Also, it is interesting to note synergies between goals as a design pattern accounting for 10% of Helping events.
Rock Band 2 was the only game that used this pattern-since players' goals include finishing notes, and the other players' performance has a great impact on group performance.
Figure 6 shows totals of observed Helping events for all session.
Table 5 shows averages, standard deviation, and 95% confidence interval per game.
The results show that Kameo is significantly in the lead here.
Rock Band 2 is last with no overlap with other games.
We deduce from our observation and analysis of gameplay videos that Kameo was the most difficult game for players given all the other games.
This may be due to the splitscreen 3D game.
But it was also obvious that many participants had a lot of problems with the controller and the obstacles within the game.
This caused them to seek each others' help, and thus may explain the lead of Kameo.
Rock Band 2, on the other hand, is a concentration game that didn't really give players time to help each other.
Figure 8 shows totals of observed Global Strategies events for all sessions; table 6 shows averages, standard deviation, and 95% confidence interval per game.
Rock Band 2 and Little Big Planet following.
The significant gap between Kameo and Lego Star Wars on the one hand, and the Rock Band 2 and Little Big Planet on the other, shows that action adventure games support this CPM.
Figure 9 shows relations between Global Strategies and causes.
Complementarity and shared character design patterns account for the majority of these events.
Together, they account for 58% of this metric.
Kameo supports four different characters with differnet abilities that players switch between dynamically during gameplay.
This feature makes it possible for players to assume different roles and develop tactics based on their desired character abilities.
Likewise, Lego Star Wars uses the shared character pattern named Jar-Jar-the player who takes the role of Jar-Jar is responsible for big jumps that solve the platform puzzles in this game, but this character is vulnerable to enemies, and thus the other player has to support him.
Figure 10 shows total events observed for all sessions for the Waited for Each Other metric, while table 7 shows averages, standard deviation, and confidence intervals per session.
Like with Global Strategies, Lego Star Wars and Kameo are in the lead, overlapping in their confidence interval.
Also, Rock Band 2 and Little Big Planet follow with little overlap in their confidence intervals.
Looking at the causes for these events , it is surprising to see that the camera pattern accounts for 47% of these events.
When we take a closer look at the studied games, we see that in Lego Star Wars, the camera requires players to wait for each other to proceed.
Conversely, Kameo has a split screen style, which gives players the freedom to get solve puzzles independently.
However, the shared puzzle structures in Kameo are designed in such a way that players need to reach the same checkpoints while progressing through the game levels.
This caused players to wait for each.
It should be noted that Rock Band 2 has a pausing mechanism that players could use but didn't choose to in any of our sessions.
In conclusion, we present table 9, showing some of the significant cooperative patterns identified based on our results.
Specifically, complementarity, shared goals, shared puzzles, and shared objects had a major impact on the identified CPMs.
This is evident by the significant results we discussed, specifically in the Global Strategies CPM where Lego Star Wars and Kameo were clearly in the lead due to their use of shared goals, shared puzzles, and complementarity cooperative patterns.
In addition, the results suggest that, for the age group we had , split screen and camera led by the first player caused Waited for Each Other and Got in Each Others' Way CPMs, which may have a negative impact on the play experience.
Thus, designers need to be careful when designing camera settings.
Furthermore, analysis of laughter and excitement shows that visual style and animation as well as cut scenes caused much of the Laughter and Excitement Together .
Another interesting point to note for cooperative designs is that Helping occurred when the game was difficult for players--the number of events observed was significantly higher for Kameo, which was rated the most difficult game by our participants.
Thus, this CPM is directly tied to difficulty and can be used to tune difficulty of the game.
Figure 12 shows total of observed events of Got in Each Others' for all sessions, and table 8 shows averages, standard deviation, and confidence intervals.
As it can be seen, there is overlap between confidence intervals among all games.
This insignificance may be due to the fact that the CPM was observered for many causes.
To summarize, designing effective cooperative patterns is an important area for the game industry, and has a direct impact on the development of educational as well as informal learning games.
Developing methods for evaluating or analyzing players' cooperative play is still an untapped research area.
In this paper we presented several contributions.
First, we proposed several cooperative game design patterns extending previous work.
Third, we presented results of a study analyzing the experience of 60 players playing cooperatively in groups of 2-3 four cooperative games: Rock Band 2, Lego Star Wars, Kameo, and Little Big Planet.
The analysis resulted in valuable design lessons, which form another contribution of this paper.
These results were further validated through interrater reliability measures.
In future research, we will extend this work by running additional experiments with different age groups and game types.
We also wish to thank our funders.
The study was funded by MITACS , a Canadian Network Center of Excellence , and Bardel Entertainment, a virtual worlds company in Vancouver, British Columbia.
A. Jacko, Human-Computer Interaction Fundamentals: CRC, 2009.
B. Fulton, "Beyond psychological theory: getting data that improve games," in Game Developers Conference, 2002.
K. Isbister and N. Schaffer, Game Usability: Advancing the Player Experience: Morgan Kaufmann, 2008.
J. P. David, K. Steury, and R. Paygulayan, "A Survey method for assessing perceptions of a game: the consumer playtest in game design," The international Journal of Computer Game Research, vol.
A. Drachen and A. Canossa, "Analyzing Spatial User Behavior in Computer Games using Geographic Information Systems," in MindTrek 2009, 2009.
A. Tychsen, "Crafting User Experience via Game Metrics Analysis," in Workshop Research Goals and Strategies for Studying User Experience and Emotion, part of NordiCHI 2008, Lund, Sweden, 2008.
L. Nacke, M. Ambinder, A. Canossa, R. Mandryk, and T. Stach, "Game Metrics and.
Biometrics: The Future of Player Experience Research," in Future Play, 2009.
P. Sweetser and P. Wyeth, "GameFlow: a model for evaluating player enjoyment in games," Computers in Entertainment , vol.
M. Csikszentmihalyi, Flow: The Psychology of Optimal Experience.
Intelligent Paradigms in Computer Games 2007, pp.
N. Lazzaro, "Why we play games: Four keys to more emotion without story," XEODesign 2004.
J. Nielsen and R. Molich, "Heuristic evaluation of user interfaces," in Proceedings of the ACM CHI '90, 1990, pp.
H. Desurvire, M. Caplan, and J.
A. Toth, "Using heuristics to evaluate the playability of games," in CHI, 2004.
M. A. Federoff, "Heuristics and usability guidelines for the creation and evaluation of fun in video games."
N. Schaffer, "Heuristic Evaluation of Games," in Game Usability, K. Isbister and N. Schaffer, Eds., 2008.
D. Pinelle, N. Wong, T. Stach, and C. Gutwin, "Usability Heuristics for Networked Multiplayer Games," in Cupporting Group Work, 2009.
J. P. Zagal, J. Rick, and I. Hsi, "Collaborative games: Lessons learned from board games," Simulation & Gaming, vol.
S. Bjork and J. Holopainen, Patterns in Game Design.
J. Zagal, M. Mateas, C. Fernandez-Vara, B. Hochhalter, and N. Lichti, "Towards an Ontological Language for Game Analysis," in Digital Interactive Games Research Association Conference , 2005.
B. Rocha, S. Mascarenhas, and R. Prada, "Game Mechanics for Cooperative Games," in ZDN Digital Game, 2008.
E. Adams and A. Rollings, Fundamentals of Game Design: Prentice Hall, 2006.
R. Rouse, Game Design Theory and Practice: Wordware Publishing Inc., 2000.
S. Benford, B. Bederson , K. Akesson, V. Bayon, A. Druin, P. Hansson, J.-P. Hourcade, R. Ingram, H. Neale, C. O'Malley, K. T. Simsarian, and D. Stanton, "Designing Storytelling Technologies to Encourage Collaboration Between Young Children," in Human Factors in Computing Systems , 2000.
J. Cohen, "A coefficient of agreement for nominal scales," Educational and Psychological Measurement, vol.
J. R. Landis and G. G. Koch, "The measurement of observer agreement for categorical data," Biometrics, vol.
