For the majority of us, inter-personal communication is an essential part of our daily lives.
Instant Messaging, or IM, has been growing in popularity for personal and workrelated communication.
The low cost of sending a message, combined with the limited awareness provided by current IM systems result in messages often arriving at inconvenient or disruptive times.
In a step towards solving this problem, we created statistical models that successfully predict responsiveness to incoming instant messages - simply put: whether the receiver is likely to respond to a message within a certain time period.
These models were constructed using a large corpus of real IM interaction collected from 16 participants, including over 90,000 messages.
The models we present can predict, with accuracy as high as 90.1%, whether a message sent to begin a new session of communication would get a response within 30 seconds, 1, 2, 5, and 10 minutes.
This type of prediction can be used, for example, to drive online-status indicators, or in services aimed at finding potential communicators.
Unlike face-to-face communication, users of IM cannot easily detect whether a buddy is available for communication or not.
As the use of IM is growing, and in particular in the work place, the inability to detect a buddy's state can often result in communication breakdowns with negative effects on both communication partners.
For the receiver, communication at the wrong time might be disruptive to their ongoing work.
If, on the other hand, receivers simply decide to ignore communication, the initiator's productivity might suffer as they are left waiting for a piece of information needed for their work.
If, however, we were able to accurately predict whether a user was likely to respond to a message within a certain period of time, then some of these breakdowns could be prevented.
For example, models could be used to automatically provide different "traditional" online-status indicators to different buddies depending on predicted responsiveness.
Alternatively, models can be used to increase the salience of incoming messages that may deserve immediate attention if responsiveness is predicted to be low.
One could also imagine a system whose role is to allow its users to locate others who are available for conversation  while hiding those who aren't.
This would benefit users looking for help, whose messages would be more likely to get a response, as well as busy users who would be able to stay on task uninterrupted.
The work presented in this paper describes the creation of accurate statistical models that are capable of predicting a user's responsiveness to incoming messages - simply put: whether the receiver is likely to respond to a message within a certain period of time.
For example, of the models presented in this paper, one was able to predict with 89.4% accuracy whether a user will reply to a message within 5 minutes and another with 90.1% accuracy a response within 10 minutes .
Inter-personal communication through Instant Messaging, or IM, is gaining increasing popularity in the work place and elsewhere.
IM programs, or clients, facilitate one-onone communication between a user and their list of contacts, commonly referred to as buddies, by allowing them to send and receive short textual messages .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
A number of benefits of using IM have contributed to its increasing popularity.
With its near-synchronous nature, IM is positioned somewhere between synchronous communication channels  and asynchronous communication channels .
Since IM is inherently asynchronous, users can choose when or whether to respond to an incoming message.
As noted by , users welcome the ability to use "plausible deniability" when electing not to respond to messages.
IM is thus often regarded as less disruptive than other synchronous communication channels.
In fact, IM is sometimes used for communication even between users who share the same physical work-space in an attempt not to disrupt one another's work.
This asynchrony means that messages often arrive when a user is engaged in other tasks.
Indeed, research shows that users often multitask when using IM .
Particularly in the work place, messages may thus arrive when a user is engaged in important and potentially urgent work.
This means that while it is convenient and desirable for the sender to initiate a conversation, it may be undesirable and often inconvenient for the receiver.
The receiver must then choose between staying on task and engaging in conversation.
Staying on task and not responding may come at a cost to the initiator, who may need some information from the receiver.
The receiver herself may incur a social cost from being portrayed as unresponsive.
Engaging in conversation, on the other hand, will often come at a cost to the receiver's ongoing work .
One of the most important features of IM clients is the ability to provide some awareness of presence.
IM clients typically provide this information by indicating whether a user is online and whether the user is currently active or idle .
Most IM clients also allow users to set additional indicators to signal whether they are busy or away from the computer.
Those, however, are often insufficient as they require users to remember to set and reset them .
As noted in  and , knowing whether a person is present, however, does not necessarily provide an indication of whether or not that person is available for communication.
A user who is not present  is indeed not available for communication.
On the other hand, a user engaged in an important task and unavailable for communication will be indicated by an IM client as present .
Since the content or topic of an incoming message is typically unknown to the user before it arrives, users generally have to attend to all messages.
While the tool presented in  increases alerts to some messages based on their content, it does not prevent default alerts from taking place.
As a result, users will sometimes elect to turn their IM client off when they are busy, refusing incoming messages altogether .
As Isaacs et al note, however, most IM conversations held in the workplace are work-
This makes closing the IM client a less desirable strategy.
Similar to the use of Caller ID in phones, a user can typically also see who the sender of the message is before attending to the message.
However, even this brief interruption can, in and of itself, be disruptive .
Results from  and  suggest that, given information about the receiver, senders would be able, and willing, to time their messages to accommodate for the receiver's state.
Incoming instant messages join an ever growing number of interruptions a person is exposed to.
Those include interruptions external to the computer, such as telephone calls or people stopping by to ask a question, as well as interruptions from various computer applications, including alerts of incoming email, calendar notifications, or notifications of new items from RSS feeds.
Unlike face-toface interaction, most computer-generated or computermediated interruptions occur entirely without regard to whether the receiver is ready to accept them.
A number of studies have been performed showing the negative effect of interruptions on people's performance.
Field studies on the effects of interruptions in the workplace observed that, while interruptions can be beneficial to people's work , some perceive them to be such a problem that they will physically move away from their computer or even offices to avoid them .
In the particular case of IM, we observed a number of managers who refused to use IM for fear of being interrupted.
In previous work  we have demonstrated the ability to create statistical models that predicted, with relatively high accuracy, time periods reported by participants as highly non-interruptible.
Availability for inter-personal communication is a concept not easy to define.
Many factors can contribute to a person's availability: their current mental task, the proximity to the next breakpoint, the identity of the conversation partner, established organizational norms and culture, and so on.
Unfortunately, getting at a person's "true" availability is near impossible.
Furthermore, a person's stated availability, how available they claim to be, may not match their demonstrated availability - their actual responsiveness to communication.
For example, a person may be busy and state that they are unavailable for communication, while organizational norms coerce that same person to respond to incoming communication, thus demonstrating availability.
While stated availability is of great interest to us and others, we have decided to focus our initial efforts on predictions of demonstrated availability, more specifically, on the ability to predict responsiveness to incoming communication.
We are hopeful that this work will allow us to further understand the relationship between responsiveness, demonstrated availability, and finally availability for communication overall.
Responding to a voice-prompt  or to a survey on a PDA  or sitting for a long period of time to label past events  can be socially and attentionally costly, and quite time consuming.
Another problem with self-reports is that they reflect individuals' subjective interpretation of what is asked of them, an interpretation that can vary from individual to individual.
In contrast with the work mentioned above , the work presented in this paper describes the creation of predictive statistical models trained using naturally occurring human behavior.
One added benefit of using naturally occurring behavior as the source for learning is that a model deployed as part of a system would be able to continuously observe user behavior to train and improve its performance without requiring any intervention from the user.
These considerations led to the design of the data collection mechanism described in the next section.
In the remainder of this paper we describe the data collection method we used and give an overview of the data collected.
We then go on to describe in detail the predictive models that we constructed, followed by discussion of the work presented, its limitations, its implications for practice, and conclude with our plans for further research.
In order to create a predictive model using machine learning techniques referred to as supervised learning, one must first gather data along with labels that represent ground truth about the data.
For example, a set of email messages along with labels provided by a user, indicating messages as either `spam' or `legitimate', can be used to train a model to identify spam email messages.
Previous related work, including , collected naturally occurring behavior as data, using participants' self reports as labels of ground truth.
Other work, such as  used the behavior of subjects participating in a lab experiment to create their predictive models.
The work presented in  and  , for example, gathered its labeled data by asking participants, at different intervals, to provide self-reports of their interruptibility on a scale of 1-5.
Horvitz et al asked participants to observe video recordings of their day and assign a monetary value to a hypothetical interruption , and Nagel et al had participants fill out a short survey on a PDA at random intervals .
Our data were collected using a background process implemented as a custom plug-in module for Trillian Pro, a commercial IM client developed by Cerulean Studios , and running on the Windows operating system.
We chose to use Trillian Pro as it supports the development of dedicated plug-ins through a Software Development Kit  giving access to most of the client's functionality.
Like a number of other IM clients, Trillian allows a user to connect to any of the major IM services  from within one application.
Trillian Pro is further capable of communication with other IM services, including Jabber and Lotus Sametime  .
Using Trillian Pro thus allowed us to recruit participants without concern for the specific IM service they were using.
In fact, 8 of the 16 participants used two or more IM services during their participation, and using Trillian Pro allowed us to observe their interactions over all channels.
Another important reason in our decision to use a commercial client such as Trillian Pro, rather than develop a client on our own, was that it provided functionality beyond the simple exchange of text messages.
For example, it allows file sharing, audio and video chats, sending images, etc.
This reduced the likelihood of participants using other IM clients, which support these features, during the course of their participation in our study.
Our plugin is written in C and implemented as a Dynamically-Linked-Library  that is run from inside Trillian Pro.
The plugin automatically starts and stops whenever Trillian Pro is started or stopped by the participant.
The following events are recorded: IM events: * Message sent or received * Trillian start or stop * Message window open or close * Starting to type a message * Status changes 
These log files were compressed by the plugin "on-the-fly", encrypted, and stored locally on participants' machines.
Participants were required to use Trillian Pro for all their IM interactions for a period of at least four weeks.
The compressed log files were collected from participants' computers at the end of their participation and instructions were given to them for removing the plugin.
Finally, for determining that two events were associated with the same buddy we used an MD5 cryptographic hash of the buddy name instead of the buddy name itself.
Data was recorded from 16 participants in two phases.
The first phase, which started in May 2005, included eight participants, all Masters students at our department.
During their participation, each of these participants was engaged in a number of group projects as part of their studies.
Six of these participants ran the recording software on their personal laptops.
One participant, who used a laptop at school and a desktop computer at home, ran the recording software on both machines.
The eighth participant ran the recording software on his account on a shared desktop computer in the Masters students' lab.
The remainder of this paper will refer to this group of participants as the "Students" group.
In the second phase, which started in July 2005, we collected data from eight employees of a large industrial research laboratory who used IM in the course of their everyday work.
One group consisted of three first-line managers and three full-time researchers.
We will refer to these six participants as the "Researchers" group.
The second group consisted of two temporary summer interns at the laboratory.
Since these last two participants not only worked at the research lab but were also graduate students, we suspected that the patterns of IM use they display will lie somewhere in between that of the Students and that of the Researchers.
We refer to the last two participants as the "Interns" group.
All participants in phase 2 ran the recording software on their work laptops.
For confidentiality reasons, we did not record the text of messages from any of the participants in the "Researchers" or "Interns" groups.
All of our participants except one were new to Trillian Pro but were able to automatically import the list of all their buddies into Trillian Pro.
None of the participants had any difficulty making the transition to using Trillian Pro , although some assistance was required with customization of specific options to match the preferences that individual users were accustomed to.
All participants ran the recording software for a period of at least 4 weeks.
2 of the participants voluntarily continued their participation for a total of approximately 3 months.
We have taken a number of measures to preserve, as much as possible, the privacy of participants and their buddies.
Unless we received specific permission from the participant, the text of messages was not recorded and messages were masked in the following fashion: Each alpha character was substituted with the character `A' and every digit was substituted with the character `D'.
Alerts notifying buddies of the participation in the study were sent to each buddy the first time that our participant opened a message window to that buddy and the buddy was online.
Using Trillian Pro as the client on which we based our data collection resulted in the successful recording of a very high volume of IM events.
Table 1 provides a summary of data collected in both phases.
We collected a total of approximately 5200 hours of recorded data, observing over 90,000 incoming and outgoing instant messages.
Two of the participants in the Researchers group recorded significantly fewer messages in their logs .
However, we did not remove their data from our models and analyses.
To accommodate the fact that data were recorded only when Trillian was running, we provide separate fields in Table 2 indicating the amount of time recorded, as well as the total participation time .
Since participants in the second phase only recorded activity during business days, their participation time is multiplied by 5/7.
The number of recorded hours per day did not vary significantly between groups .
Participants in the Students and Interns groups exchanged an astonishing average of 19.25 and 19.54 messages per hour recorded respectively.
In other words, when Trillian was running, they exchanged, on average, a single message almost every 3 minutes!
By comparison, the Researchers exchanged an average of 7.42 messages per hour, or a single message every 8 minutes.
There was no significant difference between the Interns and the Students groups .
Figure 2 shows the delay between 500 consecutive messages between one of our participants and one of their buddies.
This pattern is similar to the pattern of email exchanges discussed by Barabasi in .
In our data set, 92% of messages are responded to within 5 minutes .
This means that a system that always predicts that a user will respond to any incoming message within 5 minutes will be correct 92% of the time.
However, the majority of messages occur as part of a rapid exchange of messages - what we will call an IM session.
Once a session has been established, responsiveness is likely to be high and can be explicitly negotiated between parties if needed .
Consequently, predicting responsiveness to an incoming instant message is interesting primarily for messages that can be defined as initiating a new session, rather than those inside a session proper.
We define an IM session to be a set of instant messages that are exchanged within a certain time delay between one another.
Unlike a conversation, a session is not determined by the content of its messages.
Indeed, a single conversation may extend over multiple sessions, while a particular session may contain many conversations.
We also did not use the closing of a message window to segment sessions since different IM users exhibit different patterns of closing message windows .
We identify an incoming message from a buddy as a "Session Initiation Attempt"  if the time that has passed since the participant sent a message to that same buddy is greater than some threshold.
In the work presented in this paper we used two thresholds: a 5-minutes threshold , similar to the threshold used by Isaacs et al , and a more conservative 10-minutes threshold .
Note that any message identified as a SIA-10 is necessarily also identified as a SIA-5.
72% of messages in SIA-5 and 71% of messages in SIA-10 were responded to within 5 minutes, compared to 92% of the full set of messages.
The median response time for messages in SIA-5 and SIA-10 was 37 seconds, compared to the median of 15 seconds for the full data set.
Our base measure of responsiveness, "Seconds until Response", was computed, for every incoming message from a buddy, by noting the time it took until a message was sent to the same buddy.
A histogram of "Seconds until Response" for incoming SIA-5 messages is presented in Figure 3.
From this base measure we then created five binary classification labels by indicating, for every message, whether or not it was responded to within each of the following five time periods: 30 seconds, 1, 2, 5, and 10 minutes.
We were now ready to train models to predict each of these binary classifications using the generated features.
Before beginning to create the model we processed the raw user-data to produce, for every incoming or outgoing message, a set of 82 features describing IM and desktop states and a set of classes that the models should learn.
Table 2a shows a partial list of the IM features associated with every message.
We adapted our desktop features from features used in  and .
Those include the amount of user activity and the most-used application, in the 0.5, 1, 2, 5, and 10 minutes time intervals that precede the message arrival time.
We associated applications with a general set of application types .
Table 2b shows a partial list of the desktop features associated with every message.
Day of week Hour Is the Message-Window open Buddy status  Buddy status duration Time since msg to buddy Time since msg from another buddy Any msg with others in last 5 mins log Is an SIA-5 App.
This section presents the performance of statistical models of responsiveness to instant messaging, more specifically to Session Initiation Attempts over each of the classes described above.
Other classification techniques were also explored but generated models with lower accuracy.
For our decision-tree models we used a wrapper-based feature selection technique .
This technique selects a subset of the available features by incrementally adding features to the model and testing the model performance until no added feature improves the performance of the model.
Each of the models in the process is evaluated using a 10-fold crossvalidation technique.
That is, each model is created over 10 trials, with each trial using 90% of the data to train, and the remaining 10% to test the model's performance.
The overall model accuracy is then presented as the average over these 10 trials.
Finally, a boosting process took place using the AdaBoost algorithm .
The performance of ten models created for both SIA thresholds and predicting responses within 0.5, 1, 2, 5, and 10 minutes, is presented in Table 3  and also presented in Figures 1 and 4.
A comparison of accuracy between models created using the SIA-5 and the SIA-10 data sets revealed no significant differences in accuracy.
Following model generation we examined the features that were automatically selected for the 20 models presented above.
These features represent those providing the most useful and predictive information to the model.
Models built from the full set of features selected on average 12.3 features, while user-centric models selected, on average, 10.4 features .
In order to understand the role that buddy state and identity play in our predictions, we next examine ten predictive models of responsiveness created after removing all buddyrelated features.
We thus term these "user-centric" models.
User-centric models are interesting also as they offer a different solution from a practical standpoint.
Models that use the full feature-set  may predict, at the same time, different levels of responsiveness to different buddies.
In contrast, user-centric models are oblivious to information about the source of the message, and will predict, at any point in time, the same level of responsiveness to all buddies, basing the prediction only on information that is "local" to the user.
A comparison of accuracy between the models presented above and the user-centric models is presented in Table 3.
Figure 4 shows a graphical comparison for models created with the SIA-10 set.
As expected, the user-centric models performed slightly worse than the models using the full feature set, however this difference was not significant.
In fact, in some of the models described earlier, the automated feature-selection process selected no buddy-related features even when they were made available.
Since the combined total of distinct features selected by all models was high , for this discussion we group together features describing similar user activity and application information regardless of the time interval they describe .
We further group features into 3 highlevel categories: buddy-related IM information, user-centric IM information, and desktop information.
The top 10 selected features for both types of models are:
Of features related to IM, the time since the last outgoing message, as well as the duration of the current online-status of the participant appear in both lists.
It is possible that the duration of status was frequently selected by our models as it could indicate a recent change of state.
Finally, we can see that two features describing IM interaction with other buddies were frequently selected for models built from the full set of features for predictors of responsiveness.
Indeed, predictive models of responsiveness can be applied in a number of useful ways.
For example, models can be used to automatically provide different "traditional" onlinestatus indicators to different buddies.
Alternatively, models can be used to increase the salience of incoming messages that may deserve immediate attention  if responsiveness is predicted to be low.
Models could also be used by a system that will show a list of potentially responsive buddies to users who are looking for help or support, while hiding others.
We now discuss a number of issues regarding the practical use of predictive models of responsiveness:
Next we examined the distribution of feature selection by high level category.
When moving from these models to user-centric models, the distribution of selected features shifts to 62.6% desktop features and 37.4% IM features, suggesting that the void left by the removal of buddy-related IM features was filled, for the most part, by user-centric IM features.
As described above, desktop features accounted for over 50% of the features selected by our models.
The desktop features we generated looked at different time intervals .
Figure 5 shows the percentage that features with different time intervals were selected for both full-data models and user-centric models.
It is interesting to observe that desktop-features using longer intervals are selected more frequently, potentially because they provide information that is less susceptible to small changes and noise or because longer trends have more predictive importance.
One of the key benefits of IM is users' ability to respond to messages at a time that is convenient to them .
The insufficient awareness provided by most IM clients is at the source of the problem that we are trying to solve with our models.
However, it is the ambiguity inherent in this insufficient awareness that provides users with `plausible deniability'; that is, it allows them to claim that they did not see a message or even that they were not at their computer.
It is thus important to warn against a naive use of predictions of availability.
Providing prediction of responsiveness to buddies "as-is", would substantially reduce plausible deniability and should be avoided.
Instead, careful consideration of the application and presentation of predictions is required .
In the previous section we have presented statistical models that are able, with high accuracy, to predict responsiveness of IM users.
Specifically, these models are able to predict whether a user is likely to respond to an incoming message within a certain time period.
In all current IM clients, users can see their own onlinestatus.
This allows them to be aware of and control the presence that they expose to others.
Similarly, any system providing automatic predictions of responsiveness to others should reflect this information back to the user.
One danger, of course, is that users will attempt to learn which factors determine the system's predictions.
For example, in a system that uses responsiveness to determine whether to include a user in a set of possible communicators, a user may try to "game" the system in order to always appear as non-responsive.
The system, however, can potentially avoid such a situation by making use of predictions from multiple models.
A greater number of models, and potentially a greater number of features, could reduce the overall effect of any one feature in the prediction.
Finally, allowing users to override the predictions will likely eliminate the need to "game" the system.
However, the use of user-centric models also has implications for practice.
Specifically, a predictive model that takes into account features describing the state and history of a user's interaction with different buddies will, inherently, predict different levels of responsiveness to different buddies.
On the other hand models that use only information about the state of the user are guaranteed to provide the same prediction regardless of the identity of the buddy initiating the session.
This difference should be carefully considered by the system designer when deciding which type of models to use.
One limitation of the models presented is this paper is that they are unaware of the content of messages sent and received.
A large number of messages do not in fact require immediate responses.
Avrahami and Hudson list different levels of responsiveness expected for different types of messages .
A model for predicting responsiveness that does not use the content of messages will use other features to explain the lack of a response, potentially leading to inaccurate predictions.
Predictions of responsiveness without using content may also result in misinterpretations of availability.
An example of a case where mere responsiveness incorrectly reflects availability is that of responses used for deferral.
For example, a user responding quickly with a message saying "can't talk, in a meeting" would demonstrate high responsiveness but low availability.
A model unaware of the content of the message is likely to misinterpret this behavior.
In order for such events to be classified correctly they should, more appropriately, be noted in the training data as "no response".
This, however, would be impossible to detect without the content of the messages .
The work presented, for example, in  described the creation of statistical models that used input from a person's calendar as well as sensors external to the workstation.
Those included a door sensor, sensing whether the door was open or closed, a phone sensor, sensing whether the phone was on or off hook, simple motion detectors, and speech sensors, implemented with microphones installed in the person's office, or the microphone built into participants' laptops.
When designing the data collection for the work presented in this paper we decided not to use sensors external to the desktop.
While we believe that it is reasonable to expect events and activities external to computer usage to be reflected in that usage , we suspect that improvement to our models could potentially be generated from features that use such sensor data.
As the collection of software events is possible on most all computers and is extremely low cost in comparison with other sensors, we plan to investigate the correlation between software generated events and external events.
As we mentioned at the beginning of this paper, we are interested in a better understanding of the concept of availability.
In the future we plan to collect both behavioral data , as well as collect participants' self-reports, in order to understand the relationship between stated and demonstrated availability.
For future improvements to our models, we plan to look at the content of messages provided by four of our participants.
We plan to test the ability to automatically detect the topic of a message.
This will allow us to address the limitations discussed above as well as introduce other content-based features to our models.
Our plans for further exploring the predictions of responsiveness include the creation of models that predict the time until a user responds as a continuous measure.
In this paper we presented models capable of successful predictions for 5 different time periods, however, a system might require a model that can provide finer grain predictions of responsiveness.
As a first step in this direction we plan to use regression models to try and estimate users' response times.
Instant Messaging is an important communication channel increasing opportunities for inter-personal communication between both distributed and co-located people.
The low cost of initiating communication over IM, combined with its currently limited awareness support, results in messages often arriving at times that are inconvenient or distracting for the receiver.
An attempt to start a conversation may then either result in a disruption to the receiver's work, or if the receiver decides to ignore it, may result in the initiator left without a needed piece of information.
In the work presented in this paper we focused our efforts on predictions of demonstrated availability - more specifically, on the ability to predict responsiveness to incoming communication.
We described the collection of a large corpus of IM interaction and the creation of statistical models that successfully predict a person's responsiveness to incoming messages, in particular responsiveness to incoming attempts at initiating a new IM session.
We further investigated the performance differences between models that provide different responsiveness levels for different buddies, versus "user-centric" models that predict the same responsiveness for all buddies.
This means that considerations for the particular use of the models will allow a system designer to choose between these two alternative model types.
Ultimately we are interested in understanding the factors that govern availability .
We believe that the ability to predict the behavioral manifestations of availability, namely responsiveness, advance us in that direction.
ACKNOWLEDGEMENTS We would like to thank Mike Terry, James Fogarty, Darren Gergle, Laura Dabbish, and Jennifer Lai for their help with this work.
We would also like to thank our participants for providing us with this invaluable corpus.
Finally we would like to thank the 61C cafe for providing us with a great environment for writing.
This material is based upon work supported by the Defense Advanced Research Projects Agency  under Contract No.
Behavior & Information Technology .
Avrahami D. & Hudson S. E. QnA: Augmenting an instant messaging client to balance user responsiveness and performance.
The origin of bursts and heavy tails in human dynamics.
Begole J., Matsakis N. E. & Tang J. C. Lilsys: Sensing unavailability.
Begole, J., Tang, J.C., Smith, R.E., and Yankelovich, N. Work rhythms: Analyzing visualizations of awareness histories of distributed groups.
Cerulean Studios - Trillian Pro http://www.trillian.cc 7.
Cutrell, E., Czerwinski, M. and Horvitz, E. Notification, Disruption, and Memory: Effects of Messaging Interruptions on Memory and Performance.
Dabbish L. and Kraut R. Controlling interruptions: Awareness displays and social motivation for coordination.
Examining the robustness of sensor-based statistical models of human interruptibility.
Fogarty J., Ko A. J., Aung H. H., Golden E., Tang K. P. & Hudson S. E. Examining task engagement in sensorbased statistical models of human interruptibility.
