We investigate how to reveal an initial mid-air gesture on interactive public displays.
This initial gesture can serve as gesture registration for advanced operations.
We propose three strategies to reveal the initial gesture: spatial division, temporal division, and integration.
Spatial division permanently shows the gesture on a dedicated screen area.
Temporal division interrupts the application to reveal the gesture.
Integration embeds gesture hints directly in the application.
We also propose a novel initial gesture called Teapot to illustrate our strategies.
Our main findings from a laboratory and field study are: A large percentage of all users execute the gesture, especially with spatial division .
Users intuitively discover a gesture vocabulary by exploring variations of the Teapot gesture by themselves, as well as by imitating and extending other users' variations.
Touch-based interaction is the common modality for public displays.
However, distant interaction through mid-air gestures has several advantages for public display interaction.
Several interaction techniques  have been proposed to guide the execution of gestures in the context of penbased or touch interfaces.
Usually executing a gesture requires to initiate the gesture by pressing a button or touching an interactive surface.
Triggering a help system or menu usually requires to touch or press and wait for one second .
For mid-air gestures on public displays, a registration or initial gesture to define the beginning of advanced interaction is not yet established.
Due to the novelty of the interaction technique it is unclear to users how to initiate the interaction.
The context of public displays introduces additional challenges for revealing the initial gesture, especially for midair gestures: First, many users approach the device for the first time.They are unaware that the system can capture midair gestures, which gestures are available and how to execute them.
Second, users interact with the system for a short time  and thus the system only has a couple of seconds/minutes for communicating the initial gesture.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In this paper we investigate the question of how to reveal an initial mid-air gesture on public displays to enable advanced interactions such as navigating through a menu.
We propose three strategies to reveal gestures: spatial division, temporal division and integration.
Spatial division permanently shows the gesture on a dedicated screen area .
For temporal division the running application is interrupted to reveal the gesture in full screen .
For integration hints are directly embedded into the application, similar to product placement techniques in movies .
We also propose the Teapot gesture, a novel initial gesture for mid-air gestural interaction with public displays.
Users touch their hip to enclose an inner area with their body and their arm in their contour image .
We show that the Teapot gesture is easy to recognize by the system, comfortable for the user, socially acceptable, and easy to understand.
We use the Teapot gesture to illustrate our gesture revelation strategies in a laboratory and a field study.
The laboratory study shows that  users do not notice the hint with spatial division.
From these observations we derived improved versions of the most promising technique for each strategy to compare them against each other in a field study.
The main findings of our field study are:  A large percentage of all users execute the gesture, especially with spatial division .
This is a surprisingly high number for an inthe-wild study, especially since users are free to do what they want, are not instructed by experimenters, and the game alone is already fun to play.
This provides us with a user-defined gesture set.
Only a few public displays support mid-air gesture interaction.
While  investigate direct manipulation through ergotic gestures,  and  investigate the use of symbolic gestures for the execution of commands.
Still, no field studies have investigated the revelation of symbolic mid-air gestures in the field.
The closest to our work is , who investigates touch gestures for a public multi-touch table in a field study.
They find that gestures are integrated into a continuous flow of gestures and the choice of gesture is influenced by previous gestures and social context.
However, these results can not be transfered to mid-air gestures because prolonged interaction with an interactive table differs from playful interaction with a vertical display.
Mid-air gestures in front of public displays can also be described as performative interaction .
This concept proposes that users are simultaneously in three different relationships:  the interaction with the public display,  the perception of themselves within the the situation and  acting about a role for others to observe .
Important concepts for performative interaction are manipulations and effects  because they impact social learning and the honeypot effect .
Manipulations refer to the performer's gestures while effects refer to the visible result of the gestures on the display.
Gestures can be described in three phases :  registration that clearly marks the beginning of the gesture,  continuation which is the dynamic part and  termination that marks the end of the gesture.
In the case of a touch screen, these phases could be  touch the screen,  swipe finger and  release finger.
Especially for mid-air gestures, the registration and termination phases appear less obvious, since there is no explicit delimiter that marks the beginning and the end of a gesture.
For example a user could say "put that ..." while pointing at an object, then point at another location saying "...
However some modalities may be unavailable or inappropriate on interactive public displays.
Moreover, discovering additional modalities itself introduces new problems.
A possible clutching mechanisms for mid-air gestures may be a virtual and invisible curtain that the user's hand needs to penetrate in order to initiate the gesture tracking.
Still it is not clear how this surface should be shaped and positioned.
If it is too close it may generate false positiveand if it is too far away it is prone to false negative detection.
In this paper we refer to the definition of gestures of Kurtenbach and Hulteen : "A gesture is a motion of the body that contains information".
Several classifications or taxonomies  have been proposed to categorize gestures.
For instance Cadoz  proposes three types of gestures depending on their function: Semiotic , ergotic , and epistemic .
While ergotic gestures are usually used for direct manipulation of virtual objects, semiotic gestures are used for the execution of commands.
Semiotic gestures can be further subdivided into symbolic, deictic, iconic, and pantomimic gestures.
Second, these systems have been designed for a context where users want to achieve a goal.
In this scenario users are aware of available commands and explore the system for them.
In contrast, users of public displays often do not have a specific goal .
The interaction is spontaneous and initiated by curiosity or playfulness.
Third, for GestureBar and ShadowGuides users were already instructed that they should operate a gestural interface.
They were aware of "the concept of gestural commands and how to use them" .
In contrast, passers-by are generally not aware that public displays are interactive, how to interact with them and whether gestural interaction is supported .
In consequence, passers-by should understand that gesturebased interaction is possible and how gestures are invoked, both in a very short time as passing-by interaction is generally quite short  .
Crib-sheets: Another alternative is the use of crib-sheets .
Most of them are displayed on demand by pressing a help button.
In Tivoli , users press and hold to get information about commands and gestures.
But this technique is not compatible with immediate usability of public displays.
Another strategy may be to always display the crib-sheet on the screen.
For traditional platforms, permanent crib-sheets are often criticized because they require a lot of space, especially for large gesture sets.
The spatial division techniques presented in this paper are similar to permanent crib-sheets.
A major difference to our approach is that not all the available gestures are shown, but only one: the initial gesture.
This single gesture would serve as a registration for advanced gestures, to access a larger set of gestures, or perform other interactions.
We believe that presenting several gestures will confuse or overload users by displaying too much information simultaneously.
Finally, while different kinds of labels have been used in crib-sheets , they have not been evaluated or compared in the context of distant interaction with public displays.
Kurtenbach  introduced the concepts of self-revelation, guidance and rehearsal for gestures.
Several techniques have been proposed for guidance or rehearsal in the context of penbased or touch interaction.
Only LightGuide  has been proposed in the context of mid-air gestures by projecting guidance hints directly onto the user's hands.
In contrast, very few techniques have been proposed for revelation , although it is an essential issue for all gesturebased systems, especially in public space.
We now detail three approaches to reveal gestures on touch surfaces: guessability, interaction techniques, and crib-sheets.
We discuss their adequacy for mid-air gestures on public displays.
Guessability: The design of guessable gestures  appears not very promising for public displays because generally users are not aware which commands are available.
However this is one major prerequisite for guessability.
Besides users of public displays usually do not have a specific goal or a command to execute in mind.
Interaction Techniques for Revelation: To the best of our knowledge, only three techniques focus on revelation of gestures in the context of mouse  and touch  interfaces.
Firstly, GestureBar  is a technique for integrating gestures into conventional WIMP interfaces.
It uses an advanced toolbar which, instead of executing the command when the corresponding icon is clicked, displays a video of how to execute the command via a mouse gesture.
Secondly, Hofmeester recently investigated the revelation of a single gesture in the context of Tablet PCs .
In the project a slide to select gesture to launch applications on Windows 8  is taught to the user.
A tutorial is not used to avoid impairing the user experience.
The authors found that visual cues that raise curiosity are an important factor to improve the discoverability of gestures.
Finally, ShadowGuides  displays various hand poses for gesture registration, once users have touched the display and dwelled for one second.
ShadowGuides also guides the gesture continuation after the user has executed the registration gesture.
As these projects , we aim at improving the discoverability of gestures.
However, our approach differs in several aspects as we focus on public displays.
First, these systems assume that users already know how to interact in a first modality .
This prior knowledge about the first modality is then used to reveal gestural interaction as a second modality.
Inspired by , we designed a simple but engaging game based on physics simulation to motivate passers-by to interact.
Passers-by see their mirror image on the screen and can use it to play with virtual cubes .
Users can toss them into a specific target to collect points.
They can also perform an initial gesture to enable an advanced operation.
While the focus is only on revealing this initial gesture and in order to keep the experiments as simple as possible, the advanced operation only consists of adding a funny bunny mask  or doctoral hat  to the users' contour.
We propose the Teapot gesture as a novel initial gesture for mid-air gestural interaction on public displays.
The gesture can be described as a full-body version of the pinch gesture , where users touch their hip with their arm to enclose a distinct inner area in their contour image .
The Teapot gesture overcomes two limitations of the pinch gesture in the context of public displays:  The inner area is large enough to be easily detected by the system, even if users are positioned a couple of meters away from the screen.
The Teapot gesture is also well suited as a gesture registration, because it can clearly indicate the beginning and the end of a gesture or interaction.
The first implementation of the recognizer was based on the skeletal tracking capabilities of the OpenNI / NiTE framework.
Though thick clothes might cause reliability issues with the contour-based recognition, it turns out to be more robust than the skeleton-based recognizer.
As a side effect, any gesture besides the Teapot gesture, that generates such an inner area would trigger the recognizer.
In the field study described below we observed, that people naturally tend to explore this set of possible gesture variations.
A Pilot study also shows that the Teapot gestures is well accepted by users .
Integration Cues: We propose three examples of cues that can be used for the integration strategy.
The Hip Button is supposed to afford users to "touch their hip" .
Instead of mirroring the user's movement, the mirror image would perform the gesture .
For the Fake User an actor has been previously videorecorded passing by the display, stopping, and executing the initial gesture.
The Fake User enters every 30 seconds for four seconds.
As people tend to imitate behavior of other people, we expect that the real user would imitate the gesture of the Fake User.
To test a reasonable number of conditions in the laboratory study, we conducted a second pilot study to determine the two most promising techniques.
Results show that the Fake User performed relatively poorly.
While 8/10 users noticed the Fake User, no one actually imitated the presented gesture.
Users reported that "The bunny guy comes in to distract me from the game!"
Apparently users paid most attention to their own mirror image and have only perceived the effect but not the manipulation of the gesture from the Fake User.
Based on the results of this pre-study, we decided to retain the Voodoo User and the Hip Button for a laboratory study.
We propose three strategies inspired by advertisement placement to reveal an initial gesture: * Spatial division: The screen is split into two areas: the game and a ribbon below explaining the gesture.
This strategy is for instance implemented as banner ads on Youtube videos.
This strategy is similar to classical television ads.
This strategy is similar to product placement, where certain products  are placed in a movie.
These three strategies suggest when and where to reveal the initial gesture.
However, it is not yet defined how to explain it.
Labels: Text can provide precise descriptions but its intelligibility depends on the user's language skills.
Icons do not have limitations related to readability but can be ambiguous or insufficient for complex or dynamic gestures.
Videos are ideal for dynamic gestures but users have to observe and memorize the entire sequence, which may require too much time and cognitive load.
Videos can also highlight the link between the manipulation  and the effect of this gesture.
Iconand video labels can also be combined with text.
To reduce the number of conditions to be evaluated in laboratory and field studies, we ran a pilot study to determine the three most promising cues among the five label variations: text; icon; video; text+icon; text+video.
Results show that the conditions that include text were more effective in triggering users to execute the gesture than those without text.
Label: Three different labels  were used for the temporal and spatial division strategies.
A text label "Touch your hip to get a bunny mask" explains both the manipulation  and the effect .
An iconic label shows the static pose, the highlighted inner area of the Teapot gesture, and the bunny mask.
A video shows an actor performing the gesture and receiving the bunny mask.
To avoid that users confuse the actor in the video with themselves, the actor's contour was rendered in a different color and position.
For the temporal strategy the label was displayed in the center of the screen.
It was presented every 30 seconds for four seconds.
Four seconds were sufficient to present a video of a user executing the gesture and show the effect, but short enough not to make users wait too long before being able to continue playing the game.
For the spatial strategy, the label was shown permanently in the lower part of the screen.
Apparatus and Participants: The system was installed in a room in close proximity to the main entrance of a university building.
We randomly invited passers-by in the entrance to participate in a five-minute experiment.
They received candies for their participation.
For the entire time of the interaction the system logged the raw depth video, a screen capture, and various events to a text file.
Instructions and Task: After introducing the interactive public display to the participants, we asked them to play with it as they would do in a public place.
Participants did not receive any further instructions.
In particular they were not instructed about the gestural interaction, the initial gesture, the principle of the game, or the bunny mask.
The experiment was aborted as soon as the participant successfully performed the initial gesture  to trigger the effect, or after a maximum time of two minutes, an approximation of the maximal usage time for casual interaction in the field .
After the test we interviewed all of the 166 participants of the laboratory study individually for approximately 3 minutes.
The most important interview results are summarized in figure 3.
In addition to the mentioned questions, we gathered general data like age, gender, and occupational background from participants.
Design: We used a between subjects design as we were particularly interested in first-time users.
The conversion rate is defined as the percentage of users that execute the gesture.
It was derived from the results of question Q2 of the interview.
Techniques: A Kruskal-Wallis test reveals no effect for spatial- and temporal division labels on the conversion rate.
However, it reveals a significant effect between the two integration techniques .
Voodoo User  triggers significantly more gestures than Hip Button .
For the Hip Button technique, we observed that a few users did not perform the gesture correctly.
They recognized the button, understood that they had to push it but did not do it in the intended way: Instead of touching their hip by pushing the button from the side, they decided to hit it in front of their body.
Ultimately, the Hip Button triggered more actions than the Voodoo Users, but in contrast to the Hip Button, the Voodoo User appears to be less ambiguous.
Strategies: A Kruskal-Wallis test reveals an effect for strategies on the conversion rate .
Temporal division  triggers significantly more gestures than spatial division  which triggers significantly more gestures than integration .
The comprehensibility rate is defined as the percentage of users that understood the manipulation of the technique.
It was derived from the results of question Q4 of the interview.
Techniques: A Kruskal-Wallis test reveals neither an effect for spatial and temporal divisions labels on the comprehensibility rate nor for the integration techniques.
Strategies: A Kruskal-Wallis test reveals an effect for strategies on the comprehensibility rate .
Users understood temporal division  significantly more often than spatial division  and integration .
For spatial division, people are distracted from the game and thus may notice but disregard the hint.
This was also reported by some participants in the interview.
Techniques: A Kruskal-Wallis test reveals neither an effect for the spatial and temporal division strategies on the noticeability rate nor for the integration techniques.
Strategies: A Kruskal-Wallis test reveals an effect for strategies on the noticeability rate .
The hint is noticed significantly more often for temporal division  and integration  than for spatial division .
Finally, we observed that 96% of the participants that understand the manipulation would actually perform the gesture.
This is independent of whether or not people did also understand the effect of the gesture.
In summary, results show that  The temporal division strategy  triggers a high conversion rate.
In order to evaluate the gesture revelation techniques in an ecologically valid setting, we conducted a field study.
We deployed StrikeAPose for five working days in the entrance hall of a university cafeteria.
The screen was oriented sideways along the main walking path.
Conditions: We tested the three techniques derived from the laboratory study .
These conditions were counterbalanced and automatically switched every 10 minutes to minimize the influence of time of day on the results.
To avoid interruptions of user interactions, the switch was delayed until no users were detected or after 15 minutes at the latest.
All hints were shown  or highlighted  every 30 seconds for 4 seconds.
System: We used the same hardware and software as in the lab study, but we updated the game assets to reflect the deployment location .
Data Analysis: We collected both qualitative and quantitative data.
As quantitative data, we recorded a screen capture as well as the raw depth video from the sensor for the entire time of the deployment.
Qualitative data was gathered from observations, interviews and from analyzing manual video recordings.
We interviewed a total number of 46 users in 20 semistructured group interviews.
They were randomly picked, regardless of whether or not they executed the Teapot gesture.
However, quantitative results of the field study were only derived from the video annotations.
Besides that, we collected general comments and opinions of users on the system.
Interviews were usually shorter than five minutes and were rewarded with candies.
Observations were conducted from an inconspicuous location on a nearby bench without interfering with the interaction.
The depth videos were manually annotated for gesture execution , gesture variations, disengagement, and temporal relation to the hint.
This user study highlighted weaknesses for each of the strategies: For the integration techniques, almost all users notice the cue, but it was too subtle to be understood.
In contrast, a lot of participants did not notice the hint for spatial division.
Finally, for the temporal division strategy, we observed that users noticed the hint and executed the gesture.
However, as they executed the gesture mostly during the inserts, it did not show any effect.
Label: As we did not observe differences between labels, we decided to use the text+icon label for the temporal and spatial division, because it shows both accurate textual description as well as a language independent iconic description of the gesture.
Highlighting Spatial Division: Since the hint was sometimes unnoticed by users for the spatial division, we decided to highlight the hint occasionally using looming stimuli : the cue jumps repeatedly towards the user to capture attention.
This highlight appears every 30 seconds for 4 seconds.
Feedback for Temporal Division: To allow users to observe the effect of the gesture during the hint in the temporal division, we decided to fade the mirror image while the hint was blended in .
Comprehensibility for Integration: We built on the Hip Button of the affordance technique as it is noticed better than the Voodoo User.
But as this subtle cue was not understood by the users, we decided to make the hint more explicit.
We propose to attach a Speech Bubble to the mirror image of the user as shown in Figure 1c.
The Speech Bubble uses the same text+icon label as proposed for temporal and spatial division and appears every 30 seconds for four seconds.
Communicating Manipulation Only: For all techniques, we decided not to communicate the effect of the gesture but only the manipulation as it did not seem to effect whether users executed the gesture.
During the five days of deployment 558 individual users interacted with the screen while 274 of them performed the gesture at least once.
The conversion rate for temporal division is 47%.
Timing: We annotated when the gesture was performed within five seconds after the hint appeared or was highlighted.
Assuming that people would perform the gesture randomly, this value would be 5 / 30 = 16.7%.
We observed that integration  and temporal division  generates a high probability that users perform the gesture during the appearance or highlighting of the cue.
Throughout all three conditions people interacted with the screen for about 41 seconds in average .
Disengagement: The hint appears or is highlighted every 30 seconds.
Assuming the hypothesis that the hint would not trigger disengagement, the expected random disengagement rate would be only 16.7%.
However, we observed that 27.4% of the users of the temporal division leave within five seconds after the hint appears while 13.8% of the users do so for spatial division and 18.8% for integration.
Discussion: Overall, with 56% for spatial division, a surprisingly large percentage of all users executed the gesture.
This shows that gesture revelation works very well for public displays.
It seems that the lack of attention observed in the laboratory study was resolved by the periodic highlighting.
Temporal division also performs well  but seems to make a large percentage of users leave while the cue is shown.
This relates to the finding of Huang , who observed that when people look at a film on a public display, they will mostly leave when the film ends or there is an interruption.
Design Recommendations: In order to communicate an initial gesture on a public display, it is recommendable to use a gesture revelation strategy like spatial division.
A large percentage of users can be expected to execute the gesture.
All revelation strategies work well, but have different benefits and drawbacks.
One should not assume that users will casually play with a screen before executing a symbolic gesture.
Almost one quarter of all users executed the gesture before actually start playing.
There is also a growing body of evidence that interruptions on public displays make people leave.
Thus, interruptions should be avoided or used very carefully.
Others executed the Teapot gesture with one hand  and then tried to lift their virtual doctoral hat with the other hand or to continue to play the game .
If they executed the Teapot gesture with both arms or held an object in the other hand, they became more creative.
They continued playing using their head, their shoulders, and their legs.
Some users even punched each other in the face in the mirror image  while executing the gesture .
Users also engaged in very expressive behavior.
For example they wildly swung their hip, posed as on a catwalk, or performed skillful dances while performing the Teapot gesture .
Users sometimes discover the Teapot gesture inadvertently through a flow of gestures.
For example, one user performed the gesture while putting a cigarette behind his ear.
Another user pulled a wallet from his back pocket, inadvertently executing the gesture.
Discussion: The fact that gestures are not performed in isolation, but are rather linked into interwoven sequences, was also observed by Hinrichs and Carpendale  for a multitouch table.
They describe that users perform different gestures depending on their previous motion for physical ease  but also social functions.
This interweaving of gestures is much more pronounced for midair gestures.
For multi-touch users, there is a separation between touch gestures for manipulating the screen and mid-air gestures for communicating and manipulating the world.
For mid-air gestures, this separation almost disappears, and gestures of various kinds for various purposes melt into a continuous flow, with the same gesture often fulfilling multiple purposes .
Design recommendations: The initial gesture will be woven into a continuous flow of gestures, and this needs to be supported.
For example, the Kinect Guide Gesture requires users to stand still while stretching their arm at 45 for about two seconds.
Users are not allowed to move their other arm or legs during this time.
This would be counter-productive in public settings, as it interrupts the natural flow of gestures.
Similarly, forcing users to use a particular hand, as it is done for the Kinect, would be unsuitable for public situations.
A large proportion of users carry objects, like coffee, bags, or jackets in one hand, so that they would prefer to use the other hand for executing the gesture.
Some users may execute gestures inadvertently, which may be beneficial for gesture discovery.
However, this did not occur very often in our case and a separate gesture revelation technique may still be necessary.
As it was the case for the Teapot gesture, the gesture should be easy to avoid if desired.
Finally, because gestures are interwoven into a gesture flow, it should be easy to recognize beginning and end of gesture for system and user .
Users did not perform the Teapot gesture in isolation.
They were engaged in a constant flow of gestures, be it symbolic, deictic, iconic, pantomimic, or ergotic gestures.
For example, when approaching the screen, some users did a symbolic waving gesture towards the screen.
Others pulled their friends by the arm to make them abort interaction.
They were also pointing at the screen  while talking to others.
One user iconified the depth sensor with both hands while talking to the friends, and another user pantomimed the walking style of his friend with crutches.
The biggest category of gestures, however, were ergotic gestures, movements to manipulate the environment.
Users play with cubes on the screen, push their friend, or grab objects.
A variety of gestures were also executed simultaneously with the Teapot gesture, using every part of the body.
Interestingly, the Teapot gesture supports these two levels of variations.
At a low level, users are not forced to precisely touch their hip .
At a high level, it raises curiosity to explore a potential gesture vocabulary, an important factor to improve the discoverability of other gestures .
Once users discovered the Teapot gesture, they performed many variations of it.
These variations include the modification of the location or the size of the inner area as shown in Figure 5.
Location was the most frequent modification, and includes switching arm  or using both arms .
User also touch their head to define the inner contour image area with the left , right , or both  arms.
Finally, some users also used their legs to span an area.
Users also explored different sizes for the area.
Minimal size included pinch with one hand  while maximal size included the formation of a circle with the arms in mid-air above the head , on the left  or on the right side  as shown in Figure 5.
Users also explored size by coming closer or using various objects .
Additionally, multi user gestures as shown on Figure 1 or 5 were explored.
This may be either by active collaboration where people try to define very large areas by holding their hands together or passively by using the body of another user as a border for the enclosed area.
Some users freely interpreted the instruction "touch your hip" by touching the hip of their friends .
Discussion: While not actively encouraged users, users explored variations of the Teapot gesture.
They not only try to execute the gesture, but try to discover other gestures and additionally identify the limits of the system.
We believe that several users consider the Teapot gesture as part of a gesture vocabulary.
Indeed users rarely tried arbitrary gestures but restrict their exploration to some variations of the Teapot .
This observation supports our main hypothesis: revealing the initial gesture is necessary to discover it but it is also sufficient for several users to discover advanced gestures.
These novel gesture variations form a whole that can for instance be used to organize commands in a gesture-based menu.
Variations of gestures have been observed in the context of multi-touch surfaces .
However, it is interesting to notice that these variations occur at two different levels of abstraction.
In  authors observe that users adopt different strategies to perform the same gesture .
Authors conclude that designers should provide a variety of ways to perform a gesture.
In that study users do not intentionally explore these variations.
In contrast, our observed variations occur at a different level of abstraction and seek to to determine a gesture vocabulary: It is not a variation of the way to execute the gesture, but a variation of the gesture itself leading to different gestures.
Design recommendations: Public displays can rely on users exploring the gesture vocabulary if the initial gesture has been chosen carefully.
To do so, designers should design an initial gesture which is part of a gesture vocabulary .
The initial gesture should also raise curiosity to support the discoverability of the other gestures.
The Teapot gesture is one example of an initial gesture supporting these two properties.
We were surprised by the number of imitations occurring between users, mostly within groups, but also between groups .
As soon as one user in a group performed the gesture, there was a high probability that within a few seconds others would perform the gesture, too.
We distinguish direct and indirect imitation.
The first imitation occurs when people directly observe a user performing the gesture.
In contrast, some people who played together in the same group rarely looked at each other but at the screen.
Therefore, they probably indirectly copy the gestures from the mirror images on the screen.
Finally, spectators in the environment seemed to position themselves so that they can see both the users and the screen, although sometimes the screen was occluded.
They seemed to copy gestures more from directly observing the bodies of other users.
Users did not only copy the Teapot gesture, but also variations of it as well as other gestures.
For example, in Figure 7a, one user discovers that he can execute the gesture by touching his head and this is copied by another user .
Interestingly, users did not only try to simply imitate gesture variations.
Instead, when they saw someone performing a variation of the gesture, they tried to build on this and find different, more interesting gesture variations.
This sometimes led to a kind of competition, where each creative variation of the Teapot gesture by one user was answered by a more unconventional variation by another user.
Discussion:  reports on two cases of imitative behavior .
In these cases, the observation of the manipulation and the effect  is merged.
In contrast, in our case there is a difference between users in the same group, imitating from seeing the effect , and in other groups, rather imitating from seeing the manipulation.
In our case, imitation behavior seems to be an important factor for the revelation of the gesture, in particular when someone in the same group has already performed the gesture.
Groups also seemed to quickly explore the gesture vocabulary by building on each others gesture in a competitive game.
Design recommendations: Imitation is an important part of gesture revelation.
In particular, a registration gesture should be easy to recognize not only for the system, but also for bystanders, and easy to imitate.
Because players concentrate on the screen, it is recommendable if the gesture is imitable by only seeing its effect on the screen, for example through a mirror image.
In addition, a large set of interesting gesture variations should be supported, in order to enable groups of users to discover the vocabulary by competitively trying to find more interesting gestures.
In this paper, we investigated how to reveal mid-air gesture on public displays.
In this section, we discuss the generalizability of our results as well as potential limitations.
In this paper, we proposed and used Teapot as an initial gestures.
The focus of this paper is not to find the optimal initial gesture, and we do not claim that the Teapot gesture should be used for all mid-air gesture interfaces on public displays.
However, our studies reveal that the Teapot gesture has several advantages: It is easy to recognize for the system , apparently comfortable and socially acceptable, easy to understand even with very short description, encourages exploration of the gesture vocabulary, is easy to imitate, and can be held while performing other gestures simultaneously .
Further investigations are advised to compare Teapot to other gestures .
We tested several revelation strategies both in laboratory and field studies.
Several questions remain: First, do these techniques provide similar results if applied to other initial gestures?
We believe that the temporal and spatial division strategies are quite independent of the chosen gesture and interaction paradigm .
In contrast, we believe that the integration strategy can provide different results depending on the adequacy of the visual cues to the gesture.
Additionally, in a different interaction paradigm , integrated cues would need to look differently.
Second, some decisions have been taken for the timing of strategies.
Currently, all cues are highlighted or shown every 30 seconds for four seconds.
Further investigations are necessary to determine the optimal timing.
Finally, we were surprised by the low performance of our integrated techniques.
These visualization techniques have a lot of parameters 
Our user studies are based on a simple but engaging game to motivate people to interact.
More generally, it would be useful to investigate interaction between the context 
In this paper we investigated mid-air gesture revelation strategies for public displays.
We proposed three strategies that have been shown to be efficient to make users execute the gesture.
For spatial division, 56% of all interacting users executed the gesture, followed by temporal division  and integration .
Spatial division is very effective, does not interrupt the content, and does not cause people to leave while the cue is shown.
However, it constantly consumes screen space.
Temporal division does not have this problem, but interrupts the content and causes users to leave while the cue is shown.
Integration seems to be less effective, but can show different cues to different users.
They do not only imitate gestures from other users, but try to go beyond other users' gestures in a kind of competition.
Finally, the Teapot seems to be a promising initial gesture.
We hope that our work can provide a foundation for the investigation of initial mid-air gestures on public displays.
