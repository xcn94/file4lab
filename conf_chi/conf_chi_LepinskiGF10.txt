Despite the considerable quantity of research directed towards multitouch technologies, a set of standardized UI components have not been developed.
Menu systems provide a particular challenge, as traditional GUI menus require a level of pointing precision inappropriate for direct finger input.
Marking menus are a promising alternative, but have yet to be investigated or adapted for use within multitouch systems.
In this paper, we first investigate the human capabilities for performing directional chording gestures, to assess the feasibility of multitouch marking menus.
Based on the positive results collected from this study, and in particular, high angular accuracy, we discuss our new multitouch marking menu design, which can increase the number of items in a menu, and eliminate a level of depth.
A second experiment showed that multitouch marking menus perform significantly faster than traditional hierarchal marking menus, reducing acquisition times in both novice and expert usage modalities.
Menus should provide a logical categorization, support efficient access, and save screen real estate for the user's primary work .
While traditional menus are widespread in conventional computing interfaces, they are not necessarily well-suited for multitouch displays.
Because the activation area of a finger is considerably larger than that of a mouse cursor, touch interfaces often drastically increase the minimum size of widgets needed for accurate selection .
For a menu with many items, this may simply require more screen real estate than is available or result in large, visually displeasing menus.
While recent research  has presented methods to refine the gross movement of traditional multitouch into finer movements that may be suitable to select from smaller, traditional menu systems, we believe these methods may not provide the same level of performance as new menu systems tailor-made for a multitouch environment.
Another type of menu systems which may be more appropriate for the direct finger input of multitouch systems is marking menus .
Marking menus save screen real estate, by only popping-up when being used, require directional accuracy instead of positional accuracy, and support gestural activation which is a desirable mode of interaction to support a "natural"  user interface experience.
However, a marking menu design, which takes advantage of multitouch input, has not been investigated.
Marking menus are a well studied menu design outside the multitouch domain, and their performance gives good reason to examine how they may be applied to multitouch.
Multitouch interfaces are characterized as computing interfaces that are touch sensitive, and allow a user to issue multiple touches simultaneously .
Multitouch interfaces have become popular in recent years, both in research and in consumer devices.
Despite the considerable quantity of research directed towards multitouch technologies, a set of standardized UI components have not been developed .
In particular, menu systems specifically designed for multitouch platforms have received little attention.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In this paper, we design and evaluate a new multitouch marking menu system .
Our design is guided by an initial empirical experiment, which collects data on users' ability to perform directional chording gestures.
This initial experiment finds that users can accurately gesture in 8 directions while articulating multifinger chords.
Based on this data, we propose our new multitouch marking menu design, which uses chorded input to quickly indicate the top-level menu category.
In addition, we outline a new recognition technique to disambiguate between the chords.
In a second experiment, we find that our new design outperforms traditional single touch marking menus, when selecting from a set of 64 possible menu items.
This result was found for both novice and expert usage modalities.
One of the appeals of multitouch interaction is its potential to provide a richer set of inputs to interactive systems.
In general, this input is used to manipulate the data itself through "natural"  gestures , but it can also be used for interface controls, such as command activation  and cursor control .
In many cases, command activation in multitouch systems can be achieved through gesture sets .
While gestures can provide a fun interaction experience, recent research has shown that it may be more challenging than once believed to provide "natural" gestures that user's will be able to immediately learn .
Furthermore, while gestures may be practical for a limited set of commands, it would be impractical to develop a library of gestures for applications which possess hundreds of commands.
For such a scenario, it would seem to be more practical to use a menu system.
While menus have been developed and used for multitouch systems , few actually take advantage of the multiple finger input stream which is available.
Wu describes a tool which allows the user to choose a radial menu item with one finger, and then place the item with a second finger .
Brandl describes a menu which activates a menu with one hand, and selects the items with the other .
Neither of these designs attempt improvements upon traditional marking menus.
In our work we integrate multifinger chords into marking menus and provide an empirical comparison to traditional, single point marking menus.
Marking menus  are a gesture-based menu system which displays menu items in a radial layout around the cursor.
The user drags the cursor, or "marks", in the direction of the desired item.
In hierarchal marking menus, a mark can select a particular path through a menu hierarchy.
Marking menus support both novice and expert modes.
In novice mode, after a short dwell time, the menu is displayed and the user can move towards the desired item.
In expert mode, the location of the desired item is already known, and the user can quickly perform the marking gesture without waiting for the menu to be displayed.
Numerous alternative designs have been proposed to improve upon the initial designs, such as the hotbox , multi-stroke menus , zone and polygon menus , and flower menus .
However, we are unaware of work which applies multitouch input to the design of marking menus.
One of the most important aspects of marking menus is determining how many menu items should be placed at each level.
The angular accuracy which the user can achieve will determine appropriate menu breadth.
As the breadth increases, more items are placed at each level, reducing the angle between each item.
Kurtenbach and Buxton  recommended that to maintain acceptable levels of accuracy, the breadth of the menu should be 8 with a depth of 2 levels.
With this design, the menu can contain 64 items.
Zhao and Balakrishnan's multistroke marking menus  allowed for an additional level of the menu, but the menu breadth was still limited to 8.
These results have been found for mouse and pen input .
Numerous technologies have been used for sensing hand and multi-finger input, such as capacitive sensing  and vision-based solutions .
Each of these technologies provides slightly different input streams, making the design of technology-independent interactions challenging.
For example, the SmartSkin system can sense finger positions while they are above the surface .
In contrast, FTIR systems only sense contact points , but has the ability to also sense pressure .
To reduce dependency on a specific hardware platform, our designs do not rely on these additional input streams.
Chorded input involves the simultaneous use of multiple fingers.
Chording input has been most commonly seen in the text-entry literature, where it has been shown to significantly increase typing performance .
Previous gesture sets for multitouch technologies have taken into account the number of contact points as a mechanism to specify gestures, such as using one finger to rotate and two fingers to scale .
Less work has gone into using different combinations of fingers , which can increase the command vocabulary.
A barrier to the use of such chords is the absence of finger identification technology.
We describe a simple vision based solution to allow for accurate sensing of an increased number of chords.
Recognizing the potential power of using different combinations of fingers as a method of command input, we believe that menus could be efficiently operated through what we define as Directional Chording Gestures.
Since chording in text entry occurs on physical devices, the chords cannot be directional, and are only used to press a button .
However, applied to an interactive multitouch surface, chords could also provide directional information .
Applying this principle can provide a large combination of relatively simple gestures.
We apply this idea to develop two types of directional gestures: Simple-Stroke and Lift-and-Stroke gestures.
The chords are initiated by using the various combinations of the 5 fingers on one hand.
In total, there are 31 different combinations: 5 single finger chords , 10 two-finger chords, 10 three-finger chords, 5 four-finger chords, and 1 five-finger chord .
To create our Simple-Stroke Gestures, we combine each of these chords with a direction.
Figure 4 shows the 8 gestures for one of the chords.
This design gives us a large gesture set, without any compound strokes , or iconic shapes .
As a software design solution to this potential limitation, we developed the Lift-and-Stroke gestures.
These gestures are the same as the Simple-Stroke gestures, except the user is first required to depress all five fingers .
This calibrates the system, so it knows where each finger is located.
The user then lifts the fingers not required for the gesture , and then performs the directional stroke .
This solution actually has an additional benefit - the placement of all five fingers could be used to put the system in a command mode, where it would know to accept the menu input.
In contrast, before using a Simple-Stroke gesture, the user would first need to enter a command entry mode.
We conducted an experiment to understand how well users can perform directional chording gestures on a multitouch surface.
In particular, we wanted to test the effects of different chords  and directions on the angular accuracy and speed of these movements, as well as ascertain which chords are easier to articulate.
Furthermore, we wanted to compare the SimpleStroke gestures to the Lift-And-Stroke gestures.
This will be important data to consider when designing our multitouch menus, which we describe after this experiment.
We hypothesized that the directional accuracy and speed of the movements would be affected by both the chord and the direction.
Further, we expected there to be a difference in performance and ease of articulation depending on the chord chosen, with `simple' chords, consisting of fewer fingers, for example, being faster and easier.
Once touching the start box, the chord and direction to perform was displayed in a gesture area , at the center of the screen.
The participant then lifted their finger from the start box, moved to this gesture area, and performed the indicated gesture.
The gesture area was illustrated with a circle rendered with a gradient fill that faded out at the borders.
For a trial to be considered successful, we required a minimum travel distance of 135 pixels, and an angular accuracy of 45 degrees.
We did not perform chord recognition, so the only additional requirement was that the right number of contact points were used.
However, an experimenter was present to ensure that users were not "cheating", but performing the correct chord for the trial.
Following a gesture, the screen glowed green or red, indicating whether the trial had been completed correctly or not.
In cases where the trial was not completed, the participant repeated the trial, so that we would obtain a complete data set.
Users were told to complete the trials as quickly as possible while minimizing errors.
A repeated-measure within participant design was used.
The independent variables were Technique , Chord , and direction .
Participants performed the experiment in one session lasting approximately 40 minutes.
The session was broken up into 62 blocks, 31 of Simple-Stroke and 31 Lift-and-Stroke.
Each block consisted of one chord, with all eight directions appearing in a random order.
The appearance of the chords was randomized.
The technique ordering was counterbalanced, with half of the participants starting in Simple-Stroke mode and the other half starting in Lift-and-Stroke mode.
For practice at the start of each experimental mode, participants were given ten randomly chosen chord/direction combinations.
We recruited ten participants , ranging between 20 and 26 years of age.
All participants were righthanded and used their right hand for the study.
Participants were between 157cm and 188cm in height and reported no problems with mobility or motion.
None of the participants had extensive experience with multitouch systems.
Pilot studies indicated that an accurate way to measure the stroke angle was use the angle between the center-point of the chord when first articulated and the center-point when the first finger was released.
Angular error was calculated as the absolute difference between this stroke angle, and the required angle for the trial.
Angular accuracy was very good, with an average angular error of only 5.6 degrees .
In total, 98.2% of the gestures were completed with an angular accuracy of less than the 22.5 degrees which would be needed to select between eight radial items.
Our experiment was conducted on a Microsoft Surface system which is a combined tabletop multitouch input and rear projected display device.
The surface was raised to a height of 86cm, which was a comfortable height for users.
The display and interaction area measures 63.5cm by 48cm.
The software was implemented in C# and the touch tracking was handled by the multitouch libraries included with the Microsoft Surface SDK.
Our hypothesis that Chord would influence angular error was rejected, with a maximal per-chord angular error being 6.4 degrees, and minimal error being 5.0 degrees.
This is a positive result, showing that directional chords can be accurately used for command activation.
Trial completion time was measured as the time between lifting the finger from the start box, until lifting the fingers after performing the gesture.
To provide a more comprehensive analysis of these results, we divided the trial completion time into two distinct phases: the articulation time, and the movement time.
Articulation time was defined as the time until the chord was articulated.
In general, the act of lifting certain fingers from the surface, while keeping some of the fingers down, was extremely difficult.
Chords with "gaps", that is, requiring non-consecutive fingers to be depressed, tended to produce higher articulation times.
As seen in Figure 9, there was a greater variation in articulation time for Liftand-Stroke, and less for Simple-Stroke.
The data obtained from our first experiment has some important implications to the design of multitouch menus.
On the down side, the Lift-and-Stroke gestures are difficult to perform, and should be avoided.
This means that SimpleStroke Gestures should be used, but a method of recognizing the individual fingers will be needed.
On the upside, we found that the majority of chords were easy to articulate.
The exception was chords that had gaps, which tended to produce higher articulation times.
Once articulated, the complexity of the chords had negligible effects on the speed and angular accuracy of the directional stroke.
This leads us to believe the directional chords, and in particular the Simple-Stroke Gestures, can be used as a command activation mechanism for multitouch systems.
While the Simple-Stroke directional chords seem to be a promising gesture set, we are still in need of a menu system which these gestures can drive.
Without a menu system, we only have a large set of unorganized gestures that the user would have to memorize.
We place a marking menu system on top of theses gestures, so that they can be structured in an organized fashion, and interactively revealed to the user.
The movement time was defined as the time taken to perform the stroke, once the chord had been articulated.
In general, chords with more fingers resulted in higher movement times .
This was mostly likely due to added friction with more contact points.
Errors were recorded when a trial was not completed successfully on the initial attempt, either because the wrong chord was articulated, or the movement was made in the wrong direction.
There were higher error rates for chords involving more fingers.
Our observations indicated the errors were often a result of failed tracking by the Microsoft Surface, rather than behavioral errors, and in particular, dropping contact points that had a high velocity.
In many vision based multitouch systems, including the Microsoft Surface which we are using, objects in close proximity to the touch screen are visible to the camera.
Our solution uses raw images from the tracking cameras to determine a bounding box for the user's hand .
We then compare the location of the finger contact points relative to this bounding box.
For example, the thumb is almost always at the left edge of the bounding box, and the middle finger is almost always at the top border.
This analysis allowed us to effectively increase the number of chords which we could recognize.
Although we did not formally study recognition rates, our pilot tests indicated that this strategy could be used to accurately recognize the set of 14 chords illustrated in Figure 11.
This set was selected because the chords could be recognized unambiguously based on finger count and their location within the shadowbox.
The marks required for marking menu selection are scale independent.
This means that marking menus can function across a variety of screen sizes.
This is important for usage with multitouch technology, as the platforms can vary in size, from large, wall size displays , to smaller, personal devices, such as the UnMousePad  or iPhone .
The main idea behind multitouch marking menus is that each level of the menu is accessed with a Simple-Stroke directional chord, instead of a single point mark.
The chord indicates a top level menu category, eliminating an entire level of depth from the menu.
Here we describe the specific design and implementation details.
A limitation of the multitouch devices we are aware of is their inability to disambiguate between different fingers.
Our initial hope was that Lift-and-Stroke gestures could be used to calibrate finger locations, eliminating the need for finger recognition, but our initial experiment demonstrated that these gestures are difficult solution.
For simple-strokes, if the technology does not have any capabilities to detect individual fingers, then certain chords will be impossible to disambiguate .
The angular breadth is the number of directions which are supported by the menu.
Previous research has shown that users have difficulty with marking menus with more than eight items displayed, and this has traditionally been the maximum limit of items per menu level .
The results of our preliminary study showed that users are also capable of marking with directional chords in eight directions with a very low error rate.
As a result, we have retained the maximum limit of eight directions.
Because each level of the menu is accessed through a directional chord, the breadth of the menu is also determined by the number of supported chords.
The number of chords chosen will depend on the system the menu will be used for, and the number of commands and categories it contains.
In our implementation, we used 8 different chords, so that both the chording and angular breadth would be 8.
We choose 8 chords which provided strong results in our first experiment, and could be easily recognized with our shadowbox recognition technique.
With an angular breadth and chording breadth of 8, our menu system has a total of 64 selectable menu-items.
This allows users to quickly browse multiple categories.
In a traditional marking menu, the user would need to constantly back track to do this.
The display of the menu items is the same as traditional marking menus; they are organized in a radial layout around a center point.
However, in our design, we offset the entire menu 50 pixels above the top of the shadowbox.
Similar to recent techniques for pen-based interaction , this alleviates the occlusion problem associated with direct touch interfaces.
A cursor is displayed at the origin of the menu, which is offset and controlled by the first finger which contacts the display.
The recognition of angular marks made by this offset cursor was done using the same algorithms used by the original marking menus .
As with the original marking menus, the menu items are only displayed after a short delay.
Thus, an expert user can articulate a chord and perform the directional gesture, in a single fluid movement, without having to display the menu.
As with traditional marking menus, menu cancellation is achieved by returning the cursor to the center dead zone.
The depth of the menu defines the number of levels which the menu contains.
Even with a single level, our menu supports 64 items.
Since this already replicates the upper limit of what could accurately be achieved with a traditional two-level marking menu , we kept the menu depth to 1.
This also avoids potential problems caused by including additional menu levels .
Thus, all 64 menu items can be accessed through a single, directional chording gesture.
We discuss potential designs for increasing menu depths in our future work section.
In traditional marking menus, direction is used to make the final item selection.
To maintain this design, our menu organizes items by chord - items in the same category are activated through the same chord.
The selection of the item within a category is done through the direction of the chording gesture.
As such, the interaction model is to articulate a chord to select a category, and then perform a directional gesture with that chord to select an item.
Our second experiment was conducted to compare user performance of a traditional hierarchical marking menu  to our multitouch marking menu  design.
We compared performance in both novice and expert user scenarios.
The novice user scenario was simulated by giving the user a word to select, requiring them to categorize the word and then use this category to search for the word within the menu system.
The expert user scenario was simulated by showing the user the mark they were to draw  before the trial began.
We display a chord map, illustrating the eight chord patterns, at the top of the screen relative to the user's location.
This chord map could be displayed when the system enters its command mode, so it does not occlude the display during application usage.
The chord map shows what category is associated with each chord .
A repeated measures within-participant design was used.
The independent variables were Technique  and Mode .
Each participant performed the experiment in one session lasting approximately 45 minutes.
The session was broken up into two sections, one for each technique.
Each section was further broken up by the expertise mode, with all trials for one mode being completed before proceeding to the next mode.
For each mode, there were four blocks of 16 trials each.
Across the four blocks, all 64 items were selected exactly once.
Before the first block for each technique-expertise level combination, participants were given one block of 16 random trials with which to familiarize themselves with that particular mode.
The design was counterbalanced by randomly assigning participants to one of four ordering groups.
These groups were divided by which technique was completed first, and which order of expertise mode was applied to the trials for each technique.
Six of our participants were female and six were male, all between 20 and 26 years of age.
All participants were right-handed, between 157cm and 188cm in height and reported no problems with mobility or motion.
Participants were computer users, but none had extensive experience with multitouch.
Articulation time was defined as the time until the chord  was articulated.
Multitouch marking menus had a much higher articulation time in novice mode than marking menus  because the user first had to find the desired category on the chord map .
With the marking menu, users immediately placed their finger down to bring up the top level menu.
In expert mode, the articulation times were virtually equivalent .
We believe part of this "articulation" time could be reduced for multitouch marking menus in the novice mode, with iteration on the design of the chord map, which users sometimes had trouble using to perform their visual search.
Similar to our first experiment, participants stood in front of the multitouch display within comfortable reach of the working area.
Participants were instructed to touch and hold a button in the centre of the screen, while their instruction was shown.
Following the release of this button, the instructions would disappear the trial would begin.
If the user forgot the instruction, they could press a help button in the corner of the screen, which would restart the trial.
The mechanics of the trial depended on the technique and experience level variables.
We developed a menu consisting of 8 generic but identifiable categories , and 8 items for each category .
The chord map was only displayed after a trial began.
For control purposes, we included an equivalent "direction map" for the marking menu technique, which showed which directions were required for each category.
In novice mode the instruction presented the target menu item, while in expert mode, the instruction provided a graphical depiction of the required gesture.
For the marking menu this consisted of a compound hierarchic mark, while for the multitouch marking menu, this was a directional chording gesture.
There was no interaction effect between Technique and Mode.
In novice mode, average movement times were 1949ms for MTMM and 3298ms for MM.
In expert mode, average movement times were 528ms for MTMM and 767ms for MM.
This demonstrates the main benefit of the multitouch marking menu technique.
The initial menu traversal is replaced by a quick chord posture.
Trial completion time was measured as the time between lifting the finger from the start box, until lifting the finger after performing the gesture.
Errors were defined as trials where the participant selected the wrong menu item.
The average error rates were 6.4% in novice mode and 14.9% in expert mode.
Our observations indicated that the high error rate in expert mode was due to problems with the Microsoft Surface tracking.
The discrepancy between error rates in Novice and Expert modes, despite identical chord sets, supports our belief that errors are not primarily caused by difficulty articulating chords.
This could potentially be addressed by allowing dropped contact points to be recaptured before dismissing the menu.
We also need to consider how the technique can be made self-revealing for first-time users.
Our hope is that the chord map may be beneficial in such scenarios, but this will need to be studied further.
In addition, we did not explicitly investigate fatigue and other potential ergonomic issues arising from long-term usage of our technique.
Our studies indicated that an hour of continuous use did not cause undue strain on the user, but this could be investigated further.
Another potential limitation is the relatively high error rates measured in our studies.
While we believe these were due to tracking errors, it is still an issue which needs to be addressed.
For example, our results could guide hardware developments of multitouch systems to ensure maximum velocities could be accurately tracked.
Given the variety of multitouch technologies, it is important to discuss how our research can be applied to other platforms.
Our technique had two technological requirements.
The first needed ability is to identify up to 5 distinct contact points, for each finger of that hand.
This should not be problematic, as the most common multitouch technologies used today all have this ability.
The second ability is to disambiguate fingers.
Using our shadowbox recognition technique, we are able to reliably infer some finger identities.
While other vision-based systems should be able to do this, capacitive systems may not.
However, a capacitive system will still be able to identify the 5 chords defined by the number of contact points.
Aside from these two properties, we do not assume any other information streams, such as tracking state input  or pressure .
If such data streams become standard for multitouch technologies, it may be interesting to consider how they could be utilized within our design.
For example, the chording map could be displayed when the user hovers over the display surface.
Our results may also need to be reconsidered for different display surface configurations, such as size and orientation.
For example, different chords may be harder to articulate on a vertical multitouch surface.
Because our menu design provided a large menu breadth , we limited the menu depth to a single level.
With such a large breadth, the menu size would explode if a second level were added.
For example, if the second level had the same breadth, the total number of menu items would be 2482 = 61504.
Obviously this is beyond the needs of any multitouch application, and would suffer a drawback that the user would need to change the chord articulation between levels.
A more viable alternative would be to keep the chord constant throughout the compound stroke, giving the second level a breadth of 8, defined by the second direction.
With this restriction, the menu could contain up to 64x8 = 512 items.
For the sake of our evaluation, we used a fixed user location.
However, in a real usage scenario, the user could be standing anywhere around the display.
This would not be problematic for our technique, since the menus pop-up in place, and do not need to be accessed from a specific location.
Furthermore, given recent development in finger orientation recognition , we would be able to appropriately rotate the shadowbox to maintain our accurate recognition of the chords.
Our implementations were also restricted to a single user scenario.
However, a benefit of multitouch platforms is their appropriateness for collaborative usage .
In a multi user scenario, our technique could still be used given the location and angle independence described above.
In addition, territorial research shows that in general, users will work in their own personal spaces , so contact point conflicts should not be problematic.
In our work, we focused on dominant-hand usage, for both our empirical investigation of directional chords, and usage of the multitouch marking menu.
However, it may be desirable is some scenarios to support non-dominant hand usage, for example, if the dominant hand is using a pen .
Finally, while our shadowbox recognition technique supported accurate identification of chords, it did not provide identification of all 31 possible chords.
Furthermore, it may not be robust to significant changes in the user's hand posture.
For example, if the hand was postured with only index finger pointing, the middle finger would no longer be at the top of the bounding box.
Thus, future work could look at more advanced vision based techniques, such as recognition of hand postures , to improve the chord recognition.
We did not investigate mode switching techniques to transition between application usage and the command activation mode needed for menu usage.
This would be necessary to prevent interference with main operations.
A thorough investigation needs to be conducted, similar to those carried out for pen-based applications .
We foresee the use of mode-switching techniques such as reserved command-zones, bimanual input, or invocation gestures as potential candidates.
Our second experiment showed a statistically significant performance increase for multitouch marking menus over traditional marking menus, reducing execution times in both novice and expert usage modes.
These results indicate that multitouch marking menus could be an efficient menu system for use within multitouch applications.
