We introduce BodyScape, a body-centric design space that allows us to describe, classify and systematically compare multi-surface interaction techniques, both individually and in combination.
BodyScape reflects the relationship between users and their environment, specifically how different body parts enhance or restrict movement within particular interaction techniques and can be used to analyze existing techniques or suggest new ones.
We illustrate the use of BodyScape by comparing two free-hand techniques, on-body touch and mid-air pointing, first separately, then combined.
We found that touching the torso is faster than touching the lower legs, since it affects the user's balance; and touching targets on the dominant arm is slower than targets on the torso because the user must compensate for the applied force.
Kinect let users interact by pointing or moving their bodies, although most interaction involves basic pointing or drawing.
Most research in complex device-free interaction focuses on hand gestures, e.g.
Charade's  vocabulary of hand-shapes that distinguish between "natural" and explicitly learned hand positions, or touching the fore-arm, e.g.
Skinput's  use of bio-acoustic signals or PUB's  ultrasonic signals.
However, the human body offers a number of potential targets that vary in size, access, physical comfort, and social acceptance.
We are interested in exploring these targets to create more sophisticated body-centric techniques, sometimes in conjunction with hand-held devices, to interact with complex data in multi-surface environments.
Advances in sensor and actuator technologies have produced a combinatorial explosion of options, yet, with few exceptions , we lack clear guidelines on how to combine them in a coherent, powerful way.
We argue that taking a body-centric approach, with a focus on the sensory and motor capabilities of human beings, will help restrict the range of possibilities in a form manageable for an interaction designer.
This paper introduces BodyScape, a design space that classifies body-centric interaction techniques with respect to multiple surfaces according to input and output location relative to the user.
We describe an experiment that illustrates how to use the design space to investigate atomic and compound bodycentric interaction techniques, in this case, compound mid-air interaction techniques that involve pointing on large displays to designate the focus or target of a command.
Combining on-body touch with the non-dominant hand and mid-air pointing with the dominant hand is appealing for interacting with large displays: both inputs are always available without requiring hand-held devices.
However, combining them into a single, compound action may result in unwanted interaction effects.
We report the results of our experiment and conclude with a set of design guidelines for placing targets on the human body depending on simultaneous body movements.
Multi-surface environments encourage users to interact while standing or walking, using their hands to manipulate objects on multiple displays.
Smartphones and devices such as Nintendo's Wii permit such interaction via a hand-held device, allowing sophisticated control.
However, holding a device is tiring  and limits the range of gestures for communicating with co-located users, with a corresponding negative impact on thought, understanding, and creativity .
Krueger's VIDEOPLACE  pioneered a new form of whole-body interaction in which users stand or walk while pointing to a wall-sized display.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Multi-surface environments require users to be "physically" engaged in the interaction and afford physical actions like pointing to a distant object with the hand or walking towards a large display to see more details .
The body-centric paradigm is well-adapted to device- or eyes-free interaction techniques because they account for the role of the body in the interactive environment.
However, few studies and designs take this approach, and most of those focus on large displays .
Today's off-the-shelf technology can track both the human body and its environment .
Recent research prototypes also permit direct interaction on the user's body  or clothes .
These technologies and interaction techniques suggest new types of body-centric interaction, but it remains unclear how they combine with well-studied, established techniques, such as free-hand mid-air pointing, particularly from the user's perspective.
Although the literature includes a number of isolated point designs, we lack a higher-level framework that characterizes how users coordinate their movements with, around and among multiple devices in a multi-surface environment.
Previous models, such as the user action notation , separate interaction into asynchronous tasks and analyze the individual steps according to the user's action, interface feedback, and interface internal state.
These models do not, however, account for the body's involvement, including potential interaction effects between two concurrent input movements.
Our goal is to define a more general approach to body-centric interaction and we propose a design space that:  assesses the adequacy of specific techniques to an environment or use context; and  informs the design of novel body-centric interaction techniques.
We are aware of only two design spaces that explicitly account for the body during interaction.
One focuses on the interaction space of mobile devices  and the other offers a task-oriented analysis of mixed-reality systems .
Both consider proximity to the user's body but neither fully captures the distributed nature of multi-surface environments.
We are most influenced by Shoemaker et al.
The relative location and body positions of the user thus play a central role in the interactions she can perform.
For example, touching a tactile surface while looking at a screen on your back is obviously awkward.
This physical separation defines the two first dimensions of BodyScape: User Input  and System Visual Output .
Using a body-centric perspective similar to , we identify two possible cases for input and output: Relative to the body and Fixed in the world.
Such body-environment relationships have been considered in Augmented Reality systems , but never applied to a body-centric description of interaction techniques.
D1: Input - User input may be relative to the body and thus performed at any location in the environment, e.g.
Different technologies offer users greater or lesser freedom of movement.
Some interaction techniques require no devices, such as direct interaction with the user's body  or clothes .
Others require a hand-held device, constraining interaction, but not the orientation of the body.
Others further restrict movement, such as mid-air pointing at a wall display, in which the user holds the arm, which is tracked in 3d, in a fixed position relative to an external target.
D2: Visual Output - Multi-surface environments are inevitably affected by the separation of visual output over several devices .
Users must adjust their gaze, switching their attention to output devices that are relevant to the current task by turning the head and - if that is not sufficient - turning the torso, the entire body, or even walking.
Visual output relative to the body is independent of the user's location in the environment, e.g., the screen of a hand-held device.
It does not constrain the user's location or body position, except if a limb must hold the device.
Conversely, visual output fixed in the world requires users to orient the head towards the target's physical location, e.g., where it is projected on a wall.
Users' location and body positions are constrained such that they can see the visual output effectively.
The BodyScape design space differentiates between Touchbased and Mid-air user input, since these can affect performance and restrict the position of the body.
Body movements and their coordination depends upon the physical connection with the environment .
For example, Nancel et al.
Multi-surface environments may add additional constraints, such forcing users to walk to an interactive tabletop in order to touch it.
The Input dimension clearly restricts body movement more than Visual Output, and Touch is more restrictive than Mid-air gestures, when the input is fixed in the world.
For example, one can watch a fixed display from a distance, at different angles, whereas touch input devices require physical proximity.
BodyScape builds upon Card et al.
These in turn were inspired by early research on how people adjust their bodies during coordinated movements, based on constraints in the physical environment or the body's own kinematic structure .
They help identify appropriate or adverse techniques for a given task, as well as the impact they may have on user experience and performance, e.g., body movement conflicts or restrictions.
BodyScape presents a taxonomy of atomic body-centric interaction techniques, organized according to Input and Visual Output.
Together, Input and Visual Output dictate the body's remaining degrees of freedom  available for other potential interactions or body movements.
Note that Body Restriction is not necessarily negative.
For example, assigning each user their personal display area in a collaborative multi-surface environment restricts their movement, but can prevent common problems that arise with interactive tables  such as visual occlusions, collisions, conflicts and privacy concerns.
Figure 1 shows various atomic interaction techniques in terms of their level of body restriction and the total number of involved and affected body parts, and shows how combining them into a compound technique further restricts body movement.
D3: Body Involvement - BodyScape offers a finer grained assessment of body restriction by considering which parts of the user's body are involved in an interaction technique.
Every interaction technique involves the body with varying degrees of freedom, from simple thumb gestures on a handheld device , to whole-body movements .
We define a group of limbs involved in a technique as the involved body parts.
For example, most mid-air pointing techniques involve the dominant arm, which includes the fingers and hand, the wrist, the forearm, the upper arm and the shoulder.
A technique may involve a group of limbs and also affect other limbs.
For example, on-body touch interaction involves one hand and the attached arm, and the limb touched by the hand is the affected body part.
This implies further restrictions on the body, since affected body parts are unlikely to be involved in the interaction and vice versa, especially when interaction techniques are combined.
We define five groups of involved body parts: the dominant arm, the non-dominant arm, the dominant leg, the non-dominant leg and the torso.
We omit the head when considering involved and affected body parts, since the location of the visual output is the primary constraint.
Although head orientation has been used to improve access to data on large displays , this is only a "passive" approach in which the system adapts itself to the user's head orientation.
Figure 2 lays out atomic body-centric interaction techniques from the literature along the Input and Visual Output dimensions, illustrating their impact on body restrictions in the environment.
Each technique involves performing an elementary action, e.g.
Relative Input / Relative Output - The least restrictive combination lets users move freely in the environment as they interact and obtain visual feedback.
VirtualShelf  is a mid-air example in which users use the dominant arm to orient a mobile phone within a spherical area in front of them to enable shortcuts .
Armura  extends this approach with wearable hardware that detects mid-air gestures from both arms and projects visual feedback onto the user's body.
The dominant arm is involved and the non-dominant arm is affected by the pointing.
Relative Input / Fixed Output - A more restrictive combination constrains the user's orientation and, if the distance to the display matters, the user's location.
Shoemaker's  midair technique involves pointing to a body part and pressing a button on a hand-held device to select a command.
Visual output consists of the user's shadow projected on the wall with the available commands associated with body locations.
Users press imaginary buttons on their palm  and see visual feedback on the fixed TV screen.
One arm is involved in the interaction; the other is affected.
Fixed Input / Relative Output - The next most restrictive approach requires users to stand within a defined perimeter, limiting movement.
Here, touch is more constrained than mid-air gestures: standing within range of a Kinect device is less restrictive than having to stand at the edge of an interactive table.
Both examples involve the dominant arm and affect the non-dominant arm, which carries the handheld device.
Fixed Input / Fixed Output - The most restrictive combination constrains both the user's location and visual attention.
A common mid-air technique uses the metaphor of a laser pointer to point to items on a wall-sized display.
Conventional touch interaction on a tabletop or a large display is highly restrictive, requiring the user to stand in a fixed location with respect to the surface.
Body Involvement - Figure 1 shows that most body-centric techniques only involve and affect one or two groups of body parts, usually the arms.
We know of only a few "whole-body" techniques that involve or affect the entire body: V IDEOPLACE  and its successors for games and entertainment and PinStripe , which enables gestures on the users' clothing.
Touchprojector is thus considered a "touch fixed input and fixed output" technique in BodyScape.
The advantage of minimizing body restrictions with relative-to-thebody technique is overridden by requiring a fixed input.
Even so, Touchprojector offers other advantages, since users can interact directly with a remote display without having to move to the display or use another interaction device.
Our work with users in complex multi-surface environments highlighted the need for interaction techniques that go beyond simple pointing and navigation .
Users need to combine techniques as they interact with complex data spread across multiple surfaces.
The BodyScape design space suggests a number of possibilities for both atomic and compound interaction techniques that we can now compare and contrast.
This section illustrates how we can use the BodyScape design space to look systematically at different types of body-centric interaction techniques, both in their atomic form and when combined into compound interaction techniques.
We chose two techniques, illustrated in Figure 2d, O N -B ODY TOUCH input, and 2g, M ID - AIR P OINTING input, both with visual output on a wall display, which is where our users typically need to interact with their data.
Although the latter has been well-studied in the literature , we know little of the performance and acceptability trade-offs involved in touching one's own body to control a multi-surface environment.
Because it is indirect, we are particularly interested in on-body touch for secondary tasks such as confirming a selection, triggering an action on a specified object, or changing the scope or interpretation of a gesture.
Finally, we want to create compound interaction techniques, so as to increase the size of the command vocabulary and offer users more nuanced control.
However, because this involves coordinating two controlled movements, we need to understand any potential interaction effects.
The following experiment investigates the two atomic techniques above, which also act as baselines for comparison with a compound technique that combines them.
The two research questions we are addressing are thus: 1.
Which on-body targets are most efficient and acceptable?
Users can take advantage of proprioception when touching their own bodies, which enables eyes-free interaction and suggests higher performance.
However, body targets differ both in the level of motor control required to reach them, e.g., touching a foot requires more balance than touching a shoulder, and in their social acceptability, e.g., touching below the waist .
Complex tasks in multi-surface environments combine several interaction techniques:  in series, e.g., selecting an object on one touch surface and then another; or  in parallel, e.g., simultaneously touching one object on a fixed surface and another on a handheld device.
Serial Combination - a temporal sequence of interaction techniques.
The combined techniques can be interdependent , but the first action should end before the second starts.
For example, the user can select an object on a tactile surface  and then apply a function to this object with a menu on a mobile device.
Serial compound techniques do not increase the restrictions imposed by each atomic technique in the sequence, nor the involved or affected body parts.
However, one must still design serial combinations to avoid awkward movements, such as having to constantly walk back and forth, move a device from one hand to another or repeatedly switch attention between fixed and relative displays.
Parallel Combination - performing two techniques at the same time.
The techniques may be independent or dependent.
For example, the user might touch two tactile surfaces simultaneously in order to transfer an object from one to the other .
Unlike serial combinations, these compound techniques may significantly restrict the body's movement and raise conflicts between involved and affected body parts.
The constraint on body movement is determined by the more restrictive of the combined techniques.
Thus, combining a "fixed-in-the-world" with a "relative-to-the-body" technique will be as restrictive as "fixed-in-the-world".
What performance trade-offs obtain with compound bodycentric interaction techniques?
Users must position themselves relative to a target displayed on the wall and stabilize the body to point effectively.
Simultaneously selecting on-body targets that force shifts in balance or awkward movements may degrade pointing performance.
In addition, smaller targets will decrease pointing performance, but may also decrease O N -B ODY TOUCH performance.
We recruited sixteen unpaid right-handed volunteers ; five had previous experience using a wallsized display.
All had good to excellent balance  and practiced at least one activity that requires balance and body control.
All wore comfortable, non-restrictive clothing.
Participants wore passive infra-red reflective markers that were tracked in three dimensions by ten VICON cameras with sub-millimeter accuracy at a rate of up to 200 Hz.
Markers were mounted on a wireless mouse held in the user's dominant hand to track pointing at a target on the wall, on the index finger of the non-dominant hand to track on-body touches, and on protective sports gear - belt, forearms, shoulders and legs - to track on-body targets.
The latter were adjustable to fit over the participants' clothing.
VICON data was filtered through the 1Euro filter .
The system made an orthogonal projection from the index finger to the touched limb segment using a skeletonbased model to calculate the closest body target.
Wall targets were randomly placed 4700px  from the starting target.
P OINTING MOVEMENT TIME: from initial cursor movement to entry into goal target.
C URSOR READJUSTMENT TIME: from leaving goal target to relocating cursor onto goal target.
B ODY REACTION TIME: from appearance of trial stimulus to leaving starting position.
B ODY POINTING TIME: from leaving start position to touching on-body target.
B ODY ERRORS: number of incorrect touches detected on body target2 ; includes list of incorrect targets per error.
We debriefed participants at the end of the experiment and asked them to rank on a Likert scale:  perceived comfort of each body target according to each M ID - AIR P OINTING condition ; and  social acceptability of each on-body target:"Would you agree to touch this body target in a work environment with colleagues in the same room?"
Each session lasted about 60 minutes, starting with a training session, followed by blocks of trials of the following conditions, counter-balanced across subjects using a Latin square.
B ODY ONLY: Non-dominant hand touches one of 18 on-body targets  P OINTING ONLY: Dominant hand points to one of three target sizes  P OINTING +B ODY: Combines touching an on-body target with selecting a wall target x5 replications = 270 trials Participants were thus exposed to 75 unique conditions, each replicated five times, for a total of 375 trials.
Starting position: non-dominant hand at the hip and/or dominant hand points to a starting target on the wall display.
B ODY ONLY and P OINTING ONLY are atomic conditions; P OINTING +B ODY is compound: a body touch triggers the selected wall target.
P OINTING ONLY trials were organized into blocks of five and all wall pointing trials were counterbalanced across difficulty.
The two atomic interaction techniques, B ODY ONLY and P OINTING ONLY serve as baseline comparisons for performance with the compound interaction technique, P OINTING +B ODY.
TASK : Participants were asked to perform trials as quickly and accurately as possible.
They were asked to point and select on-body targets using their non-dominant hand's index finger in the B ODY ONLY condition, and to point and select walltargets using a mouse device held in the dominant hand in the P OINTING ONLY condition.
The compound P OINTING +B ODY condition asked users to point to the wall-target and keep the cursor inside before selecting an on-body target.
The trial begins when an image of a body silhouette appears on the wall, with a red circle indicating the location of the on-body target to acquire.
The participant touches that target with the index finger of the non-dominant hand as quickly and accurately as possible.
Participants were asked to avoid crouching or bending their bodies, which forced them to lift their legs to reach lower-leg targets.
The trial ends only when the participant selects the correct target; all intermediate incorrect selections are logged.
Body parts involved when touching the  torso,  arm,  leg;  mid-air pointing; and  in parallel, when the dominant hand points in mid-air and non-dominant hand touches the dominant arm.
The trial begins with the appearance of a body-target illustration and the goal target on the wall display.
The participant first points the cursor at the goal target, then completes the trial by touching the designated on-body target.
The trial ends only when the on-body touch occurs while the cursor is inside the goal target on the wall.
As in the B ODY ONLY condition, multiple body parts may be involved, sometimes with adverse effects.
5e shows the interaction between the dominant arm, which is trying to point to a target on the wall and the non-dominant arm, which is pointing at the dominant arm.
Figure 5 shows how different body parts interact for different on-body targets.
The non-dominant arm is always involved, since it is responsible for pointing at the target.
The trial begins when the starting target disappears and the goal target appears between 0.5s and 1s later, to reduce anticipatory movements and learning effects.
The participant moves the dominant hand to move the cursor to the goal target and selects by pressing the left button of the mouse bearing the optical marker used for pointing.
The trial ends only when the participant successfully clicks the mouse button while the cursor is inside the goal target.
Participants began by calibrating the system to their bodies, visually locating, touching and verifying each of the 18 body targets.
They were then exposed to three blocks of six B ODY ONLY trials, with the requirement that they performed two onbody touches in less than five seconds.
They continued with three additional blocks to ensure they could accurately touch each of the targets.
Next, they were exposed to all three levels of difficulty for the P OINTING ONLY condition: easy, medium and hard, in a single block.
Finally, they performed three additional blocks of the compound P OINTING +B ODY technique.
Our first research question asks which on-body targets are most efficient and which are socially acceptable.
We conducted a full factorial ANOVA on the B ODY ONLY condition, with PARTICIPANT as a random variable based on the standard repeated measures  technique from the JMP 9 statistical package.
We found no fatigue or learning effects.
Figure 6 shows the times for touching all 18 on-body targets, grouped into the five body areas.
We found significant effects of B ODY TARGET on B ODY POINTING TIME: touching lower body targets is slower.
Since B ODY POINTING TIME is consistent for targets within a given target group, we report results according to target group, unless otherwise stated.
A post-hoc Tukey test revealed two significantly different groups: body targets located on the upper torso required less than 1400ms to be touched whereas targets on the dominant arm and on the lower body parts required more than 1600ms.
Results are similar for B ODY POINTING TIME with a significant effect of B ODY TARGET GROUP only for the D UPPER group , specifically, targets on the dominant thigh are touched more slowly than those on the shoulder or torso.
For B ODY REACTION TIME, despite a significant effect, values are very close for each B ODY TARGET GROUP .
Participants were able to quickly touch on-body targets with an accuracy of 92.4% on the first try.
A post-hoc Tukey test showed that targets on the dominant arm were more prone to errors than other body target areas .
Most errors obtained when targets were close to each other, i.e.
Touching lower body parts is, not surprisingly, slower, since these targets are further from the starting position and require more complex movements.
However, the difference is small, about 200ms or 12% of global trial time.
Figure 7a shows that participants' preferences  for and within each B ODY TARGET GROUP were consistent with performance measures: targets on the upper body parts were preferred over lower body parts  and the torso were slightly more preferred than on the dominant arm.
The latter is surprising, given that the most popular location for on-body targets in the literature is on the dominant arm.
This suggests that interaction designers should explore alternative on-body targets as well.
P OINTING REACTION TIME is also significantly slower for difficult  as opposed to medium  or easy  tasks.
P OINTING MOVEMENT TIME is significantly different for all three levels of difficulty: hard , medium  and easy .
Participants made few errors but occasionally had to relocate the cursor inside the goal target before validating the selection with the mouse.
This occurred rarely , but was significantly more likely for difficult pointing tasks   and accounts for the differences in T RIAL T IME and P OINTING MOVEMENT TIME.
The main effect of M ID - AIR P OINTING is also similar to those observed before, showing that difficult pointing tasks make simultaneous body touching slower than medium or easy pointing task.
Obviously, these are all significantly slower than the B ODY ONLY baseline.
In fact, despite the fact that our task required body target selection as the final action, the reaction times indicate that both tasks start almost simultaneously .
ND UPPER and D UPPER remain the groups of targets that require less time to be touched.
In summary, the compound P OINTING +B ODY task involves interaction effects between the two atomic techniques, which not only incur a time penalty when the tasks are performed simultaneously, but also degrades pointing performance for M ID - AIR P OINTING  when combined with a body-relative technique that involves and affects multiple limbs.
However, our results also reveal that O N -B ODY TOUCH on the lower parts of the body significantly impair the movement phase of pointing, and that the overall negative impact increases with the difficulty of the pointing task, especially when targeting on the pointing arm.
We illustrate BodyScape by examining the effects of combining two multi-surface interaction techniques: mid-air pointing and on-body touch.
This novel combination enables an eyes-free interaction with on-body targets while offering a rich set of mid-air pointing commands to access a remote virtual target on a large display.
We ran a controlled experiment to study both techniques individually and in combination, investigating performance and acceptability of 18 on-body targets, as well as any interaction effects that obtain when the two techniques are combined.
Participants were most efficient with targets on the torso and least efficient with targets on the lower body and on the dominant arm, especially in the combined condition: Reaching targets on the lower legs requires additional balance and touching the dominant arm impairs the precision of mid-air pointing because of the force applied on the pointing arm.
Users consistently preferred targets located on the upper body.
Our results suggest three guidelines for designing on-body interaction: G1 Task difficulty: On-body targets should be placed on stable body parts, such as the upper torso, when tasks require precise or highly coordinated movements.
G2 Body balance: Anticipatory movements, such as shifts in balance, can be detected to accommodate corresponding perturbations in a primary task, e.g.
The precision of a pointing task can be adversely affected if users must also touch on-body targets that require a shift in balance or coordination, in particular, touching the dominant arm while it is performing a separate task.
G3 Interaction effects: Designers should consider which body parts negatively affect users' comfort while touching on-body targets as well as side effects of each task, such as reduced attention or fatigue that may lead to unexpected body positions or increases in errors.
Future work will develop more detailed descriptions of each limb's involvement in the interaction.
We also plan to increase the predictability of BodyScape, following Card et al.
The BodyScape design space uses a body-centric approach to classify both existing and novel interaction techniques.
The distributed nature of multi-surface environments highlights the need for combining interaction techniques, in series or in parallel, to accomplish more complex tasks.
A bodycentric approach can help predict possible interaction effects of body movements by  analyzing the spatial body-device relationship and  proposing ways to decompose individual techniques into groups of body parts that are either involved in or affected by the interaction.
We argue that studying compound interaction techniques from a body-centric perspective will lead to powerful guidelines for interaction design, both with and without physical devices.
