We introduce CrashAlert, a system aimed at improving safety while on the move.
CrashAlert captures and displays information beyond the user's peripheral view using a depth camera attached to a mobile device .
The depth camera's field-of-view is orthogonal to that of the eyesbusy operator for increased peripheral awareness.
Unlike navigation aids for the visually-impaired which rely on audio or vibro-tactile cues , CrashAlert displays a small slice of the depth camera's image as a minimalfootprint display on the mobile's screen.
With an extended field-of-view, users can take simpler and early corrective actions upon noticing a potential collision.
The display also alerts users of obstacles immediately in front of the user through a red alert, prompting users to immediately stop or lift their heads.
Mobile device use while walking, or eyes-busy mobile interaction, is a leading cause of life-threatening pedestrian collisions.
We introduce CrashAlert, a system that augments mobile devices with a depth camera, to provide distance and location visual cues of obstacles on the user's path.
In a realistic environment outside the lab, CrashAlert users improve their handling of potential collisions, dodging and slowing down for simple ones while lifting their head in more complex situations.
Qualitative results outline the value of extending users' peripheral alertness in eyesbusy mobile interaction through non-intrusive depth cues, as used in CrashAlert.
We present the design features of our system and lessons learned from our evaluation.
Researchers have introduced walking user interfaces   to improve mobile usage efficiency with tasks that require significant visual attention or eyes-busy mobile interaction.
These interfaces include audio feedback , enlarged soft buttons , twohanded chorded keyboard input , and adaptive methods to compensate for extraneous movement .
WUIs primarily focus on task efficiency instead of user safety.
Eyes-busy mobile interactions limit much of the user's peripheral vision, resulting in users tripping on curbs, walking into traffic or deviating from their intended path .
The year 2008 registered a twofold increase from the previous year in eyes-busy interaction-related accidents .
This has forced municipalities to consider safety policies that ban mobile device usage while walking .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To the best of our knowledge CrashAlert is one of the first explorations of a safety-aware WUI.
Our contribution is threefold:  a prototype implementation of CrashAlert, a system designed for safer eyes-busy interaction,  a set of visualizations aimed at minimizing screen real-estate and optimizing information about obstacles outside the user's field-of-view, and  a study of CrashAlert showing improved handling of potential collisions and an increased perception of safety, without loss of task performance.
We noted the holding angle of the device, the number of hands used, the number of steps taken before users lift their heads , the type of obstacles commonly avoided, patterns in walking speed and how many steps users took while typing.
We noticed that when walking, people handle potential collisions with varying degrees of safety `cost': from slowing down to dodging obstacles, then lifting their heads and/or ultimately coming to a full stop to avoid a crash.
Users rely on their peripheral awareness to notice obstacles early on and to take simpler corrective actions .
As their walking continues, the obstacle is reevaluated and, if needed, further corrective actions are taken .
Limited peripheral vision means that obstacles are noticed later on, restricting the suitable corrective actions to higher cost ones .
These observations led to the following design requirements .
A WUI supporting safer walking should therefore prompt users to take simpler corrective actions early on by encouraging dodges , and alerting on imminent collisions .
The color image is a slice of the picture taken with the color camera .
The depth image is obtained by applying a binary threshold to the depth capture for a fixed distance  and assigning the max value of each column to all of its pixels .
The masked image uses the depth image  as a mask on the color image; this way it shows the full color version of the closest objects on a black background .
All bands presented a red alert when the obstacle was <2 m away.
We designed CrashAlert  to let users act safely in eyes-busy mobile interaction.
CrashAlert has two main components: an ambient visual band and visual alerts for near-by objects.
Our system uses both a depth and a regular camera to capture the region in front of the user but outside their eyes-busy field-of-view .
We extract only a small slice of the camera's image and process this to present obstacle positions and distances on the small footprint ambient band.
The band conveys a glance-able representation of the elements in front of the user and outside their FoV .
Visual alerts are generated from the depth image for objects that are 2 meters away or less from the user.
Their appearance  is quite salient, prompting the user to raise her head to better cope with obstacles .
We explicitly excluded other feedback modalities  due to their limitations in the wild  and need of headphones, and to isolate the effects of our visual approach on safety.
We generated different visualizations for the ambient band through a design workshop with eight participants who interacted with 11 different visualizations.
The CrashAlert prototype operates on an Acer A100 7'' tablet computer, a laptop computer, and a Microsoft Kinect .
The laptop is carried in a backpack together with a 12 volt battery to power the Kinect in a mobile setting.
The laptop receives images from the Kinect via USB, processes and transforms them, and sends them to the tablet via Bluetooth.
The tablet receives images at approximately 10-11 frames per second.
The application is written in C#.NET.
It interfaces with the Kinect, processes the images with OpenCV, and communicates them over Bluetooth.
The tablet software is an Android 2.3.3 application.
We conducted our experiment to observe participants' safety behaviors using CrashAlert.
We recruited eight university students, from various disciplines, who habitually text and walk .
All participants text while walking, but agreed that such practice is dangerous.
On average, our participants reported having a dozen collisions over the last year.
Conditions were counter-balanced with an incomplete Latin-square design.
The camera was fixed at a 0 angle  and participants were asked to hold the tablet in a natural way.
The depth slice covered the middlelow 2/5 of the camera image.
We used the univariate ANOVA test and the Bonferroni correction for post-hoc pair-wise tests for our analysis.
Figure 5-left-top shows the percentage distribution of collision handling maneuvers  for each condition.
Post-hoc analysis showed differences only between the no-feedback  condition and all the others, but not between the various visualizations.
The apparent increase in near crashes was not significant.
These results show that with CrashAlert participants avoided more obstacles by dodging and slowing down, rather than by heads up.
Moreover, this better handling came at no cost in playing the game .
We asked participants to play a whack-the-mole game while walking through the university cafeteria.
Each trip  consisted of starting the walk at the near-by bookstore and looping around the entire food court .
Participants were asked to walk as normally as possible while playing the game.
Their objective was to tap on as many moles as possible during their trajectory.
Participants were asked to naturally avoid collisions with people and obstacles.
We ensured that participants would face at least four collisions during each trial.
This was achieved by asking an `actor,' unknown to the participant to provoke potential collisions.
The `actor' would do one of the following: cut the participants' path orthogonally, would stop right in front of them, would come toward them at a fast pace, or would walk beside them but then immediately swerve in their lane.
None of our participants suspected the presence of the `actor'.
Participants also faced obstacles from other people and objects in the cafeteria.
The experimenter recorded participants' behavior during any potential collision.
We captured the user's total walking time, the number of moles they hit, as well as the number of times they performed a `dodge/slow down,' a `heads-up,' a `full stop,' or a `crash'.
Each experiment lasted roughly 30 minutes.
Each condition was done twice, resulting in 8 participantsx4 conditionsx2 trials = 64 trials in total.
We also interviewed the participants between trials and had a longer debriefing at the end of the experiment to collect data  about their perceived safety, efficiency, alertness, walking speed, understandability and glance-ability of each condition.
If a mole was not hit within 2.5s we recorded an error and the mole was shown as being destroyed.
Participants wore a backpack containing the battery pack to which the Kinect was connected.
We first explained the task and briefly explained the visualizations.
We did not inform them of the planned collisions and asked them to behave naturally while trying to hit moles in the game as efficiently as possible.
Participants walked through the cafeteria as per the assigned path.
For subjective ratings we used the Friedman 2 test.
There were no main effects on the other factors.
We coded their answers  into 3 topics: abstraction, navigation, alerts.
In terms of the abstraction level, participants said that even though the color and the masked images provide higher levels of detail, they were harder to read, requiring more attention and generating more stress when executing the task ; for example P8 said "I have to check the  image much more and longer".
Participants whacked moles at an average rate of 1.91 moles/second .
There were no significant differences between conditions on the number of moles hit,
Moreover, participants reported depth images as falling into the background to the point where some were convinced they had used them unconsciously.
Participants talked about the different ways that CrashAlert enhances their navigational senses  beyond simply alerting about obstacles and potential collisions, by:  allowing participants to walk within the dark regions shown on the ambient band, and  by interpreting the alert in unforeseen ways.
Some participants found it useful to simply relax and follow the darker areas of the depth images, as they trusted that these areas would not have obstacles.
In a different situation, when walking through a narrow and crowded corridor, a participant knew the person in front of her  was walking in the same direction and so she decided to follow the position of the alert to way-find through the crowd.
Finally, participants noted that a system based only on depth alerts  would be a marked advantage over current systems.
Moreover, participants indicated the need for different alert types.
One such type are alerts based on direction and speed; for example, participant 1 said "I couldn't tell whether people where coming toward me or moving further away".
Another type of alert would be based on the type of object  and their related hazard estimation; for example P3 noted " a significant level indication of obstacles like how much danger if collision occurs", and P5 said "perhaps I could be alerted about different objects in different ways... moving people and static chairs require me to take action differently considering time and predictability".
We presented CrashAlert, a mobile device augmented with a depth sensing camera that shows users out-of-periphery objects in their path while walking.
CrashAlert shows salient information such as distance and position about potential obstacles.
The information is displayed on a minimal footprint ambient band on top of the device's display.
Study results show that users took simpler corrective actions early on in their path upon noticing an obstacle, felt safer with our system and use it in unexpected ways to help navigate around the environment.
This improvement came with no negative impact on performance, showing that even minimal environment information outside the user's periphery can provide for safer usage of mobiles while walking.
We summarize three key benefits of CrashAlert:  Depth and color images orthogonal to the user's FoV can facilitate safe navigation ;  Only a slice of the camera's image is needed to observe a benefit in extending users' peripheral alertness;  Visual alerts based on depth information can support safer walking when interacting with a mobile device.
This initial exploration was limited by a low image rate , a bulky hardware set-up, and naive detection of obstacles .
However limited, our system demonstrated the value of considering safety in WUIs.
Future work should investigate alternative visualizations , varying alert styles, such as a growing or shrinking boxes based on distance and speed, other feedback modalities, impact on complex tasks, dynamic selection of the image slice, scene analysis and object recognition .
