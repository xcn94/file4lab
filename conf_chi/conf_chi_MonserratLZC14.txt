Figure 1: A physics lecture video based on Angry Birds is played on the L.IVE system .
A lecturer uses an embedded text annotation to indicate a quiz is coming .
The video is automatically paused with an in-place quiz that asks the speed required to hit the green target.
Users' answers will be assessed and feedback given based on their individual responses .
Users see a comment tag above the green target, and move the cursor over as the tag fades in .
Clicking on the comment tag displays the comments on the side.
Hovering over the comments reveals an associated annotation, which can be hand-drawn  or shown as a linked video .
Correctly answering the quiz question advances the video .
In this paper, we introduce L.IVE: an online interactive video-based learning environment with an alternative design and architecture that integrates three major interface components: video, comment threads, and assessments.
This is in contrast with the approach of existing interfaces which visually separate these components.
Our study, which compares L.IVE with existing popular video-based learning environments, suggests advantages in this integrated approach as compared to the separated approach in learning.
Together, they offer a great variety of courses and attract millions of users each month .
Although the content, design, and layout differ among these environments, they all have three interface elements to support key educational activities: videos to deliver the lecture content , comment threads for sharing , and assessments to check learning progress  .
This potentially creates additional burdens for learners to cognitively link different sources of information together, thereby impairing the effectiveness of their learning .
For example, a comment placed below the video may contain useful information for learners to understand the key lecture concepts; however, it can be easily overlooked, as most people tend to focus on the video.
Furthermore, because the duration of each video typically spans at least a few minutes, it can be difficult for learners to identify which part of the video the comment is associated with.
Similarly, while it may seem natural to assess users' learning after watching a video segment, visually separating assessment questions from video content can make it harder for learners to recall the relevant information for answering questions.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
While the previous work has provided inspirations, there has not been an attempt to integrate the three components of online video-based learning environment together.
Furthermore, there hasn't been a formal evaluation to reveal the potential benefits/drawbacks of such integrated approach for learning and education, hence providing motivation for our work.
Therefore we propose an alternative design to visually integrate comment threads and assessment questions tightly with the video.
We created the L.IVE  prototype to test the potential benefit of the proposed integrated approach as compared with the current separated approach.
Our evaluation of 18 participants indicated that they learned more efficiently with the L.IVE integrated approach than with the baseline approach: participants showed 20% greater scores, measured as post-test performance compared with pre-test performance, when they experienced content with L.IVE.
Our postexperimental questionnaire also indicated that most participants preferred the integrated approach as compared with the separated approach.
The concept of adding either comment thread or assessment to videos is not new.
However, based on our analysis, users' benefit may be maximized when both comment thread and assessment are tightly integrated with the video.
We developed L.IVE, the first system and design to integrate all three components and apply them in the context of online video-based learning.
Further, we also contribute an architecture to facilitate the implementation of this integrated design on the web, where video-based learning systems are becoming increasingly popular.
Finally, we conducted the first empirical evaluation to reveal the potential benefits of this integrated design for online video-based learning.
Our vision for the L.IVE system is for it to be an interactive video player that allows users to view lecture video, discuss with both the teaching staff and peers, and assess their learning within or in close proximity to the video itself.
The goal is to tightly connect information presented from the different sources to provide an enriched, holistic learning experience.
Figure 1 illustrates an example of using the L.IVE system in a physics lecture on projectile motion.
As in the example in the figure, educational information is often presented in three main formats: video, comment threads, and assessments.
Though used somewhat differently, all three are equally important in working together to enhance the user's ability to learn efficiently.
Existing systems separates these elements and may hamper with the user's ease of learning of abstract and complex information.
Thus, there is a need for an interface design and underlying software architecture to support integrated organization and presentation of information that connects the three source components and allow users to easily interact with them.
Numerous works have added various types of interactive elements in videos.
Due to space constraints, we can only list a few representative ones.
The Hvet Design  used links in videos to external information to improve user learning.
NoteVideo reverse-engineered the video to convert visual glyphs of lecture notes into clickable links to make it easier for learners to access the different parts of video and has shown the new interface improves various learning tasks .
CWaCTool implemented in-place annotations and comments in video for sharing and discussing annotated information .
In-place annotation and in-context comment threads have also been tried in documents .
In addition, learning assessments have been included as part of games , and most recently be experimented in video .
We constructed a data structure to organize the different interactive elements based on their spatial and temporal relationships to the video as well as their logical relevance to the lecture content.
Both are JSON documents that are based on SIVA's interactive video XML document design but are modified to allow in-place objects in video rather than at pre-defined places  .
They have a starting time of appearance and an ending time of appearance.
The location of the element in the video space is defined by its x and y coordinates.
User interactions on elements will usually call these action triggers to issue a change in the interface.
Apparatus: The study was conducted using a desktop computer running on Windows 7 OS with the usual mousekeyboard input.
The L.IVE system's implementation was as previously described.
The baseline system was implemented using HTML5, CSS, and JavaScript.
Task and stimuli: Two 10-minute biology videos from Khan Academy were selected as the lecture videos1.
Each video has 10 comments and 3 assessment quizzes.
All comments were selected from the existing comment threads on these videos from Khan Academy.
We made some modifications to the comments by removing unrelated posts and answered some questions with text explanations and/or links to external information and videos.
We also added x and y coordinates to fix appearance of linked videos on the L.IVE interface.
The structure and appearance of comments for both conditions were the same.
We also developed the assessment quizzes.
Because assessments were not embedded in the baseline, they were implemented in the usual way: after every 2-3 minutes of a video segment, the users were taken to another page to complete the assessment.
The timing of the assessment in the baseline is the same as in L.IVE.
Design and procedure: A within-participant design was used.
Each participant watched both videos  using both interfaces .
The order of the interfaces was counter-balanced while the order of the videos remained the same .
To measure the knowledge gain, for each video, a 10question pre-test  and a 10-question posttest  were administered to each participant.
The difference in the number of questions answered correctly in the pre- and post- tests was recorded.
After they finished watching both videos using the two interfaces, participants shared their preferences and experience in an interview.
Before the experiment, participants were asked to spend about 2 minutes each to familiarize with the two interfaces.
Each participant performed the entire experiment in one sitting, including breaks, in approximately 1 hour.
The separation of the comment thread descriptor and the interactive video file descriptor allows users to share the interactive video with their peers without copying the comment threads of the original interactive video.
A comment thread descriptor contains a list of comment objects and annotation objects.
A comment thread starter type has data connecting it to a scene object and holds x and y coordinate and a timestamp of the video.
This visually integrates it to the video.
A comment reply is connected to another comment data object, creating a comment thread.
Each annotation object can be any of these types depending on their encapsulated object data: free-hand drawing, text, image, or video.
The annotation object's location is defined by its x and y data coordinates.
The system is implemented using three main web technologies: HTML5, Cascading Style Sheets , and Javascript , which are responsible for the structure, style, rendering, and interactivity of all objects and action triggers in the L.IVE interface.
The current mechanism of authoring a L.IVE video is through manually editing the markup language in a text editor.
The WYSIWYG interface for authoring the L.IVE video is in current development.
Participants: Eighteen participants , ranging in age from 20 to 29 years were recruited from within the university community to take part in the experiment.
15 of them have previous experience with existing online videobased learning environments.
Two tailed t-Tests with 5% alpha-level revealed that there was a significant effect of interface type on difference of pretest and post-tests scores.
The L.IVE interface resulted in an additional 22% score as compared with the baseline interface.
However, the overall learning time participants spent using the L.IVE interface  were not significantly different from time spent using the baseline interface  .
For embedded assessments, all participants eventually completed the assessments correctly after doing the experiment.
Participants answered L.IVE and baselines assessments incorrectly, on average, 1.11 and 1.83 times, respectively.
This indicates an average of 0.72 or 65% more unsuccessful attempts when using baseline.
In addition to the results above, the majority of participants preferred the design of the L.IVE system  over the baseline design .
Participants expressed that the incontext annotations, comment threads and assessments were helpful in getting to know the bigger picture of the information.
The ease of access to information in comments while watching video helped them understand and absorb more information.
The in-context assessments also helped them to recall information.
20% score gain with the former.
Our post-experimental questionnaire and interviews revealed that most participants preferred the integrated approach as compared with the separated approach.
Their input also highlighted potential challenges to watch for when deploying the system.
In the future, we would like to evaluate L.IVE in a real online course setting and explore other values or experiences our integrated approach can bring to areas other than learning.
The feedback of the participants also provided additional insights that can guide the design of future video-based learning environments.
Although most participants preferred the L.IVE interface, a few participants still preferred the baseline interface as it allowed them to focus on the video first.
This suggests that the L.IVE system may not be suitable for everyone; it is recommended that future video-learning environments provide an option for users to switch back to the traditional interface if they prefer.
In addition, while 10 comments are easy to manage, participants pointed out that an excessive number of comments may clutter the video interface and distract their attention.
It is suggested that comments and annotations should be monitored and managed so that irrelevant ones can be filtered out and potential visual clutter can be minimized or avoided .
Lastly, participants suggested that it would be useful to allow personalized comments targeted to specific audiences instead of the entire viewing group.
For example, a student may want to raise a question only for the teaching staff to look at; alternatively, one may want to start a discussion only with several of her close friends.
In this paper, we proposed an alternative design to visually integrate comment threads and assessment questions tightly within video.
We implemented this alternative design in a prototyping L.IVE system and contributed a system architecture for organizing and presenting information from the three components on the web.
