Many online communities are emerging that, like Wikipedia, bring people together to build community-maintained artifacts of lasting value .
Motivating people to contribute is a key problem because the quantity and quality of contributions ultimately determine a CALV's value.
A field experiment with 197 contributors shows that simple, intelligent task routing algorithms have large effects.
We also model the effect of reviewing contributions on the value of CALVs.
The model predicts, and experimental data shows, that value grows more slowly with review before acceptance.
It also predicts, surprisingly, that a CALV will reach the same final value whether contributions are reviewed before or after they are made available to the community.
35,000 RateYourMusic.com users are building themselves a music database, while freedb.org's online music service receives thousands of weekly CD submissions.
The common theme: groups of volunteer contributors building community-maintained artifacts of lasting value .
The content of CALVs is meant to be persistent and have value to the entire community.
This raises many issues, from "who is the community?"
In this paper, we focus on two fundamental, related problems that communities building CALVs must solve: motivating people to contribute and ensuring that contributions are valuable.
Research Question 1: How does intelligent task routing affect contributions to a CALV?
Thorn and Connolly analyzed the problem of encouraging contributions using discretionary databases , an abstract model that applies reasonably well to CALVs.
A key problem is that discretionary databases are public goods .
That is, everyone can consume the information without using it up for others.
It is rational for individuals to consume information but not to produce it because contributing has costs.
Some people contribute despite the cost , but the community as a whole suffers because all would be better off if all contributed.
Reducing costs can encourage contributions.
Wikipedia allows almost anyone to edit almost anything, making it easy to find work to do.
Distributed Proofreaders uses mentors to teach new members.
The ESP Game makes contributing fun.
RateYourMusic.com piggybacks on the value people gain by maintaining their music collections.
We explore a computational approach to reducing contribution costs, intelligent task routing, that leverages social psychology theory to match people with appropriate tasks.
The collective effort model  suggests a number of factors that affect people's motivation to contribute to groups.
Online communities that know something about their members may be able to match people with tasks based on attributes of the tasks that map to factors in the collective effort model.
We develop several general and simple task routing algorithms that most communities could implement by choosing items:
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Research Question 2: How does reviewing contributions before accepting them affect the value of CALVs?
Just contributing is not enough.
Because Wikipedia allows anyone to contribute, and because these contributions become visible right away, it has become a lightning rod for the problem of ensuring contributions are valuable.1 Reviewing contributions is a natural strategy, but designing effective review systems is hard because reviewing behavior is complex.
For example, the timing of contributions affects whether they receive adequate review in Slashdot .
In our work we focus on how the structure of review mechanisms affects contribution behavior.
We studied whether review is needed at all in prior work.
It is, but in our domain, peers were as effective as experts .
Here we focus on another important structural question: should contributions be reviewed before being added to the CALV, or can they be added first and reviewed later?
Distributed Proofreaders and RateYourMusic require contributions to be reviewed before they are added to the CALV.
In Wikipedia, contributions are immediately available and are reviewed by their consumers.
Both approaches can also fail.
The ChefMoz.org restaurant directory is impeded by its slow contribution review process, while a 2005 LA Times experiment with wiki-based editorials ended, overrun by vandals, in just two days.
We develop simple mathematical models of contributors' behavior to explore the effect of reviewing contributions before and after inclusion.
The models predict that review before inclusion hurts the CALV's short-term value because of review overhead.
Surprisingly, the models predict no gain in long-term value for review before inclusion.
We compare the models to data from MovieLens, where some contributors used a review before inclusion system while others used review after inclusion.
It turns out that the model is correct that making contributions available immediately wins in the short term, and it fits the data fairly well.
However, we do not have enough data to evaluate the long-run prediction that both systems will achieve the same level of value.
It keeps a modest amount of information about movies, including directors and actors, movie genres, languages, release dates, and video availability.
Members can use this information to filter recommendations.
Its movie information is incomplete.
For most of its life the MovieLens database has been maintained by a single movie guru.
When the guru is busy, the database suffers.
Sometimes he does not add actors and directors, movies released on DVD are not always updated as "new DVDs," and so on.
About 1/3 of the fields in the database are blank.
This has a direct impact on the value of MovieLens, for example, when searches fail to return relevant movies.
To improve the database, we created an interface that allows MovieLens members to edit information for movies .
Theoretical justification People often find recommended items valuable; personal value increases motivation.
Editing is easier if you know the movie; easier tasks increase motivation.
Further, rareness implies a special ability to contribute; knowing that contributions matter increases motivation.
More missing information allows more valuable contributions; knowing that contributions matter increases motivation.
Practical justification Almost any community that models its members can compute some notion of relevance.
Also easy to implement, and targets effort toward items that might not otherwise be corrected.
Chooses tasks that provide the maximum potential to increase value.
Similar to Wikipedia's Community Portal.
Easy and provides wide coverage.
We publicized the ability to edit movie information by asking people to edit selected movies to edit on the MovieLens main page .
Prior to this, only the guru had the power to edit movies in the database.
The main page displays a list of chosen movies and a list of recent movies.
The chosen list contains movies selected by one of four intelligent task routing algorithms.
The recent list, similar to Wikipedia's recent changes, contains the most recently edited movies.
Each list showed five visible movies and a link to 15 more.
Members were also able to edit information for any movie they saw while using MovieLens.
More nuanced value metrics are possible, such as asking members to flag high- and low-value items or using read and edit wear  to estimate value.
Another indicator of a community's success is how many members participate in the CALV's upkeep; communities that spread work among many members are more inclusive and robust.
This leads to the third metric, Neditors, the number of people who edited at least one movie.
We supplement these metrics with results from a survey we conducted after the experiment concluded.
119 people, most of whom used the editing features, responded to the survey.
MovieLens members who logged in during the experiment were randomly assigned one of four task routing algorithms shown in Table 1.
They were also randomly assigned to one of two contribution review systems: Wiki-Like, where contributions were made visible immediately, or Pre-Review, where contributions needed to be reviewed by another PreReview subject before becoming visible.
We placed few restrictions on editing.
Members who had been with MovieLens for at least one month were eligible.
We limited subjects to 10 edits day to make the analysis less sensitive to huge outliers.
We debated this--why not let users do what they want?--but it also has reasonable practical justifications: involving more users increases community robustness, while industrious subjects are discouraged from writing scripts to automate contributions using data from other websites .
Finally, Wiki-Like subjects were not allowed to edit movies pending review by a Pre-Review subject, and Pre-Review subjects were not allowed to review their own edits.
We collected behavioral data for 53 days in summer 2005.
A total of 2,723 subjects saw the contribution interface, with 437 subjects editing at least once.
They performed a total of 2,755 edits.
Of these editors, the mode was one edit, though two subjects edited well over 100 times--quite a feat considering the 10 edit per day limit.
Editing activity appears to follow a power law distribution, a pattern we have seen in many aspects of MovieLens, from movie popularity to the number of logins, ratings, and forum posts per user.
A crucial aspect of whether a community succeeds in building a CALV is how much value the community creates.
A coarse metric, Nedits, counts the number of edits people make.
A finer-grained metric, Nfields, counts the number of non-blank fields for a movie.
Nfields is not very precise because it cannot detect when bad information is corrected.
We chose it because many communities should be able to develop similar syntax-based met-
We turn now to our first research question: How does intelligent task routing affect contributions to a CALV?
A community might want to match members with tasks for a number of reasons.
For example, it might be useful if people who review recent changes in Wikipedia know the topic they are reviewing.
User modeling research shows how to build profiles of user interests  and expertise  that can help match people with topics.
Another reason to match people with tasks is to reduce their workload.
Wikipedia's recent changes list is long because people make dozens of changes per minute.
A personalized view that shows only pages the viewer is likely to work on might increase viewers' ability to contribute.
A community might also prefer to focus on tasks that seem most needed.
Wikipedia could highlight large recent changes rather than small ones or solicit people to expand short articles rather than review recent changes.
But because NeedsWork selects movies that have the most missing information, its per-edit improvement  is much higher than for the other strategies .
Finally, HighPred is dominated by RareRated on all metrics and by Random on Nedits and Nfields.
Its per-item improvement is especially low.
Karau and Williams' collective effort model  calls out factors that influence people's motivation to contribute to groups.
These factors include how much the person values the task, how much effort the task requires, and how much they believe their contribution matters to the group.
Communities that model members' behavior can try to operationalize these factors and use them to stimulate contributions.
We developed four algorithms to match people with movies: HighPred, RareRated, NeedsWork, and Random.
Table 1 gives a brief description for each algorithm along with practical justifications and reasons why they might motivate contributions based on the collective effort model.
We chose simple, single-strategy algorithms because they are easy to understand and easy to implement.
Further, they represent algorithms used by real communities.
Wikipedia's Community Portal is based on a NeedsWork-like algorithm, while Slashdot assigns meta-moderation using Random.
When a subject logs in, the algorithm ranks all movies, removes movies chosen for any subject in the last four hours , and populates the subject's chosen list with the top 20 movies.
We computed statistics about the movies each algorithm chose in order to better understand these differences.
Table 2 shows that the four algorithms had distinct patterns of selecting movies.
The first three rows tell us that the algorithms accomplished their goals: HighPred chose movies with high predictions, RareRated chose movies with relatively few ratings, and NeedsWork chose movies missing the most information.
Otherwise, the algorithms chose movies of roughly the same quality and predicted rating.
HighPred chose relatively popular movies, while RareRated and NeedsWork both choose relatively obscure movies.
NeedsWork chose many fewer distinct movies than the other algorithms because it is not personalized--it always chose the movies that needed the most work.
Similarly, the usermovie pairs row from Table 2 shows that HighPred and RareRated tended to show the same movie to the same user multiple times.
In particular, several subjects in the RareRated group complained they wanted to see movies in their chosen lists that they had not already edited.
Designs that choose different movies each time they ask a user to contribute might perform better.
To concentrate on the algorithms' effect on behavior, here we limit the analysis to the five movies visible on subjects' chosen lists.
For this experiment we collected data for 24 days in August 2005.
In these data, 197 of 1,982 subjects edited at least one movie chosen for them.
Figure 3 shows the performance of each algorithm on Neditors, Nedits, and Nfields.
We use Random as our baseline.
The difference on Neditors was striking: over 22% of RareRated subjects edited at least one movie, compared to about 6% for the other groups.
RareRated was the most effective strategy for convincing people to contribute.
It has a number of aspects that might increase people's motivation to participate according to the collective effort model.
First, people are more likely to know about movies they have seen; editing known movies is easier and easier tasks increase motivation.
Second, people are more apt to like movies they have seen; personal value increases motivation.
Third, being one of a few people who has seen a given movie might induce people to feel their contribution is harder to provide and thus matters more; knowing that contributions matter increases motivation .
We used our post-experiment survey and a logistic regression model to help tease apart these factors.
The survey asked subjects people questions corresponding to the three factors above.
Table 3 shows people were most likely to agree that having rated a movie matters and least likely to agree that rareness matters.
We also built a logistic regression model to predict whether a movie was edited using four attributes: whether the subject had rated the movie, the subject's predicted  rating for the movie, log of the movie's popularity, and the movie's Nfields score.
The first three correspond to the three reasons RareRated might increase motivation, while the last seemed important to include.
We used only movies shown to the Random group to create an unbiased sample.
Having rated the movie and the movie's Nfields score are useful predictors ; liking the movie and movie popularity were not useful.
Taken together, these results suggest lead us to hypothesize that a RatedNeedsWork algorithm would be a good alternative to explore in future research.
There are many design questions when building a system for reviewing contributions, including whether review is needed at all, who can be a reviewer, and whether contributions need to be reviewed before the community can use them.
We have explored the first two design questions in the context of adding movies to MovieLens .
We found that at least some review is required both to prevent anti-social behavior and to encourage people to contribute.
We also found that peers were about as good at reviewing as experts, at least in MovieLens.
Further, people were just as willing to contribute whether peers or experts provided review, supporting the goal of using peer-based review systems as a mechanism for building scalable, robust, and valuable CALVs.
Here, we concentrate on the timing of review.
As we saw in the introduction, both including contributions immediately and reviewing contributions before accepting them can succeed.
We tried both approaches during the field experiment, randomly assigning members to one of two conditions.
In the Wiki-Like condition, contributions went directly into the MovieLens database.
In the Pre-Review condition, contributions went to a queue of pending edits and only went live after being reviewed  by a second member.
Subjects saw a recent list that contains the items most recently edited by others in their group.
The recent list displayed five movies with a link to another 15.
Figure 4 shows how the interface differed for the two groups.
Below we present a model of how review timing affects CALV quality, then use our experimental data to test the model's predictions.
We combine modeling with a field experiment to build support for our main result: accepting contributions immediately is a win in the short term and will do just as well in the long run.
These results show that intelligent task routing has large effects.
NeedsWork is intuitively appealing from the community's point of view, maximizing the value the CALV gets per edit.
However, it fails to consider individual motivation.
RareRated did well because it both personalizes the movies shown and, importantly, chooses movies people have seen.
The poor performance of HighPred suggests that information retrieval-style relevance by itself is not enough.
Having rated an item was so important that any task routing algorithm should consider members' experience with items when possible.
A natural question is, given this, what should a task routing algorithm consider next?
We explored this question by building a second logistic regression model to predict editing of movies shown to the RareRated group.
Nfields and the person's rating are useful predictors , while popularity and average ratings were not useful.
Since Nfields matters, developing useful measures of item quality is a logical next step in improving task routing.
Survey respondents believe Wiki-Like systems would increase value more quickly while Pre-Review systems would result in higher long-term value.
We present a model that suggests they are half right.
The model predicts how review timing will affect the quality of CALVs.
We make a number of simplifying assumptions in order to get at the heart of the effect of review timing, following Axelrod: "If the goal is to deepen our understanding of some fundamental process,
Many of these assumptions can be lifted by adding complexity to the model, but we believe the high-level predictions of the simple model apply to many CALVs.
In the model, a CALV consists of a number of items, each of which has some value.
Value might simply be the presence of an item: a MovieLens movie, a Wikipedia page, or a response to a Usenet post.
Value might account for quality, perhaps by asking people to flag low-quality items.
Value might also include frequency of use, by counting page views in Wikipedia or by weighing fields in MovieLens based on their use in searches.
We assume an item's value ranges from 0 to some maximum, that the value of a given item does not change unless someone contributes to it2 , and that the number of items remains constant over the modeling period.
The CALV as a whole then has a value, V , ranging from 0 to Vmax , the sum of the maximum value of all items.
Let Vt be the value of a CALV at time t. We assume that time proceeds in discrete periods.
During a time period, the community can increase the value of items, e.g.
We model the amount of value the community creates in a given time period as Gt , or "good work".
The community also sometimes destroys value , which we model in the aggregate as Bt , or "bad work".
Good and bad work can overlap during the same time period.
We can state a basic model of how a CALV's value evolves: Vt+1 = Vt + Gt - Bt 
A little algebra shows this equation is satisfied when Pt = /, allowing us to compute the equilibrium:  Vlim = Vmax   In other words, a Wiki-Like system reaches a value equilibrium below its potential that is determined only by the proportion of good to bad effort members are willing to expend.
Gt and Bt are not constant; otherwise, the equation above suggests that CALVs can grow without limit.
Many factors influence how much value is created and destroyed in a given time period, including the CALV's current state and the motivation and abilities of community members.
The intuition is that as Vt grows, members will find it harder to locate useful work and easier to damage already-existing value.
Much of the work the community would do, then, is expended in finding work rather than doing it.
Let  be the amount of good effort the community as a whole is willing to expend in a given time period to improve the CALV, and let  be the similar amount of bad effort it would expend to harm the CALV.
Remembering that 0  Vt  Vmax , we let Pt = Vt /Vmax be the proportion of its potential value a CALV possesses at time t. We extend equation 1 to incorporate the task of finding work, by multiplying  by the probability that a given item needs work and  by the probability that a given item is already correct.
We now turn to modelling the Pre-Review system, where a second person must approve contributions before they are added to the CALV.
Here, value is divided into two parts: working on items, and approving  work.
Now, the formulae for Gt and Bt must account for the fact that some of the community's contributions are allocated to editing items and some to checking those edits.3 One of four things can happen to a contribution, depending on whether the editor and the checker are good or bad.
Table 4 shows a payoff matrix that models "bad" people that maliciously destroy content, one of the most common threats seen in wikis.4 If both the edit and check are good, value is created.
If both are bad, value is removed.
If one is good and one bad, value is unchanged: either a bad edit is appropriately rejected or a good one is incorrectly rejected.
We now compute how the value of the CALV changes during a given time.
To do this we must first know how much total work is done.
3 The work of checking in Wiki-Like systems is represented implicitly in  and  : some of the effort of  at time t +1 will repair errors introduced through the effort in  at time t. 4 But not the only possible matrix.
Bad actors might sometimes create value, e.g., a spammer might approve good contributions in order to increase the community's value as a market.
The formulae for all four squares in the contingency table have the same structure.
To compute the number of times a Bad Edit followed by a Good Check occurs, we multiply this probability by the total number of edits performed: Wt /2 since half the work goes into edits.
Table 5 presents all four formulae along with a numeric example.
Now we are ready to apply equation 1 and the payoff matrix from Table 4.
In this case Gt is just the number of edits that wind up in the Good-Good quadrant, while Bt is the number of edits that wind up in the Bad-Bad quadrant.
Again, we can figure out the equilibrium for the value of the CALV, Vlim , which happens when: 1 1 = 2 Wt 2 Wt Ht = C t  = Pt  We saw this equation when finding the Wiki-Like equilibrium; the equilibrium for a Pre-Review system is:  Vlim = Vmax   which is surprisingly the same as Equation 3!
That is, if Vmax ,  , and  are the same, checking before accepting contributions does not improve the eventual value equilibrium the CALV reaches, compared to accepting them right away.
We first apply the model to a slightly fictionalized version of adding movies to MovieLens.
Our movie guru took over the MovieLens database about 6 years ago, when it had about 4,000 movies.
Based on his criteria for adding movies to MovieLens, there are about 10, 000 movies eligible to include.
Based on his behavior, we can estimate that  = 40 and  = 0 if only the guru adds movies.
We will discuss how to estimate these parameters later, when we evaluate the model against the experimental data.
We can use the model to look at how the database might have evolved if we had allowed more people to participate.
Based on data from , we estimate the community's  = 160 and  = 40 for the task of adding new movies to MovieLens.6 With these estimates, we can compare what might have happened had we allowed the community to add movies.
Figure 5 shows that the value of the database would have grown much faster than it did, and further, it would have grown even faster if contributions were made visible right away.
The guru would eventually create a more valuable database--in five years.
The model relies on a number of assumptions about how editors and checkers interact through the CALV, which we collect here for convenience.
One time step completes before the next begins.
Good and bad activities within each time step may overlap.
The value of the CALV is the sum of the value of individual items.
The value of an individual item only changes when someone changes it.
There is a fixed maximum attainable value for the CALV.
The total amount of available effort is stable while the value of the CALV changes.
Users randomly encounter items to work on.
The effort to fix an item, to check whether an item needs fixing, and to check whether a fix is correct is about the same.
In Pre-Review, as many checks happen as edits.
Anyone can edit or check in Pre-Review.
For instance, people may give up on a CALV with low value or one that grows too slowly, violating Stable Motivation.
Rapidly growing CALVs like Wikipedia violate Value Ceiling.
Cumulative Value fails to model the fact that some items are more popular than others.
Many review systems violate No User Roles by having relatively few editors who are allowed to check contributions.
We nevertheless believe these are reasonable assumptions for understanding the broad effects of review timing.
The model makes several interesting predictions.
The most surprising is that the final quality will be exactly the same for the Wiki-Like and Pre-Review methods of making changes to a CALV.
Proponents of both models, from people discussing Wikipedia on Slashdot to several of this paper's authors, argue that their favorite model is better.
Finding they should yield the same quality in the long run was a surprise.
A second interesting prediction is that Wiki-Like will converge much faster to this long-run quality.
The reason is simple: Wiki-Like does not waste as much effort checking.
Survey responses confirm that people prefer editing to checking .
Pre-Review had a higher Nfields per movie changed in the database than Wiki-Like .
The average increase in Nfields for both groups on the initial edit of a movie was almost identical, so the increase happened when PreReview reviewers made changes that improved the original contribution.
Note that the model does not account for this increase; by using the payoff matrix in Table 4, it assumes that reviewers can only approve or reject changes without adding value of their own.
We now compare the model to the experimental data.
The model makes three high-level predictions about the WikiLike and Pre-Review systems.
We will first fit the model to our data, then evaluate how well it fits and whether its predictions are accurate.
We use the Nfields metric.
The sum of Nfields when the experiment began was V0 = 47, 280.
To estimate  and  , we use observed behavior for the first half of the experiment.
The ratio of good to bad edits was Gt /Bt  60.
This lets us estimate Bt  11 and Gt  660.
Similar calculations for the Pre-Review group give   3, 240 and   54.
Based on those estimates, Figure 6 compares the model's predictions to actual behavior.
We now examine what happened during the experiment, then use that data to evaluate the model.
Table 6 shows subjects' behavior under the two systems.
Pre-Review outperformed Wiki-Like on total Nedits and Neditors.
However, Wiki-Like subjects made more edits that appeared in the database.
These differences happen because of checking in the PreReview group.
This left 212 edits pending approval at the end of the experiment--wasted work because these contribu-
The predictions fit reasonably well, overestimating somewhat for both groups.
The quality of the predictions also depends on how much data is available for estimates.
Had we only collected data for one or two weeks, our estimates of  would be 30 to 40 percent higher because contributions taper off faster than the model predicts.
Its predictions over the experimental period are nearly linear because  +  is small relative to the maximum value of repository.
Finally, we do not have enough data to evaluate the model's third prediction that Wiki-Like and Pre-Review will converge to the same value equilibrium in the long run.
Note that the model missed on Pre-Review in two ways.
First, our payoff matrix from Table 4 was incorrect.
Checking added about 40% more value to an initial edit because reviewers were allowed to improve contributions they checked.
This means 1.4 would have been a more accurate value in the Good-Good quadrant.
Second, the model did not account for wasted work because of the Edits Equal Checks assumption.
In this setting, about 1/4 of edits were never checked.
These two effects roughly balanced each other here.
This is not likely to be true in general, and a more complex model that accounts for these effects might be more directly useful in designing systems.
Although Wiki-Like systems accumulate value more rapidly, they also allow more bad content into the CALV that is eventually corrected by other members.
This is often held against Wikipedia: even though there is much good content, members may not know which content to trust, thus limiting Wikipedia's usefulness as a reference--and perhaps reducing their motivation to contribute.
Recent pages on fake pop stars and articles modified by marketers show that this concern is real.7 On the other hand, bad content is often quickly removed, with obvious vandalism disappearing in minutes .
Incorrect content may take longer to fix.
Extending the model to consider the amount of bad content seen by members might be useful.
The model could be extended in a number of other ways as well.
In the experiment, we noted that reviewers can add value and that work can be wasted.
The model assumed these effects away, but incorporating them would not be hard.
More complete  models that reduce the number of necessary assumptions may be more useful to designers.
Enhancing the model's ability to account for the cost of finding work is a natural next step.
Systems that help people find work, perhaps using intelligent task routing, will be able to redirect effort from finding useful work to doing it.
This will improve their ability to create value, and the model should account for that as well.
The model is an interesting starting point for thinking about designing systems that encourage contribution, but it is by no means the last word.
These results suggest that Wiki-Like systems create value faster than Pre-Review ones.
More people contributed under the Pre-Review system overall, but since people prefer editing to checking, a backlog of wasted work builds up.
The model also suggests that the Pre-Review group will do about the same as the Wiki-Like group in the long term.
Our estimates of  and  put Vlim near Vmax for both systems because  is small in MovieLens.
This prediction would be easier to test in a system with more bad contributions.
At a high level, the model accurately reflects the relative behavior of the Pre-Review and Wiki-Like systems.
However, contributions taper off faster than predicted.
As a new feature, the contribution interface might have been used more heavily at first than it would be in the long-term, violating the Stable Motivation assumption.
This would explain the overestimation and the rapid taper.
It might also be that  and P are not the right probabilities of finding useful work to do.
Finally, it may be that counting all fields equally made our value function too simple.
Members rarely search for films available on VHS; perhaps they would not notice or care enough to fix errors in a movie's VHS release date.
Pre-Review systems may increase people's willingness to contribute  or deter people from damaging the system  compared to Wiki-Like.
Here, the PreReview group had more editors and total contributions, while prior work showed that review before acceptance reduced antisocial behavior compared to a system with no review .
Designers might use the model to reason about trade-offs between short-term speed and long-term quality.
Fielding a Wiki-Like system until contributions taper off and then switching to a higher-equilibrium Pre-Review system may let designers have it both ways.
This work takes a number of steps toward improving system design for community-maintained artifacts of lasting value.
Even simple algorithms have large effects.
We are excited by the potential of CALVs to increase the value and scope of community on the web.
We predict a similar rise of groups that build these lasting, valuable community-specific resources.
Understanding how to build the tools that will help these groups survive and thrive is an important next step for the CHI and CSCW communities.
