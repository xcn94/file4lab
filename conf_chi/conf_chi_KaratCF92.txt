We investigated the relative effectiveness of empirical usabitit y testing and individual and team walkthrough methods in identifying usability problems in two graphical user interface office systems.
The findings were replicated across the two systems and show that the empirical testing condition identified the largest number of problems, and identified a significant number of relatively severe problems that were missed by Team walkthroughs the walkthrough conditions.
About a third of the significant usability problems identfled were common across all methods.
Cost-effectiveness data show that empirical testing required the same or less time to identify each problem when compared to walkthroughs.
Use of walkthroughs has been encouraged by development cycle pressures and by the adoption of development goals of efficiency and user-centered design .
Many questions remain about how walkthroughs compare to empirical methods of usability assessment, and when and how walkthroughs are most effective.
Software development teams work within cost, schedule, personnel and technological constraints.
In recent years, usability engineering methods appropriate to these constraints have evolved and become increasingly incorporated into software development cycles.
I-Iuman factors practitioners currently rely on two types of tectitques to evaluate representations of user interfaces:  empirical usability testing in laboratory or field settings; and  a variety of usability walkthrough methods.
These latter methods have substantive differences and are referred to as pluralistic walkthroughs, heuristic evaluations, cognitive walkthroughs, thinkaloud evaluations, and scenario-based and guidelinebased reviews .
Empirical usability testing and walkthrough methods differ in the experimental controls employed in the former.
If the methods differ in their effectiveness in identifying usability problems in user interfaces, do these differences persist across different systems?
Or is the effectiveness of an evaluation method system dependent, based on the type of interface style and metaphor used in the interface?
What is the relative cost-effectiveness of the two techniques in identifying the usability problems in an interface?
What amount of human factors involvement is necessary in the use of the two techniques?
What issues arise in analyzing and interpreting data?
Are walkthroughs more effective when conducted individually or in teams?
Social psychology has documented that groups seldom perform up to the level of their best member .
One exception in this area is that groups do offer the possibility of more accurate judgments than individuals, especially when working on complex tasks .
The use of interaction-enhancing procedures may heighten group productivity as well .
Are members of development Evaluator expertise.
Permisslon to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and tts date appear, and notice is given that copying ISby permission of the Association for Computing Machinery.
To copy otherwise, or to republish, requires a fee and/or specific permission,
Prescribed tasks versus se~-guided exploration.
In a usability walkthrough with prescribed tasks, evaluators step through a representation of a user interface  or the actual system while performing representative end user tasks.
Other walkthrough procedures rely on self-guided exploration of the interface by evaluators who may or may not generate scenarios for that purpose.
Do evaluators in walkthroughs think that one approach is more useful than the other in identifying usabfit y problems?
What is the role of usability Utility of guidelines.
Are heuristics useful and necessary for experienced members of development teams?
The design-test-redesign cycle is reduced to minutes through the use of low-technology prototypes to illustrate alternative designs, and through the presence and cooperation of individuals with the varied skills required to complete the work.
This technique highlights the value of rnultidkciplinary activity in design  and group problem solving , and the iterative design possible within tight time constraints.
A question arises about the group facilitation skills and procedures required for human factors engineers to achieve high group productivity and accurate judgments in the walkthroughs .
Nielsen and Molich  tested heuristic evaluations completed individually by evaluators who were not human factors experts.
In three of the four studies reported, evaluators received a lecture on nine usabiMy heuristics and related reference material.
No prescribed task scenarios were employed.
Aggregates of data from five computer science students or industry computer professionals generally found about two-thirds of the problems that the authors had previously identified in the interfaces.
While the study measured how hard it was to fmd problems, there was no measure of the severity of the identified usability problems.
There is some evidence that evaluators other than UI specialists can carry out useful and successful thinkAlso, aloud and heuristic walkthroughs .
Jorgensen  and Wright and Monk  both provide evidence of the success of a think-aloud technique used by developers who had rniniial training on the procedure and limited use of human factors resource.
Developers observed users who thought aloud while working through tasks on the developers' systems.
Walkthroughs were done by individuals in the former study and by teams in the latter.
Questions arise about the numbers and types of problems that were not identified in these studies, and how data on problems were interpreted and analyzed.
Recent studies provide some data on these issues.
Jeffries, Miller, Wharton, and Uyeda  compared the effectiveness of usability testing, guideline, heuristic, and cognitive walkthrough  methods in identifying The heuristic method diverged user interface problems.
Prescribed task scenarios were employed in usability testing and cognitive Results showed that the heuristic Walkthroughs.
Usability testing was generally the second-best method of the four in identifying problems.
A number of questions arise: Was evaluator expertise the key component in the effectiveness of both the heuristic and usability testing methods?
What was the amount of overlap in the problems identified by the four methods?
What was the inter-rater reliability y of problem identification?
Desurvire, Lawrence, and Atwood  compared the effectiveness of emptilcal usability testing and heuristic evaluations in identifying violations of usability guidelines.
The heuristic method differed from others  in that evaluators rated guidelines on bipolar scales for each of a set of tasks.
Laboratory testing identfled violations of six of ten relevant guidelines, while the combined results from the heuristic evaluations identified only one violation of a guideline.
The heuristic ratings from IJI experts and empirical usability test participants were predictive of laboratory user performance data.
Non-UI experts' ratings were not predictive of performance.
Iieuristic ratings were effective in identifying tasks where problems would occur, but not the specific user interface problems themselves.
UI experts' "best guess" predictions of performance to their heuristic ratings.
Our study had three goals: The f~st goal of the study was to better understand the relationship between empirical testing and walkthrough results.
This information would improve the understanding of the tradeoffs in selecting one rather than another in a particular situation.
This goal included assessment of the number and severity of usabilit y problems ident~led by the two methods and the resource required to identify them.
The second goal was to determine whether the results regarding the relative effectiveness of empirical and walkthrough methods were reliable and would replicate across systems, or whether these results were system dependent.
The third goal of the study was to understand how well walkthroughs work in user interface evaluation and how to improve their effectiveness.
An effective walkthrough method would be one that identifies most usability problems in an interface, and especially the most severe usability problems.
The role of individuals and teams, evaluator characteristics, scenarios, self-guided exploration, and usability heuristics in walkthroughs were explored as part of this third goal.
Three user interface evaluation methods were assessed: test, empirical usabiiit y individual usability waikthrough, and team usability waikthrough.
The usability waikthrough procedure developed and used in this study included components to maximize effectiveness of usability problem identflcation, based on previous research.
The procedure utilized a set of 12 usability guidelines.
Walkthroughs were conducted individually by six evaluators in one condition and by six pairs of evaluators in another condition.
The evaluators in the team condition conversed with each other about issues and problems during the sessions.
The evaluators were responsible for documenting the usability problems they identfled in the waikthroubs.
The usability problems identfled through use of the three methods were categorized using common metrics.
Thus data could be compared across methods on dimensions including number and severity of usability problems identfled in the interface.
The three methods were each applied to two competitive software systems in order to assess the reliability of the fmdmgs.
The usabfity tests and walkthroughs were completed as if part of a realistic development schedule with resource constraints so that the data could provide practical information on usability engineering in product development.
The two systems selected for the study were commercially available GUI office environments with integrated text, spreadsheet, and graphics applications.
They will be referred to as Systems 1 and 2, The two systems difTered substantially in the type of interface style and oflice metaphor presented.
Human factors staff consulted with end users and developed a set of nine generic task scenarios to be used with both systems.
These scenarios were representative of typical office tasks involving text, spreadsheet, and graphics applications, and use of the system environment.
The tasks included a range of one to thirteen subtasks and covered document creation, moving and copying within and between documents, linking and drawing, printing, interface updating documents, customization, finding and backing up documents, and use of system-provided and user-generated macros.
A two-page document of guidelines was developed for the evaluators in the waikthrough conditions, The document told evaluators their assignment was to identify usability problems with the interface initially by exploring the interface on their own and then by walking through typical tasks provided to them.
A usability problem was defined as anything that interfered with a user's ability to efllciently and effectively complete tasks.
Evaluators were asked to keep in mind the guidelines about what makes a system usable and to refer back to them as necessary.
Following the precepts of minimalism , the document provided brief definitions and task-oriented examples of twelve guidelines.
These guidelines were compiled from heuristics used by Nielsen and Molich , the 1S0 working paper on general dialogue principles , and the IBM CUA user interface design principles .
The twelve usabdit y guidelines included:
For each system, the empirical test and individual waikthrough each utilized six participants, and the team waikthrough utilized six pairs of participants.
A total of 48 participants took part in the study.
Participants were randomly assigned to methods; team members did not know each other.
Participants had not previously used the GUI system they worked with in the study.
The six groups were comparable based on background data gathered prior to usability sessions.
Participants were predominantly end users and developers of GUI systems, along with a few UI specialists and software support stti.
Most of the participants had advanced educational degrees; used computers in home, work, and school settings; and had used a variety of computers, operating systems, and applications.
They used computers approximately 20 hours a week, including over 10 hours a week on GUI systems.
Except for more formal education, the participants were typical of those who would participate in usability walkthroughs and empirical testing of GUI systems in product development.
Usability sessions for all methods each took about three hours.
The f~st half of the usability sessions included an introduction by the usability engineer who administered the sessions, and a self-guided exploration of the system by the participants.
During the selfguided exploration, participants could go through cmline tutorials, read any of the hard copy documentation shipped with the system, use and modify example documents created using the dtierent applications and system functions, or create new application documents and experiment with system functions.
In the second half of the sessions, participants worked through a set of nine typical tasks presented in random order and completed a debriefing questionnaire given by the administrator.
The emptilcal testing and walkthrough sessions differed in human factors involvement in the session and in how usability problems were documented.
In empirical testing, two usability engineers adminktered each session in its entirety with an individual user.
One person in the control studio interacted with users , controlled the videotape equipment, and observed usability problems.
The second person in the control studio logged user comments, usability problems, time on task, and task success or failure.
Usability staff involvement in the waLkthrough sessions was limited to test the resource requirements of the method, One administrator was available on-call during the session in case of unexpected events.
A few sample sessions were videotaped and observed by human factors sta~ no session logging occurred.
One administrator introduced the session and instructed walkthrough evaluators in the use of the guidelines for usability walkthrough document and the usability problem description forms.
The administrator emphasized in both individual and two-person team walkthrough conditions that the problem ident~lcation sheets were the deliverable for the session.
In the team evaluators were given additional inconditions, structions to help each other by providing relevant information .
They were told that if either one of the team members thought something was a usability problem, they should record it.
The classification completed a content analysis of the problems and prepared the data for subsequent problem severity ratings.
The hierarchical model consisted of a total of 47 categories of potential user interface problem areas.
Because of the functional differences between the systems, all 47 categories applied to System 1 while only 43 applied to System 2.
Subcategories were created when a main category had several problems that were related, yet addressed different aspects of the higherlevel category.
We distinguished between problems that were pervasive through the environment and those that were application specKIc.
For example, we classfied icon complaints  as pervasive office-level problems, while confusion about specific spreadsheet functions 
To assess inter-rater reliability, 50 problem statements were randomly selected from the data for the three conditions and classtiled by two usability engineers who had not observed the participants or been involved in data analysis.
They each classified the data using the generic model of usabilhy problems.
The inter-rater reliability scores between the third-party usability staff and the staff involved in the study were 870/0 for the empirical testing data, 70~0 for the individual walkthrough data, and 710/0 for the team walkthrough data.
For each empirical testing and walkthrough group, data were analyzed regarding the number of usability problcm tokens , usability problem types , and problem areas  in the generic model.
These problem areas were assigned Problem Severity ClassKlcation  ratings.
A version of the PSC measure, which is used in IBM, was employed in the study.
It provides a ranking of usability problems by severity that can be used to determine allocation of resources for addressing user interface problems .
PSC ratings are computed on a two-dimensional scale, where one axis represents the impact of a usability problem on end user ability to complete a task, and the other represents frequency .
Categories of the impact dnension  and frequency dimension  were combined to form an index of PSC ratings that ranged from 1-3 where 1 is most severe.
High impact was defined as a problem that prevented the user from completing the task, moderate impact represented sigrMcant problems in task completion, and low impact represented minor problems and inefficiencies.
Given the small sample sizes in the condi-
Evaluators read the guidelines document and the administrator then left the studio.
After the selfguided exploration phase, the administrator returned briefly to present the task scenarios and emphasize that it was more important to identify usability problems than to complete all the tasks.
For example, if three or more of the six evaluators  reported a problem that caused sigtilcant d~lculty  in completing a task, a PSC rating of "l" would be assigned to the problem area.
Problem Severity Classification rating matrix.
The dtierence in the distribution across the groups of the total number of tokens found was statistically si@lcant for each system at the p <.01 level according to X2 tests.
Empirical testing also identified the largest number of usability problem types , followed by team and individual walkthroughs.
For both systems, the total number of usability problem types found by empirical testing was about twice the total number found by the team walkthroughs, and three times the total number found by individual walkthroughs.
Again, the diierence in the distribution of the total number of problem types found in the three groups was statistically si@lcant for each system at the p <.01 level.
The data on PSC ratings assigned to problem areas for Systems 1 and 2 are presented in Table 3.
For both systems, empirical testing identified a larger total number of signMcant problem areas  assigned PSC ratings of 1-3 than did either team or individual walkthroughs, I-Iowever, the variation in total number of SPAS across methods was statistically si@lcant for System 1  but not for System 2.
For both systems, there was no bias or tendency towards more severe ratings  in one group as compared to another.
We generated PSC ratings for each of the categories in the generic model of user interface problems.
There were 47 PSC ratings for System 1, and 43 for System 2.
To generate PSC ratings in the empirical conditions, the human factors staiT calculated the frequency of users experiencing a problem and assigned an impact score.
Disagreements about impact scores were discussed until consensus was reached.
In the walkthrough conditions, human factors staff calculated the frequency data and averaged the impact scores provided by the evaluators.
Problem areas that did not have problem tokens from at least two participants or teams were assigned a PSC rating of 99 .
Problem areas with PSC ratings of 1-3 were called sigdcant problem areas , and those with ratings of 99 were called "no action" areas.
For the walkthrough conditions, evaluator questionnaire data on aspects of the walkthrough procedure were collected during the debriefing sessions and analyzed.
For the empirical conditions, data on time on task, completion rates, and the debriefing questionnaire were collected but are not reported here.
For Systems 1 and 2, empirical usability testing identified the largest number of usability problem tokens , followed by team walkthrough and then individual walkthrough .
For both systems, -- Empirical Test System 1 Problem Tokens Problem Types System 2 Problem Tokens Problem Types Table 2.
Table 4 provides information on the number of unique usability problem areas identified by each of the methods.
A problem area that is unique to a method is a SPA that is identified by only one method.
For Systems 1 and 2, empirical testing ident~led the largest number of unique problem areas.
For both systems, two-thirds or more of these unique problem areas were assigned a PSC rating of 2, representing relatively important problems in the user interfaces.
A common usability problem area across methods occurred when a SPA was ident~led by all three methods.
To analyze the proportion of problem areas that were common, the total number of SPAS for each system was computed.
The total number of SPAS ident~led for System 1 was 41 .
The total number of SPAS identified for System 2 was 29 .
13mptilcal Test 1 13 2 8 usability Team Individual Walk 0 2 areas.
Table 5 shows the cost-effectiveness data for the three methods on the two systems.
This analysis includes the time required by human factors staff and participants; no laboratory facility costs are included.
Iluman factors time includes preparation of all materials , administration of sessions , and data analysis .
Time to analyze the data using the generic model of problem areas and the PSC matrix are included for all groups.
As expected, the total hours required  for a method was highest for empirical testing for System 1 and System 2.
Additional analysis of the effectiveness of team as compared to individual walkthroughs was conducted by studying the total number of problem tokens found by each individual walkthrough evaluator or walkthrough evaluator team.
This analysis showed that teams found more problem tokens than did individual walkthrough evaluators for each system .
For System 1, the average number of problems identified by the walkthrough teams was 19 while the average for individual walkthrough evaluators was 13.
For System 2, these values were 18 for team and 11 for individual walkthroughs respectively.
However as shown in Tables 2 and 3 above, while more problem tokens and types were identiiled by team walkthrough conditions, the total number of SPAS identified was similar for both team and individual walkthroughs, and the pattern held across systems.
During the debriefing, evaluators rated the relative usefulness of scenarios as compared to self-guided exploration in identifying usability problems in the systems.
The evaluators used a 5-point scale where a score of 1 was the most positive response for use of scenarios.
All walkthrough groups favored the use of scenarios over self-guided exploration; the average score across systems was 1,8.
Evaluators were also asked about the added value of using the guidelines during the walkthrough.
A 5-point scale was again used and a score of 1 was the most positive response for guidelines.
For both systems, the walkthrough evaluators thought the guidelines were of liiited added value to them in identifying usability problems; the average score across systems was 3.9.
The evaluators said they thought the brief document was very effective in explaining and giving examples of the guidelines, and that they would not change the format.
They stated that because of their experience with GUI and other systems, they were already fiuniliar with the guideline concepts, but that less experienced users would fmd it very useful.
It was noted that almost all evaluators tried to take the guideline document with them at the end of the session.
I-Iowever, empirical testing needed only about half as much time as the walkthroughs to fuld each usability problem type.
For System 1, the hours required to identify each significant problem area were fairly similar across techniques.
For System 2, the resource required for team walktlu-ough was higher than for both empirical testing and individual walkthrough.
The findings regarding the relative effectiveness of empirical testing and walkthrough methods were generally replicated across the two GUI systems.
It is not clear whether these patterns would be replicated on non-GUI systems, however, the significant differences in the style and presentation of the two GUI systems in the study support the reliability of the results across these types of systems.
The empirical testing condition identified the largest number of problems, and identified a sig@cant number of relatively severe problems that were missed by the walkthrough conditions.
These data are consistent with Desurvire et al.
All three studies do provide strong support for the value of UI expertise though.
We recognize that the basis of our empirical usability testing results was the experimental controls employed, the skiUs required to conduct the test, experience with the two GUI systems prior to observing test sessions, and the UI expertise required to recognize and interpret the usability problems encountered by the users.
Our data suggest that this type of empirical usability testing should be employed for baseline and other key checkpoint tests in the development cycle where coverage of the interface and identification of all sign~lcant problems is essential.
W alkthroughs of the type in this study are a good alternative when resources are very limited  and may be the preferred method early in the development cycle for deciding between alternative designs for particular features.
This might be explained partially by the differences in procedures.
Data for the methods in our study were collected across a three-hour time period.
In the Jeffries et al.
About a third of the significant problem areas identitled were common across all methods.
The degree of overlap is encouraging, but it should caution human factors practitioners about the tradeoffs they are making in employing one method rather than another.
These methods are complementary and yield different results; they act as different types of sieves in identifying usability problems.
Jeffries  stated that there was less overlap in problems found by any two of the methods in their study  than in ours.
The higher degree of overlap in our study might be partially due to the fact that all methods used the same scenarios.
These scenarios were rich and complex examples of typical work that end users need to perform and may have greatly aided in the evaluation of the systems by all methods.
The overlap between the two methods that used their scenarios in Jeffries et al.
The fact that any differences emerged between the team and individual walkthroughs is encouraging.
The brief period of the usability session, the lack of an established working relationship between team members, and the small size of the teams may have contributed to the small differences found.
Many usability walkthroughs in product development are done by moderate-sized teams  because of the wide range of skills and backgrounds necessary to identify and then resolve usability problems.
Therefore, due to practical and organizational considerations, team walkthroughs may be an area warranting future research.
AU walkthrough groups favored the use of scenarios over self-guided exploration in identifying usability problems.
This evidence supports the use of a set of rich scenarios developed in consultation with end users.
And as evaluation work attempts to predict what will occur in real world settings, the use of well-founded scenarios can provide some assurance that real world problems will be identfled.
The evaluators, who were all experienced GUI users, generally thought that the guidelines for usability were of limited added value to them in conducting the walkthroughs, but would be helpful for less experienced users.
These data are consistent with the Desurvire et al.
Guidelines may serve to promote consensus about usability goals for development teams, and may be more useful for less experienced evaluators during walkthroughs.
The results also demonstrate that evaluators who have relevant computer experience and represent a sample of end users and development team members can complete usability walkthroughs with relative success.
Specflc UI or human factors expertise may be very heipful but is not required, and there are a multitude of practical, individual, end user, organizational, and product benefits to be achieved by involving more members of development teams in walkthroughs , Cost-effectiveness data show that empirical testing required the same or less time to identify each problem as compared to walkthroughs.
The differences between these data and the cost-benefit data for usability test and heuristic methods in Jeffiies et aL  maybe due to the differences in the walkthrough procedures utilized and in the type of data analysis performed in the two studies, If our walkthrough data had been analyzed in other ways or by different individuals , the resource required might have varied significantly.
However, the resource and skills applied to data analysis may be reflected in the quality of the Ultianalysis and the resulting changes to systems.
AnaJysis of data from one iteration in isolation is of limited utility.
The ident~lcation of usability problems is not an end in itself.
Rather, it is a means towards eliminating problems and improving the interface.
The part of the development process concerned with making recommendations for change based on the usability problems ident~led is not covered in this study.
The data from this study show that the empirical and team walkthrough conditions have the advantage over the individual walkthrough conditions in this area.
The empirical test data contained four times as many problem tokens describing a si~lcant problem area and providing context about it as compared to team walkthrough data, and teams produced 33-50% more information as compared to individual walkthroughs.
The quality of the data analysis completed and the recommendations that arise from them are issues for future research.
I low could walkthrough methods be improved?
Users in the empirical testing sessions were given opportunities to provide recommendations for changes to the usability problems they encountered, and the walkthrough sessions could be improved to capture evaluator recommendations as well.
Another area of walkthrough procedures that needs attention is the difficulty of interpreting problems.
Walkthrough evaluators used different language than the staff who analyzed the problem reports, and the data analysts' job of unrJerstanding these problem statements was made more difficult by a lack of context and lack of session observation.
The dificulty experienced in interpreting walkthrough data was supported by the lower inter-rater reliability data reported for walkthroughs compared to usability tests.
Moreover, from walkthrough sessions that human factors staff observed, it became evident that evaluators misattributed the sources of problems.
We also observed that evaluators sometimes became so involved in the task scenarios that they forgot to document problems they encountered and identfled.
We attempted to overcome this demand characteristic of the walkthroughs by emphasizing the importance of problem ident~lcation over task completion, but it was not effective in some cases, and further refinement of intervention strategies should be explored .
A better debriefing of evaluators that included reviewing identified usability problems, capturing undocumented ones that evaluators mentioned in passing, and collecting evaluator recommendations for changes might improve walkthrough effectiveness.
User interface design in large corporations: Coordination and communication across disciplines.
Group tasks, group interaction process, and group performance effectiveness: A review and proposed integration.
8, Academic Press, New York, 1975. International Business Machines Corporation, Systems Application Architecture, Common User Access, Guide to User Interface Design, , 1991. International Standards Organization.
Karat, C. Cost-benefit and business case analysis of usability engineering.
ACM SIGCHI Conference on Human Factors in Computing Systems, New Orleans, LA, April 28-May2, Tutorial Notes.
Karat, C. Cost-benefit analysis of usability engineering niques.
Lewis, C., Poison, P., Wharton, walkthrough methodology for techFL,
Nielsen, J. Usability engineering at a discount.
Elsevier Science Pulishers, Amsterdam, 1989, pp.
Nielsen, J. and M olich, R. Heuristic evaluation of user interfaces.
