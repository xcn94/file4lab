Although technology can support compelling interactive play experiences and enhance interaction and communication between players, evaluating the success of interactive play environments is an open research challenge.
Human-computer interaction research  has been rooted in the cognitive sciences of psychology and human factors, in the applied sciences of engineering, and in computer science .
Although the study of human cognition has made significant progress in the last decade, the idea of emotion, which is equally important to design , is still not well understood, especially when the primary goals are to challenge and entertain the user.
This approach presents a shift in focus from usability analysis to user experience analysis.
Traditional objective measures used for productivity environments, such as task performance, are not applicable to collaborative play.
The first issue prohibiting good evaluation of entertainment technologies is the inability to define what makes a system successful.
We are not interested in traditional performance measures, we are interested in what kind of emotional experience is provided by the play technology and environment .
Although traditional usability measures may still be relevant, they are subordinate to the emotional experiences resulting from interaction with the play technology and with other players in the environment.
Once we determine what makes an entertainment system successful, we need to resolve how to measure the chosen variables.
Unlike performance metrics, the measures of success for collaborative entertainment technologies are more elusive.
The current research problem lies in what emotions to measure, and how to measure them.
These metrics will likely be interesting to researchers and developers of games and game environments.
Our goal is to develop an evaluation methodology for entertainment environments that: 1. captures usability and playability through metrics relevant to ludic experience; 2. accounts for user emotion; 3. is objective and quantitative; and 4. has a high evaluative bandwidth.
Researchers are using emerging technologies to develop novel play environments, while established computer and console game markets continue to grow rapidly.
Even so, evaluating the success of interactive play environments is still an open research challenge.
Both subjective and objective techniques fall short due to limited evaluative bandwidth; there remains no corollary in play environments to task performance with productivity systems.
This paper presents a method of modeling user emotional state, based on a user's physiology, for users interacting with play technologies.
Modeled emotions are powerful because they capture usability and playability through metrics relevant to ludic experience; account for user emotion; are quantitative and objective; and are represented continuously over a session.
Furthermore, our modeled emotions show the same trends as reported emotions for fun, boredom, and excitement; however, the modeled emotions revealed differences between three play conditions, while the differences between the subjective reports failed to reach significance.
Emerging technologies in ubiquitous computing offer exciting new interface opportunities for entertainment technology, as evidenced in a recent growth in the number of conference workshops and research articles devoted to this topic .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Current methods of evaluating entertainment technologies include both subjective and objective techniques.
The most common methods are subjective self-reports through questionnaires, interviews, and focus groups  and objective reports through observational video analysis .
Subjective reporting through questionnaires and interviews is generalizable, convenient, and amenable to rapid statistical analysis.
Some drawbacks of questionnaires and surveys are that they are not conducive to finding complex patterns, and subject responses may not correspond to the actual experience .
Subjective techniques are good approaches to understanding the attitudes of the users, but subjects are bad at self-reporting their behaviours in game situations .
In addition, participants' reaction to new play environments might be skewed by the novelty of the entertainment technologies.
Using video to code gestures, body language, facial expressions and verbalizations, is a rich source of data.
However, coding observational data as an indicator of human experience is a lengthy and rigorous process that needs to be undertaken with great care to avoid biasing the results .
The main drawback of observational video analysis is the enormous time commitment.
There are a few consulting firms that specialize in observational analysis of entertainment technologies ; however, many researchers rely on subjective data for user preference, rather than objective observational analysis.
Standard discount usability methods, such as heuristic evaluation, are useful for uncovering usability issues within play environments; however, there has been minimal research on using heuristics to evaluate the playability of an entertainment technology , or to evaluate the impact of emerging technologies.
Most importantly, these discount methods do not involve actual users, but are administered by usability specialists.
When research involves incorporating novel technologies into a play experience, there are no "experts".
At this point, experts can only guess how the technologies will impact users.
Think-aloud techniques  cannot effectively be used with entertainment technology because of the disturbance to the player, and the impact they have on game play.
To avoid disrupting the player during the game, researchers can employ a retrospective think-aloud technique.
Although informative, this technique qualifies the experience, rather than providing concrete quantitative data.
In addition, retrospective think-aloud does not occur within the context of the task, but in reflection of the task.
Figure 1: Current methods for evaluating entertainment technologies.
Evaluators have a lot of choice, but there is a knowledge gap in the quantitative-objective quadrant.
Heuristic evaluation can be quantitative since experts can provide ratings for how well software adheres to heuristics.
Although observational analysis can be used for quantitative or qualitative results, it is not used quantitatively to evaluate play due to the time commitment and required expertise.
Traditional evaluation methods have been adopted, with some success, for quantitative-subjective, qualitativesubjective, and qualitative-objective assessment of play technologies.
Metrics of task performance are used for quantitative-objective analysis of productivity systems, but task performance is not relevant to play .
As such, there is a knowledge gap for quantitative-objective evaluation of play technologies .
In addition, the described techniques all suffer from low evaluative bandwidth .
Subjective techniques only generate data when a question is asked, and interrupting game play to ask a question is too disruptive.
Heuristics also give an overview, rather than examining change over time.
Researchers in human factors have used physiological measures as indicators of mental effort and stress .
Psychologists use physiological measures to differentiate human emotions such as anger, grief, and sadness .
However, physiological data have not been employed to identify a user's emotional states such as fun and excitement when engaged with entertainment technologies.
Used in concert with other subjective and/or qualitative evaluation methods, researchers can triangulate data sources and form a complex, detailed account of user experience.
We designed an experiment to create and evaluate a model of user emotional state when interacting with play technologies.
We record users' physiological, verbal and facial reactions to game technology, and apply postprocessing techniques to objectively and continuously measure emotional state, hence filling the knowledge gap in the objective-quantitative quadrant of Figure 1.
Our ultimate goal is to create a methodology for the objective evaluation of entertainment technology, as rigorous as current methods for productivity systems, providing more choice and robustness for evaluators.
The cardiovascular system includes the organs that regulate blood flow through the body.
Measures of cardiovascular activity include HR, interbeat interval , heart rate variability , blood pressure , and BVP.
Electrocardiograms  measure electrical activity of the heart, and HR, IBI, and HRV can be computed from EKG.
It has been used to differentiate between positive and negative emotions with further differentiation using finger temperature .
HRV refers to the oscillation of the interval between consecutive heartbeats.
When subjects are under stress, HRV is suppressed and when they are relaxed, HRV emerges.
Similarly, HRV decreases with mental effort, but if the mental effort needed for a task increases beyond the capacity of working memory, HRV will increase .
To collect EKG, we placed three pre-gelled surface electrodes in the standard configuration of two electrodes on the chest and one electrode on the abdomen.
Researchers in the domain of human factors have been concerned with optimizing the relationship between humans and their technological systems.
The quality of a system has been judged not only on how it affects user performance in terms of productivity and efficiency, but on what kind of effect it has on the well-being of the user.
There are many examples of the use of physiological metrics in the domain of human factors .
To provide an introduction for readers unfamiliar with physiological measures, we briefly introduce the measures used, describe how these measures are collected, and explain their inferred meaning.
Based on previous literature, we chose to collect galvanic skin response , electrocardiography , and electromyography of the face .
Heart rate  was computed from the EKG signal.
The measures we used will each be described briefly including reference to how they have previously been used in technical domains.
Electromyography  measures muscle activity by detecting surface voltages that occur when a muscle is contracted .
In isometric conditions  EMG is closely correlated with muscle tension .
When used on the jaw, EMG provides a very good indicator of tension in an individual due to jaw clenching .
On the face, EMG has been used to distinguish between positive and negative emotions.
EMG activity over the brow  region is lower and EMG activity over the cheek  is higher when emotions are mildly positive, as opposed to mildly negative .
We used surface electrodes to detect smiling activity  from zygomaticus major activation and frowning activity  from corrugator supercilii activation.
The disadvantage of using surface electrodes is that the signals can be muddied by other facial muscle activity, such as talking.
Needles are an alternative to surface electrodes that minimize interference, but were not appropriate for our experimental setting.
GSR is a measure of the conductivity of the skin.
Located in the palms of the hands and soles of the feet, these sweat glands respond to psychological stimulation rather than simply to temperature changes in the body .
For example, many people have cold clammy hands when they are nervous.
In fact, subjects do not have to even be sweating on the palms of the hands or soles of the feet to see differences in GSR because the eccrine sweat glands act as variable resistors on the surface.
As sweat rises in a particular gland, the resistance of that gland decreases even though the sweat may not reach the surface of the skin .
Galvanic skin response is a linear correlate to arousal  and reflects both emotional responses as well as cognitive activity .
GSR has been used extensively as an indicator of experience in both non-technical domains , and technical domains .
Physiological metrics have only recently been used in the domain of HCI.
Researchers have used GSR and cardiovascular measures to examine subject response to video and audio degradations in video conferencing software , and to investigate user response to welland ill- designed web pages .
HRV has been used as an indicator of mental effort and stress when interacting with simulators  and to distinguish between attentive states of a user .
Partala and Surakka  and Scheirer et al.
Partala and Surakka measured EMG activity on the face in response to affective audio intervention, while Scheirer et al.
Our previous work has examined physiological responses to different interactive play environments .
We showed that GSR and EMG of the jaw were higher when playing against a friend, over playing against a computer, and we found many correlations between normalized physiological activity and normalized subjective measures, including strong correlations between GSR and fun, and EMG and challenge.
We also showed how physiological measures provide a rich, continuous, and objective source of information about user experience with interactive entertainment technologies.
Based on these results, we believe that physiological metrics can be used to model user emotional experience when playing a game; providing continuous and objective metrics of emotion.
There has been a long history of researchers attempting to use physiological data to identify emotional states.
William James first speculated that patterns of physiological response could be used to recognize emotion , and although this viewpoint is too simplistic, recent evidence suggests that physiological data sources can differentiate among some emotions .
Opinions vary on whether emotions can be classified into discrete emotions , or whether emotions exist along multiple axes .
Both perspectives have seen limited success in using physiology to identify emotional states .
The arousal-valence space  used by Lang  classifies emotions in 2-D space defined by arousal and valence .
Using pictures as stimuli, Lang and colleagues mapped individual pictures to emotions as defined by the space.
Based on their circumplex model of emotion, the Affect Grid is a tool to quickly assess affect along dimensions in AV space.
Subjects place checkmarks in the squares of the grid, as a response to different stimuli .
One problem with the AV space method of classifying mood is that arousal and valence may not be independent and can impact each other.
For example, Lang et al.
It seems that if an image is truly unpleasant, it cannot also be calm, suggesting some interplay between these two axes.
In addition to the difficulties in classifying emotions, when using physiological data sources there are methodological issues that must be addressed , and theoretical limitations to inferring significance .
Discussing these issues are beyond the scope of this paper.
As with our previous work, we were not interested in whether there was a difference between playing against a friend, a stranger, or a computer.
We have observed many groups of people playing with interactive technologies, and we know that these three play conditions yield very different play experiences; rather, we were interested in whether our model of emotion could detect the differences between the conditions.
Twenty-four male participants age 18 to 27 took part in the experiment.
Before the experiment, all participants filled out a background questionnaire, used to gather information on their computer use, experience with computer and video games, game preference, console exposure, and personal statistics such as age and handedness.
Participants were recruited in pairs to ensure that they would be playing against a stranger in only one of the colocated conditions.
We wanted all of the participants to be independent subjects, statistically unrelated to any of the other participants, so we only treated one player in each pair as the participant.
As such, we designed the experiment for 12 participants in 12 pairs, and we report data for 12 participants; one member of each pair.
All participants were frequent computer users.
When asked to rate how often they used computers, all 12 subjects used them every day.
Participants were also frequent gamers, playing either computer games or console games regularly.
Participants played the game in three conditions: against a co-located friend, against a co-located stranger, and against the computer.
Order of the presentation of the conditions was fully counterbalanced.
Participants played NHL 2003 by EA Sports in all conditions .
The stranger remained constant for all participants, and was a 29 year-old male gamer, who was instructed to match each participant's level of play to the best of his ability.
Each play condition consisted of one 5-minute period of hockey.
The game settings were kept consistent within each pair during the course of the experiment.
All players used the Dallas Stars and the Philadelphia Flyers as the competing teams, as these two teams were comparable in the 2003 version of the game.
All players used the overhead camera angle, and the home and away teams were kept consistent.
This was to ensure that any differences observed within subjects could be attributed to the change in play setting, and not to the change in game settings, camera angle, or direction of play.
The only difference between pairs was that experienced pairs played all conditions in a higher difficulty setting than non-experienced players.
The experiment was conducted in a laboratory at Simon Fraser University.
NHL 2003 was played on a Sony PS2, and viewed on a 36" television.
A camera captured both of the players, their facial expressions and their use of the controller.
The game output, the camera recording, and the screen containing the physiological data were synchronized into a single quadrant video display, recorded onto tape, and digitized .
The recording also contained audio of the participants' comments from a boundary microphone, and audio output from the game.
Physiological data were gathered using the ProComp Infiniti system and sensors, and BioGraph Software from Thought Technologies.
Based on previous literature, we chose to collect galvanic skin response , electrocardiography , and electromyography of the face .
Heart rate  was computed from the EKG signal.
We only collected physiological data for the participant, not for the friend or stranger.
To maintain the perception that both players were participants in the experiment, we treated both players as if their physiological signals were being collected.
We fitted both players with sensors, tested the sensor placement to ensure that the signals were good, and plugged the extra sensors into ports on the back of the unit.
Upon arriving, participants signed a consent form.
They were then fitted with the physiological sensors.
Before each experimental condition, participants rested for 5 minutes while listening to a CD containing nature sounds.
The resting period allowed the physiological measures to return to baseline levels prior to each condition.
In prior experiments we saw that the act of filling out the questionnaires and communicating with the experimenter altered the physiological signals .
The resting periods corrected for these effects.
After each condition, subjects rated the condition using a Likert Scale.
They were asked to consider the statement,
The same technique was used to rate how challenging, exciting, frustrating, and fun the condition was.
The html-based questionnaire was filled out using a laptop computer to reduce the physiological impact of communicating with the experimenter .
After completing the experiment, subjects completed a post-experiment questionnaire.
We asked them to decide in retrospect which condition was most fun, most exciting, and most challenging.
The subjective data from the questionnaires were analyzed using non-parametric statistical techniques.
In terms of the physiological data, EKG data were collected at 256 Hz, while GSR, respiration, and EMG were collected at 32 Hz.
HR was computed at 4 Hz.
Physiological data for each rest period and each condition were exported into a file.
Noisy EKG data may produce heart rate  data where two beats have been counted in a sampling interval or one beat has been counted in two sampling intervals.
We inspected the HR data and corrected these erroneous samples.
HR data were interpolated since HR was sampled at a lower frequency than the EMG or GSR signals.
Each data signal was smoothed with a moving average window of 4 frames , with the exception of GSR, which was filtered using a 5-second window .
We then normalized each signal into a percentage between 0 and 100.
There are very large individual differences associated with physiological data, and normalizing the data is necessary in order to perform a group analysis.
We transformed each sample into the percentage of the span of that particular signal, for that particular participant across all three conditions.
Using GSR as an example, a global minimum and maximum GSR were obtained for each participant using all three conditions and the rest period,
We used the normalized GSR, HR, EMGsmiling, and EMGfrowning signals as inputs to a fuzzy logic model.
To generate values for user emotion, we modeled the data in two parts.
First, we computed arousal and valence values from the normalized physiological signals, then used these arousal and valence values to generate emotion values for boredom, challenge, excitement, frustration, and fun.
To generate a model of emotion, we used half of the participants , reserving the other six participants for validation of the model.
Details of how the fuzzy system was designed  can be found in .
The current paper presents a high-level description of the model, the comparison of the model to reported emotion, and its potential use in HCI evaluations.
To make use of the continuous nature of physiological data, we used the complete time series for each input.
Our model of physiology to AV space had four inputs  and two outputs  .
Inputs were normalized signals , while outputs were percentages of the possible maximum  value for arousal and valence.
For each input signal, the membership functions were generated using characteristics of that particular signal over all participants and conditions.
The 22 rules were grounded in the theory of how the physiological signals relate to the psychological concepts of arousal and valence.
GSR correlates with arousal, and increasing GSR was mapped to increasing arousal.
The extreme high and low levels of GSR were modulated by HR data; if HR was contradictory, arousal was altered, otherwise arousal was maintained.
Valence increased with increasing levels of EMGsmiling, and decreased with increasing levels of EMGfrowning.
A full discussion of the membership functions and rules for the model can be found in , while Figure 5 shows the surfaces generated from the model.
Using the Affect Grid , developed from the circumplex model of emotion , we translated our arousal and valence values from the first model into a language of emotion.
Five emotions were modeled: boredom, challenge, excitement, frustration, and fun.
These are the same five emotions that participants rated after each play condition.
As such, our AV to emotion model  had two inputs , and five outputs .
Membership functions for the outputs, and the rules were generated by dividing emotions into four states based on AV space: very low, low, medium, and high .
A comprehensive discussion of the membership functions and rules for the model can be found in .
Inputs and outputs were represented as percentages of the possible maximum.
To analyze the effectiveness of our model, we used data gathered from the six subjects not used in the generation of the model.
Data were smoothed and normalized using the previously described method.
Both models were applied to the data and the time series for each emotion were averaged to compare modeled emotion to the subjective responses.
Although there were no subjective differences between conditions, plotting the means reveals that there were definite trends .
Furthermore, plotting the modeled emotion means reveals the same trends for boredom, excitement, and fun .
To determine how closely the modeled  emotion resembled reported  emotion, we correlated the two data sources for each emotional state.
We used Spearman's rho, since reported emotion is non-parametric, while modeled emotion is parametric.
The subjective and physiological emotional state were significantly correlated for fun , and excitement ; the same two emotional states where the model revealed significant differences across play conditions.
Although the same trends were present for reported boredom and modeled boredom, the values for modeled boredom were very low and similar; the same problem existed with frustration.
Both of these modeled emotions suffered from issues with scaling, which is discussed later in this section.
There was a correlation for challenge , but the correlation was inverse, as seen in Figure 8 and Figure 9.
There were no significant differences from play condition for either modeled or reported challenge; however, the correlation reveals an inverse relationship.
In modeling challenge, we assumed that a player's arousal would increase with challenge; however, upon further examination, this pattern was not always true.
Some participants' comments revealed a strategy to attempt to relax when challenged, in order to improve their performance.
Obviously, how participants handle challenge in a game is an individual strategy and additional work is required before challenge can be modeled accurately.
We also examined the subjective results from the postexperiment questionnaires.
Frequencies of responses for which condition was deemed the most fun, most challenging, and most exciting were tabulated, as were frequencies for the play condition with the maximum modeled fun, challenge, and excitement.
For fun, subjective choice and modeled choice were matched for 5 of the 6  participants; for excitement, subjective choice and modeled choice matched for all 6  participants.
These results corroborate aforementioned results.
Although the trends between conditions are similar for most of the emotions, there are apparent differences in the relative strength of the emotions.
Mean modeled emotions  were analyzed using a repeated measures MANOVA with the five emotions as dependent measures, and play condition as a within-subjects factor.
Play condition significantly impacted fun and excitement, but not frustration, boredom, or challenge .
Post-hoc analysis revealed that players were having more fun when playing against a friend than when playing against a stranger or a computer, and that playing against a stranger was more fun than playing against a computer.
Playing against a friend was more exciting than playing against the computer, while playing against a stranger was marginally more exciting than playing against the computer.
Computer games are generally fun, enjoyable experiences.
Although a user may be frustrated, and may rate this frustration as fairly high on a 5-point scale, this frustration will be low when compared to the frustration experienced by getting a flat tire on the way to an important appointment, or by trying to contact technical support for a lousy local internet provider.
By the same logic, the boredom reported by subjects will be much lower than the boredom experienced during a really boring lecture given by a monotonous professor.
We asked participants to agree with the statement "this condition was frustrating".
Had we asked them to rate their response as a ratio of how frustrating it was compared to a flat tire on the way to an appointment, we probably would have seen much different subjective results.
In contrast, our model takes a global approach to the scaling of emotion, so a user's frustration is given as a percentage of the maximum possible frustration, given the available data.
As seen in Figure 8 and Figure 9, boredom, challenge, and frustration are significantly lower for modeled emotion, while fun and excitement are only somewhat lower.
This result is expected, since playing a computer game can be quite fun and exciting, but perhaps not as fun and exciting as riding a rollercoaster or attending a rock concert.
The mean values shown in Figure 8 are derived from a time series for the five modeled emotions.
Figure 10 shows one participant's modeled frustration over time for the three play conditions.
The mean values reveal that participant three was most frustrated when playing against the computer, , followed by playing against a stranger , and playing against a friend , but means alone do not tell us whether the tonic level was raised or whether there were more phasic responses.
Figure 10 shows that not only were there more phasic responses  when playing against the computer over playing against a friend or stranger, but that these frustrated episodes lasted longer and were greater in amplitude.
When playing against a friend, the frustrated episodes were fewer in number, and smaller in amplitude, showing that both tonic level and the number of phasic responses were reduced.
Modeled emotion pinpoints moments in time when a user's frustration was changing.
This is particularly beneficial when there is no baseline or comparative condition.
Researchers and developers can uncover individual moments when a user begins to get stressed, starts having fun, or becomes bored.
For the other emotions, the trends were similar between the subjective and objective methods, but the relative strength was not.
Modeled emotions took the maximum potential experience into consideration, whereas the same was not true of reported emotion.
To scale reported emotion, one could choose to ask questions that contained scaling elements.
In addition to integrating the modeled emotion with other evaluation methods, there are other research directions to consider.
We developed models for five emotional states that we felt were relevant to interaction with entertainment technology.
We would like to consider other relevant emotional states that can be described by arousal and valence, such as disappointment, anger, or schadenfreude.
In addition, we would like to see if our method can generalize to interaction with other play technologies, specifically, to study user behaviour in ubiquitous play  environments.
Once generalized, modeled emotion can be used to dynamically adapt play environments to keep users engaged.
When the software determined that players were getting bored, the challenge of the task could increase, or the challenge of the task could decrease if players were becoming overly frustrated.
Furthermore, the techniques described in this paper could be adapted to analyze a user's emotional response to productivity software, or other workrelated interactive technologies.
Mean emotion modeled from physiological data provides a metric to fill in the knowledge gap in the objectivequantitative quadrant of evaluating user interaction with entertainment technologies.
In addition, the emotion of the user can be viewed over an entire experience, revealing the variance within a condition, not just the variance between conditions.
This is especially important for evaluating user experience with entertainment technology, because the success is determined by the process of playing, not the outcome of playing .
The continuous representation of emotion is a powerful evaluative tool that can be easily combined with other evaluative methods, such as video analysis.
Given a time series of emotional output, researchers can identify interesting features, such as a sudden increase or decrease in an emotional state, then investigate the corresponding time frame in a video recording.
This method would drastically reduce the time required to qualitatively examine video of user interaction with entertainment technologies.
Modeled emotion corresponds to reported emotion for most of the emotions that we investigated.
We have presented a method of modeling user emotional state when interacting with play technologies.
Modeled emotions can be a powerful evaluation approach because they are objective and quantitative ; they account for user emotion; and they present a method of continuous evaluation over an entire condition, revealing process as well as variance.
Furthermore, the modeled emotions show the same trends as reported emotions for fun, boredom, and excitement; however, the modeled emotions revealed differences between play conditions, while the differences between the subjective reports failed to reach significance.
We have shown that there is great potential for using physiological metrics to model emotional experience with interactive play technologies.
