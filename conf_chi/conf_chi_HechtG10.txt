This study explores language's fragmenting effect on usergenerated content by examining the diversity of knowledge representations across 25 different Wikipedia language editions.
This diversity is measured at two levels: the concepts that are included in each edition and the ways in which these concepts are described.
We demonstrate that the diversity present is greater than has been presumed in the literature and has a significant influence on applications that use Wikipedia as a source of world knowledge.
We close by explicating how knowledge diversity can be beneficially leveraged to create "culturally-aware applications" and "hyperlingual applications".
We begin by suggesting that current technologies and applications that rely upon Wikipedia data structures implicitly or explicitly espouse a global consensus hypothesis with respect to the world's encyclopedic knowledge.
In other words, they make the assumption that encyclopedic world knowledge is largely consistent across cultures and languages.
To the social scientist this notion will undoubtedly seem problematic, as centuries of work have demonstrated the critical role culture and context play in establishing knowledge diversity  on a large scale.
Yet in practice, many of the technologies and applications that rely upon Wikipedia data structures adopt this view.
In doing so, they make many incorrect assumptions and miss out on numerous technological design opportunities.
To demonstrate the pitfalls of the global consensus hypothesis - and to provide the first large-scale census of the effect of language in UGC repositories - we present a novel methodology for assessing the degree of world knowledge diversity across 25 different Wikipedia language editions.
Our empirical results suggest that the common encyclopedic core is a minuscule number of concepts  and that sub-conceptual knowledge diversity is much greater than one might initially think--drawing a stark contrast with the global consensus hypothesis.
In the latter half of this paper, we show how this knowledge diversity can affect core technologies such as information retrieval systems that rely upon Wikipedia-based semantic relatedness measures.
We do this by demonstrating knowledge diversity's influence on the well-known technology of Explicit Semantic Analysis .
In this paper, our contribution is four-fold.
First, we show that the quantity of the world knowledge diversity in Wikipedia is much greater than has been assumed in the literature.
A founding principle of Wikipedia was to encourage consensus around a single neutral point of view .
For instance, its creators did not want U.S. Democrats and U.S. Republicans to have separate pages on concepts like "Barack Obama".
However, this single-page principle broke down in the face of one daunting obstacle: language.
Language has recently been described as "the biggest barrier to intercultural collaboration" , and facilitating consensus formation across speakers of all the world's languages is, of course, a monumental hurdle.
Consensus building around a single neutral point of view has been fractured as a result of the Wikipedia Foundation setting up over 250 separate language editions as of this writing.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Third, this work is the first large-scale and large-number-of-language study to describe some of the effects of language on user-generated content.
Finally, we conclude by describing how a more realistic perspective on knowledge diversity can open up a whole new area of applications: culturally aware applications, and its important sub-area, hyperlingual applications.
Wikipedia has in recent years earned a prominent place in the literature of HCI, CSCW, and AI.
It has served as a laboratory for understanding many aspects of collaborative work  and has also become a gamechanging source for encyclopedic world knowledge in many AI research projects .
Yet the vast majority of this work has focused on a single language edition of Wikipedia, nearly always English.
Only recently has the multilingual character of Wikipedia begun to be leveraged in the research community, ushering in a potential second wave of Wikipedia-related research.
This new work has been hailed as having great potential for solving language- and culture-related HCI problems .
Unfortunately, while pioneering, most of this research has proceeded without a full understanding of the multilingual nature of Wikipedia.
In the applied area, Adafre and de Rijke  developed a system to find similar sentences between the English and Dutch Wikipedias.
Potthast and colleagues  extended ESA  for multilingual information retrieval.
Hassan and Mihalcea  used an ESA-based method to calculate semantic relatedness measurements between terms in two different languages.
Adar and colleagues  built Ziggurat, a system that propagates structured information from a Wikipedia in one language into that of another in a process they call "information arbitrage".
A few papers  attempted to add more interlanguage links between Wikipedias, a topic we cover in detail below.
We argue that although this "information arbitrage" model in many cases provides risk-free informational profit, it does not always do so.
In fact, if such an approach is applied heedlessly it runs the risk of tempering cultural diversity in knowledge representations or introducing culturally irrelevant/culturally false information.
Our results can be used in tandem with approaches such as information arbitrage to provide a framework for separating "helpful" arbitrage, which can have huge benefits, from "injurious" arbitrage, which has negative qualities.
While fewer in number, some recent papers have focused on studying the differences between language editions.
These papers fall more in line with social science positions regarding the nature of knowledge diversity.
Our prior work  demonstrated that each language edition of Wikipedia focuses content on the geographic culture hearth of its language, a phenomenon called the "self-focus" of usergenerated content.
In a similar vein, Callahan and Herring  examined a sample of articles on famous persons in the English and Polish Wikipedia and found that cultural differences are evident in the content.
In order to question the veracity of the global consensus hypothesis, quantify the degree of knowledge diversity, and demonstrate the importance of this knowledge diversity, a large number of detailed analyses are necessary.
The following provides an overview of the various stages of our study.
Step One : First, we develop a methodology similar to previous literature  that allows us to study knowledge diversity.
This methodology - highlighted by a simple algorithm we call CONCEPTUALIGN - aligns concepts across different language editions, allowing us to formally understand that "World War II" , "Zweiter Weltkrieg" , and "Andre verdenskrig"  all describe the same concept.
Since the methodology leverages user-contributed information, we also provide an evaluation to demonstrate its effectiveness.
Step Two : After describing the concept alignment process, we can analyze the extent of diversity of knowledge representations in different language editions of Wikipedia.
These analyses take place at both the conceptual  level and the subconceptual  level, and serve two purposes:  they show that the global consensus hypothesis is false, and  they provide the first empirically derived quantitative descriptions of the extent of knowledge diversity across numerous Wikipedia language editions.
Though innovative, much of the previous work implicitly or even explicitly adopts a position consistent with the global consensus hypothesis.
The global consensus hypothesis posits that every language's encyclopedic world knowledge representation should cover roughly the same set of concepts, and do so in nearly the same way.
Any large differences between language editions are treated as bugs that need to be fixed, discrepancies that will go away in time , or a problem that should simply be ignored.
For example, Sorg and Cimiano  state that since English does not cover a large majority of German's concepts, there is "clearly" something wrong with the links.
Our test case is Explicit Semantic Analysis , a technology that has been widely applied in fields ranging from HCI to AI to NLP.
Step Four : Finally, we conclude with a discussion of the implications for design that result from each of the previous analyses.
We highlight the importance of considering language and culture in application building, and introduce two ideas: culturallyaware applications and hyperlingual applications.
For example, an ILL exists that links the English article "Computer Science" to the Catalan article on the same topic, "Informatica".
On the live Wikipedia site, ILLs included in an article can be viewed and clicked on under the "languages" 
In the 25 languages used in our study, we parsed out around 52 million separate ILLs.
German ILL has been manually added.
However, it is crucial to our study that we have an accurate collection of interlanguage links in order to ascertain the degree of knowledge diversity present.
While the bots propagate these links across the various Wikipedia language editions, their lack of formality means that we cannot be sure that they are exhaustive.
To address this lack of formality and help maximize the information value from the user-generated ILLs, we implemented an algorithm, CONCEPTUALIGN, which formally categorizes every article in every language into a single concept group.
ILLs are typically viewed as pairwise dictionary-like entities, e.g.
However, the ILL dataset can also be viewed as a set of directional edges  and nodes  that form an enormous number of individual connected components in a graph containing all the articles in our study .
Each connected component represents a concept that is described in the articles in the component.
CONCEPTUALIGN was designed for this graph-based view of interlanguage links.
The algorithm picks a node at random and does a breadth-first search, ignoring edge direction, until it finds all connected nodes in the component.
It then labels all those nodes as belonging to the same concept.
Next, it picks another node , and continues the process until all nodes have been labeled with a concept.
CONCEPTUALIGN effectively adds to the ILL dataset new ILLs that make each separate component fully connected; all articles in a concept group are connected to all other articles in the group with an ILL following the operation of the algorithm.
In this way, CONCEPTUALIGN maximizes the information content derived from the user-generated ILL dataset.
In other words, there need not be a large number of Wikipedians proficient in both Korean and Slovak.
CONCEPTUALIGN only requires is that there be a large number of Wikipedians proficient in Korean and English, and Slovak and English .
This is an approach much more suited to the real-life patterns of bilingualism.
A large data preparation process preceded the studies described in this paper, all of which were executed using an extension of our open-source WikAPIdia software.
WikAPIdia is a MySQL-based Java API to the database dumps made available by the Wikimedia Foundation.
While support for any existing language edition can easily be added to WikAPIdia, our current implementation uses the 25 different language editions in Table 1 .
Table 1 - A brief overview of the size of some of the 25 language editions in our study.
Other languages included are: Czech, Danish, Finnish, Hungarian, Indonesian, Korean, Polish, Portuguese, Romanian, Slovak, Swedish, Turkish, and Ukrainian.
Therefore, we evaluate in two different ways the combined effectiveness of the ILL dataset and CONCEPTUALIGN.
The first approach is based on the algorithm's nature as a "missing link finder" and places it in the context of state-of-the-art machine learning techniques that have been developed to accomplish the same task .
In the second evaluation, we examine the effectiveness of the algorithm using bilingual human coders.
Human Evaluation Study: Despite the effectiveness of CONCEPTUALIGN relative to the state of the art in missing ILL detection, in order to determine the degree of world knowledge diversity across Wikipedia language editions it is important to get an accurate estimate of the number of ILLs that are likely missing after running the algorithm.
While it would be impractical to evaluate the coverage of our dataset for all 25*24 = 600 language pairs in our study by hand, we did perform this evaluation on three pairs of languages: English paired with Spanish, Japanese, and Italian.
Large-Wikipedia languages were used because they have the biggest impact on the results we present in the latter part of this paper.
Moreover, due to patterns of bilingualism around the world, these languages are most likely to play an important role in connecting concept components in the ILL graph.
After CONCEPUALIGN was run on our entire dataset, six human coders  were recruited, each highly proficient in both languages L1 and L2.
Each coder was given 75 primary Monolingual  articles and 75 secondary Monolingual articles, as identified by CONCEPTUALIGN.
Monolingual articles are Lx articles representing concepts that CONCEPTUALIGN has indicated do not have articles in LY.
For each article, coders were instructed to attempt to find a conceptual equivalent in the other language of the language pair.
They were told to use any tool or information they wished to accomplish this task, but were limited to five minutes per article.
Coders were also given a list of 25 Bilingual articles to evaluate the precision of CONCEPTUALIGN .
For these articles, coders were instructed to determine whether or not each bilingual article pair covered the same concept.
For each language pair, we used the union of the coders' results.
Any disagreements between coders were resolved by discussion.
Finally, coders were told to ignore any disambiguation pages in the sample.
Figure 1: A simplified ILL graph.
Interlanguage links are represented as edges, and articles as nodes.
For example, the article on concept A in English  has an interlanguage link to the article on concept A in Spanish .
Importantly, note that even though BEN and BES have zero interlanguage links to each other, they are still identified as part of the same concept.
The same is true for AES and ADE.
Algorithm Evaluation Study: The first step in examining CONCEPTUALIGN's effectiveness in finding missing ILLs is to compare its performance to existing state of the art techniques in missing ILL detection.
English ILLs found by their machine learning approach that makes use of a support vector machine  on a subset of German articles.
CONCEPTUALIGN found ILLs for 95.8% of the German/English "missing ILLs" in the dataset.
Interestingly, 91.8% of these links had been added manually into Wikipedia since the time of Sorg and Cimiano's study.
The remaining 4.0% of ILLs were discovered by CONCEPTUALIGN.
The precision of the SVM that generated the missing links is far from perfect3 and some found links are to articles not included in our study .
This leads us to conclude that CONCEPTUALIGN  is at least as effective as the SVM.
We write "at least" because we have no idea how many additional missing links CONCEPTUALIGN would have found on the same sample of articles on which Sorg and Cimiano tested their SVM.
As shown in Table 2, the probability of a missing link is low, especially in the case of Japanese/English.
In addition, the precision of the ILL graph appears to be good, as not a single incorrect link was found.
The results from both evaluations together provide evidence supporting the relatively high quality of concept alignment, and also provide us with a reasonable estimate of the amount of error we are likely to find as a result of missing ILLs.
Given the differential in article quantity in Table 1, this assumption is most often instantiated in the corollary that the English Wikipedia, given its massive size, is a near-complete superset of all the other Wikipedias.
Every other language edition thus is supposed to cover some subset of the English editions' articles.
Second, we explore the global consensus hypothesis at the sub-conceptual level.
Here the hypothesis posits that two articles about the same concept in two different languages will describe that concept roughly identically.
For instance, all articles on "Psychology" would describe "Psychology" in close to exactly the same fashion.
Before continuing with our analyses, it is important to briefly discuss in more detail the two information levels investigated.
At the higher conceptual level, the topic of the article matters and the way that topic is defined is unimportant.
The exploration of the global hypothesis at the sub-conceptual level is predicated on the higher-level assumptions being true.
To further support this last point, we ran the same experiment using only the three largest Wikipedias  and the largest six Wikipedias .
In the first case, around 80 percent of concepts remained single-language, while only 7 percent were in all three.
In the second case, 77 percent of concepts were singlelanguage, while only 1.5 percent are in were in all six.
Our 25-language analysis also revealed the extremely small number of concepts that are covered in all 25 language editions: 6,966 concepts .
While small in number, these concepts are revealing and could be considered "globally relevant".
Table 4 lists a few of these concepts and their corresponding articles in English, Japanese, Slovak, and Dutch.
Conceptual diversity is measured by the degree of concept coverage overlap across different language editions of Wikipedia.
Concept coverage overlap is determined as follows: if one or more articles exist on a concept C in a language L1's Wikipedia, then C can be considered to be covered by L1's Wikipedia world knowledge representation.
As such, if an article on C exists in both languages L1 and L2, C can be considered to be covered by the shared world knowledge of L1 and L2.
One can examine the conceptual overlap between n languages using the same method, but including only concepts that are covered by the knowledge intersection of all n languages.
For instance, we can approximate shared "global knowledge", or the "encyclopedic core", with n = 25.
Our results  demonstrate that a surprisingly small amount of concept overlap exists between languages of Wikipedia, refuting the global consensus assumption at the concept level.
Over 74 percent of concepts are described in only one language.
It is important to note that we have not yet taken into account the small number of missing ILLs that may remain after CONCEPTUALIGN is run.
While it is impossible to predict exactly how missing ILLs would effect the distribution in Figure 2, our earlier studies allow us to establish a realistic range.
A conservative estimate would be to assume a "best case scenario" for the global consensus hypothesis: that all of the missing ILLs would link singlelanguage concepts to articles in other languages, that no missing ILL is a reflexive "duplicate" of another, and that the Italian performance rate  occurs across the entire dataset.
This still represents a great deal of conceptual diversity.
In addition to examining the global properties of the concept overlap among all languages, we can perform a more detailed examination of the pairwise diversity across languages.
As shown in Table 3, the English-As-Superset corollary to the global consensus hypothesis does not hold.
Despite its massively larger size, the English Wikipedia  covers no more than approximately three-quarters of any other Wikipedia in our study.
The case of overlap between German and English - two very mature language editions - is quite illustrative.
English is more than three times the size of German, but only covers slightly more than 50 percent of its concepts.
Fortunately, perfect clarity is by far the norm in our dataset, with only rare instances of very low clarity.
In fact, only about 2,700 concepts in total have a clarity of less than 0.5, which can still be reasonable in some cases.
These statistics are bolstered by the precision results from our coders.
Further analysis of the rare cases of low clarity can be found in .
Even when two language editions cover the same concept , they may describe that concept differently.
If a significant phenomenon, this sub-concept diversity would add extensively to the overall diversity of world knowledge representations present in Wikipedia.
In this sub-section, we show that sub-concept diversity is a prominent force, although similarities do exist on average between articles on the same topic.
As a result, while false overall, the global consensus hypothesis does have some truth at the sub-concept scale.
Our experiment on sub-concept diversity borrows from  the idea of using outlinks, or links in one article pointing to another article, as a "highly focused entity-based representation of ."
In other words, outlinks4 provide a decent structured, canonical/ languageindependent summary of raw text.
Operating under this assumption, we compared the outlinks of each of the "global concepts" to determine the degree to which the articles covered the same content.
If, on the other hand, there is great sub-concept diversity, these articles would link to very few articles about the same concepts.
If links in two different languages are pointed at the same concept, the destination of both links would be articles belonging to the same ILL graph component.
Our metric in this experiment was the Overlap Coefficient , first used in the UGC domain in  and calculated as follows:
At first glance, many of these statistics may appear rather remarkable.
However, any dedicated Wikipedian could provide ample anecdotal support.
For instance, the entry on Prinzipalmarkt, a key commercial district in the mid-sized city of Munster, Germany, remains German-only, despite the district's local significance.
Similarly, the American country music duo Big and Rich, who have sold millions of records in the United States, have an English-only article.
Even seemingly "major" concepts in a culture, say minor league baseball, are not pervasively included across all Wikipedias in our study.
In fact, only 40 percent of languages in our study have an article on minor league baseball.
Of course, to the global sports fan, this makes sense.
Why would the Poles, Finns, Russians, etc.
The overlap gets even smaller when one takes a step down the specificity hierarchy and considers minor league baseball teams.
For instance, the Lansing Lugnuts, a Class A minor league team, have an English-only article.
OC is the size of intersection of the two sets of links divided by the size of the smaller of the two sets.
In a small number of cases, more than one article per language belongs to a single concept.
If prominent across our dataset, this could have unpredictable effects on our results.
We measured the average number of languages per article in a concept , a ratio we call conceptual clarity.
In this way, OC provides the "best case" scenario for the global consensus assumption in terms of these systematic differences.
Calculation of the OC was straightforward with one exception relating to links to time-related concepts.
Because the norm about linking to these concepts is different in each of the Wikipedias studied, we used WikAPIdia's spatiotemporal package to filter out all links to years, dates, and months in our analyses.
In languages where timerelated links occur, these form a substantial percentage of outlinks on many pages, creating a signal that needed to be neutralized in order to explore more general sub-concept diversity.
For this study, we used a sample of concepts in the global concepts list, filtered for perfect clarity.
We also required that each article have at least three outlinks, in order to make our experiment non-trivial, and three inlinks, to ensure that each article was relatively integrated into its Wikipedia.
For each concept, we calculated the OC for every L1, L2 pair, leading to 600 pairs per concept.
Our final sample consisted of over 217,000 of these pairs.
For instance, in the example of "Psychology", the Spanish article links to "Biologia"  but the German one does not, even though both discuss biology.
The last major factor is seemingly random differences in descriptions.
While some of these could be less obviously or indirectly cultural differences, it is not unreasonable to think that two people of the same culture with access to the same information would describe a concept differently.
Examining this phenomenon represents an important area of future work.
In this section, we explore the effect that the large diversity of representations established above has on technologies that use Wikipedia as a source of world knowledge.
As a case study, we use one of the most generally applicable and popular technologies developed around Wikipedia, the semantic relatedness measure Explicit Semantic Analysis .
ESA was first introduced by Gabrilovich and Markovitch , and has been shown to mimic human judgments on standard datasets better than any other semantic relatedness  measure.
The semantic relatedness between two concepts can be defined as some measure of the number and strength of relationships between the concepts.
Semantic similarity, perhaps more familiar to some readers, is a subset of semantic relatedness in which the only relations considered are hypernymy/hyponymy .
In addition to their implicit import as a model of human judgments, semantic relatedness measures play an essential role in a variety of technologies including information visualization , information retrieval, word sense disambiguation, text summarization and annotation, determining the structure of texts, and lexical selection .
Our goal here is to determine if the diversity in world knowledge representations described in the previous study causes significantly different ESA scores for any two concepts C1 and C2.
The mean overlap coefficient for our sample was only 0.41 .
This means that, on average, the longer of two articles on the same concept contains only 41 percent of the outlinks in the shorter of the articles5.
Adding in the assumption in , a longer article on a concept C only covers 41 percent of the content of a shorter article on C. While the main driving force behind the concept-level diversity seemed to be cultural , the causes in the sub-concept context, on the other hand, seem to be more mixed in nature.
Certainly, cultural forces are very prominent.
In the case of the concept that is called "Psychology" in English, for example, the Spanish article  contains many outlinks to Latin American countries not contained in the German article .
These links come from a section in the "Psicologia" page about Latin America's contribution to psychology.
While many of these culture-specific instances of sub-concept diversity are evident in links to articles on geographic entities , this is by no means always the case.
The article on "Paz"  in Spanish contains links to Christian concepts in a discussion on peace in the Bible, links that are not in the English Wikipedia's article on "Peace".
However, two other factors also seem to be at play.
The first is linking behavior.
There are some cases in which articles in two languages describe the exact same content,
We built an ESA implementation based on the descriptions in .
ESA models concepts as vectors of their abstract  relationships with a set of other concepts.
These "other concepts" are ESA's world knowledge representation, and are defined using Wikipedia articles.
ESA compares the vectors of two input concepts C1 and C2 and the more similar the vectors, the higher the ESA value .
In implementing ESA for a large number of languages, we were forced to make several changes to the original implementation.
Portuguese, Romanian, English, German, French, Italian, and Danish.
To validate our implementation, we tested our English version against a canonical human gold standard dataset in semantic relatedness  and achieved correlations that were comparable to the original ESA implementations .
Since in our experiment we would be comparing between many named entity pairs, we also evaluated our implementation against the only named entity SR dataset available  , which comes from the bioinformatics domain.
Interestingly, our English ESA  performed comparably to Pedersen et al.
ESA is optimal as it allows us to change the concepts that are used for modeling.
In other words, we can plug in world knowledge that varies in language and content relatively easily.
Using this flexibility, we performed two experiments with ESA.
In the first, we used as world knowledge the 8,264 perfect clarity concepts  that existed in the intersection between the 10 languages in the study .
This models the pure effect of sub-concept diversity on ESA without any concept-level effect.
If C1 = "Argentina" and C2 = "Sigmund Freud", how much will ESASPANISH differ from ESAGERMAN due to the sub-concept diversity in the "Psychology" article?
In the second experiment, we used as world knowledge 10,000 randomly selected articles from each language .
This is done to model another way in which ESA is typically implemented and because it includes concept-level diversity as well as sub-concept level diversity.
To understand the possible effect of concept-level diversity on ESA results, consider C1 = "Country Music" and C2 = "Duo".
If the "Big and Rich" article appeared in the ESAENGLISH 10,000 concepts, ESAENGLISH would understand a relationship between C1 and C2 that none of the other ESAs would be able to understand, increasing the resultant SR value relative to the other ESA values.
For both experiments, we tested on 2,000 C1,C2 concept pairs randomly selected from the list of global concepts discussed above.
For each language, C1 and C2 were set to the title of the article of the concept in each language.
We only used concepts that have single-word titles in all ten languages.
Two pairs where C1 = C2 were included, and this is in line with typical SR human gold standard datasets such as WordSim353 .
Because of the differentiation in content, pairs that may appear very related according to one ESA may not be considered related according to another.
Consider, for example, the concept pair "Germany" / "Saxony-Anhalt".
In most of the languages, this pair receives high ESA scores, but in Italian and Danish, ESA perceives no relation at all.
This is because in Italian and Danish, the articles that make up the world knowledge do not mention "Germany" and "Saxony-Anhalt" together  whereas the other languages do.
Likewise, the "Triumphal Arch" article in English mentions both an arch in Thessaloniki and in Iraq, whereas the German article only mentions the Thessaloniki arch.
Since "Triumphal Arch" is one of the ten-language concepts, this leads to "Thessaloniki" / "Iraq" having a much higher SR value in English than in German.
That is not to say there was not widespread "agreement" amongst all ESAs on certain concept pairs.
Obviously, these words occur frequently together in Wikipedia articles, regardless of the language.
Similarly, many pairs such as "DVD" / "Djibouti" are not related in any language.
The results of the second experiment show similar trends to the first: mean r = 0.16 .
In this experiment, there are two forces behind the low correlations: concept differences and language differences.
To tease out the effect of the language difference, we compared the values from the multilingual experiment to those from an identical experiment using 16 random English-only 10,000-article sets as world knowledge.
In other words, given two ESA implementations, no matter which two 10,000-article sets are used as world knowledge, if those sets are from the same language they will lead to more similar SR scores than if the sets are from different languages .
For researchers in HCI, AI and NLP, the rejection of the global consensus hypothesis has important implications for technologies that operate on Wikipedia directly.
At the concept level, this work places an important boundary condition on the utility of ideas like "information arbitrage" , which seeks to "leverage articles in one or more languages to improve content in another".
For instance, a running example in  is articles on the concept "Jerry Seinfeld", a concept that does not exist in eight of the languages explored in this study.
It is likely that for at least some of these eight language groups, Jerry Seinfeld has insufficient cultural import to warrant and maintain an article.
More generally, it is possible that information arbitrage would be of little utility for a portion of the Wikipedia articles that only exist in a single language.
Ideas such as information arbitrage are also affected by subconcept diversity.
The wide-ranging extent of sub-concept diversity captures a great deal of culture-specific content and researchers must be aware that sub-concept diversity does not simply represent information "inefficiencies" that need to be fixed.
Propagating culture-specific information such as that found in the "Psicologia" article to other Wikipedia language editions, for example, would likely be detrimental to end-users.
Readers of the Danish Wikipedia likely would not consider a section on psychology in Latin America to be very relevant in the Danish "Psykologi" article.
The research challenge ahead is learning to automatically separate culture-specific information - such as geographically focused examples - from information that is largely globally relevant, like dates of birth, etc.
This challenge plays an important role in future work.
The lack of global consensus also has large indirect effects on the larger class of Wikipedia-based applications.
These are applications that use Wikipedia as a source of world knowledge to do non-Wikipedia actions.
This directly impacts end-user applications that implement ESA.
For example, Bergstrom and Karahalios's recent research  on the clustering of conversation topics on a shared display relied upon an English Wikipedia-based ESA metric.
However, if the group conversing is a multinational team of scientists, our results suggest that clustering will be biased toward any native English speakers in the group, as it would be their world knowledge used for the clustering.
The results for a conversation that involved Thessaloniki and Iraq would be different than if the German Wikipedia were used.
For instance, if a system needs to calculate the semantic relatedness between two entities for a group of Romanian immigrants to the United States, the Romanian Wikipedia could be swapped in and the English Wikipedia swapped out.
Another important and exciting subset of culturally-aware applications is hyperlingual applications, which consider world knowledge from multiple languages simultaneously.
For instance, the work of Bergstrom and Karahalios could be extended hyperlingually if a weighted combination of the native languages of the participants were considered.
A hyperlingual approach can provide enormous benefits in terms of access to new world knowledge not available in any particular language edition .
Researchers have not yet taken advantage of the articles lying outside the concept intersection between languages, despite the fact that these articles far outnumber articles in the intersection.
In other words, by donning a hyperlingual lens, technologists can utilize the diversity in knowledge representations as well as the similarities.
We are currently working on two hyperlingual applications that leverage the concept and sub-concept diversity findings in this paper.
The first is effectively a "cultural reading level" application that will help people writing for an international audience to identify parochial or regionspecific references, as well as suggest appropriate alternatives.
Secondly, we are building a system to help foster the understanding of concept and sub-concept diversity in Wikipedia.
The system will allow users to view the intersection and union of world knowledge on a particular concept.
Importantly, it will also highlight the difference between the union and intersection for the user.
We used a text-only nascent implementation of this system to identify some of the examples in this paper.
In this paper, we have provided four key contributions:  we have shown that knowledge diversity across Wikipedias is large and defined its extent,  we have demonstrated that this diversity has a significant effect on technologies,  the first census of the effect of language on UGC repositories was executed, and  we have discussed design implications of these findings while introducing the ideas of culturally-aware applications and hyperlingual applications.
Moving forward, we hope this work will inform and inspire a new generation of multilingual Wikipedia applications.
While knowledge diversity can create problems in existing technologies, it opens up opportunities for developing new approaches to technology design.
In particular, the potential for culturally-aware applications is enormous.
