Co-located collaborators often work over physical tabletops using combinations of expressive hand gestures and verbal utterances.
This paper provides the first observations of how pairs of people communicated and interacted in a multimodal digital table environment built atop existing single user applications.
We contribute to the understanding of these environments in two ways.
First, we saw that speech and gesture commands served double duty as both commands to the computer, and as implicit communication to others.
Second, in spite of limitations imposed by the underlying single-user application, people were able to work together simultaneously, and they performed interleaving acts: the graceful mixing of inter-person speech and gesture actions as commands to the system.
This work contributes to the intricate understanding of multi-user multimodal digital table interaction.
To answer this question, we performed an observational study investigating how people used two multi-user speech and gesture wrappers built over existing single user applications.
As we will see, our analysis verifies and adds detail to the role that speech and gesture commands play as consequential communication.
Previous research explored how people could interact over existing single user applications  displayed on a digital table that recognized both speech and expressive hand gestures .
They listed a number of behavioural foundations motivating this multi-user, multimodal interaction.
In particular, they hypothesised that one person's speech and hand gestures used to command the application also produced consequential communication that others could leverage as cues for validation and assistance.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We observed 6 computer-proficient participants : 5 males and 1 female, ages 21-30 years.
Pairs were seated side by side along the front edge of the digital table displaying an `upright' single user application .
Participants interacted with the application using speech via noise cancelling headsets and gestures via a DT107 MERL DiamondTouch table.
Speech and gesture input was mapped to GUI commands understood by the application in a manner similar to .
Participants used gestures as commands by directly touching the table surface.
Feedback of successful speech and gesture recognition was indicated by the application's visual response and by an audio tone for speech commands.
Spoken commands were designed to be easily understood by both the computer and other collaborators .
A printed list of recognizable speech and gesture commands was posted in front of participants, and pairs were encouraged to practice speech and gesture input prior to each trial.
Tasks consisted of two scenarios, described below.
Travel Planning: Pairs used Google Earth to plan a European student's three day, all expenses paid, trip to Boston, New York and Chicago.
Typical speech commands were "fly to " and "layer ", while gestures included using one finger to pan or annotate, two fingers to zoom the camera in and out, and five fingers to tilt the camera.
Pairs had to select four or five key places to visit in each city by using the "scratch pad" speech command, circling the area of interest and numbering the attractions in the order they would be visited.
Home Layout: Pairs used The Sims by Maxis to lay out furniture in a bed room, living room, kitchen, and washroom of a newly purchased two story home for a four person family.
We recorded and then transcribed a total of 476 minutes of speech and gesture actions from each participant, as recognized by the system at 15 events per second.
Over all pairs, we coded 416 commands: 164 speech, 194 gesture and 58 multimodal commands .
Our open coding revealed five categories of speech and gesture commands: Assistance: People invoke commands as actions that directly respond to other people's explicit or implicit requests for help, for example,
While systems exist that log single person multimodal interactions , we needed a way to capture the interactions of multiple people with our multi-user multimodal system.
First, we video recorded sessions and took field notes during the experiment.
Second, we created a transcription tool that recorded synchronized streams of gesture and speech acts from both participants as seen by the system for playback with recorded video.
Figure 1 shows a screen snapshot of our logging tool.
The top shows the video.
This is synchronized with the middle activity graph: a visualization of both participants' speech and gesture actions and how they were recognized by the system.
The bottom pane includes manual transcription notes.
Assistance, validation and affirmation are all examples of commands that are positively included as conversational elements.
Clarification requests indicate that the command did not fit well as a conversational construct, while redundancy is an indication that a person viewed the action as distinct communication and command elements.
Figure 2 shows the average breakdown of the 416 coded speech and gesture command used across both tasks .
When these numbers are considered by task , we saw that the Travel Planning Google Earth task had slightly higher validation and assistance rates than the Home Layout Sims task.
We believe this is because many Google Earth commands performed global actions that would affect the entire work area, and for this reason, participants would converse with their partners before issuing the command.
We now consider how people interact in this multimodal tabletop setting.
We were particularly interested in whether the single user nature of the underlying application  forced a situation in which people predominantly worked sequentially , or whether they were able to converse and interact simultaneously over this surface.
Our coding results clearly show that speech and gesture commands directed to the system also served double duty as communication to other collaborators.
98% of our 416 observations were coded as assistance, validation, or affirmation.
Only 2% - the clarification and redundancy categories - indicated commands that were not included well within the conversational context.
Our own subjective appraisal of pair interactions confirms what these numbers suggest: people integrated speech and gesture commands into their joint conversations and actions.
To explain our results, Clark  describes how speech acts can be broken up into two tracks: track one describes the business of the conversation and, track two describes the efforts made to improve communication.
With commands, track one becomes the act of issuing a command to the computer, while track two serves a communication role to other collaborators.
We deliberately crafted speech commands so they were both machine and human recognizable .
Our results suggest that pairs' used speech commands as dual purpose speech acts that fit into both tracks.
Similarly, consequential communication happens when one monitors the bodies of other's moving around the work surface .
For example, as one person moves her hand in a grasping posture towards an object, others can infer where her hand is heading and what she likely plans to do.
In our system, gesture commands are designed so that they provide consequential communication to others when used.
For example, using five fingers to pick up a digital couch also produces awareness to collaborators around the table.
First, we used our logger to mark each person's gesture and speech actions as either on or off: speech is on when it is above a volume threshold, while a gesture is on whenever the logger detects a finger or hand posture placed on the table.
Thus for any instant in time we can determine if a simultaneous speech and gesture act is occurring.
We then examined those times when at least one person was speaking and/or gesturing .
For about 14% of this 53%, we found that the other person was also speaking and/or gesturing at the same time.
This number actually underestimates simultaneous activity, as it only includes those gestures which are direct touches to the table.
In actual practice, we saw many gestures occur immediately above and around the table, as well as nodding and many other forms of body language.
We observed  that participants were highly engaged in each other's task and actions; it was rare to find a participant idling.
They were involved both in how they attended to each other, and in the interleaving of their speech and gestures when talking about what they were doing.
This supports other people's findings of simultaneous interaction over tables .
Next, we examined how people worked together during those episodes in which we saw at least one person direct speech and gesture commands to the application.
Here, we analyzed our video transcriptions using an open coding method  to look for different styles of interleaving actions.
Our analysis revealed that even though the underlying application could not recognize simultaneous activity, people managed to cooperate through interleaving acts: a graceful mixing of people's speech and gesture actions in the construction of commands.
We saw four different interleaving interactions that can be described along the dimensions of coupling  and the input modality used.
This category occurs when one person issues the speech component of a command and the other issues the gesture component.
For example, the following interaction separates one's decision of creating a chair from the specification of the location for it.
One person discusses or gestures over what should be done while the other person performs the command on the system.
These interleaving acts were primarily used for two purposes.
First, people used them to support coaching, validation and assistance.
By suggesting what command should be performed next, participants are implicitly seeking validation of their suggestion from their partners.
Second, we saw this mode used for cooperative error correction.
In particular, when a person was having problems getting the system to recognize a particular speech or gesture command as valid input, the other person would often provide support by issuing the same command on their behalf.
To digress momentarily, cooperative error correction within this mode is extremely important: it provides an additional level of robustness to multimodal systems.
Previous empirical studies described how multimodal systems can add robustness; each modality provides a check for erroneous recognition .
For example, a "create stove" speech command would be ignored by our system if no location-indicating gesture followed.
Cross person error correction adds further robustness over this system correction.
To illustrate, we noted 84 speech recognition errors in our transcriptions where the system failed to correctly recognize a speech command.
Of these, partners stepped in ~1/3 of the time to correct another's error.
Most participants would start by trying to reissue the command themselves.
Two or more failed speech recognition attempts might be seen as an implicit request for assistance according to Clark's  description of track two efforts to improve communication, and repair conversation.
One person issues the next speech command while the other is finishing their gesture, i.e., they overlap command sequences, which the system then queues to the underlying single user application.
This allowed pairs to efficiently issue overlapping multimodal commands without having to wait for the other person to finish their action.
We noticed that each participant peripherally monitored the workspace to find an appropriate place to insert their next command; they rarely overlapped commands in ways that resulted in system confusion.
One person issues a speech or gesture command within a conversation to assert informal floor control of not only the application, but of the conversational direction.
For example in the travel planning task, people would often assert control of the map to signal that it was their turn to speak or to advance the discussion in a new direction.
The other person would follow this lead.
In summary, we were pleasantly surprised that people were able to converse and communicate using simultaneous speech and gesture commands, much as they do in real world interactions when working over work surfaces.
Similarly, people were able to do fine-grained mixing of their actions, conversation, and commands using what we called interleaving acts.
Of course, there is much left to do.
Our study is small; larger studies are needed to confirm our numbers and to investigate additional details.
Another obvious next step is to build multimodal tables running true multi-user applications, and to see if their use differs from what we saw here.
Limitations aside, this paper contributes to the understanding of multi-user multimodal interactive systems.
We saw that speech and gesture commands directed to the computer also serve double duty as implicit communication to others.
We saw that people's simultaneous interactions were not inhibited by the underlying single-user application.
Similarly, we saw that people were able to compose sequential actions through interleaving acts: the graceful mixing of both participant's speech and gesture actions as commands were being constructed.
They suggest that people can use multi-user multimodal tabletops - even when limited by single user application constraints - in much the same way as they work over visual work surfaces.
