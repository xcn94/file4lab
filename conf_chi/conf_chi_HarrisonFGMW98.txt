This paper reports on the design and use of tactile user interfaces embeddedwithin or wrapped around the devices that they control.
We discuss three different interaction prototypes that we built These interfaces were embedded onto two handheld devices of dramatically different form factors.
We describe the design and implementation challenges,and user feedbackand reactionsto theseprototypes.
Implications for future design in the area of manipulative or haptic user interfacesare highlighted.
Again, in these examples a specialized input device controls a separate electronic display.
These extensionsto graphical user interfaces seemlogical in view of the widespreadsupport and acceptance of direct manipulation interfaces  and of real-world metaphors, such as trash cans and file folders .
We believe that suchphysical user interface manipulatorsare a natural step towardsmaking the next UI metaphorthe real world itself: real objectshaving real propertiesthat are linked to or embedded in the virtual artifacts that they control.
We set out to further explore this new area.We have been influenced by several previous research prototypes that reflect elements of an "embeddedphysicality" approach.
Fitzmaurice , Rekimoto , and Small & Ishii  attachedsensorsto small handhelddisplays and subsequently used these displays to "scroll" through or view a larger virtual space.Movement of the display is mappedto correspondingmovementsin the virtual space,such as changing the view perspective or to the degreeof magnification .
These prototypes demonstratedthe intuitiveness of this embeddedphysicality approach.The work we report here incorporatesmanipulations different from theseprevious examplesto further improve our understandingof the breadthand potential of thesenew kinds of interactions.
Our work differs from the previous work on "physical handles" in one particularly interesting way.
We are investigating situations in which the physical manipulations are directly integrated with the device or artifact, such as a small PDA, that is being controlled.
We are not exploring separate input devices,but rather making the physical artifact itself becomethe input device by meansof embedded sensortechnologies.
The goal of this paper is to share our experiencesin designing, building, and using severalprototypesof such user interface techniquesand devices and in reflecting on new ways for thinking about this new class of user interface.
In these cases,unique physical input devices are cleverly matched to the requirements of the specific application domain .
The affordancesof the input devicesare well matched to the virtual representationof the object that they represent.
We chose three diverse and previously unexamined user tasks.
This allows us to explore alternative kinds of manipulations, test different sensing technologies,and more thoroughly probe the researchspace.Additionally, we selected tasksthat represented two different types of interaction: active user interaction via explicit physical manipulations or via passive user interaction, sensed implicitly.
Finally, we selected tasks that were relevant for other PARC research groups who were implementing applications for portable documentdevices .
For this reason, we focusedon portablepen-based systems.
By implementing new user tasks,we hope to contribute to the generalbody of knowledgeaboutphysically manipulatable interfaces.
We believe that this experiencewill assist us in formulating a more generalframework,designprinciples, and theoretical foundations for physically afforded interfaces.
We choseseveralsimple tasks:navigation within a book or document, navigation through long sequential lists, and documentannotation.
In the next section,we describemanipulation of real world, traditional artifactsand outline the task representation,user actions, and feedbackfor each of our selectedtasks.
Following this, we describeour three task Uf designsin termsof how thesereal world manipulations were mappedto the devices we selected.
Again we discuss our designs in terms of task representation,user actionsrequired, and feedback We then highlight some of the implementation decisions and tradeoffs that impacted the interaction design.
User actions are manipulation via turning the knob and stopping at a desiredlocation.
Visual feedbackincludes the flipping of items or cards,the rate of flipping, and a new destination item or card.
Auditory feedbackis the sound of the cards flipping.
Kinesthetic cues include finger pressure,extent of rotational movement,and direction of rotation.
We defined this task as hand written annotationon a pageby-pagebasis , where the original page contains margins and white spacewithin which the annotations are made.
User actions are typically bimanual:the non-dominanthand anchorsthe pagewhile the dominant hand writes the annotations wherever there is room.
Visual feedbackis the appearance of the annotations.
There is minimal auditory feedback.Kinesthetic cues are the pen pressurein the hand, anchoringpressurein the nondominanthand,and the pen/writing movementand friction.
This task is of particular interestto us in that we introduced new capabilities not available in the real world annotation task.
As such, it representsan opportunity for computationally enhancedinteraction.
In traditional page annotation users must fit the annotations into existing, limited white spacebasedon the static layout of the page.
During annotation,their writing hand also often obstructs the text that they are commentingon.
We decidedto optimize annotation by maximizing the amount of white spaceand its position within the page.
We detect the handedness of the user and then dynamically shift the text "away" from the writing hand thereby maximizing white spacedirectly under that hand .
We describethis design and the implementationof it in subsequent sections.
The task representationassumes that the book or document has a number of predictable properties.
These include physically manipulatablepageunits, a sequentialorganization, a thicknessor "extent", and obvious start/endpoints.
These properties afford page-by-pagemanipulation and movementthrough blocks of pagesrelative to the start/end points of the whole book or document.
The user actions we looked at were flicking comers of pages and thumbing into a location of the book/documentby moving blocks of pagesrelative to the beginning or ending.
Manipulation of these traditional artifacts provides feedback in the form of visual cues , auditory cues , and kinesthetic cues .
Our design criteria were that the devices chosenwould be handheld, support pen-basedinput, allow serial port input ,have a developmentenvironment for custom applications, and be cost effective .
Ideally, we wanted severaldeviceswith different form factors.
We decidedto usetwo different portable devicesto test our manipulations.We chose to use a handheld computer for the pageturning andhandedness detectionmanipulations .
For the list navigation manipulations, we chose a Palm Pilotm.
Clearly, a number of other devices could have beenselected- thesetwo were chosenfor their ubiquity.
Generally, users conceptualizelists in diierent ways than books or documents.
A flick on the upper left comer from left to right would indicate "back one page".
These actions were highly similar to the real world actions.Visual feedbackwas similar; pageschanged, and the new destination page and page number becamevisible after the user action.
After a page turning manipulation, both the page number and the contents change to reflect either the preceding or next page, dependingupon the direction of the stroke.
However, we did not implement sound effectsin this iteration and some kinesthetic feedbackwas lost , Figure 1 shows a "real-world" page-turning gesture , and the implemented interface to the pageturning command on the CassioT"f.
We decided to try this approach since this allowed us to "retro-fit" pressuresensing technology onto a normally pressure-insensitive device.
Also, we did not need to use valuable screenreal estatefor the large areagraphicsneededto display a finger operatedbutton.
Finally, this would provide us with opportunities to later usethe sensortechnology in other application contextsand acrossapplications.
The extent and start/end points were not obviously represented.
Hence moving forward or backwardby chunks relative to the beginning or ending of a document was more difficult to represent for virtual documents.
We decidedto use a grasping manipulation at the top of the device, where the relative position of the graspdeterminedthe relative position within the document.
Far left corresponded to page 1 and far right corresponded to the last page.
While this was not tightly analogous to known real world metaphors,it appealedto the well-known GUI metaphorof the scroll bar.
A grasp gesturewill move to a new location in the documentand display the new location's pagenumberand contents.
This interaction requires that the left and right upper comer detecta finger press,the direction of a stroke, and a release of pressure.
Several implementation options are possible.
Within each application where documentreading occurs, a touch sensitive display can detectpressurepoints and their origin, determineif this aligns with a documentupper corner, track the path of pressureto determine the stroke direction, and execute the appropriate page turn.
Based on our chosen representation for moving by "chunks" and the correspondinguser action, we again decided to use pressuresensors.
To detect a grasp with the thumb and a finger, pressurestrips were attachedalong the front top edge of the device.
Grasping any portion of the strip moves to a document position relative to the beginning or ending of the document,where the beginning maps to the far left of the strip and the end mapsto the far right of the strip.
For example,Figure 2 showsa "grasp" gesture moving the display to roughly 2l3 of the way through of the document.
On the Pahn PilotThi, the user action was in fact a tilt movement away from a neutral resting position and not a rotational turn of a knob .
Insteadof having a rate or speedof turning we usedthe extent or degreeof tilt .
Turning "harder'  moves faster through the list, similar to Rekimoto .
To stop at or select a particular item, the user either ceasesto tilt , or squeezes the device, mimicking a grasping gesture .
For the handedness detectiontask we neededto understand something about how users hold and operatethe intended device.
We designedthis such that no specialmanipulation was neededother than picking up the device and/or stylus.
The handedness detection is immediately visible when the user invokes any application that wants to be "handedness-aware".In the specific task we implemented,for a right-handed user, text is immediately left justified and annotationspaceremains at the right side of the text .
The reverseis true for left handed users.
When both hands are used or the device is set down , the text appearscentered.
Feedback from the annotation remains consistent with the real world case;ink trails appearas the pen writes.
Finally, we examinedsensoroptions for unobtrusively determining handedness.
Several implementation paths were considered.
Heat sensorson either side of the device could potentially detectwhether contact with a hand occurred on the right or left side of the device .
However, this detection would be complex since physical extremities such as hands and feet generate heat levels comparableto many other environmental factors, including that of the device itself.
This is somewhat problematic since these algorithms are complex and the system can only take effect after the user has started writing.
We decided to use pressuresensorsagain, this time to determinethe points of contact and detecthow the device was being held .
Pressuresensing pads were attachedto the back of the device, on the left and right sides,in alignment with positions usedfor holding the device.
In our systemthe tilt angle is converted into a Pbit value and transmittedto the Palm PilotTh'across an RS232 link after being prefixed with the 4-bit sensorID, a total of 8 bits for eachtilt sample.
By mounting a tilt sensorof this type to the caseof a Palm Pilotm*, with the sensorplates parallel to the plane of the display, we were able to use the sensorreadingsas a crude measureof the computer's orientation relative to gravity.
We arrangedit so that the Pilot generateda neutral reading at the 45 degreepoint and produced8 readingsforward and backwardsfrom that position: 45 degreesbeing close to the angle that most people use to read from the display of the Pilot.
Even though the range of angles detectableis thus very coarsely defined, we found that it has been adequate to implement and supportthe Rolodex-like metaphor.
In addition to sensing tilt, the system must differentiate betweeninadvertent tilting, such as when walking with it, and intentional tilting, when the user wishes to navigate.
There are two possible ways of addressingthis issue.
The first method is to apply higher threshold values to the tilt sensing itself, thereby removing manipulations which are not of extremes and hence presumably retaining only deliberate user requests.This was infeasible in our desired application since we wished to userangesof tilt to indicate the rate of list movement.Another possible solution is to create a second specific manipulation that indicates user intention.
In our case,we decidedto use an initial squeeze of the device to indicate the desire to navigate through the list, followed by a second squeezeto `grasp" the desired item, thereby ending the navigation task.
To avoid muscle stress, usersdid not have to maintain the squeezingpressure during navigation.
The device was paddedwith foam to further suggestsqueezingcapability.
To achieve the squeezefeature, we attachedpressuresensors along both sides of the Palm Pilotm in positions that aligned with the users' fingers and thumb .
To differentiate squeezingfrom holding the device, we tested 10 users and derived an appropriatethresholdvalue for the pressuresensors.
The "squeezing" gesturehas several advantages for usersof a hand-held device.
It doesn't require that the user reposition either hand, or that the usersalter their viewing angle or viewing distance to the device, requiring only a momentaryincreasein the pressureusedto hold the device.
While the sensorsreport which finger are exerting this pressure,at presentour algorithms make no use of this additional informationThe list navigation task provides two levels of user feedback Since the device is often moved about, the "tilt feature" is initially disabled.
When users wish to navigate through lists they commencemovement by squeezingthe device.
At this point the device is tilt-enabled.
The Casio device was augmentedwith a network of pressure sensors.
Two overlaid strips on the top edgedetectthe page turning manipulations .
The pressure sensor nehvork reports its current values through an interface connectedto the RS232 port on the device.
A simple communicationsprotocol was devised,where each packet indicates the ID of the reporting sensor, and the current value of the sensor.Packetsare only sent when the value changes.Absolute values, rather than deltas, are reported so that we can recover from dropped/damaged packets.The documentreading application runs as a multithreadedapplication under Windows CE: one thread performs user I/O, while the other monitors the sensorstream.
To implement the pageturning manipulations,hvo pressure sensorsare overlaid at the top edge of the device.
One type of sensor strip reports pressure,but not spatial location.
The secondtype reportsspatial location, but not pressure.
Unfortunately, the spatial sensor tended to have a great deal of jitter.
In order to compensate for this, hvo measurements were made from the spatial sensor:the first measuringthe distancefrom the left end, the secondmeasuring the distance fi-om the right end The sum of thesehvo values should be a constant- if they differ too much from this constant,the values are rejected.
Otherwise, the average of the two values is used.
The {location, pressure} values are stored from the moment of pressure-downto pressure-up.
If the sum of the inter-location differencesis negative, the user is deemedto be stroking from right-toleft.
If the sum is positive, the user is deemedto be stroking from left-to-right If, regardlessof this sum, the range of spatial locations is in a narrow range,the user is deemed to be pressingat a certain spot .
Navigation through Sequential Lists In order to implement a tilt detection mechanismfor continuous list scrolling on a handheld computer, we investigateda number of sensors.
The commercially available tiltsensordesign we choseis basedon an electrolyte bordered on two sidesby a pair of conductiveplates.As the device is angled towards or away from either plate, the amount of electrolyte in contactwith the plate varies.The areaof fluid in contact with each plate will affect the impedancepresented by the contacts of the sensor.
Clearly, a different message and a different meansof conveying tilt-enabled would be better.
Independentof this or any message, it is visually obvious when tilt-based navigation is enabled.Tilting works as described and userscan seethe list flipping through entriesat varying rates of speedin the appropriatedirection, dependingupon the direction and magnitudeof the tilt The display ceases moving when the usereither holds the device in the neutral position or againsqueezes the device, therebydisabling tilt.
Since no explicit commandsor manipulations were needed,users seemedamazedthat the device recognixed and optimized for handedness.
They were unable to tell how this was accomplishedwithout us explaining it.
This suggestsnot only that passive manipulations can be powerful, but that they greatly impact a user's interaction experiencewhen well integratedwith the form factor of the device.We clearly needto explore more passivemanipulations to seeif this is a generalproperty.
Additionally, this illustrates an opportunity for computationally augmented task representations that provide more than the real world analogy.
Since these handheld devices are typically gripped by the edge with one hand, while they are used with the other hand, we detectedhandedness by mounting two pressuresensitive pads on the back surface.If the pressurepad on the back left is pressed,the sensorthread of the program concludesthat the user is holding the device with  the left hand If the pad on the back right is pressed,the program concludesthat the user is holding the device with the right hand.
A numberof designissuesaroseas we iterated through the design and developmentof the prototypes.We did a number of in-laboratory, informal user teststo estimatethreshold values for sensors.Typically this was done with our immediate researchproject group and a few other interested people .
Once the sensorvalues were initially determined,we then carried out informal user testing and interviews on 15 different peopleoutsideour researchproject group.
Userswere not instructedon how to hold the devices.They were given only brief descriptions of what the sensors would do .
Following this, we observedthem and recordedtheir comments.We askedthem specific questionsabout what they expected to occur, what problems they encountered,and what they liked most and least.
Severalinteresting usageobservationswere made in these tasks.
Becauseof our need to overlay pressure sensors, usersnow had to exert greaterpressurethan they expected for the page-turningmanipulations.
Users try out manipulations basedon their expectationsfrom the real world.
A page turn in a paperbackbook, for example, takes very little pressure.All of our users initially attemptedexactly the samemanipulation on the device, which was too light to be sensed.However, they were able to quickly adjust with practice.
In general, we believe that users will attempt to exactly replicate the analogous real-world manipulation, when those metaphorsare used; and they will expect them to work Jf we are striving for enriched interaction experiences,the more exactly we can support or match theseexpectationsthe better.Making our sensorsmore sensitive to detectlighter pageturning strokeswould clearly be an improvement.
Users had no problem in discovering the manipulation neededfor "previous page" once they had tried the "next page" manipulation.
Despite slight differencesin the pressure required over that of real-world interaction, usersrelied on extending their understanding of the real-world metaphor to guide their further assumptionsabout what was possible with the device-embodied interface.
As in GUI design,small inconsistenciesin metaphorseemto be forgiven.
Users neededto have the "navigation by chunks" mechanism describedto them.
Almost certainly this was because the device did not resemblea book, nor did the manipulation map directly to the manipulation in the real world.
Graspingalong a strip to indicate relative position is unique to this interaction.
Once describedor briefly demonstrated, users had no trouble remembering this manipulation or applying it.
In general, our test users found the manipulations "intuitive", "cool", and "pretty obvious in terms of what was going on."
Some users needed quick demonstrationsto understandthat their manipulations would actually be interpreted.Our usershad little or no exposureto physically embeddeduser interfaces and therefore often did not expect interaction with the deviceto be understood.
Undoubtedly, conveying the basic paradigmwill be necessary in the sameway that users neededto understandthe conceptual foundation for direct manipulation interfaces and mice.
Once users understoodthe basic paradigm, they immediately began to explore the range of interaction.
Just as GUIs users try to find out what is "clickable" by moving around the screen with the cursor and clicking, our test userstried a variety of manipulationson the prototypesto see what the range of detectablemanipulations was.
For example, to turn pagesthey tried long and short strokes,
Our disambiguating algorithm would sometimescomputethis incorrectly, which would surprise the users.Since users' finger widths vary and we also want to support short strokes,this problem is not easily solved We need to re-examine  whether there are better sensingtechnologiesavailable or a different sensor con@rration that would solve this,  whether minor alterationsto the manipulations usedwould help differentiate the two, or  whether there is a better metaphorfor navigation by chunks.
In general, the "navigation by chunks" task illustrates the tradeoff behveen intuitive real-world mappings which try to stay true to the real-world  versus learned mappings .
At this point, it is unclear how much learning is reasonablegiven that the overall goal is enrichedinteraction experienceand intuitive interfaces.
Navigation through Sequential Lists The list navigation task revealed some additional design issues.
One issue was determining the range of angles for the tilt operationand the value for the neutral angle where the device remains in a resting state.
We determinedthe initial neutral angle by in-laboratory testing.
The range of tilt angles was partly basedonjust noticeabledifferences,both in terms of discemabletilt anglesand in terms of discernable list scrolling speeds.
The range of perceptibletilt is clearly an important determinant in setting and assigning values for the tilt manipulation's parameters.At present the 16 sensedtilt anglesmap to 6 different rates of scrolling.
One result of broaderusertesting identified the difficulty in stopping at a particular item within a list.
Users would scroll quickly to the correct general arm then attempt to scroll slowly to the desireditem.
We now believe that our slow scrolling speedis still set too fast, as users tend to overshoot the target item.
We believe that it is generally necessaryto fine tune continuously issued gestural commands that control rate and/or direction of a corresponding action.
We are investigating this issue further to determine how much individual differenceamongstuserseffectstheir ability to precisely control list manipulation.
Trained parameterizationor user-customizedsettings may also help alleviate this problem.
Finally, as a consequence of using tilt to control list navigation, display visibility was an issue.
In particular, we avoided use of extreme angles of tilt, since the Palm Pilotm display was not readable at these angles.
Different devices and/or displays have different viewing angle restrictions with must be taken into account if the display is the primary feedbackmechanismor if the display plays a centralrole in the task.
The passive sensing used in the detection of handedness worked amazingly well.
It detected and responded correctly, and users did not need to alter their usage of the device in any way from what seemednatural.
All users remarked on the "magical" nature of this feature.
We believe that the successof this feature is partly due to the inlaboratory, pre-testing we carried out.
We tested 15 different usersto fine-tune the placementof the pressurepads to accommodatedifferent sized hands, slight differences in method for holding the device, and whether the left and right hand were usedin exactly the samepositions.
A possibility for the strong positive reactions to the seamingly "magical" featurewas the augmentationof real-world capabilities.
By optimizing annotation space,we have created a function that does not exist in the corresponding real-world scenario.
In order to create computational enhancements ,the systemmust accurately "know" what the user wants to do.
These underlying assumptionsand the subsequent matching of systemsresponses to user expectation are crucial.
In this case, annotation optimization worked well becauseour assumptionsaccuratelypredicted user goals.
One aspectof this interaction that still requires adjustment is the changeoverfrom one hand to another.
If users momentarily shii the device from one hand to the other, the contents of the screen immediately move as well.
Some users commented that seeing the screen contents jump aroundwas disconcerting.However, it is unclear how often this scenario would arise in actual day-to-day usage.
One improvement would be to briefly delay the screenchange to determinethat the user is not merely rapidly shifting the device to be more comfortable.
Determining this time duration for detecting a "resting state" versusa "transient and temporarychange"might improve the current interface.
In this paper we have explored the design, implementation and testing of three specific casesof physical manipulation prototypes.
We discussedissues raised in designing and implementing these prototypes.
And we have briefly outlined someof the results of user testing of theseprototypes.
Our strategy is to implement other novel manipulations, new device-embodiedtask representations,and test new sensortechnologies.We are interestedin further exploring explicit manipulations as well as seamlesslysensedpassive manipulations,with a goal of better understandinghow this new paradigmcan enrich the user's interaction experience.
This new type of interaction can be very dependenton the form factor of the device being augmented.We are interestedin incorporating new manipulationsinto a number of different devices including tablet computers,conventional scanners, copiers,and monitors.
User expectationis often basedon real-world experience and feedback,particularly if strongly analogoussituations are represented.
We would like to augmentthe simple visual feedback of the current prototypes to also include auditory feedback, animation, and potentially increased tactile feedback.
We would also like to prototype somedeviceswithout displays and determinewhat kinds of manipulationsand status indicators are possible and what types of computational power canbe enhanced by suchdevices.
These should lead us to a better understandingof physically embedded user interfacesas a new paradigm, its limitations and strengths, and design principles to guide othersexploring this area.
We wish to particularly thank Tom Moran of Xerox PARC for his many substantivesuggestionson the structure of this paper and for his extensivecollaborative efforts in defining possible theoretical frameworks for this work .
We also thank the CHI reviewers for their helpful commentsand suggestions.
Our researchadditionally benefited from input given by membersof the Portable Document Readergroup at Xerox PARC, DpiX, and FXPAL.
Finally, thanks to Rob Burtzlaff, patent attorney extraordinaire, without whom this papermight not have beenreleasedfor publication.
Passive Real-World Interface Props for Neurosurgical Visualization, Proceedings of CHZ'94, pp.
Ishii, H. and Ulhner, B. Tangible Bits: Towards Seamless Interfacesbehwen People, Bits, and Atoms.
Rastogi,A., and Grodski, J. J. Telerobotic Control Using Augmented Reality.
The Future of Interactive Systems and the Emergenceof Direct Manipulation.
12.Smal1,D., and Ishii, H. Design of Spatially Aware GraspableDisplays.
Extended Abstracts of CHZ'97, pp.
A Morphological Analysis of the Design Spaceof Input Devices.
Fishkin, K. P., Moran, T., and Harrison, B. L. Design Principles for Manipulative User Interfaces.
Xerox Palo Alto Research Center.Palo Alto, CA.
