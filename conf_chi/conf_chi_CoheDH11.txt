3D transformation widgets are commonly used in many 3D applications operated from mice and keyboards.
These user interfaces allow independent control of translations, rotations, and scaling for manipulation of 3D objects.
In this paper, we study how these widgets can be adapted to the tactile paradigm.
We have explored an approach where users apply rotations by means of physically plausible gestures, and we have extended successful 2D tactile principles to the context of 3D interaction.
These investigations led to the design of a new 3D transformation widget, tBox, that can been operated easily and efficiently from gestures on touch-screens.
Our approach is different, and complementary.
We have designed a touch-based 3D transformation widget called tBox that favors the direct and independent control of 9 DOF; 3 translations, 3 rotations, and 3 scalings.
The design of this widget has been guided by initial observations of users interacting with standard 3D transformation widgets on touch-screens.
The main conclusions of this preliminary study were that the selection of the DOF controls is difficult as soon as the graphical elements project close to each others on the screen.
This is very frequent, in particular when all the DOF controls are displayed at the same time .
Moreover, we observed that users were sometimes perturbed by occlusion and ergonomic issues, and they had to think about how to position their hand.
Consequently, we have explored an alternative approach for the design of tBox, where precise and small mouse displacements are replaced by finger inputs that better fit the tactile paradigm.
Such a touch-based approach may open 3D modeling to many users who are not 3D experts.
This allows him to transform an idea he has in mind into a 3D scene.
Our final goal is not to replace all the mouse/keyboard modeling tools by their touch-based counterparts.
Since the introduction of Skitters and Jacks , 3D transformation widgets used in the manipulation of 3D objects have little evolved.
These 3D user interfaces  have been mainly designed for mouse-based systems where the user benefits from accurate pointing, distant interaction, an unobstructed view of the screen, and direct access to numerous buttons and keyboard shortcuts.
Touch-screens have none of these qualities as noted by Moscovitch .
Consequently, 3D transformation widgets need to be reinvented to adapt to the tactile paradigm.
An example of new 3D transformation widgets has been proposed by Schmidt et al.
Others have explored multi-touch controls where several degreesof-freedom  can be manipulated at the same time.
In particular, Reisman et al.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The tBox widget appears as a wireframe box, with face culling enabled .
Rotations are performed from physically plausible gestures, translations rely on the selection of the box edges, and scaling benefits from dual-touch input.
This design favors a direct access to all DOFs, while keeping a good separation of the actions to be applied.
Similarly to standard 3D transformation widgets, tBox is always visible, the widget being displayed on top of rendered objects.
The transformations applied to tBox are directly applied to the transformation matrix of the object being manipulated.
The box-shaped form-factor has been chosen for two main reasons.
First, it is an elementary 3D shape that provides good visual affordances for making it spin from "natural" gestures.
Second, this form-factor enhances axes selection from finger inputs, as we will discuss later in this paper.
In our approach, we have introduced new mechanisms aiming at enhancing interaction when users interact directly on touch-screens, from finger gestures.
Second, we observed that two strategies were used to make the cube spin.
In some cases, subjects followed the 3D orientation of the touched face, as if they were grabbing the cube .
In other cases, they follow a direction that is tangent to the rotation, at the targeted edge.
This corresponds to a push strategy where energy is minimized .
In both cases, the gestures rely on physically plausible behaviours, as we had expected.
During the experiments, 3 subjects used the push strategy only, 3 used the grab strategy only, and 4 used both strategies.
The mean scores about the pertinence and the easiness of the gestures applied by the subjects are very high , which tend to show that an approach based on such gestures could be easily understood and used.
We have observed in our preliminary study that one of the main difficulty when using standard widgets on touch-screens came with the control of rotations.
Thus, we initially focused our investigations on these DOFs.
Because motions with physical behaviors appear to be compelling on tactile screens, we investigated an approach where users rotate the widget as they would do with a real cube.
Our intuition was that the 2D gestures used to make a box spin around one of its primary axes can be characterized.
We conducted a pilot study to investigate this assumption.
We used a guessability study methodology, as proposed in .
The idea is to present the effects of gestures to participants, and elicits the causes meant to invoke them.
The study was performed on a Dell Latitude XT Tablet PC.
A video corresponding to a spinning cube and a static image of the same cube were displayed .
The subjects were asked: "Which gesture would you draw on the static cube to obtain the movements of the moving one?".
In addition, we asked them to assess their gesture by answering the questions "the gesture I did is a good match for its intended purpose"  and "the gesture I did is easy to perform" , as done in .
We used two Likert scales with ordinal responses from 1 = strongly disagree to 7 = strongly agree.
The subjects completed the task with thirty videos corresponding to the rotations in both directions around the three primary axes, for five viewpoints.
Ten subjects aged from 22 to 50  participated in this experiment.
Three were left-handed and seven were right-handed.
We recruited subjects with no experience in 3D modeling in order to avoid gestures guided by previous habit.
Interestingly, we obtain very comparable gestures from one subject to another, and from one view to another.
From these results, we made two main observations.
First, when they exist, the inner edges appear as the visual references from which users draw their gestures.
Consequently, these edges should have priority to the other edges in the rotation algo-
From the results described above, we have designed the rotation algorithm as follow.
If the continu ation of v intersects with the screen-space projection of an  inner edge at point I , we see if v 's 3D projection on the cube  is tangent to the corresponding rotation , or if v is aligned with the tangent line  at point I , projected in 2D .
If one of these conditions is verified, then a corresponding rotation should start.
Otherwise, we see if similar conditions are verified with a crossed exterior edge.
Note that we increase the edge length when computing intersections.
Hence, users do not need to cross the cube edges exactly.
When the box is almost aligned with the viewing direction, we also consider the depth-oriented edges that are not visible, as illustrated in Figure 3.
To make the cube spin, users "push" or "grab" it by way of straight gestures.
Technically, a linear mapping is applied between users' gestures and tBox rotations.
Fast and brief inputs correspond to flicking gestures.
The rotation continues with a given inertia, and it stops after a short time of decreasing speed or when the user stops the movement with a tap input.
Slower and continuous gestures allow the rotations to be refined.
Moreover, by using their two hands on both sides of the cube, users can adjust rotation angles from successive inputs while keeping a good visualization of the manipulated object.
Figure 4 illustrates tBox being rotated.
The translation mode and, consequently, the effective translation of the manipulated object starts when the cylinder collides with the other edges of the tBox widget as illustrated in Figure 6.
This approach is inspired by sliding widgets, which are very well suited for tactile interaction .
A famous example is the unlock slider of the Apple iPhone where a sliding gesture is preferred compared to a simple tap input.
The validation tests we performed have shown that these 3D sliding widgets make the rotation interface more robust.
Note that when several translations are chained, the sliding widget is used to activate the first one only.
In order to well separate it from rotations, we have based the activation of translations on the  selection of tBox edges.
To apply a translation along a primary axis, the user selects a colored segment that is centered on one of the corresponding edges of the tBox widget.
Compared to standard 3D transformation widgets based on triplets of arrows, a box-shape form-factor is valuable when used on touchscreens.
Indeed, the visual components to be caught are spread around the focus area, which limits the selection issues linked to the fat finger problem .
Moreover, two to three box edges per direction can be easily selected at any time .
Consequently, users can choose the best edge to be caught in order to avoid ergonomic and occlusion issues.
Beyond single axial translations, the design of the tBox widget also favours multi-finger input.
First, by chaining translation operations with two or three fingers, users can quickly reach 3D locations with successive refinements along the primary axes x, y, z.
Then, the simultaneous input of two fingers on axis edges allows translations on planes.
A double-tap input on an edge with a second finger, attaches the first finger movements to the corresponding translation.
Hence, the translation-in-plane mode can be controlled from a single finger input.
A sliding widget appears when a colored segment is touched .
The translation starts after the slider collides with the other edges of the box .
When the projected size of an edge is too small, the translation slider is adapted, so the gesture performed before starting a translation is similar for any situations .
For scaling operations, tBox relies on the standard dual-touch metaphor that allows users to resize 2D objects with pull apart and shrink gestures.
This 2D metaphor is very convincing as the semantic link between the user gesture and the resulting action works well.
In our approach, a pull apart or shrink gesture inside or on both sides of the tBox widget implies the widget, and consequently the attached object, to be resized uniformly .
In addition to uniform scaling, bi-directional scaling along the three primary axes of the tBox widget can be controlled by selecting two opposite edges of a face, and moving them away from each other.
Both fingers can be moved in opposite directions at the same time for symmetric scales.
If one finger moves while the second one remains static, then the manipulated object is extended in the direction of the moving finger .
First experiments with tBox have shown that the rotation gestures tended to start unwanted translations, when the finger motion were involuntary started on a tBox edge.
Consequently, we introduced a new mechanism to prevent from erroneous inputs.
To start a translation along one of the x, y, or z axis, the user touches a corresponding colored segment.
However, the translation mode is not activated directly.
Concretely, when two edges have their middle point that project at a distance which is smaller than the finger contact area, we keep the one being closer to the observer only.
Hence a large tolerance area can be maintained around the edges that can be selected, which ensures easy selection on touch-screens.
Moreover, the edges whose projections are too small are disabled to prevent from inaccurate depth-oriented translations.
For selecting or deselecting an object, we use double-tap gestures.
In addition, standard multi-touch camera controls are used to zoom , orbit , or pan the view .
Hence, users can directly access all the controls without menus or additional buttons, which favours fast and direct interaction on touch-screens.
In this paper, we have presented an approach that adapts 3D transformation widgets to the tactile paradigm, inspired from what has been learned these past few years in the scope of 2D touch-based interaction.
We have conceived tBox, a new UI that favors the independent control of 9 DOF, and that is complementary to other 3D multi-touch techniques .
Preliminary experiments have shown that tBox can be used easily both by 3D expert and novice users for 3D manipulation tasks on a touch-screen.
Further user studies need to be conducted to better understand user performance, in particular for precise and fully-controlled 3D tasks.
One research direction that also seems interesting is to continue investigating the relation between 3D shapes perception and user actions, as we started to study with the cube rotation task.
We hope that our work will inspire new research in the scope of interaction with 3D content on touch-screens, with the final goal of making rich interactive 3D applications more accessible to everyone.
We conducted an informal user study with eight participants.
Four of them had no familiarity with 3D modeling, and four were 3D experts.
We asked the participants to play with the interface, with no precise goal.
The experimental scene was composed of several objects that can be assembled together for the creation of a character .
The participants were encouraged to "think aloud".
We informed them that they were able to control translations, rotations, and scaling as well as the camera view from touch gestures, but we did not explained them how to proceed.
We observed that the participants discovered almost all the functionalities by themselves very quickly.
This confirmed that the affordances provided by the widget allow a good understanding of its functionalities.
Moreover, the multi-touch gestures appear to be known as standard gestures by the participants, even for those who are not familiar with multitouch systems.
Note that none of the participants discovered the translation-in-plane mode, which appeared as an advanced feature.
Both expert and novice users managed to assemble the character as they wanted.
The participants reported that they liked the rotation mechanism based on physical behaviors.
They indicated that it worked well, and that they managed to apply the rotation they wanted easily.
We observed that the activation of translations and scaling was performed without any difficulty.
None of the subjects appeared to be disturbed by the sliding widget.
This is very beneficial as this mechanism prevents from erroneous activations.
3D transformation widgets have shown undeniable benefits for mouse and keyboard systems.
Unfortunately, these UIs are of limited usability when used on touch-screens, as input spaces largely differ.
This excludes numerous 3D ap-
Skitters and jacks: interactive 3d positioning tools.
M. Hancock, S. Carpendale, A. Cockburn, and NZ.
Shallow-depth 3d interaction: design and evaluation of one-, two-and three-touch techniques.
In CHI '07: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 1147-1156.
M. Hancock, T. ten Cate, and S. Carpendale.
Sticky tools: Full 6DOF force-based interaction for multi-touch tables.
Iterative design of an interface for easy 3-d direct manipulation.
In CHI '92: Proceedings of the SIGCHI conference on Human factors in computing systems, pages 135-142.
A. Martinet, G. Casiez, and L. Grisoni.
The effect of dof separation in 3d manipulation tasks with multi-touch displays.
Contact area interaction with sliding widgets.
In UIST '09: Proceedings of the 22nd annual ACM symposium on User interface software and technology, pages 13-22.
A screen-space formulation for 2d and 3d direct manipulation.
In UIST '09: Proceedings of the 22nd annual ACM symposium on User interface software and technology, pages 69-78.
R. Schmidt, K. Singh, and R. Balakrishnan.
Sketching and composing widgets for 3d manipulation.
User-defined gestures for surface computing.
In CHI '09: Proceedings of the 27th international conference on Human factors in computing systems, pages 1083-1092.
