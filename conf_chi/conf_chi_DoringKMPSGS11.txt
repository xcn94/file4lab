One trend for tertiary task input devices is to place them into spaces previously reserved for primary and secondary devices.
The available space on the steering wheel for example is now often used for interacting with the entertainment system, navigation system or mobile phone .
The advantage of using space on the steering wheel is that the buttons or thumbwheels are very close to the driver's hand so there is no need to move the hand away from the steering wheel, improving safety.
However, the arrangement of physical input devices is fixed and the space for mechanical buttons is limited.
To further explore the potential of the steering wheel as a location for tertiary task input and output and the advantages that touch gestures might offer, we built a fully functional prototype of a multi-touch enabled steering wheel.
Our research is motivated by the following: 1.
Driver Distraction: Bringing tertiary tasks to the steering wheel has already proven to be a "best practice" in the design of many existing cars.
Nevertheless, no scientific studies are yet publicly available that compare driver distraction regarding steering wheel and middle console input for infotainment systems.
Gestural Input: Gesture-based input on multi-touch surfaces allows the execution of many different commands in a limited space - from basic to complex - and for a variety of applications and tasks.
Graphical Input/Output: A multi-touch steering wheel can also contain a display, i.e.
This leads to questions about how visual output on the steering wheel should appear, and how it might affect driving.
Cars offer an increasing number of infotainment systems as well as comfort functions that can be controlled by the driver.
In our research, we investigate new interaction techniques that aim to make it easier to interact with these systems while driving.
We suggest utilizing the steering wheel as an additional interaction surface.
In this paper, we present two user studies conducted with a working prototype of a multi-touch steering wheel.
In the first, we developed a user-defined steering wheel gesture set, and in the second, we applied the identified gestures and compared their application to conventional user interaction with infotainment systems in terms of driver distraction.
The main outcome was that driver's visual demand is reduced significantly by using gestural interaction on the multi-touch steering wheel.
New media and communication technologies  provide increasing entertainment and communication opportunities while driving.
Furthermore, functions like adaptive cruise control and lane-keeping assistance support drivers, reducing their mental workload, and increasing their capacity to share their attention between driving and consuming media content.
Nevertheless, these tasks  demand attention as they force the driver to interact with built-in systems or nomadic devices.
Currently, the central surface of the steering wheel is not used as an input or output element as there is the airbag underneath in most designs.
In the case of an accident the surface breaks to release the airbag.
We expect that with emerging display technologies this is not longer a limitation, as displays will be constructed to break or will be made of segments that allow the airbag to open.
In this paper, we describe the design challenges and setup of a prototype multi-touch enabled steering wheel.
We present two user studies.
In study 1, we investigated which gestures users chose for a number of defined actions conducted on the steering wheel while driving.
Study 2 builds on the results of study 1 and applies the identified gestures in a comparative study.
Using eye tracking in a driving simulator, we measured the driver distraction when interacting with the steering wheel as well as with the middle console.
The central finding is that interaction using a multitouch steering wheel strongly reduces the visual demand to control a radio and to control a navigation system.
As gestures can potentially support an intuitive form of interaction, an important research topic has been the design of free hand gestures on tabletop surfaces.
Nevertheless, the design of a suitable set of gestures is a challenging task for system designers.
Among their results was a user-defined set of gestures for 27 actions and the insight that users generally do not care about the number of fingers used for a gesture.
A discussion has also begun on the "intuitiveness" of so-called "natural" user interfaces , suggesting a critical look at the learnability and memorability of gestures.
In the following sections, we focus on the potential of gestural input on a steering wheel and on interacting with specific functions typical for in-car use.
A number of researchers have investigated using the steering wheel for interaction, specifically for text input .
For gestural input, a touch screen mounted on the vertical center stack was used.
Their results indicated that gestural interaction is slower than touch or haptic interaction, but can reduce eye glances while interacting with the radio.
They observed an almost eyes-free interaction with the pie menu after a training phase.
Harrison and Hudson  investigated a combination of a touch sensitive surface and physical buttons for nearly eyes-free interaction.
They developed a visual display with deformable areas so that physical buttons can be produced flexibly but on fixed positions on the surface.
Beside kiosks and ATM systems they investigated the use of a dashboard comprising such a flexible display.
One could imagine using this on the steering wheel as well.
Multi-touch technologies allow direct gesture-based interactions with fingers on interactive surfaces .
Derived from the three issues mentioned in the introduction, a number of design challenges have to be addressed on the use of a multi-touch steering wheel in cars.
We focus on the following questions: 1.
Driver Distraction: Can we reduce the cognitive load of interacting with infotainment systems with a multitouch steering wheel?
Obviously, the functioning of the steering wheel as well as the visibility of all instruments should not be affected.
Gestural Input: Can we find gestures such that the driver should not have to move her hands from the steering wheel or her eyes from the road?
A closer look at thumb gestures appears to be promising.
Graphical Input/Output: By converting the steering wheel into a multi-touch surface, the whole space can be used for touch input and graphical output.
This leads to questions of where to define interaction areas and what kind of visual feedback to display on the steering wheel.
We addressed these issues by building a multi-touch steering wheel prototype and integrating it into a driving simulation apparatus for two user studies.
The prototype is described in the following section.
Thereafter, we describe study 1, which addressed questions regarding gestural input.
Finally, study 2 built upon the results of study 1 and validated a set of identified user-defined gestures, comparing driver performance and driver distraction to middle console device interaction.
An 11 mm thick round clear acrylic glass with a radius of 35 cm  was fixed to a special mounting and used as the steering wheel "body".
We applied the FTIR  principle  to enable multi-touch input and attached infrared LEDs beneath the steering wheel cover as well as a silicone layer and tracing paper on top of the acrylic glass.
The whole setup was mounted on a rotatable stand.
A camera and a projector were attached on a board at the bottom of the mounting.
For image processing we used the open source software CCV2, which sent touch events in TUIO protocol format  to a Flash application that was responsible for the visual representation of interactive elements on the steering wheel.
In order to have a setting for identifying user-defined gestures and to investigate whether driver distraction could be reduced with multi-touch steering wheels, we installed a driving simulator setup in our lab .
An HD projection of 3x2 meters was used to show driving scenarios.
A WiiRemote was attached to the steering wheel and delivered steering information .
BlueSoleil3 and EIToolkit4 were used for the communication between the WiiRemote and the driving simulations.
EIToolkit is a component-based architecture that allows proxy-like objects to exchange messages over a general communication area, e.g., via UDP.
We proposed 20 commands that could be useful to perform directly on the steering wheel.
We chose two exemplary applications, a music player and a navigation system, including commands for general menu access and list navigation.
Table 1 gives an overview of the commands that participants were asked to perform on the steering wheel.
We provided a simple driving task and, after running several pilot studies, decided to restrict the interaction opportunities by equipping our multi-touch surface with two input fields close to the left and right edges of the steering wheel .
Interactions on the screen were only recognized in these areas, so that the driver could leave both hands on the steering wheel, when choosing the thumbs for interaction.
Apart from displaying the interaction areas, we did not provide visual feedback on the steering wheel during this study.
Performed gestures were recorded by a screen record program capturing the finger detection images as well as by a webcam mounted above the setup.
Thus, the video footage provided the finger movement as well as the gesture trails and was analyzed later to collect the user-defined gestures.
We used the CARS driving simulation software5 to provide a simple driving task without collecting driving performance measures.
We presented the user a two lane endless highway, where participants had to change lane when an obstacle blocked their way.
After being introduced to the driving setup, the participants carried out a test drive without any additional task to get used to the driving simulator.
While driving thereafter, they were asked to create gestures for the 20 different commands as listed in table 1.
All participants performed the commands in a randomly assigned order.
Participants could take as much time for each task as they wanted.
Furthermore, they were free to choose which and how many fingers they would use for the gesture but at least one hand had to remain on the steering wheel during driving.
They were instructed to think aloud.
After accomplishing all 20 commands, participants filled out a questionnaire that contained questions concerning ratings of the gesture interaction on the steering wheel.
Further open-text explanations for their statements were collected, e.g., advantages and disadvantages, as well as demographic data.
Each experiment took between 20 and 30 minutes.
Through video analysis we collected 240 gestures in total, 12 individual user-defined suggestions for each of the 20 commands.
For each command, we selected the most frequently occurring gesture.
Overall, we observed that participants had little difficulty in inventing a gesture for each action.
For commands like zooming where gestures had already been seen or used by the participants in other contexts and on other devices , we found similarities to existing gestures .
Nevertheless, driving has special constraints and the gestures had to be adapted.
Gestures with thumbs were found to be especially well suited to the driving, where hands should ideally remain at the steering wheel.
In figure 2 we show 6 resulting map interactions with the navigation system, which were mainly conducted with two thumbs, one in each of the interaction areas.
The two-thumb gestures provided basic operations for interaction with maps.
When asking the participants, it became clear that they had already formed a mental model for this type of interaction based on personal experience with multi-touch devices or having seen other people using them.
The essential part of the zoom gestures is a convergent or divergent movement of two fingers or thumbs of the same or different hands.
The gestures for moving the map left, right, or up and down were inspired by interactions that users would carry out if interacting with a physical map.
The most popular gestures included touching and grasping a map with two fingers/thumbs and then moving both fingers/thumbs synchronously.
All 12 participants suggested very similar gestures for movements.
Two further gestures to control the navigation system were a "rotate map" command and the "new destination" command .
All 12 participants decided to execute the rotate command by putting either thumb and index finger of one hand onto the surface and rotating the hand 180 degrees or making a similar gesture trail using both thumbs.
The agreement on the "new destination" command was the weakest: 3 of 12 participants chose to draw the first letter of the word "destination".
4 participants traced the "play symbol"  used on HiFi systems.
For gestures indicating the next and previous song a strong similarity in the gestures was observed.
Similarly, volume control was similar between participants.
The least agreement was on a gesture for the stop action.
For the general menu access, the agreement also was low: 3 participants decided to tap with two fingers/thumbs onto the steering wheel to trigger the menu.
For the selection in the menu the agreement was higher: 6 people chose to draw a circle for "music player" and the first letter "N" for navigations system.
10 of 12 participants drew a question mark to enter the "help" command.
On the list interaction, all participants agreed and performed an up/down movement with one thumb/finger as in the "volume up/down" command.
In order to analyze and structure gestures, Wobbrock et al.
Among these, they suggest distinguishing between "symbolic", "physical", "metaphorical", and "abstract" gestures.
For the menu command "navigation system" and the navigation system command "new destination" no real symbols were found by the participants and thus, the majority chose to draw the first letter of the command words as gestures.
Abstract gestures can be found for the "menu access" command  and for the music player "stop" command .
Further -and fairly basic- metaphorical gestures were chosen for "list up/down" and music player interactions  in the form of single finger or thumb strokes in the direction that participants mapped the interaction to .
These are well known mappings that are based on embodied conceptual metaphors .
Overall, the participants stated in the questionnaires that they preferred gestures, even compared to buttons on steering wheels.
We found a strong desire to control a music player on touch-enabled steering wheels  whereas only 5 of 12 users stated that they would like to control a navigation system on the steering wheel.
This might be due to participants' skepticism regarding having to look at the steering wheel to see the visual output.
For our second study, we selected the 6 navigation system gestures  presented in figure 2 and the 6 music player gestures  presented in figure 3 in order to validate them and to see, whether participants can remember and conduct them without errors.
Furthermore, we were interested in the level of driving distraction caused by gestural interaction.
In order to evaluate the gesture set we compared using gestures on a multi-touch steering wheel to a conventional car radio and navigation system in the middle console .
While existing steering wheel buttons only offer certain input functionalities, middle console devices provide comparable input and output functions to those feasible on the multi-touch steering wheel .
To simplify our setup, we compared the two UIs and left the steering wheel buttons out.
To get reliable and comparable driving performance data we used the Lane Change Task   in this study.
LCT calculates the mean deviation between a normative model and the actual path followed and is in the process of becoming an ISO standardized tool6.
The main task of the LCT is steering the car along a 3-lane highway and changing lanes when overhead signs indicate this.
Because LCT ISO draft prescribes a constant speed of 60 km/h we chose a setup without pedals and instead set the speed directly to 60 km/h.
The experimental setup is shown in figure 4.
As laboratory tests are the standard method for testing the impact of automotive UIs on driver distraction and offer a safe procedure during first tests, we chose a driving simulator setup for this user study .
We developed a music player and a navigation application able to be controlled by gestures.
The applications provided functions for each user generated gesture.
For the navigation system, maps are shown directly on the screen of the multi-touch steering wheel.
A within-subjects design was employed, with each subject performing the task in all conditions in counterbalanced order.
We discriminated the following conditions: conventional middle console car radio , conventional middle console navigation system , touch gestures for the radio , and touch gestures for navigation .
When interacting with the radio  the users had to perform 6 different actions .
For interacting with the map  we selected 6 different interaction tasks with the navigation system  while driving.
The gestures for the multi-touch conditions  had to be executed as illustrated in figure 2 and figure 3, using thumbs or fingers, but always remaining one hand at the steering wheel.
Only gestures performed on the interaction fields on the left and right side of the steering wheel  were recognized.
Each run lasted exactly 3 minutes and was dedicated to one of the four interaction conditions .
Participants were asked to perform as many actions as they felt comfortable with during the run.
Thus, in the analysis of driving behavior, rather than controlling for the frequency of actions during the experiment, which would limit the user's freedom during the driving task, we decided to make this a random variable that was controlled for after the fact by removing its effect on other dependent variables, if any, through analysis of covariance.
With the beginning of the run, the experimenter gave a verbal instruction, e.g., "Please move the map one step to the left side".
After the participant had performed the action, the experimenter read the next instruction to him, in randomized order, and, if all 6 actions had been performed, starting over again.
Thus, we could assess the number of actions performed  in each 3-minute-drive as one dependent variable.
Further dependent variables included driving performance data  as well as data on the visual demand , i.e.
In order to quantitatively assess the task performance in each condition, we recorded the number of successfully performed tasks during each run under each condition.
The numbers of interface actions were compared with repeated measures ANOVAs for the radio task and for the navigation task.
Mean numbers of interface actions are shown for each condition in figure 5.
First, the participants received a brief introduction to the driving simulator setup and were asked about their radio and navigation usage while driving in a questionnaire.
We showed the participants how to drive in the simulator with the LCT.
The users could familiarize themselves with driving in order to explore how the virtual car reacted to their interaction with the steering wheel.
As driving a virtual car with our prototype steering wheel differs a bit from steering a real car, users generally need some test-driving to get familiar with the steering.
Afterwards, the experimenter explained how to use the conventional radio and navigation system and demonstrated the different gestures for the radio and navigation application with his thumbs while leaving his hands at the steering wheel.
Participants got 5 minutes time to try out all interactions and to commit them to memory.
Before driving under each condition participants got the opportunity to try out all interactions again.
The first run after this introduction was recorded as the initial reference drive .
The following 4 runs were performed while interacting with different media and modalities.
After 4 runs interacting under the different conditions, one run was performed without interaction .
In the second part, all 4 conditions were repeated .
The final run was again a reference drive  without interacting with the system.
A typical experiment would look like this: RefS, n, rg, r, ng, RefM, ng, rg, n, r, RefE.
For the navigation task, there were main effects of both interface condition, F=24.80, p<0.01, and time, F=64.25, p<0.01, but no interaction, with more actions being carried out with the gestural interface and more actions tending to be carried out in the second trial than the first: on average participants carried out 17.2% more actions with the gestural interface in the first trial and 22.2% more in the second trial.
Participants carried out 18.3% more actions with the gestural interface in the first trial and 18.0% more in the second trial.
As the frequency of interface actions varied between conditions, subsequent quantitative measures were compared controlling for this effect where appropriate as a covariate in an analysis of covariance.
Controlling for the effect of frequency of actions, there was a main effect of interface condition that approached significance, F = 3.80, p=0.058, with participants tending to deviate less from the lane in the gestural conditions.
If the driving performance was compared without controlling for the effect of frequency of actions, there was also no effect of interface condition for either the navigation task, F = 1.98, p>0.05 or the radio task F=0.38, p>0.05.
Thus, participants were able to carry out more actions with the gestural interface without affecting driving performance.
The estimated marginal mean lane deviation by condition is shown in figure 6.
For the navigation task, the covariate, frequency of actions, was not significantly related to the number of glances at the interface for the navigation task, F=1.63, p>0.05.
There was a significant effect of interface condition, with participants looking at the interface less in the gestural conditions than the console conditions, F=17.65, p<0.001.
There was no main effect of time.
Across the two trials, participants looked at the interface on average 58.1% less often with the gestural interface than with the console.
Controlling for this, there was a main effect of interface condition, F=85.36, p<0.001, with participants looking at the interface less often when using the gestural interface.
Looking at the estimated marginal means , participants looked at the gestural interface 77.2% less often than they looked at the console.
There was no effect of time.
Figure 7 presents the estimated marginal means for the number of glances by condition.
For the second measure of visual demand, the total time spent looking at the interface, there was no relationship with the covariate, frequency of actions in the navigation task, F=0.25, p>0.05.
There was however a main effect of interface condition, F=15.55, p<0.01, with participants spending on average 59.7% less time looking at the interface when using the gestural interface.
For the radio task, the covariate, frequency of actions, was significantly related to the total time participants spent looking at the interface, F=8.28, p<0.01.
Controlling for this, there was a main effect of interface condition, F=23.93, p<0.001, with participants spending 67.1% less time  looking at the interface when using the gestural interface.
Figure 8 presents the estimated marginal mean time spent looking at the interface by conditions.
Operating a navigation system requires more visual attention than operating a radio.
There is also a very clear and statistically significant difference for the same task using different interfaces.
For both the navigation task and the radio task using the multi-touch surface in the steering wheel substantially reduced the required visual demand, operationalized as the number of glances and total time spent looking at the interface, compared to the conventional console interface.
In the questionnaire, we asked the active drivers  among the participants what types of devices they use while driving and in what way they use radio and navigation system.
The radio was used by all of them very frequently and in most cases always when driving.
All of the participants commonly used the physical controls of the radio located in the middle console of the car to operate the radio.
For the navigation system, 8 of the 11 participants reported that they used it at least once a week.
All participants were used to operating navigation systems in the middle console .
Participants were asked to rate their expressed user experience with each system on a series of Likert scales relating to: how much they liked interacting with each of the systems  ; how distracting they found each of the systems ; and how easy they found each of the systems to use  .
Wilcoxon tests were used to follow up this finding.
A Bonferroni correction was applied, so all effects are reported at a 0.0167 level of significance.
The gestural radio interface was reported to be more enjoyable than the conventional radio interface .
The gestural navigation interface was also reported as more enjoyable to use than the conventional radio interface .
The gestural radio interface was also more enjoyable to use than the gestural navigation interface .
There was also an effect of interface condition on how distracting the participants found the task to be  = 22.41, p<0.001.
Post-hoc Wilcoxon tests with a Bonferroni correction indicated that the conventional radio interface was more distracting than the gestural radio interface ; that the conventional navigation interface was more distracting than the gestural navigation interface .
Differences in ratings of how distracting the gestural radio and gestural navigation interfaces were approached significance , with the navigation interface being rated as more distracting.
Finally, there was an effect of interface condition on how easy participants reported it was to use the interface, = 22.07, p<0.01.
The gestural radio interface was reported to be easier to use than the conventional radio interface ; the gestural navigation interface was rated as easier to use than the console navigation interface ; and the gestural radio interface was rated as easier to use than the gestural navigation interface .
In our experiment, we decided to compare interaction with the center console and the multi-touch steering wheel.
We see both options as extreme positions: all controls and visualizations in the middle console versus all controls and visualization on the steering wheel.
There are many cases in between, e.g., some controls and visualization in the middle console and some on the steering wheel.
Most cars currently on the market have the majority of controls and the visualization for infotainment systems in the middle console and a small set of additional physical controls on the steer-
In order to have an experiment with a small number of conditions and to make it easier reproducible we chose a clear separation and looked only at the two different options.
As we found a number of significant results in our user study with 12 participants, especially with regard to required visual attention, we consider the chosen setup as a good initial data point to show that having flexible input and output unified on the steering wheel is potentially superior to interaction on the middle console.
In future experiments, it could be useful to explore further variations on the study design  and integrate larger groups of participants.
Our results indicate that gestural control on the steering wheel can serve as a viable option for future car user interfaces.
The reduction in gaze time required to operate controls when using a multi-touch surface on the steering wheel is the major finding.
Intuitively, one would expect that physical controls with haptic properties  would help users to operate them without looking.
However, our experiments showed that gestural input on the steering wheel is superior with regard to the visual demand compared to UIs in the middle console.
One reason for this seems to be that users do not have to hit a specific spot to perform input.
Users could keep their hands on the steering wheel all the time; potentially increasing safety.
Overall, we have shown that two safety critical parameters, namely demand on the driver's visual attention and positioning of the hands while driving, can be improved by moving controls onto a multi-touch surface in the steering wheel.
The user-defined gesture set identified in study 1 seemed well suited to many of the participants in study 2.
It took little effort to learning gestures and they commented positively on this.
With many devices on the market, in particular smart phones and tablets, users have already learned what gestures they consider "natural".
Several of our participants had no previous experience with gestural input on multi-touch surfaces personally, but their expectations and suggestions were driven by what they had seen other people doing or what they learned from advertising.
Hence, we expect as gestures become very common in humancomputer interaction, a basic set  will become commonly agreed.
And, as our study showed, users transfer those expectations from one device to another, e.g., from the phone to the steering wheel.
Therefore, we think it is essential to support users by designing gestural interaction that conforms to their expectations, but also fits the interaction environment.
Our experiments looked at two tasks with different visual demands.
Controlling a radio has no inherent visual demand other than to find and use the controls, as the output is not visual.
In contrast, manipulation of a map requires visual attention in order to complete a task.
Our results show that tasks that have no inherent visual demand can potentially benefit significantly from using gestural input.
The reduction of gaze time on the control by 67% and of number of glances at the interface by 77 % for the radio interface indicates that such tasks can benefit strongly from this type of interaction.
For the navigation task, we see a reduction of gaze time of 58% and number of glances by 60% due to the fact that users have to look at the display to complete the task.
However, during our experiments we observed that the time people look at the multi-touch steering wheel display is largely spent on the task and not for finding controls or interacting.
Overall, our results indicate that the effect of moving controls onto a multi-touch steering wheel are strongest for applications that require little or no visual attention for the task itself.
With using the entire surface of the steering wheel as an I/O surface, the flexibility to design interactive controls in the car increases.
There are interesting options with regard to the positioning of content and controls:  they can stay horizontal, independent of the rotation of the steering wheel,  they can rotate with the steering and  they can stay next to the user's hand.
Depending on the functionality provided, these options may be combined.
E.g., a design for a phone book could include the contact details always horizontally in the middle  and the controls to make a call in reach of the driver's fingers .
We have not investigated the usability of these combined visualizations yet, and we expect that further studies will explore the new design space of multi-touch steering wheels.
In this paper, we introduce the idea of a multi-touch steering wheel that allows gestural input as well as visual output.
By integrating the interaction surface into the steering wheel, users can interact and still leave their hands in the preferred position for driving.
In a first study with 12 participants we collected a gesture set for 20 typical interactions for controlling the infotainment system in a car.
In a second experiment, we compared gestures on a multi-touch steering wheel with interaction via traditional physical controls positioned in the middle console.
The main finding is that interaction using a multi-touch steering wheel reduced the visual demand by a large degree.
In the case of controlling typical functions of a radio, a reduction of 67-77% was observed, depending on the measure.
In the case of a navigation task, where the task requires visual attention, a reduction of 58-60% was seen.
Our observations during the user studies suggest that the multitouch steering wheel is a step towards controls than can be used without visual attention and at the same time can offer visual feedback for fast recovery in the case of a problem.
The driving performance measured with LCT showed no significant difference between the modalities.
This means that participants conducted more actions with the gestural interface without affecting driving performance.
In addition to the quantitative results participants provided very positive feedback on the gestural interface.
They found the use of the multi-touch steering wheel instantly understandable and easy to use.
In future research, we plan to integrate this technology into a car in order to explore the potential of a multi-touch steering wheel for different applications more comprehensively.
