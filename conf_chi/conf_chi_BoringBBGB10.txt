Hasso Plattner Institute August-Bebel Str.
88, 14482 Potsdam, Germany {sean.gustafson, patrick.baudisch}@hpi.uni-potsdam.de While the metaphor is still interesting, the environments and usage scenarios have changed since that time.
In this paper we revisit this metaphor and investigate its suitability for mobile use.
We present Touch Projector, a system that enables users to interact with remote screens through a live video image on their mobile device.
The handheld device tracks itself with respect to the surrounding displays.
Touch on the video image is "projected" onto the target display in view, as if it had occurred there.
This literal adaptation of Tani's idea, however, fails because handheld video does not offer enough stability and control to enable precise manipulation.
We address this with a series of improvements, including zooming and freezing the video image.
In a user study, participants selected targets and dragged targets between displays using the literal and three improved versions.
We found that participants achieved highest performance with automatic zooming and temporary image freezing.
Touch Projector allows users to manipulate content on distant displays that are unreachable, such as  displays outside a window, or  a tabletop system crowded with people.
It allows users to manipulate devices that are incapable of touch interaction, such as  a wall projection or  a laptop.
Users point the device at the respective display and manipulate its content by touching and dragging objects in live video.
The device "projects" the touch input onto the target display, which acts as if it had occurred on itself.
Cameras observed industrial machinery and allowed users to manipulate mechanical switches and sliders over a distance by clicking and dragging within the live video image with a mouse.
This was made possible by mapping portions of the video frame to the respective parts of the remote hardware.
The system was revolutionary in that it established a particularly direct type of affordance -- in many ways similar to the affordance of direct touch.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In this paper, we investigate how to apply "interaction through video" to these new scenarios and to what extent mobile devices can offer the required flexibility.
We build on recent advances in mobile augmented reality  combined with techniques for manipulating objects at a distance .
As illustrated by Figure 1, Touch Projector allows users to manipulate content on displays at a distance, including those that would otherwise be unreachable.
It further allows users to manipulate devices that are be incapable of touch interaction, such as a wall projection, or a laptop computer.
Users aim the device with one hand and then manipulate objects by touching and dragging it in the live video using the other hand.
Touch input is "projected" onto the remote display, as if it had occurred on it.
With Touch Projector, users manipulate targets using both hands in concert.
Walkthrough of the original metaphor: The user aims at a display  and touches the item of interest .
When moving the device off-screen, a thumbnail of the dragged item is showing .
After reaching the destination display , the item can be positioned precisely by moving the finger .
When the finger is released, the item has been transferred successfully .
Touch Projector preserves immediate feedback : when content on the target display is changed, users immediately perceive these changes through the live video.
This allows for a close connection of action and reaction as both occur on the mobile device.
All touch events received by a Touch Projector device are also routed through the environment manager .
The manager handles touch events that require adjustment when performing drag operations across displays.
If a screen leaves a Touch Projector's camera during a drag operation, both the touch point and item are removed from this screen .
Similarly, when the mobile device reaches another display, the touch point is projected onto it and the item is added .
If a touch ends while the device is not pointed towards a screen, the dragged item is returned to its original position.
Figure 2 shows how content is transferred using Touch Projector:  the user aims at the desired display.
The content is seen in the live video on the mobile device.
As long as the device is pointed at the original display, the object keeps moving.
It disappears as soon as its display leaves the device's viewing angle.
In order to allow mobile use, Touch Projector continuously tracks itself with respect to interactive displays in its surrounding using its built-in camera.
It identifies displays around it and computes its spatial relationship between itself and any identified display.
Knowledge about this spatial relationship is necessary for Touch Projector to transform the user's interaction on the mobile device into the target display's coordinate space.
Touch event handling when interacting across displays.
Compared to Tani's original system, Touch Projector offers three advancements: 1.
The device tracks its spatial relationship to other displays, eliminating the need for modeling the environment.
Since the tracking is purely based on the target display's visual content, the tracking can deal with new and rearranged display environments.
This allows Touch Projector impromptu access of displays.
Similar to Stitching , it can start and dismiss connections opportunistically.
Coarse/fine bimanual motion supports large target displays and dragging across distances  as well as precise local manipulations .
Touch Projector brings multi-touch input to single-touch, mouse-based, and even non-interactive displays.
This means that multi-touch software can be used adequately on older and less interactive hardware.
When performing cross-display operations , the unified interaction also solves compatibility problems, similar to Pick-and-Drop .
The system consists of the Touch Projector device , software on all display systems in the environment, and a server that controls the interaction , all of which communicate over wireless LAN.
On startup, all displays register with the environment manager and transfer their contents.
The manager also receives updates if local changes are committed on a single display.
The tracking system works for regular screen content .
While the original "interaction through video" metaphor transfers well to mobile use, its implementation does not.
Fixed cameras always produce steady images, but on mobile devices, instable images are created by minor hand movements .
This influences the fine positioning of the dominant hand which in turn makes accurate interaction difficult.
Fixed cameras further assume a constant distance between themselves and the device they are pointed at.
These distances may greatly vary when the camera device is mobile.
While zoom works with fixed cameras, the instability of the camera image increases due to the aforementioned hand movement.
These limitations need to be addressed in order to successfully transfer interaction through video to mobile use.
In the remainder of this paper, we present a series of modifications to the original metaphor for mobile devices.
In augmented and virtual reality, Head Crusher allows users to select objects by positioning thumb and forefinger around the object in their 2D projected image plane .
The Go-Go technique allows users to seamlessly reach both near and distant objects .
All these input strategies still require an indirect pointing device leading to similar effects seen in relative pointing .
Accuracy of absolute pointing is limited by the user's fine motor skills.
Motion errors are amplified with distance .
In the context of 2D touch screens, Sears et al.
The Touch Projector fits into their taxonomy as a spatial technique with a perspective display configuration which uses a closed-loop control method.
Touch Projector is inspired by the specific type of bimanual interaction proposed by Toolglasses and Magic Lenses .
Both techniques position a  transparent device with the non-dominant hand to enable the dominant hand to interact within it.
Comparisons between pure bimanual techniques  and dependent techniques  show that the latter perform better .
These results were confirmed by Guimbretiere et al.
They found merging command selection and direct manipulation to be the most important factor.
Bimanual interaction had previously been studied by Buxton et al.
They found that bimanual input outperformed onehanded input for selection, positioning, and navigation tasks .
Mobile display devices have been used to access remote content.
Sweep uses optical flow analysis to enable continuous relative control of a remote pointer on a large screen .
Both  techniques use pointers on the remote screen.
Devices based on augmented reality add a local display into this model.
The Chameleon, a spatially aware handheld computer, enabled users to browse information in 3D situated information spaces .
The Boom Chameleon is a spatially aware display mounted on a boom stand .
Peephole Displays simplify the metaphor to 2D .
Users of Point & Shoot take a photo of an object in order to interact with it .
Similarly, Shoot & Copy allows transferring content based on its photographic representation .
Interaction on these devices consists of two distinct phases: i.e., taking a photo and manipulating it.
The recognition of such markers is an established technology for the identification of augmented objects and the interaction with them.
Several at-a-distance techniques have been proposed to help when touch is unavailable.
Relative pointing can be transferred to distant screens: PointRight allows mapping the mouse pointer to individual screens in multi-display environments .
Perspective Cursor accomplishes the same based on the user's perspective view .
A limitation of such systems is that users are required to locate/track their pointer among a potentially large number of other pointers.
Absolute pointing techniques address this .
XWand allows users to point with a virtual laser .
Content can be brought to users to shorten the interaction distance: Drag-and-Pop shows content proxies in arm's reach .
Tablecloth extends the concept to screens with arbitrary content .
Other techniques create "portholes" that allow users to reach distant contents.
WinCuts lets users use a mouse to "cut" regions of interest from a distant display in order to interact with them on a local screen .
Instead of transferring single elements, the world in miniature metaphor allows users to reach content by manipulating a scaled down complete version of it .
The aforementioned Hyperplant system allows users to control devices in a factory through a video image .
Users could print slides by dragging them onto the video image of the printer.
Users of Sketch and Run control vacuum cleaning robots through a video image shown on a pen-based tablet PC .
In CRISTAL, users collaboratively control a variety of digital devices in the living room through a virtually augmented video shown on a tabletop .
We added two phases of automation to the zoom feature:  Touch Projector zooms out when it detects that it is no longer pointed at any objects.
This is the case when the live camera image does not include any part of a remote display.
As discussed earlier, a literal adaptation of the original static metaphor does not work.
In this section we present a series of improvements that make interaction through video work on mobile devices.
We apply the following three improvements:  zoom,  temporarily freezing the preview, and  a virtual preview for optimized quality.
Precise interaction requires the ratio between target size and viewing distance  to be reasonably high.
While our tracking algorithm is comparably robust against small apparent sizes , interacting with small targets is difficult due to the fat finger problem  and positioning jitter from the unstable video.
We address the precision problem by adding variations of zoom.
The zoom factor is calculated by means of the distance between Touch Projector and the target display.
The apparent size of any item  remains constant independently of the distance to the target screen.
On However, zooming decreases the stability of the camera image, as a slight tilt of the mobile device is amplified to a large motion in the camera image.
At increased zoom levels, the loose navigation of the non-dominant hand is too coarse for users to control it.
While the bimanual navigation of Touch Projector partially counters this effect, we address it with a simplified type of image stabilization: the freeze feature.
Touch Projector allows users to temporarily freeze the live camera image by pressing a button .
The frozen image establishes a fixed reference frame within which the dominant hand can achieve higher precision.
Freezing further eliminates the necessity to hold the device still or pointed at the screen, avoiding unnecessary fatigue.
Live video is re-established by pressing the button again.
Naturally, the user can "zoom" by moving closer to the display.
However, as shown in Figure 4, the user has to get very close to obtain a reasonable object size.
This is not appropriate for situations in which distant interaction is required.
We therefore allow users to invoke a zoom feature.
By adding a slider to the Touch Projector user interface we let users manually control the zoom.
We decided against the commonly known pinch-gesture as all touch points are being projected onto the target display.
Optical zoom generally produces better image quality, but adds weight and cost to the device.
In our implementation , Touch Projector only offers digital zoom, which simply enlarges a subregion of the picture through image processing.
The freeze feature has two limitations, though.
First, the limited sensitivity of the mobile device's camera makes it difficult to take a photo without motion blur.
This is especially true in rooms that are dimly lit due to the use of projectors.
Second, while the camera image is frozen, the device cannot show live visual feedback on its screen.
To tackle these problems, we added computer-generated graphics to optimize the image quality and responsiveness.
Dragging an object with the updated Touch Projector:  the user aims at a display  causing the device to automatically zoom in.
When in freeze mode, we augment the live camera preview with computer-generated graphics.
Touch Projector obtains the imagery wirelessly from the target screen.
It uses the spatial relationship to the current target display in order to distort the computer-generated screen image accordingly.
The main advantage of the virtual live preview is that it gives immediate feedback on a temporarily frozen image.
It combines the benefits of the live preview  with the freeze feature .
Thus, the virtual live preview preserves all properties of a physical preview; in particular, it also shows ongoing interactions by other users with the same screen.
3.0 GHz Core Duo machine as manager, we are able to run the image processing at about 15 frames per second .
However, the iPhone's limited transmission bandwidth only allows up to 8 fps.
Future mobile devices  will likely offer higher bandwidth.
When a target display is started, it first sends a discovery message to the network and waits for the environment manager's response including its IP address and port.
Subsequently, a connection is established through which the display sends its content.
Similarly to the environment manager, the target display's software is also implemented in C# using the .NET Framework 3.5.
Together, zoom, freeze, and virtual live preview overcome the limitations discussed earlier.
Figure 7 shows a walkthrough of transferring content using the updated Touch Projector:  the user aiming at the desired display.
The content is then seen through the live video.
This allows a CD ratio independently of the display's distance.
As long as the device is pointed at the same remote display, the object moves on it.
Tracking and display identification in Touch Projector works by looking at the closest match between the camera image and all known displays:  the system extracts polygons in the camera image.
The remaining polygons are matched either correctly  or not  to those on the display.
The left display matches the video image whereas the right is incorrect.
Touch Projector is implemented on a standard Apple iPhone 3G.
It offers a screen diagonal of 3.5" and a display resolution of 320 x 480 pixels.
Touch Projector is implemented in Objective-C. Live video is captured using the built-in camera at a resolution of 304 x 400 pixels.
A dedicated machine runs the environment manager.
It is implemented in C# using the .NET Framework 3.5.
To allow a user to interact with a target screen, Touch Projector determines which on-screen object it is currently pointed at .
The mobile device permanently sends video frames to the environment manager.
Depending on the previous frame, the environment manager decides which strategy to use for the current frame:  if the device was not pointed at a screen before, the current video frame is fully processed.
If no screen has been detected in the previous frame, the current video frame needs to be fully processed using the following steps:  reveal the polygon edges by increasing image contrast and performing a Hough transform.
It then tests whether the other polygons in the camera image correspond to items on the same display.
If the other polygons do not match, the system returns to step 4 on the next-best candidate until either a display has been identified or no possible matches are left.
If a display has been identified, the system chooses four points  to compute the final homography.
The homography then allows the transformation of touch events into the target display's coordinate system.
The feature points are further stored for subsequent frames.
On the other hand, the maximum distance is ten times the item's diagonal between the mobile device and the item itself.
Future devices with higher camera resolutions could increase this distance substantially.
The interaction speed is further crucial to the success of such systems.
The iPhone's camera is particularly susceptible to motion blur.
Moving the device faster than 50 pixels per frame impacts the recognition rate noticeably.
Again, we assume that future devices with better cameras  will address this in part.
To validate our main design and the proposed extensions we conducted a user study.
Participants acquired targets and dragged objects between screens using four different versions of Touch Projector: the naive port presented at the beginning of the paper, as well as three improved versions, namely Manual Zoom, Auto Zoom and Freeze.
In our user study, we had four interface conditions all of which allowed interaction through video: The Original condition enabled users to look and manipulate content through the original camera image .
This system did not provide any zoom capabilities.
The Manual Zoom condition allowed users to zoom in using up to 4x digital zoom when and to what level they desired .
The Auto Zoom condition zoomed in automatically to keep a constant CD ratio independent of the screen's distance.
In our study, the apparent size of display objects remained constant at 3cm .
The Freeze and virtual preview condition allowed users to freeze the image by tapping on the freeze button .
The frozen image then switched to a computer-generated digital image of the target screen.
Tapping the button again restarted the live video.
Participants were free to choose whether to use freeze.
This condition also included the automatic zoom feature.
If a display has been detected in the previous frame, the environment manager tries to detect the feature points used in the previous frame to calculate the homography.
If they can be found in the current frame, the screen has been detected successfully and the homography  can be calculated as explained before.
If at least one of the feature points cannot be detected in the current video frame, the system has to perform full image processing on the frame as explained above.
If the environment manager still does not detect any screen it assumes that the Touch Projector device is not pointed at a screen.
Touch Projector is subject to several limitations which result from the development stage of the prototype and not the underlying concept.
The most prominent limitation is the interaction distance.
Touch Projector needs to see at least one item fully in its viewfinder to detect the screen.
With the iPhone's field of view of 45 degrees, the minimum distance is about 1.5 times the item's diagonal.
Ultra-wide angle lenses could further reduce this minimum distance.
Participants performed two types of tasks.
Both tasks required participants to use Touch Projector to interact with display content at a distance.
During the targeting task participants acquired targets on a distant screen.
As illustrated in Figure 10, all trials began with a start button appearing on the screen.
Now participants acquired the target by pointing Touch Projector at the remote display and tapping on the target with their finger.
If they missed the target an error was logged and participants had to try to acquire the target again.
Selection of the correct object completed the trial and stopped the timer.
We used a within-subjects design in our experiment.
In the targeting task we used a 4 Techniques  x 3 Apparent Sizes  design.
Technique was counterbalanced across participants in the first task.
In the second task, the presentation of Technique and Angle was counterbalanced across participants.
In both tasks, the three Apparent Sizes were presented in random order within each block.
Each task consisted of one practice block and three timed blocks for each Technique.
Each participant completed the study in 60 minutes or less.
About 25% of the entire time was spent on the first task, 75% on the second.
However, targeting is part of the second task .
For each trial, the target was one of three apparent sizes: 0.75 cm, 1.5 cm and 3 cm on the Touch Projector screen.
We varied apparent size to simulate large target screen distances that we did not have sufficient space for in our lab.
We therefore kept the target screen distance constant and instead varied the target size on the screen.
During the dragging task participants dragged an object of fixed apparent size  between distant screens.
The setup contained two screens, as shown in Figure 11.
At the beginning of each trial, the start button was shown on one screen and the target drop area on the other one.
Tapping the start button initiated the trial, i.e., showed the item to be transferred as well as started the timer.
Participants then aimed Touch Projector at the highlighted object and acquired the object with touch-and-hold.
If participants acquired the wrong object an error was logged and participants had to repeat the trial.
Participants then moved Touch Projector until the destination screen was visible in the live video image.
Participants released the object by lifting off their finger, which "initiated the transfer".
If the center of the object was located within the target area, the trial was completed.
We measured task time and percentage of the object located outside the target area, which we call the docking offset.
We hypothesized that each of the three modifications would lead to an improvement in user performance for small apparent sizes.
For small apparent sizes we expected  the zoom-enabled techniques to outperform the Original interface in terms of task time and error rate,  Auto Zoom to outperform Manual Zoom in task time and  Freeze to result in a lower docking offset than the other techniques.
We compared separate repeated measures ANOVA tests on mean completion times and error for each task.
For the targeting task, error was measured as the number of failed trials.
For the dragging task, error was the docking offset.
To determine the nature of interaction effects we performed tests on subsets of the data.
Post hoc pair-wise comparisons used Bonferroni corrected confidence intervals to retain comparisons against =0.05.
Similar to the targeting task, we varied the apparent size of objects on the target screen.
The objects on the source screen were always 3 cm, while the destination screen contained 3 cm, 1.5 cm, or 0.75 cm target areas.
Upon inspecting Figure 12a one can see that the task completion time disparity shrinks as Apparent Size increases.
We separately analyzed each Apparent Size level and found that there is no significant main effect for Technique when Apparent Size is 3 cm.
However, there is a significant main effect when the Apparent Size is smaller , indicating that this is the main source Technique x Apparent Size interaction found earlier.
The results are summarized in Figure 13a.
To discover the nature of the Technique x Apparent Size interaction we split the data based on Apparent Size levels and ran separate ANOVA tests.
All techniques perform closely when Apparent Size is 3 cm.
However, Freeze performs slightly worse causing significant differences from Auto Zoom and Manual Zoom.
For the Apparent Size of 1.5 cm all pairs are significantly different  except Auto Zoom compared to Freeze and Manual Zoom compared to the Original technique.
For the Apparent Size of 0.75 cm we observed significant differences between all pairs  except for Auto Zoom and Freeze.
We did not find a main effect for Angle or any other interaction effects.
The results are summarized in Figure 13b.
As with task completion time, Figure 12b indicates that the main source of the Technique x Distance interaction was that the amount of failures decreased as the Apparent Size increased.
For both task completion time and number of failed trials, all techniques performed similarly when the Apparent Size was 3 cm.
For all Apparent Sizes, the Auto Zoom and Freeze techniques performed similarly well.
For Apparent Sizes of 1.5 cm and 0.75 cm the Manual Zoom and Original techniques performed significantly worse , with the Original technique performing substantially worse than all other techniques when Apparent Size is 0.75 cm in terms of task completion time .
However, the number of failures was not significantly different at this level.
There are two sources of the interaction between Technique and Apparent Size.
First, there are minimal differences in means when Apparent Size is 3 cm  and large significant differences  when Apparent Size is 1.5 cam and 0.75 cm.
The second source of interaction is the consistently low docking offset for the Freeze technique for all Apparent Sizes.
The mechanical nature of our tasks did not leave much space for thinking aloud.
However, we did get a series of comments, suggestions and feature requests.
The most prominent feature request mentioned by our participants was adding auditory or haptic feedback as indicator when a display has been detected.
Several participants also requested to hold the device in a vertical way.
We presented three extensions to the original idea that improve the performance in terms of task time and error rate.
In our experiment, we verified that zoom-enabled techniques outperform the naive approach.
Furthermore, the study revealed that freezing the live image significantly decreases the targeting offset and thus allows precise manipulation  of an item at a distance.
Automatically zooming in to gain a higher apparent size also decreased the task time.
The outcome of the experiment encourages using automatic zooming in general while allowing the user to temporarily freeze the image for high accuracy if required by the task.
In the future we plan to study the effects of completely computer-generated graphics on the mobile device as a replacement for the camera stream.
Most importantly, this would enable the system to mimic an optical zoom with a much higher focal length on mobile devices.
Additionally, we want to study the difference between giving feedback on the mobile device or the remote display.
As hypothesized, all three improved techniques significantly outperformed the Original technique in both tasks for all but the largest apparent size .
When selecting a target, participants using the Auto Zoom technique were overall 49% faster / 70% less error-prone than when using the Original technique.
For apparent sizes of 0.75 cm, participants were 90% faster / 68% less error-prone.
In general, the zoom-enabled techniques were 34% faster / 59% less error-prone than the Original technique, which supports our first hypothesis.
In the targeting task, the Auto Zoom and Freeze technique performed best of all techniques with a slight advantage to Auto Zoom for small apparent sizes.
We further found that Manual Zoom and the Original technique performed significantly worse for all small apparent sizes.
Hence, the Auto Zoom technique also outperforms the Manual Zoom, which supports our second hypothesis.
Freeze had a slightly higher task time compared to Auto Zoom.
This can be explained by the fact that users had to press the pause button in order to freeze the image before they were able to acquire the target using the computer-generated overlay.
When dragging an object between screens, participants overall were 27% faster / 132% more accurate with the Auto Zoom technique compared to the Original technique.
For the small apparent size they were 56% faster / 249% more accurate.
The Freeze technique revealed its strength by being over 10 times more accurate than the Original technique.
This supports our third hypothesis.
In the dragging task, only the Freeze condition has the advantage of retaining a low offset across all apparent sizes.
However, the extremely low targeting offset of the Freeze technique was expected  as the instability of camera images increases with a higher zoom factor.
Auto Zoom performs better than the other techniques in terms of task completion time .
However, it does not allow for the highly precise target placement which can be achieved using the Freeze technique.
Our study shows that Auto Zoom is the best performing technique for targeting tasks.
Freeze, however, outperforms Auto Zoom for precise manipulation tasks by keeping the image steady.
This suggests that freezing the image temporarily should be an optional feature that complements automatic zooming .
This work has been funded by the German state of Bavaria.
We would like to thank the reviewers for their detailed comments and suggestions.
We also thank the participants of our study for their time and patience.
Furthermore, we would like to thank Doris Hausen, Christina Dicke and Christian Holz for their valuable feedback.
