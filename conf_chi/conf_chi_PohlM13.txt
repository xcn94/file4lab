We describe the focused-casual continuum, a framework for describing interaction techniques according to the degree to which they allow users to adapt how much attention and effort they choose to invest in an interaction conditioned on their current situation.
Casual interactions are particularly appropriate in scenarios where full engagement with devices is frowned upon socially, is unsafe, physically challenging or too mentally taxing.
Novel sensing approaches which go beyond direct touch enable wider use of casual interactions, which will often be `around device' interactions.
We consider the degree to which previous commercial products and research prototypes can be considered as fitting the focused- casual framework, and describe the properties using control theoretic concepts.
In an experimental study we observe that users naturally apply more precise and more highly engaged interaction techniques when faced with a more challenging task and use more relaxed gestures in easier tasks.
Instead of requiring users to be highly engaged with their devices, we propose a class of interaction techniques that allow users to choose the level of interaction depending on the situation, so that they can also choose to casually interact with them.
Giving users the freedom to choose their desired level of engagement is empowering and has the potential to make their interactions more suitable for a given context , as well as more relaxing, physically less demanding and safer to use.
In this paper, we provide an overview of casual interactions and how they enable users to be laid back, while retaining control.
We relate this to examples of existing designs, and present an evaluation, showing how users, when given the opportunity to do so, take the chance to relax control when able to, while taking it back when required to do so by a task of higher difficulty or importance.
We conclude with challenges to the research and design worlds in creating such focused-casual systems.
The current generation of computers is designed under the assumption that users are fully engaged with them when interacting.
This is especially true of mobile, touch-enabled devices, which require users to watch the screen, as the lack of non-visual feedback makes it hard to interact in any other way, forcing them to focus on the interaction.1 However, such full engagement with their devices is not possible or desirable for users at all times, e.g., because users are physically, mentally, or socially inhibited.
Consider the user shown in Figure 1.
He is busy reading an article online when his phone rings.
While he is not sure who is calling, he still does not feel like answering.
Because he is focused on his primary task, he chooses not to devote a lot of attention to his phone.
He thus dismissively and inattentively waves over the device to signal his intent not to take the call.
The second aspect transfers some powers to the phone.
Had the user picked up the phone, glanced at the caller and then dismissed the call, it would be a clear indication that the call was not to be accepted.
The casual dismissal, on the other hand, is more vague in execution, is based on missing information and therefore leaves some room for decision making to the phone.
Imagine the user's boss is calling.
In this case the phone could insist on continuing to ring, prompting the user to give the call a second look.
Casual interaction enables users to engage more for fine control or engage less, either reducing the number of states they can drive the system to, or handing over more autonomy to their devices.
As people put on a character, they are more likely to engage in behavior that they see as potentially appealing.
Note that casual interactions are, however, not necessarily more subtle interactions, as understood in , as the casual action might be more overt than a focused one.
Mental reasons are primarily issues of distraction.
Users might be engaged in a primary task, leaving little attention for other interactions.
For example, Bakker et al.
This suggests that there is room for interfaces enabling users to carry over this behavior for their interactions.
Exhaustion after a day of work also leads to effort-avoiding behavior.
In general, the capacity to make active choices declines over the day , suggesting that users could especially benefit from casual interactions later in the day.
A commonplace in these scenarios is that the users still want to engage with their devices, just sometimes with lower effort and precision or in a more sporadic fashion.
Casual interaction mechanisms should allow users to control the level to which they engage--they do not want to give up control completely.
These are also not purely casual systems: the interaction space is continuous, spanning from very focused to very casual interactions.
Interface designers often assume that users focus on their device when interacting, but this is often not the case.
In fact, there are many scenarios where users are not able to, or do not want to, fully engage with their devices.
In general, inhibiting factors can be divided into  physical,  social, or  mental subgroups.
Physical reasons users can not fully engage with a device are often closely related to questions of accessibility.
Users who can not use one arm , for example, can not dedicate one hand to holding a device while the other interacts with it .
Thus, they can not achieve the same level of speed/accuracy as unencumbered users, and the engagement with their devices is limited by those constraints.
In general, physical impairment inhibits our achievable accuracy, for example Keates et al.
In some cases, users can obtain physical relief, and overcome strain or fatigue by changing the level of interaction.
For some temporary impairments, e.g.
Casual interaction systems try to enable interaction even in such situations where users can not engage manually as needed in focused interaction.
Social reasons are mostly concerned with the question of how much engagement with a device is acceptable in a given setting.
While taking a phone out of the pocket and checking stock quotes is perfectly acceptable when alone, this interaction would be considered rude when performed during a date.
While social settings limit some interactions, they can also strengthen others.
Users might, for example, be more inclined to perform interactions, that make them look relaxed and laid back.
The desire to seem as if one is not putting in too much of an effort in order to be seen as cool has, e.g., been described for 14-16 year old students by Warrington et al.
We postulate that casual interaction is desirable in social settings, because it can reinforce some users' desired image of being in control yet laid back and relaxed about it.
The view of casual interaction in the social sphere fits well into the concepts of front stage and back stage by Goffman .
When interacting with devices, the level of engagement with the device will differ depending on the situation .
Although the ranking will vary depending on the context and constraints, for interactions with smartphone-like devices a qualitative ranking of interactions by engagement could be: * Changes in the environment.
Increased Engagement * Presence of user in proximity.
The high-engagement extreme describes very focused interactions, in which a fully committed user is giving her entire attention to the interaction, and actions and responses are tightly coupled in time.
Playing games often falls in this category.
On the other end of the scale are interactions that are of secondary or incidental nature.
For example, muting an alarm clock by tapping it anywhere, or turning over a phone to reject a call can be done without giving the task too much attention.
There can even be levels of casual interaction within otherwise highly focused settings - the popularity of kinetic scrolling in touchscreen devices is partly because after an initial flick, the user can pull back their finger, reducing their engagement level, and wait to see where they end before stopping the scrolling, or giving it a further impetus.
So far we have presented an informal description of the differences between focused, engaged interaction and casual interaction.
We can make this difference more quantitative and objective by characterizing the casual-focused continuum in control theoretic terms, where we view the closed loop between user and computer as a dynamic system .
At every moment, humans are controlling a number of states , and this can be used to infer where they are paying attention, and what they are controlling .
Motor control studies have demonstrated that the precision of the response will depend on the uncertainty of the state information the human perceives  .
In order to create an appropriate closed-loop behavior, designers can alter the sensing and feedback mechanisms in the phone to allow both--user and software--to adapt.
This allows the user to control those aspects of their environment which appear most appropriate to the situation.
In casual control systems, users voluntarily limit their input to either a reduced, or a more abstract level of control temporarily, while preserving their option to step in and take over at any given time.
If a rider uses certain, frequent and deliberate movements, the horse follows movements exactly; as the control becomes more vague, the horse resorts to familiar behavior patterns and will take over more of the control.
This notion of being able to `loosen or tighten the reins' of interaction with an intelligent device is likely to be vital in the creation of future human-computer interaction systems, and is linked with, but not identical to, the casual-focused continuum.
The ability to move effortlessly between casual and engaged interaction will be a keystone of the approach.
This will depend on input sensors, output displays, inference of the meaning of user actions and interaction metaphors that users can grasp.
Focused interaction will typically involve the user sampling the feedback frequently and accurately, and responding rapidly via accurate, high-bandwidth inputs to changing stimuli displayed on the computer and will take place over extended periods of continuous control.
In information theoretic terms, the user will have a high channel capacity from their outputs to their senses, a measure called empowerment in , and we will see predictable responses to stimuli with a high level of consistency in the information rate of the interaction.
Casual interaction will tend to break or degrade elements of this control loop in a number of possible ways.
The interactions may be intermittent2, i.e.
The information flow in interaction is much more sporadic and unevenly distributed over time.
There may be asymmetry in the interaction loop, where the user provides good quality input but has low-bandwidth output, or vise-versa, where the input is very noisy or uncertain, while the feedback is high-quality.
Currently, mobile devices mostly rely on touch sensing, while sometimes also offering motion-sensing or vision-based interaction.
Interaction which depends on touch forces a degree of engagement which can be undesirable.
Devices can be hard to reach at times or users can be inhibited from directly touching their devices for social acceptability reasons.
Devices offering casual interaction modes will typically sense input from users over a wider range of modalities than in traditional, focused interaction.
In this section we touch on a number of technologies we see as enabling for such devices.
Nenya in particular addresses this issue and offers the socially acceptable modality of twisting a tracked ring for analog input .
Vision-based systems sense users away from the device, and the technology can be integrated into laptop touchpads , diffuse illumination systems  or directly into screens , but tend to be sensitive to lighting changes in practical use.
Capacitive sensing is usually used for tracking fingers directly on a surface.
However, fingers can still be sensed slightly off the surface, but those readings are noisier and thus less accurate, as demonstrated by Rogers et al.
They also showed that by combining coarse long-range sensing with finer close-range sensing, capacitive sensors can be used to determine the 3D position of the finger on top of a device.
Electromyographic sensing is a less explored option, but has previously been used as well to detect coarse gestures above the surface .
Similarly, electric field sensing4 is well suited for gesture recognition but less so for pointing.
Casual interaction devices could also use the user's Bodyspace  for limited bandwidth, but convenient interactions.
Given the reduced control potential  in the casual setting, there are different ways the joint system can function.
Either  the system remains a direct control interface, but adapts such that there are fewer states that can be entered in a casual interaction mode, or  it moves to a higher-level of a hierarchical control system.
Depending on how users interact with a device, the available interaction bandwidth changes.
Drawing on a touch-screen, for example, results in much more information  than pressing a physical button.
Bandwidth can be limited due to the sensing technology used, or due to user behavior.
The bandwidth of user interactions generally shrinks with increasing distance to devices.
Users are, e.g., less precise in pointing at an object 10 m away than at an object 10 cm away.
This dropoff also holds true for perceiving visual feedback.
Additionally, most sensing methods deteriorate with increased distance to the sensor.
Devices tailored for casual interaction will have to cover a larger range at varying fidelities.
Thus, such systems will often use a combination of the above mentioned sensing options, e.g., smart homes with instrumented living spaces can integrate information from a range of sensors, and on mobile devices a combination of capacitive sensing and IR distance sensors as an inexpensive and easy to build option.
In HoverFlow six such IR distance sensors provided a rough image of the space above the device and were shown to enable recognition of one-handed sweep and rotation gestures .
Capacitive sensing is an example of a sensing technology which provides high accuracy on the surface while offering reduced accuracy in above-the-surface sensing.
In many other cases , distant interactions suffer from reduced sensing capability and increased user error .
Magnetic force, for example, behaves according to the inverse cube law, resulting in fast deterioration of sensing quality.
In interaction on the surface additional opportunities for differentiation arise, e.g., by using sound features .
For multi-layer interaction above the surface,  suggests that layer thickness should be around 4 cm for users to adequately be able to interact.
To enable users to interact with a system at different control levels requires adapting input handling.
As introduced earlier, we can either reduce the degrees of freedom we control, or provide goals and autonomy to lower-level controllers.
We see several general techniques for reducing bandwidth requirements for a given interface.
These include: reducing field fidelity, spatial remapping, combining fields and dynamic combinations such as gestures.
An example application of such techniques, where users could change the lighting of a mood light using different levels of input complexity, is shown in Figure 3.
In the same way that casual interactions benefit from sensing at a distance, the feedback modalities used can be structured such that users can be provided with feedback at a range of levels of attention.
Combinations of tactile, audio and visual feedback can be used to provide the appropriate amount of information at appropriate frequencies, to allow the user to control the interactions at a suitable level of engagement.
This will tend to go from lower-precision feedback for casual interactions, possibly with sampling delays, to higher resolution, more responsive feedback for focused interaction.
There are some existing feedback mechanisms that can be used in casual interactions, e.g., ambient and peripheral displays , or glanceable displays .
Such displays can also be incorporated into textiles for feedback on-the-go .
In their Damage prototype, Williams et al.
Users could make their corresponding studs on other bracelets glow in any color, enabling them to send non-disruptive and ambient messages.
Coupling input sensors to infer whether the user was able to sense the feedback provided by device outputs 
As in the initial scenario, the meaning of a call rejection is different if the user knew who was calling or not.
Users are able to control the color of a moodlight by  precisely picking a color using sliders on the surface of the device,  manipulating the sliders together to control brightness coupled with slight rotation for changes in hue using above-the-device interactions,  in-air control of a one-dimensional hue slider at fixed brightness.
Control of sliders on the surface allows for accurate parameter changes.
If the precision for this kind of interaction is missing, e.g.
This preserves the direct control of sliders and only slightly limits the user.
Similar reductions can be applied to other kinds of widgets.
In the area of text entry there already exist several techniques allowing users to type using sloppy or constrained input, such as SHARK2  or SAK .
Spatial Remapping: Sensing is not limited to the space directly above the device.
Instead, sensing capability for many sensors extend in a rectangular frustum from the device.
Future devices might include hemispherical sensing, extending the covered space even more.
The shape of the sensing frusta results in an increase of covered sensing space when further away from the device.
This allows to  fit more targets into the space or  make targets larger.
As sensing accuracy also deteriorates with distance, increasing the target size seems more appropriate.
Figure 4 shows an example of a target remapping , where 27 surface targets are mapped to 4 crossing targets in the air.
For the 2D case, Accot and Zhai have shown that such a remapping  results in similar task times at lower error rates .
We hypothesize similar results for 3D remappings.
When the device is spun on its base on a table, the music player triggers a `jukebox shuffle' event--a playful, low-precision, low-effort gesture.
In general, gesturing can be used to decouple user commands from the devices themselves.
Systems exist that, e.g., instrument shoes  and clothing  for gesture sensing, and with the success of the Microsoft Kinect, vision-based sensing of device-free movement is becoming more prevalent.
With the previous techniques, users still had direct control, if at a lower level of engagement.
Users might instead choose to relinquish control even more and have a lower-level autonomous system take over.
Instead of picking a color directly , they would only indicate a desire for change--an action that can be communicated over a low bandwidth channel.
Related examples include Syntonetic's Moodagent app,6 where a user can slide mood levels to generate playlists, rather than picking individual music tracks.
By inferring a user's engagement level, casual interaction systems could estimate the utility of invoking an agent vs. requiring more user input, similar to previous work of Horvitz .
Combining fields: In the example interface shown in Figure 3, interaction up close allows users to change a color by modifying each color component separately.
While this provides precise control, this might not be necessary in all situations.
Instead of setting a specific color, users often merely desire to set a mood in the range from gloom to vivid.
Thus, the threedimensional control of color components can be abstracted to a one-dimensional one that only controls mood.
In effect, by combining fields, an interface tries to capture the essence of an interaction and provide an abstraction for it.
Note that users can still choose to fully engage and get full control over the color sliders.
Sensing users' hands when they are not touching a device has been the basis for several previous interaction techniques.
In particular, they showed the inversely proportional effect of layer thickness on task time.
Earlier work on interaction layers by Subramanian et al.
In particular, they looked at how to correct for drift and what kind of selection technique is most appropriate for in-the-air interaction.
Building on that work, Kattinakere et al.
In Mockup Builder Araujo et al.
This, e.g., allows intuitive extrusion of 3d shapes by finger movement through the air.
Movement and shape of users' hands in the space just above a device were monitored by Jackson et al.
While the above work investigates input of different modalities, none of them investigate those in the context of casual interactions.
We build on some of the existing techniques, e.g., investigating hand gestures similar to the ones proposed by Kratz et al.
Gestures offer an alternative way to invoke device functions, trading spatial accuracy for dynamic consistency.
Whack gestures are one example of making the main tasks of an interface available for inattentive invocation .
There has been some research into providing interfaces that allow for multiple ways to interact and seamless switching between those.
Vogel and Balakrishnan outline the design principles for public displays that offer four interaction zones from ambient to personal interaction .
Their prototype also enables all interaction levels at any point, giving users the freedom to choose how much to engage.
They look at engagement mostly as a function of distance, while casual interfaces have a user-centric view on engagement.
They combined this with a zoomable interface approach, enabling fine grained selection up close with a more coarse range of options when at a distance.
While Vogel and Balakrishnan were interested in interactions with displays, we investigate interactions with personal portable devices, which have their own set of challenges.
AirFinger enabled moving dragging and tracking operations from the surface to the space above the surface .
Their work is limited to dragging and tracking though, while we look at a wider range of inputs.
A comprehensive overview of techniques for on and above the surface interaction was provided in .
While they included a large number of techniques, they did not look at how those techniques could support casual interactions.
They showed that such peripheral interactions are less taxing with tangibles than with GUIs.
In contrast to the other works mentioned above, here two simultaneous foreground interactions are used instead of a mix of the two grounds.
While all those previous systems make a distinction between foreground and background interaction, this is mostly seen as a binary choice.
The idea of background interaction assumes that the interaction is implicit and happens with no or, at most, only slight intention of the user.
In this framework the transitions between background and foreground are often made by the systems themselves, e.g.
Instead, casual interaction is all about the users' choice of the level of engagement.
Casual interaction is never fully in the background, but also is not the primary foreground activity.
The users' choice dictates the level of ground an interface provides.
When thinking about different levels of interaction, we are building upon Buxton's foreground/background model .
Buxton makes a distinction between intentional interactions  and those that happen in the periphery of a user's attention .
Here background interactions are those performed without much attention or on-the-fly, for example, automatically rotating the screen content when a device rotation is detected.
The distinguishing characteristic between the two is the level of users' direct attention to the interaction.
Dix looks at background interactions that happen unbeknownst and secondary to an intended primary interaction, which he refers to as incidental interactions .
In his framework, incidental interactions, as users become experience with them, eventually transition to expected and finally intended interactions.
It is thus the users' experience that determines the level of user intent, instead of users' freely choosing their desired level of engagement as in casual interactions.
Around the example of the proactive Range whiteboard system, they present three implicit interaction techniques that help users keep in control of the proactive behavior.
We now present a small user study to explore and evaluate one basic principle of casual interactions: when given the freedom to pick the level of engagement, a user's choice will be influenced by the level of control needed.
We have developed a prototype in line with the aspects touched upon in the Input Sensing Requirements section for this evaluation.
For the qualitative evaluation of casual interactions, we have designed a prototype  that allows for above-the-device interaction as well as precise touch interaction.
A SHAKE SK7 sensor pack with SK7-ExtCS1 capacitive sensing extension board from SAMH Engineering Services is used as the foundation.
It provides 24 capacitive sensor pads  in a 4 x 6 arrangement .
Touch allowed for the most precise control.
After touching the surface, any finger movement directly results in a corresponding ball translation .
By using clutching, the ball can be moved to any point in the level.
Hover provides a higher C/D ratio rate control mechanism.
Here users move their hand in the space just above the device.
The zero point is set to the vertical axis through the center of the device.
Deviation from this center results in a force of proportional amplitude being applied to the ball.
Gestures can be used further atop the device, beyond the range of hover interactions.
Here, users indicate a general direction they would like the ball to move in.
This is the least precise of all three methods.
Execution of a gesture results in an impulse of constant strength and in the indicated direction being applied to the ball.
While the direct touch mode allowed for more precise control, it required participants to closely engage with the device.
The hover option on the other hand is less precise, but requires no such close engagement.
Finally, the gesture option allows participants to potentially "solve" a level with one ballistic action, but it is also the least precise option available--offering no feedback control ability.
They were given a chance to familiarize themselves with the game in a special training level.
Participants played freely until they felt confident in their ability to control the game and switch between techniques.
Afterwards, they played through a randomized sequence of 7 levels.
Participants were instructed to decide themselves how much they wanted to engage with the game at any given situation.
They were also assured that obstacle collisions were not a problem in this study and that they would not be evaluated based on their performance.
As mentioned earlier, we made sure they did not feel the need to finish a level as fast as possible.
We find that for low difficulty levels, participants exclusively used hover and gesture control .
Only at the highest index of difficulty , was a noticeable number of interactions performed by direct touch control.
Note, though, that hover can be regarded as the `default' mode.
When a user's hand is near the device this registers as a hover interaction.
Most users occasionally let go of control and allowed the ball to move around, as there was no penalty for obstacle collisions.
The ratio of touch interactions shown here should therefore be regarded as a lower bound.
In the future, combining different pad sizes could enable sensing at much larger distances, while preserving the capability for accurate onthe-surface touch tracking .
Future devices could also incorporate range sensors for coarse above-the-device tracking .
In our current prototype we use a Kinect sensor to emulate such future devices.
The Kinect is mounted on the ceiling, looking down on the sensor pack.
We use computer vision techniques to extract the hand from the depth image by thresholding and connected-components analysis.
Sensor data is combined on a PC, mapping the Kinect data to the coordinate space of the device.
In our discussion of casual interaction we have so far assumed that people are interested in letting go during interactions sometimes.
We designed an experiment to test this, by observation of peoples behavior while playing a game.
We hypothesize that with increasing index of difficulty, players will take more control of the interaction, while being more casual in easier levels.
To test this we use an obstacle course navigation type game that requires players to steer a ball to a target area.
On the way, players need to stay away from the walls.
If contact with a wall is made, the ball is reset to the starting position.
Figure 6 shows two example levels with different indices of difficulty.
The ball in the game has momentum and is simulated using a physics engine.
We compute the index of difficulty for each level according to Accot and Zhai's steering law .
For navigation along a curved path C, parameterized by s and with width changing as W , the path's index of difficulty is thus defined as: IDC = ds C W 
While there is a linear relationship of the index of difficulty to task time, this aspect was not of interest for this experiment.
We did not ask participants to be fast, nor did we show a timer or any ranking based on time.
This was done to prevent participants from trying to finish levels as fast as possible.
Instead of changes in task time, we were interested in changes of engagement depending on the varying index of difficulty.
For control of the ball, participants had to use our casual interaction prototype system .
This allowed them to exercise three levels of control:
Condition 5 here is somewhat of an outlier, because even though the path to the target area was tight, a direct vertical movement was sufficient to reach the target.
While participants often took several attempts to get the direction right, most succeeded in reaching the target that way.
This indicates that Accot and Zhai's steering law, while a good initial estimate, is not sufficient for capturing the actual level of difficulty for the set of input techniques used here.
It is also interesting to note that for a case such as condition 5, if users can get away with multiple attempts at a low-effort gesture, they appeared to prefer this over a higher engagement interaction style.
Nonetheless, participants are adapting their behavior according to the posed level of difficulty.
This supports one basic premise of the casual-focused continuum: users are willing to fall back to relaxed input if the level of control is still sufficient to achieve their desired goal.
Could this be used to provide measures throughout the interaction in bits/s?
How culturally and device-specific are these likely to be?
We will illustrate these issues with the example of TV viewing, where users are not physically inhibited, but strong mental and social factors are at play.
We could easily imagine, e.g., that after a day at work, a user is less inclined to make active choices and is more willing to give up some control.
Control in this scenario is mostly relevant in the context of channel choice and volume control, although with increasingly sophisticated Internet TV, such as BBC's iPlayer, Netflix or Apple TV, more complex interaction becomes necessary.
Current remotes actually already offer something along the lines of our engagement concepts.
Some remotes offer a `complicated' panel, which can be hidden, leaving simpler interaction possibilities.
Furthermore, users can jump to a channel immediately using a number pad.
However, this comes at a higher selection cost  and might require multiple button presses.
Instead, they can use increment and decrement buttons, stepping through the channels at a lower engagement cost.
In this case, designers need to decide whether anyone can interact with any of the remote controls, or with the TV itself, at a distance.
How many levels would be appropriate  in this context?
How do we develop interaction metaphors which can stretch across focus levels?
Casual interaction devices could extend this by giving a simplified interaction when the user is not pressing remote buttons.
They could have had swipes for channel changes, but no direct control of channel numbers, or could extend the swipe metaphor to allow for larger channel steps based on the intensity of a swiping gesture.
If the system had a user model from prior behavior, and could recognize the user, then either the system could move to an autonomous recommendation mode, where swipes just corresponded to `next recommendation', or the user could stay in direct control, but given a likelihood function for each channel, the control space could be distorted to make likely channels easier to acquire when only low bandwidth input is provided.
We have presented a conceptual framework for thinking about how interfaces could be placed on a focused-casual continuum.
When designing for casual interactions, designers have to think about a number of aspects.
They will have to, e.g., decide  Whether the levels of interaction will be on a continuum, or a set of discrete levels.
If discrete, how many levels of interaction they want to support?
How do you support control when you can reach smaller state spaces with decreasing engagement level, while having a consistent user model of the system?
In research, a wide number of questions around casual interaction open up.
7 Around-the-device interaction leads to an elevated risk of unintended activation.
The increase in active volume means there is a higher chance generating unintended interactions.
Casual interaction systems will have to include safeguards tuned to the specific system to prevent such accidental activations.
Would this make clear to the users why the channel changed, when one of them was waving his hands about something else?
As researchers investigating such a proposed system, we would like to be able to measure the interactions in such a case and quantify the level of focus needed to use the system.
When performing user studies we would need to be able to relate subjective feedback and the objective level of focus displayed in user behaviors during the study.
We would also want to see how easily the user could control the level of engagement at any point in time, and whether this itself led to frustration.
Studies of social aspects of the system should include the changes in overt behavior when multiple people are in the room compared to a single user.
It will be interesting to see whether making the engagement level `socially attentive' would be a benefit, e.g.
The ability for users to elegantly adapt their level of engagement with a computer, in a context-sensitive manner, is going to be of increasing relevance in a world populated by many interactive systems.
Especially when analyzing the design of multimodal, interactive systems with around-device interaction potential.
This paper proposes designing systems explicitly to support casual interaction, which enables users to decide how much they would like to engage with interactive systems.
We used a control-theoretic framework to describe the features of lower levels of engagement with a system .
We illustrated an aspect of user adaptation to interaction difficulty in an experiment which allowed users to achieve their goals at a range of levels of engagement, providing some initial, exploratory results showing that users do indeed change their engagement depending on the difficulty of the task at hand.
We hope that this paper will encourage designers to consider how they can instrument devices and design interactions such that users can choose to vary their level of engagement.
We anticipate that it will also lead to new developments in measures of interaction in the HCI research community.
