This work examines intermanual gesture transfer, i.e., learning a gesture with one hand and performing it with the other.
Using a traditional retention and transfer paradigm from the motor learning literature, participants learned four gestures on a touchscreen.
The study found that touchscreen gestures transfer, and do so symmetrically.
Regardless of the hand used during training, gestures were performed with a comparable level of error and speed by the untrained hand, even after 24 hours.
In addition, the form of a gesture, i.e., its length or curvature, was found to have no influence on transferability.
These results have important implications for the design of stroke-based gestural interfaces: acquisition could occur with either hand and it is possible to interchange the hand used to perform gestures.
The work concludes with a discussion of these implications and highlights how they can be applied to gesture learning and current gestural systems.
In pen and touch scenarios, a stylus held in the dominant hand is used for precise input and the non-dominant hand is used for coarse movements or gestures .
Given the benefits of two-handed interaction, it is important to consider additional ways to increase input bandwidth.
One possibility is to harness the dexterity present in the non-dominant hand, which is often treated as inferior and largely underused.
This work seeks to determine the degree to which this is possible.
Motor learning has long been interested in the generalizability of skills.
This generalizability across hands has been termed intermanual or bimanual transfer.
The ability to learn a  gesture with one's dominant hand and perform it with the nondominant hand  while encountering little decrease in performance could encourage gesture reuse.
This would thus increase the interaction input space while discouraging the growth of gesture sets .
Such transfer would also be useful in scenarios where users are prevented from interacting with their dominant hand due to injury or fatigue.
In these situations, operations could be completed efficiently with the unaffected hand, thus maintaining productivity and potentially preventing future repetitive stress injuries.
Transfer would also be important in situations where one hand is occupied while the other is free 
Although many unimanual skills transfer between the hands, there is an important distinction between those that transfer symmetrically  and those that do so asymmetrically .
Intermanual transfer has been found to be largely task-dependent.
Tasks such as catching  and using a pegboard  were found to be symmetric whereas tasks such as reaching during visuomotor rotations or load modifications , and letter or figure drawing  were asymmetric.
There has yet to be any consensus or general hypothesis specifying why certain skills are symmetric and others are not.
Many touch-based devices require simple finger motions such as taps, swipes, or two-finger linear movements.
These interactions are very natural, but do not take advantage of immense dexterity present in the hands.
The potential input bandwidth is also quite small.
There has been significant interest in how touch-based gestures can be enhanced to eliminate these shortcomings.
Bimanual interaction is one method to improve the input bandwidth of systems as both hands can be used to generate efficient,
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The degree of gesture transfer between the hands, as well as the symmetry or asymmetry of such transfer has yet to be investigated, but has implications for interaction.
If gesture transfer were symmetric, users could have the proficiency to learn and use gestures with either hand.
This could be important with devices that can detect handedness or help novices during the acquisition of gestures.
Asymmetric gesture transfer could be used to formulate a series of guidelines to increase learnability with, and transfer to, the untrained hand.
It would also require designers to accommodate situations involving the untrained hand and develop ways to lessen such behavior.
This work set out to answer many of these unknowns.
Using a touch screen monitor, participants learned four gestures using a ShapeWriter-style  keyboard and were tested 15 minutes and 24 hours after acquisition using a standard retention and transfer paradigm from the motor learning literature .
Although a large body of work on transfer exists within the motor learning literature, few outside the area have given attention to transfer.
A brief review of the motor learning literature on the intermanual transfer of skills is presented, followed by work most similar to gestures, the transfer of drawing and writing skills.
Lastly, research from within the human computer interaction community on the reuse or transfer of skills is presented.
There has been research on the intermanual transfer of `gestures' such as alphabet letters and ideograms .
Hicks , and Parlow and Kinsbourne , had participants practice drawing reversed-inverted uppercase letters using pen and paper.
Participants were not given feedback about their performance and were told to focus on producing as many letters as possible.
After 10 trials, participants switched hands and began the process again.
An analysis of the mean number of correctly written letters indicated greater non-dominant to dominant transfer.
More recently, Halsband used a digitizing tablet to teach righthanded participants meaningless ideograms over a period of 5 days .
Participants traced a picture of each target ideogram using an inkless stylus and drew each ideogram from memory for a number of trials.
During transfer, each ideogram was drawn with the untrained hand.
An analysis of mean speed revealed a significant dominant to nondominant hand asymmetry.
An analysis of time, accuracy, and movement size revealed a greater dominant to non-dominant benefit in speed and a non-dominant to dominant advantage in accuracy and movement size.
In contrast to these studies, our experiment used a novel real world activity, gesture-based text-entry, and encouraged participants to focus on the accuracy of their movements not the speed.
Our study also differs in the amount and type of feedback provided during training and transfer, using an onscreen keyboard and gesture trace.
In addition, our gestures were novel, but had implied instead of ambiguous meanings.
These methodological differences allowed for the study of skill transfer as it related to a relevant HCI task.
In the motor learning literature, the intermanual transfer of motor skills has garnered much attention.
Morton, Lang, and Bastian looked into the impact of different ball weights and arm extensions, i.e., straight versus bent, on catching .
They found that transfer across arm extensions was symmetric, but found an asymmetry in the arm used during training.
They found that load had no influence on transfer, accuracy was symmetric, but speed was asymmetric.
Using a pegboard task, Schulze et al.
Sainburg and Wang assessed the role of visuomotor rotations and found asymmetric transfer with respect to velocity and direction .
From these and many other studies, intermanual transfer and its direction appear to be task-dependent.
To date, no definitive model of transfer has emerged and the basis for transferability is unknown.
Our work provides some of the first insights into the nature of gesture transfer using touchscreens.
Human-computer interaction has been interested in the reuse and generalizability of learned skills, but has focused on the cognitive dimension of transfer.
They argued that reorganizing and combining a small set of gestures increases the functionality and learnability of systems.
They suggested that through everyday usage of a device, users inadvertently learn spatial configurations that are generalizable to novel contexts.
Additionally, many have shown that user familiarity with the QWERTY keyboard can transfer and improve performance on device independent and alternative text input systems .
These and other works focused solely on the use of one hand.
In contrast, this work focuses on scenarios where acquisition and performance could occur with either hand and analyzes the influence of gestural characteristics such as length and curvature on transfer and symmetry.
Twenty participants  from the university community were recruited to participate in the study .
All participants were right handed, as assessed by the Edinburgh Handiness Inventory  .
Participants were naive to the purpose of the experiment and had no prior experience with a gesture-style keyboard or gesture-to-unlock interface.
At the conclusion of the two day experiment, participants received a $15 honorarium for their participation.
An Acer T230H multi-touch monitor, with a resolution of 1920 x 1080 pixels was placed in front of the participant in a portrait layout .
The monitor was connected to a 3.2 GHz, 4 GB RAM, 32-bit Windows 7 computer that ran the custom C# and WPF experimental software.
The monitor displayed an underwater scene with a large hexagonal keyboard located at the bottom.
The underwater scene kept participants engaged in the activity and better mimicked a real-world scenario where numerous elements compete for a user's attention.
An ATOMIK keyboard layout was used so that the on-screen layout would be unfamiliar to participants.
Each tile in the keyboard was 125 x 125 pixels.
Participants were told that they would be learning a new way to input text using a touchscreen keyboard.
A target word was displayed in a speech bubble in the center of the screen.
To `input' each target word, participants used their index finger to draw a continuous stroke through the keyboard tiles corresponding to the letters in the target word.
As participants moved their finger along the monitor's surface, a blue line was overlain on the keyboard, providing feedback.
Similar to ShapeWriter, participants were informed that it was acceptable to draw over letters that were not found in the target word.
They were also told that they should focus on reproducing the target gesture as accurately as possible.
Pairing each gesture with its corresponding word instead of an arbitrary set of gestures allowed participants to attach meaning and intent to the gestures they were learning .
The onscreen keyboard was chosen because we did not want participants to focus on learning the gesture-word pairing first and improving the production of the gesture second.
We wanted to measure participant's ability to accurately and precisely reproduce a specific motor pattern.
Providing continuous feedback, in the form of a real-world scenario where feedback and cues were available enabled this type of evaluation.
Many other forms of feedback or cueing could have also been appropriate .
We were interested in the influences that different gesture characteristics have on transfer, thus two factors were manipulated, Length and Curvature.
Length was manipulated because most touchscreen gestures are relatively simple and short , but there are many instances where gestures could be simple but span a large area or complex and composed of multiple segments, .
Given the increase in systems supporting unstructured, user-defined gestures  , and the inclusion of curved gestures within existing gesture sets , the curvature of gestures was also of interest.
In the traditional ShapeWriter implementation, the gesture representing each word is composed of a single, continuous, curved segment.
We explicitly created template gestures that also contained a number of corners and straightline segments, similar .
Two levels of each factor were chosen , resulting in four gestures : each, a short-curved gesture with a path length of 537 pixels, mother, a longcurved gesture of 870 pixels, his, a short-straight gesture with two line segments, one corner, and a length of 462 pixels, and frost, a long-straight gesture with three line segments, two corners, and a length of 924 pixels.
Although prior work with gestures has employed larger gesture sets , four gestures were chosen so participants could complete the experiment in a reasonable time period without experiencing fatigue or decreasing their motivation or interest, which would have greatly influenced the results.
The experiment spanned two days.
On the first day, participants had two pretests , underwent a training phase , and after a 15-minute break, concluded with two follow-up tests .
On the second day, participants completed two additional follow-up tests .
Participants were randomly assigned to one of two groups, Acquisition Right or Acquisition Left.
The Acquisition Right group completed the Acquisition, Immediate, and Retention phases with their right  index finger and the Transfer phase with the left  index finger .
Participants were naive to the purpose of the follow-up tests.
Before the pretest and acquisition phases, a demonstration of the keyboard was given and participants were shown the feedback they would encounter.
In the Pretest phase, one of the four target words was presented on screen and participants were asked to make the corresponding gesture on the keyboard.
Once each gesture was made, it disappeared from view and the next word was presented.
Each word was presented four times in random order.
The Pretest Transfer test was identical to the Pretest Retention test, but used the opposite hand.
The Pretest phase lasted approximately 10 minutes.
The Acquisition phase consisted of 288 trials, grouped into blocks of 16.
The gestures within each block were presented in a randomized, blocked order.
Feedback was based on the Root Mean Squared Error  between the template and user-drawn gestures .
If participants were within 20 pixels of the target, positive audio feedback was presented and the on-screen score increased.
Otherwise, negative audio feedback played, the on-screen score decreased, and the template gesture was overlaid on the keyboard for 1500 milliseconds .
Participants were encouraged to receive as many points as possible and were told that points were based on the similarity of their gesture to the template, with a few points given for speed.
The Acquisition phase lasted approximately 35 minutes.
After the Acquisition phase, participants were given free time for 15 minutes.
After this period, the Immediate Retention and Immediate Transfer tests began.
The composition of these tests was identical to the Pretest phase and lasted approximately 10 minutes.
Twenty-four hours later, participants underwent the Delayed phase, which was identical to the Pretest and Immediate phases and lasted approximately 10 minutes.
Although 15 minutes of interference gives a good estimation of future performance, the Delayed phase allowed for interference and for the gestures to become consolidated in memory after a night's sleep .
A no-acquisition control condition was omitted from the experimental design.
It is well known within the motor learning and psychology communities that the acquisition and transferability of skills greatly improves with practice, although repeated practice is not the only factor influencing learning.
Within this experiment, the role of acquisition on gesture transfer was not the primary focus but our planned future work will explore this facet of gesture transfer.
To assess the nonacquisition trials , a 2  x 6 , x 2  x 2  ANOVA design with acquisition hand as the between subjects factor and repeated measures on the within subjects factors  was used.
These results indicate that transfer was symmetric: regardless of the hand used during training, participants were able to learn and perform the gestures at a similar level of proficiency.
Although there was not perfect transfer, the transfer is substantial enough to be leveraged by HCI designers.
In light of these results, the handedness factor was collapsed and two additional ANOVAs were performed.
The first ANOVA used the acquisition data and employed a 9 , x 2  x 2  design with repeated measures on block, length, and curvature.
In the second ANOVA, the nonacquisition data was used within a 6 , x 2  x 2  ANOVA design with repeated measures on phase, length, and curvature.
As the goal was to have each participant learn and reproduce the gestures as accurately as possible, a measure attending to scale and rotation was used.
To determine the error between the template and user-drawn gestures, each was resampled to 128 points and the RMSE was computed.
The duration of each gesture was measured from the touch down to the touch up of the finger.
The RMSE measure was used to evaluate gesture similarity, as we were interested in evaluating participant's ability to reproduce the target gesture exactly, not the cognitive association of the gesture or how `relaxed or sloppy' they could perform a gesture before a gesture recognizer could not classify it.
As RMSE is orientation and scale dependent, it enabled for the precise measurement of the deviation between the target and user-made gestures and reported a direct approximation of participant's skill.
This aligned with the instructions participants received.
A measure such as `the number of letters the user crossed though' would not have provided the fidelity needed.
A subset or collection of features that are currently in use today could have been used , but as gesture recognition features and algorithms improve, it would not be appropriate to evaluate participant behavior, ability, and skill using collections of features that will continue to be developed in the future.
Thus, the results are independent of the current state of the art and are applicable from a purely behavioral standpoint.
Paired t-tests with a Bonferroni adjustment of 0.006  revealed that during the Pretest phase, performance on both hands was similar, but very poor.
Participants made fewer errors from the Pretest to Immediate and Pretest to Delayed phases.
Participants were slightly more accurate with the trained than untrained hand during the immediate follow-up test, but after 24 hours, performance became indistinguishable.
No significant differences were found between the Immediate and Delayed results for both the Retention and Transfer phases.
Participants appeared focused on being accurate rather than fast, corroborating with the instructions they were given.
Paired t-tests  revealed that participants were significantly faster with their trained hand from the Pretest to Immediate and the Pretest to Delayed phases .
Although participants were initially slower during the pretest with the trained hand, this advantage disappeared during both follow-up phases and was likely due to the Pretest Retention test being participant's first exposure to the ATOMIK keyboard layout.
Across all other tests, the speed of the trained and untrained hands was similar and stable over time.
Across all phases, participants only exhibited a significant change in speed during the Pretest Retention phase.
The increased execution time was likely due to participants familiarizing themselves with the spatial layout of the ATOMIK keyboard instead of focusing on performing each gesture.
Once participants learned the spatial layout, they maintained a constant speed throughout the remaining experimental trials, even after the 24-hour break.
This is likely a byproduct of the scoring system used during acquisition, which favored accuracy over speed and penalized participants via delayed feedback when errors were made.
The stability found in the duration results but variability present within the error results could be due to different underlying processes at work during acquisition: one process involved in learning the spatial layout of the keyboard  and the focusing on learning the trajectory and form of the given gesture .
It thus might be possible that an effector independent representation of touchscreen gesture production was encoded in memory during the acquisition phase.
When needed during the post-acquisition phases, this flexible representation was accessible by the trained and untrained hands.
The first set of ANOVAs revealed that the acquisition hand had little to no influence on the dependent variables.
Participants who trained with their dominant hand exhibited performance that was very similar to those who trained with their non-dominant hand.
This is an important result as it implies that stroke-based gestures may be learned by either hand and could be executed with a similar level of accuracy in a real world scenario such as text entry.
The results from the second set of ANOVAs, shed light on the acquisition and transfer that occurred.
During acquisition, participants reduced the severity of their errors as the trials progressed, demonstrating that they were learning the gestures.
When looking at the pretest and follow-up tests, the trained and untrained hands complemented the results found during acquisition and were not influenced by the type of gesture being performed.
This suggests that participants were capable of using either hand to learn and perform stroke-based gestures more complex than those required today.
No differences were found between the post-acquisition Immediate and post-acquisition Delayed data or between the pre-acquisition Immediate and pre-acquisition Delayed data for the error measure.
Although a marginal difference was found between the two hands during the Immediate phase, its disappearance the following day suggests that touchscreen-based gesture transfer may symmetric.
This is likely due to the consolidation and accessibility of the acquired cognitive and motor skills in memory.
Although participants did not attain an `expert' level of performance, even after a night's sleep, they were able to recall the in-
This experiment has uncovered many interesting notions regarding the acquisition, retention, and transfer of strokebased gestures.
The implications of the work have been organized into three general guidelines that designers and researchers should be mindful of when designing and implementing future systems that use gestural interaction.
Fatigue is a common problem when working with any digital device.
The ability to perform stroke-based gestures with either hand is one way to mitigate fatigue on touchbased devices.
If users are aware  that they can perform a gesture using either hand and achieve the same level of performance, some may be inclined to switch hands from time to time, similar to those who alternate the hand they use a mouse with to lessen wrist, elbow, and shoulder pain.
Switching between the hands will not only help mitigate fatigue but could also decrease the likelihood of repetitive stress injuries in the long term.
Such encouragement does not need to be elaborate.
The movement of a gesture input area to the opposite side of the screen or the use of both hands in a video tutorial or gesture guide may be enough to remind users that they should consider using their other hand from time to time.
In addition to fatigue, supporting gestural input by both hands could greatly benefit those sharing devices or using a large touchscreen.
If users are aware they can use their untrained or non-dominant hand to perform stroke-based gestures, it could prevent the need to continually change the location or orientation of devices or prevent users from moving to a different location or reach across themselves or other users to access functionality or gesture input areas.
This will not only have social and comfort benefits, but will likely improve collaboration and user 's workflow as they will not be focused on where or if they can perform gestures but rather on the task or meeting at hand.
Lastly, the transferability of gestures may come in handy when the dominant hand is preoccupied but a user needs to perform a gesture.
For example, while writing with a stylus or holding a mobile phone, it may be necessary to flip to the next page or use a gestural keyboard to enter text.
In these situations, designers should discourage users from putting down their phone or stylus to make the required gesture.
Instead, they should integrate subtle cues to alert users that they could perform the necessary interaction using their unoccupied hand.
Such reminders will help maintain a user's concentration and workflow, while also encouraging them to interact with their device bimanually.
This will not only increase productivity and but also input bandwidth.
This offloading would enable novice users to become comfortable interacting with both hands, while simultaneously strengthening the encoding of gestures in memory.
This will ultimately help ease the transition to expert user.
With devices that detect handedness, the symmetric nature of stroke-based gestures could also enable novel methods of learning.
New gestural systems could encourage users to acquire gestures exclusively with their non-dominant hand.
Such a system would force users to focus on the intended movements, as more attention is required to maneuver the non-dominant hand.
In the long term, such systems could enable better performance and the learned movements would likely transfer to the dominant hand easily.
With handedness detecting systems, gestures could also be reused across the hands .
For example, the his gesture performed by the right hand could `redo' an action whereas when performed by the left hand could `undo' an action.
Although simplistic in nature, such reuse would help to decrease the overall size of gesture sets  while maintaining the functionality available to a user.
It would also increase the speed and accuracy with which users could perform gestures because a smaller number of gestures would need to be acquired, resulting in the continual reinforcement of mental and motor models of the gestures in memory.
Across all gestures, the length and curvature properties had little influence on transfer.
This suggests that users may be capable of acquiring and performing stroke-based gestures that are mapped to functionality in more natural, logical, and meaningful ways than what is in use today.
When defining such gestures for gesture-based picture passwords, for example, users should not be limited to circles or lines , but should rather be encouraged to use a combinations of lines, curves, and corners that represent figures or patterns that are important to them.
This will reinforce their encoding and prompt recall, while also increasing the security of such passwords.
Although our work did not evaluate a synchronous or asynchronous bimanual task, our results extend Guiard's theory of bimanual control .
In most implementations of Guiard's theory, the preferred hand performs precise actions and the non-preferred hand is restricted to coarse, simpler actions or used as a frame of reference.
As demonstrated by the work, designers may thus want to reconsider their adherence to Guiard's theory.
Users may be able to perform stroke-based interactions that are similar in complexity using either hand.
Our experiment revealed that when acquiring new skills, such as stroke-based gestures, users might not need to only use their dominant hand.
In current gestural systems, users often learn and perform gestures with their dominant hand, largely neglecting their non-dominant hand.
When a system requires bimanual interaction, it can thus be difficult for novices to use both hands; may revert to unimanual interaction.
As we have shown, gestures that are symmetric and transferrable have the potential to help mitigate fatigue and injury and be useful in situations involving mobile phones and styli.
In addition, designers may harness such results to ease the transition from a novice to expert user and increase the flow and `nui-ness' of gestural interaction.
Employing gestures that are transferrable, and encouraging usage of both hands, should be of great benefit to users, whether they are acquiring or performing gestures.
We hope that designers will take such results and use them to increase user's input bandwidth and ultimately the learnability of gestural-based systems.
Although our study has uncovered many novel facets of gesture transfer, there are a few limitations.
When considering applicability to other gesture sets, it is likely that such results would also be found with simpler gestures, i.e., those involving a single swipe, or pinch, or those appearing visually similar to common forms.
With extremely complicated gestures, the degree of transfer would likely be influenced by the mental processing required to encode and retrieve the gestures.
Gesture acquisition likely has a threshold, similar to Miller's "Magical Number Seven" findings for explicit learning , in which gestures below a certain level of difficulty are easily encoded and recalled, while others too cognitively demanding or taxing to recall being difficult to acquire.
This would in turn influence the amount and direction of transfer possible.
The particulars governing such thresholds have yet to be determined.
In prior work with alphabet and figure stimuli, feedback was not provided .
Within this study, continuous feedback was used.
Participants were thus able to reproduce the gestures, while maintaining their speed, to a very accurate degree.
The use of continual feedback may have aided gestural encoding during acquisition, and subsequently increased participant's ability to produce gestures during retention and transfer.
It is unclear what influence feedback really has on transfer.
Future work should assess the influence of various levels and durations of feedback to clarify the role of feedback in gestural systems and provide a clearer picture of gesture acquisition and transfer.
Our participants were also instructed to be as accurate as possible while learning the novel single stroke forms because mistakes would involve a penalty .
In prior work, there was no penalty for mistakes, speed was encouraged, and common multi-stroke shapes and letters were used as stimuli.
Such differences in motivation could have focused our population's attention to the novel forms they were learning more so than prior work.
The use of unfamiliar stimuli thus allowed for a more realistic picture of acquisition and transfer, as it would occur in real-world scenarios such as gesture-based text-entry.
Using a standard retention and transfer paradigm from the motor learning literature, it was found that learning a stroke-based gesture on one hand increased the accuracy to which it could be performed by an untrained hand.
Such gestures thus exhibited transfer.
It was also found that this transfer was symmetric, i.e., regardless of the hand used during training, participants could perform the gestures with their untrained hand at a level similar to their trained hand.
The experiment also revealed that the length or curvature of a gesture did not play a significant role in its transferability, even after a 24-hour period of interference.
