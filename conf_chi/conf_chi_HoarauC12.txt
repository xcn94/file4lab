Here, we are interested in structures as means to interact with the content: since structuring involves sets of objects, the actions done on an element of the structure may have an effect on several objects at once.
In current interactive systems, the use and the management of structures may be complex.
Users have to create and maintain them.
Depending on the kind of structure, some operations may be cumbersome or impossible to do, which prevents users to explore the design space of their particular problem.
Furthermore, systems that provide structuring do not leverage off the structures fully to provide users with new ways of interacting with the content.
Interactions with structure and with multiple objects through a structure have not been studied extensively in the past.
Of course, a number of past works have identified the problem , but few concepts or properties targeted it explicitly .
For example, what are the interactions that enable users to define sets of objects?
What are the available means to augment the scope of interaction i.e.
What are the concepts that may guide the design of such interactions?
The work presented in this paper aims at improving the management of structures as means to augment the scope of interactions.
Based on contextual inquires and related work, we present a number of requirements pertaining to the interactions with structures.
We then present two interactive tools that aim at fulfilling those requirements.
The first one is ManySpector, an inspector for multiple objects.
ManySpector displays all used values for a property given a set of differing objects, whereas a traditional inspector displays no value.
This reveals an implicit structure of graphics  and offers new interaction means.
The second one is based on links that users can draw between object properties to provide a dependency.
The resulting property delegation graph is a means for users to provide an explicit structure.
We then report on a user study involving those tools.
When using interactive graphical tools, users often have to manage a structure, i.e.
However, interaction with structures may be complex and not well integrated with interaction with the content.
Based on contextual inquiries and past work, we have identified a number of requirements for the interaction with graphical structures.
We have designed and explored two interactive tools that rely on implicit and explicit structures: ManySpector, an inspector for multiple objects that help visualize and interact with used values; and links that users can draw between object properties to provide a dependency.
The interactions with the tools augment the scope of interactions to multiple objects.
A study showed that users understood the interactions and could use them to perform complex graphical tasks.
When using computerized tools such as real-time editors, presentation software, GUI builders, etc.
They can edit them individually, e.g.
Users can also consider and interact with sets of objects as opposed to individual objects.
To do so, they may be required to structure the scene, by relying on concepts such as groups, styles, or masters.
According to the Oxford dictionary, a structure is "the arrangement of and relations between the parts or elements of something complex".
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We have based our work on concrete and realistic case studies.
We have conducted five contextual inquiries with "designers", the design activity being taken in its broadest sense: edition of graphics , courses schedule , architecture , or lecture presentation .
We have written a dozen scenarios that describe accurately the activities.
In order to introduce the problem, we present one of the scenarios.
This scenario illustrates a number of requirements pertaining to interactions on several objects, with or without a structure.
The steps are annotated in italic to characterize them.
We detail the annotations later in this section.
Elodie is a designer tasked with creating the graphics of a custom software keyboard for a tablet computer.
Using a graphical editor, she creates a first key.
She draws a rounded rectangle with a solid white fill and a surrounding stroke.
She adds a rectangle inside the previous one, with a blue gradient fill .
She selects both rectangles with a selection lasso  and groups them with a command in a menu .
She then adds a soft shadow effect on the group.
She overlays a label with a text `A' on the group of rectangles and centers the label and the group by invoking a `center' command on a toolbox.
She then forms another group with the label and the groups of rectangles, and names it "key" in the tree view of the graphical scene provided by the application .
This first key serves as a model to create other keys: she duplicates the key, and applies a horizontal translation to the copy.
She proceeds with this action several times in order to get a row of keys .
She then modifies the text of each key one by one .
She has to click multiple times on an object to reach the label and apply the `text centered' command.
Therefore, she estimates that it is more efficient to start over: she deletes all copies, ungroups the first key, centers the text, groups the objects again, copies and moves the copies, and modifies each letter one by one.
Elodie has finished the entire keyboard.
Some of the keys are double keys that contain two smaller labels at the top and the bottom of the key .
She wonders whether the double key labels are too small and she wants to explore new sizes .
First she has to find each double key in her design .
To do so, she zooms out to make the keyboard entirely visible.
This allows her to identify each double key.
Again, she has to change the size of the labels one by one.
The scenario illustrates several requirements.
For example, she grouped objects because she wanted to consider them as a single entity that keeps the relative positions between subparts, but also because she wanted to apply a single translation on three objects at once.
Conversely, she was not able to apply the command `set alignment' to several objects at once.
When she changes the letter `A' for `I', she realizes that the `I' text is not centered with regards to the rectangles .
The first object was specified incorrectly: if the three objects  are correctly aligned, the text of the label is not centered.
The problem was not noticeable with the first letters  since their widths are similar.
The search action requires visually scanning the graphical objects and seeking candidate objects, at the risk of forgetting some of them.
The more the objects, the more difficult it is to find out particular ones, especially if the features to search for are not pre-attentive .
As the number of keys increases, each modification gets more costly, not only because of the number of actions to repeat, but also because of the required visual search effort.
Past works have tackled the problems of managing structures, and interacting with multiple objects, either explicitly or implicitly.
We present them along three axes: interactions for structuring the content provided by interactive systems, design and evaluation of interactions for structuring, and structuring in programming.
The only operation available for a group is `ungroup', which removes the group entity and selects all objects that were part of the groups .
Selection can be seen as a transient group, with `add' and 'remove' operations by holding the shift key and selecting several elements, or holding the ctrl key and clicking on individual elements.
Some tools support heterogeneous settings, but with specific properties only e.g.
This forces the user to ungroup and apply the command on each object.
In this case, interaction with the structure is not well integrated with interaction with the content.
Similarly to classical inspectors, surrogates expose attributes that are common to objects, by automatically narrowing the surrogate to the lowest common ancestor.
This enables users to interact with those values and modify several objects at once.
The list of reversible actions is an implicit mechanism to help users not to fear possible damages .
Side Views display previews of interactive commands .
Parallel Paths support alternative exploration by relying on an arborescence of creations instead of a linear history, and on the simultaneous views of parallel results  .
Acting on a node of the creation path enables users to manipulate the subsequent designs at once .
A tree view enables users to reparent elements with a drag and drop.
However, there is no support for other operations, such as applying a color to a node in order to change all children.
For example, PowerPoint enables users to define in a master slide the appearance that other slides would inherit.
Sketchpad introduced masters as shareable objects that could be used in multiple locations in the scene .
Changing a property of the master would modify all objects that depend on this master.
This was a way to reduce the number of actions required from the user when something must be changed.
It exhibits when the structure of the information contains a lot of dependencies between parts, which implies that a small change leads to numerous adjustments from the user.
Viscosity is a hurdle to modification and exploratory design .
Since it may be costly to apply the changes, the user refrains from exploring alternatives.
A solution to viscosity consists in creating an "abstraction", a "power command" that would act on several objects .
An abstraction is a class of entities, or a grouping of elements that users will handle as a single unit e.g.
Learning, creating and modifying them require time and effort that should be balanced with investment in repeating a small sequence of actions to solve a small problem.
Design principles related to instrumental interaction, such as reification , polymorphism  and reuse  extend the scope of actions to multiple objects .
In this section, we synthesize the requirements for the manipulation of objects through structures .
The synthesis is derived from the contextual inquiries we ran, and our analysis of the related work.
Notably, the requirements are related to the set of tasks identified in  that are known to be difficult to perform with direct manipulation techniques.
We have expanded and refined them in this section.
We present 3 subsets of requirements: managing sets of objects , managing actions , fostering exploratory design .
CIS is a model that helps describe an interaction technique, analyze it, and predict its efficiency in the context of use .
CIS defines four properties for interaction techniques.
Among them, Fusion is the ability of a technique to modify several work objects by defining multiple manipulations at once , and Development corresponds to the ability offered to the user to create copies of tools with different attribute values.
The problems raised so far can also occur during development activities.
For example, refactoring tools in IDEs is an answer to the need for multiple scopes of action: if the user changes the name of a method, the system applies this change on each call of the method, possibly in many classes or files.
Changing a parameter in an intermediate node has an effect on its children.
Tags in the Tk toolkit allow the programmer to structure objects in overlapping sets .
Changes can be applied to graphical shapes or to a tag, and thus to the set of objects that hold this tag .
Prototype-based languages offer an alternative to classbased languages for object-oriented programming .
They offer a flexible creation model that allows sharing of properties and behaviors.
Such mechanisms allow users to structure a hierarchy of prototypes and to act on several clones by manipulating a prototype in the delegation hierarchy.
Morphic reifies prototypes and clones into graphic objects , and allows for their construction and edition with direct manipulation .
Tools have been designed to help structure a prototype hierarchy.
For example, Guru is an algorithm that automatically creates a well-organized graph of prototypes, by factoring shared properties into new prototypes .
Managing sets consists in searching , and designating  the objects that are part of a set.
It is also necessary to modify  the sets .
Finally, users must be able to identify  the objects that belong to a particular set, or determine the sets a particular object belongs to.
Perceiving their consequences  with appropriate feedback enables the user to realize the effects of its action after, and even before it is triggered .
In order to support exploratory design, it is important to provide users with tools that enable them to try  and evaluate  solutions during short-term exploration , and compare different versions during middle-term exploration  .
When satisfied with the results, users must be able to extend the modifications to other objects.
If the system does not support this task efficiently, users will have to repeat the same actions to propagate changes .
Finally, if structuring is a solution to the viscosity problem, it is a hurdle to exploration if required a priori.
Therefore, structuring should be made a posteriori  i.e.
We have explored a number of interaction techniques to offer new ways of interacting with multiple objects through structures.
To design them, we involved the users we interviewed in a participatory design process, with 2 brainstorming and sketching sessions, and 5 evaluation sessions, as demonstrated in .
In the following, we cite the requirements that each feature is supposed to address.
Requirements serve both as rationale to explain the design, and to help readers determine whether they are satisfied by our claims that the design fulfills the requirements.
An inspector offers two services to the user: visualizing values with progressive disclosure and modifying them .
Users can change such a value, and the system reflects the change to all selected objects.
Users are thus not informed about those values, and sometimes cannot modify them through the inspector.
We have designed ManySpector, an inspector that displays all used values for a property given a set of differing objects.
For example, in Figure 5-right, the Fill property displays all colors used by objects in the selection.
Used values reveal an implicit structure of graphics, the sets of objects that share a value for a given property.
Though not explicitly defined by the user, we think that such sets may be useful, since users sometimes think about objects with a graphical predicate .
We relied on the display of used values to design a set of interactions that offer new services for exploratory design and structurebased interaction: query and selection of objects with graphic examples, selection refinement, and properties modification on multiple objects.
The representation of a shared value in ManySpector actually reifies  both the value per se, and the set of selected objects that exhibits this property value.
As a value per se, and similarly to the interaction with the sample panel, users can drag the shared value  from ManySpector onto  objects in the main view to modify a property.
If the shared value is numerical, users can hover over it and rotate the mouse wheel to increment or decrement it .
Together with immediate feedback, this enables both exploration and precise adjustment of properties, thus reducing temporal offset  between action and feedback.
To illustrate the interactive tools, we have designed a graphical drawing application.
There are four parts: a tool palette on the left side, a workspace in the middle, a sample panel on the top right corner, and an inspector on the bottom right corner .
The workspace is the main view, where users can create a new object by clicking and resizing.
Selection is performed by clicking on an object or by drawing a rubber rectangle to encompass several items, as implemented in usual graphics editors.
A bounding box with handles surrounds selected items.
The samples panel contains a set of values for shape , fill color , stroke color  and stroke thickness .
In order to modify a property of an object in the main view, users can drag a sample and drop it onto the object.
Feedback is shown as soon as the sample hovers over the object, in order for the user to understand the action and to assess the change before effectively applying it by releasing the mouse button.
This enables the user to cancel the action, by releasing the button outside of any object .
Drag and drop of samples also applies to a selection of objects.
The interactions described so far are not entirely novel.
The next sections present two tools with novel interactions.
Keeper keeps in the selection the objects that have this shared value, and throws away the others.
Extender adds to the selection all objects that are not selected but that possess this shared value.
The instruments can also be dropped onto an object of the scene to add or remove it from the selection.
These interactions extend the set of example-based queries introduced above .
Since a shared value also reifies a set of objects, hovering over a shared value highlights the relevant objects while blurring others with a short animation .
This makes it easy to figure out which set is made of what , and to detect outliers and fix them.
Users can drag a sample  from the sample panel onto a shared value  to modify at once a property for multiple objects  .
Users can also drag a shared value  onto another shared value  .
To select objects, users can click on them in the workspace, or draw a selection rectangle.
The interaction consists in a drag and drop of the representation of the instrument onto a shared value.
Besides ManySpector, we have explored an interactive tool that enables users to structure the content explicitly.
Users can specify that a property of an object  depend on the property of another object .
A prototype is similar to a master in Sketchpad: when users change a property of a prototype by dropping a sample from ManySpector onto the prototype, all dependent clones are changed accordingly .
The interaction to specify a dependency is as follows : by clicking on an object, users can toggle the display of the properties around it.
They can press on a property, draw an elastic link, and drop it onto another object as if they were dropping a sample.
The clone object appearance reflects immediately the appearance of the clone for that property.
Users can remove a link by pressing the mouse button in the blank space, drawing across the links to be deleted, and release the button.
The system proposes two ways of creating new objects from existing ones: either by copying it or by cloning it .
Copying is the regular copy operation: properties from the copy are independent from the properties of the source.
Cloning enables users to get a clone, whose properties are entirely delegated to the copied object  .
By creating a clone, users minimize the number of actions required to specify a single difference with the prototype: if they copied instead of cloned, they would have to link all shared properties.
Explicit structuring is supposed to bring more action power, at the expense of increasing viscosity and hindering exploratory design since users have to manage a structure.
We have lowered these drawbacks with a posteriori structuring and by leveraging off ManySpector.
For example, choosing to clone or to copy may be premature at the moment of the creation of a new object from an existing one.
To solve this problem, users can decide to change them to a copy or a clone after the creation of the object .
This is made possible by tracing the history of objects, and how they were created.
Toggling between copy and clone only affects the properties that were not set explicitly by the user.
Another problem is to interact with similar objects in order to make them depend on a prototype.
A viscous solution would be to interact with each object and making it a clone of the prototype.
A more efficient solution consists in selecting the objects that are to be clones, and in dropping the property of the prototype onto an object of the selection .
Users can also drop the property onto a shared value in ManySpector , which links all objects sharing that value to the prototype.
The property delegation graph is an extension of the delegation tree found in prototype-based languages .
However, with a tree, objects cannot have multiple parents.
For example, the scene tree available in illustrator may be helpful to conceptualize the scene, but is unable to help specify cross-branches relationships.
Conversely to a tree, a node in our graph of properties can have multiple parents.
This enables users to be more specific about the parent that holds a particular property: a node can delegate `fill' to a prototype A, and `stroke-width' to a prototype B.
The interactions are consistent: they all use modeless interaction based on drag and drop, be it from or on an object on the scene, a shared value, or a prototype.
With immediate feedback and a posteriori structuring, they also support exploratory design.
The properties are immediately visible : users can try and test by hovering over and off the used values, and assess the results thanks to immediate feedback without applying the change .
The interactions we devised can be considered as a kind of surrogates .
We have expanded them by explicitly taking into account the interaction to manage the selection and explicit structuring.
Furthermore, our version exposes not only common properties but also all used values, which makes direct the access to more subsets and expands notably the scope of interactions.
Of course, existing systems enable users to obtain the same final results, and even by relying on similar concepts .
Those systems actually provide the same functionalities, but not the same interactions.
For example, existing tools do enable users to perform a graphical search, but with an indirect manipulation .
This prevents users from quickly trying and testing changes and hinders exploratory design.
In addition, interactions are not well integrated e.g.
As such, the prototypes have issues.
For example, more work needs to be done with respect to scalability: ManySpector is not able to handle very large sets of used values.
The solution with a scrollbar and progressive disclosure may not be sufficient.
The prototype/clone view also needs more work: if the links are numerous, the scene may result in a mess of tangled links.
Again, progressive disclosure is a possible solution but we are also exploring other representations and interactions .
Furthermore, the system does not check for cycle when the user tries to link two properties.
Appropriate feedback is necessary to prevent it, such as displaying the links to show a potential cycle when hovering over a property.
We have argued in the previous sections that our tools are novel, consistent and effective for performing structurebased interaction.
Assessing those claims is not a straightforward task.
We were especially concerned with the understandability of the used values concept, and the fact that they refer either to a value or the set of objects that share this value.
Would it be too difficult for users to grasp the shared value concept and linked properties?
Even if users understand them, how would they struggle when trying to use them to interact with multiple objects?
Finally, can users translate high-level problems into graphical interactions with used values and linked properties?
For examples, the task "change all circles' color" is difficult because users need to find all circles in a scene, a visual task known to be non pre-attentive and that requires a cumbersome one-byone scan of graphical objects .
Users were free to carry out the tasks the way they want, either by selecting shapes with the traditional way or using ManySpector .
The evaluation session was divided into three parts, each dedicated to one of the three questions above.
The first part was devoted to a tutorial that teaches users about used values and links, and how to interact with them in the graphical editor.
The two other parts are scenarios that were designed so that they implement the requirements.
In the tutorial, we instructed users to create a few objects, link them, change their color or stroke thickness, with a single object or a set of objects.
The tutorial lasted 10min and included 15 simple tasks.
Users were actually manipulating the mouse and performed interactions while they were listening to our instructions.
The goal of this tutorial was not only to instruct users, but also to see if they understood the design.
We assessed their understanding by observing them perform small tasks with no instructions and by asking them if they were confident in their understanding.
We did not assess discoverability since we began with a tutorial.
This aspect is left for future work.
The second part of the session was an actual test.
The test was still using the graphical editor, but this time with a scene containing multiple  differing objects .
We asked users to perform more complex tasks such as `change the thickness of all yellow circles to the maximum of all thicknesses'.
We did not give any instructions, and left users perform the tasks by themselves.
One of the expected benefits of used values is to help users select a set of objects with minimal interactions.
The third part involved a calendar application.
Users were manipulating events on a week view .
Events are represented with rectangles with a title text and a start hour text.
They are placed horizontally according to day of occurrence in the week and vertically according to the time in the day.
The screen is filled with seven columns, one per day in the week.
Instead of graphical properties, the ManySpector window contained calendar-related properties such as start, duration, title etc.
Conversely to iCal, ManySpector displays used values.
We provided a partially filled schedule and we asked users to act as if they were teachers trying to schedule lecture sessions during the week with a schedule "manager" .
For example, we asked them to place a 2-hour long lecture Wednesday afternoon.
Then we told them that when we said "place a lecture at 10am", we actually meant "10:15am", so they had to change all "10am" lecture events to "10:15am" .
The goal of this third part was to assess whether users could translate higher-level tasks to graphical interactions with our tools.
The tasks were high-level, and required users to try R3.1, perceive the consequences R2.4, evaluate R3.2 and perform short-term exploration R3.3.
We performed the tests with five subjects.
Three of them use calendar application in a day-to-day basis, one of them was a graphical designer used to applications such as Illustrator, and one was a casual user of graphical tools such as presentation software.
They were all aware about the viscosity problem that might occur when using such tools.
Only the graphical designer was involved in the participatory design process, hence four users discovered the interactions for the first time.
They struggled to understand and memorize them, which hindered their ability to devise a solution.
The four non-graphical designers found the requests much less difficult in the last part with the calendar application and meaningful tasks.
Still, all subjects were able to accomplish every tasks of the second part by themselves.
We were wondering about voluntary use.
We observed what we expected: with tasks that involve pre-attentive properties , subjects were sometimes still using a traditional selection.
However, they turned by themselves to used values with non-pre-attentive tasks, or when the number of objects was too important.
They also used links when we asked them to repeat an interaction on the same set of objects: after a number of repetitions, some subjects turned a specific object into a master.
This enabled them to be more efficient than devising a selection again with the ManySpector.
All kinds of interaction were performed , and all combinations of source and destination for drag and drop were witnessed.
We did not notice difficulties when users had to translate higher-level tasks into interactions in the calendar test .
We witnessed a tendency to use traditional selection for very simple tasks.
When we forced users to employ our interactions instead, they did not have difficulties to do so .
This suggests that the interactions can be applied to other contexts than graphical edition.
Even if we did not plan to evaluate usability, the tests revealed some issues such as the difficulty of interacting with the text boxes.
Users also found limits to the interactions we proposed: in some cases, users would have liked to keep objects based on a combination of values instead of a single one.
As expected, links lacked visibility and legibility when numerous.
All in all, the study allowed us to answer positively to our concerns: the tools fulfill the requirements since users were able to understand the interactions, could perform complex graphical tasks with them and could translate higher-level tasks into them.
Users judged ManySpector very useful .
They liked explicit structuring with links though not as much as used values .
They also praised the fact that there was no imposed strategy and that they could perform tasks their way.
We asked subjects to think aloud  while they were acting.
We observed them and logged what they tried, whether they struggled, made errors or succeeded.
At the end of the second and third part, we made them fill a questionnaire to rate the difficulty and cumbersomeness of the tasks, and the usefulness of the design with a Likert scale from 1  to 5 .
Results are given in the following, with the mean and the standard deviation.
We did not notice serious understandability problems.
Users were able to manipulate shared properties and links, and succeeded in performing simple tasks at the end of the tutorial.
When asked about their confidence, some of them felt that they needed some learning "to do it well".
We showed them many interactions, but even if the interactions are well integrated, users felt that they could not get familiar with them within such a short time.
In addition, because there were several possibilities to accomplish tasks, users were always eager to find the best way of accomplishing it, which adds to their feelings.
Our confidence into users' understandability got stronger when we witnessed that they got more capable as they were performing the second and third part.
We even observed users trying interactions that we did not designed but that were perfectly meaningful, such as using selection instruments  directly on samples to avoid the necessity to perform a selection of the entire scene, dropping a value onto a property name to apply it to all objects, or dragging a sample next to existing used values to extend the selection.
This suggests that the design was consistent and predictable.
We did notice some difficulties when users performed more complex graphical tasks in the second part .
This can be explained by the fact that users were still learning the interaction.
They also told us that the tasks were rather abstract.
A study showed that users are able to perform complex graphical tasks with them.
The examples involved a drawing editor and a calendar but the requirements and interactions are not specific to these applications, and can be applied to others.
Other designs are possible: we are currently investigating other forms of explicit structuring with no links.
We also plan to assess how well those interactions support exploratory design.
