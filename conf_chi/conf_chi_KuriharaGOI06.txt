It is tedious to handwrite long passages of text by hand.
To make this process more efficient, we propose predictive handwriting that provides input predictions when the user writes by hand.
A predictive handwriting system presents possible next words as a list and allows the user to select one to skip manual writing.
Since it is not clear if people are willing to use prediction, we first run a user study to compare handwriting and selecting from the list.
The result shows that, in Japanese, people prefer to select, especially when the expected performance gain from using selection is large.
Based on these observations, we designed a multimodal input system, called speech-pen, that assists digital writing during lectures or presentations with background speech and handwriting recognition.
The system recognizes speech and handwriting in the background and provides the instructor with predictions for further writing.
The speech-pen system also allows the sharing of context information for predictions among the instructor and the audience; the result of the instructor's speech recognition is sent to the audience to support their own note-taking.
Our preliminary study shows the effectiveness of this system and the implications for further improvements.
Lecturing and note-taking is one of mankind's fundamental communication and information processing techniques.
It is also a good example of multimodal interactions in which an instructor and the audience communicate with each other by speech, body gestures, and utilizing written materials naturally and effectively.
With advances in digital technologies, many systems have been designed to support instructors and the audience during lectures.
Some systems focus on annotating preauthored slides  and some systems are primarily designed for writing from scratch .
Writing is superior to just showing pre-authored slides in that the presentation becomes more flexible and more engaging .
In addition it saves the time that would be required to prepare complete slides.
One problem with writing is that it is tedious to write long texts by hand.
It is reported that as much as 18% of lecture time is consumed by writing on the board .
In this paper, we propose predictive handwriting to reduce the burden of manual writing for the Japanese language.
The system predicts possible next words based on speech recognition and handwriting recognition, and allows the user to choose a desired word or sentence from a list to reduce manual writing.
Prediction has been frequently used in typed text entry, but we are not aware of a previous system that has incorporated prediction for handwriting.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The biggest concern as to whether such a system will be effective is that users might not prefer predictive methods because of the cognitive overload required to choose the correct prediction .
To counter this concern and justify our approach, we first performed a user study that examines the user's behavior in Japanese writing.
The result shows that people prefer selecting to writing in general and that selection is especially preferred for words consisting of many strokes.
Based on these observations, we developed a prototype system called speech-pen to examine the possibilities of predictive writing.
Figure 1 illustrates the basic concept of the system.
This system helps the instructor's manual writing - not the entry of typed texts - by suggesting possible further writing based on speech and handwriting recognition .
If the instructor finds a correct prediction in the list, he can paste it on the board to save manual writing.
If not, he can simply ignore the predictions and continue writing.
The system uses a customized font that mimics the instructor's own handwriting to seamlessly integrate the manual writing and automatically generated texts .
In addition to supporting the instructor's writing, the speech-pen system also supports the audience's note-taking by providing similar predictions.
The result of the instructor's speech recognition is sent to each of the audience's tablet PCs and used as a context to generate correct predictions for note-taking.
We call this "ambient context" sharing because it is a kind of context-sharing usually done in the background.
One of the problems of these raw handwriting sharing systems is the difficulty of reading and reusing other peoples' handwriting.
We pursue similar goals but the shared context generated by speech and handwriting recognition is basically invisible.
Recognition technology has a long history of research and development.
However in spite of vast research efforts, recognition technology has not yet overcome the fundamental problem of recognition errors .
Given this observation, researchers have been exploring various user interface techniques to work with error-prone recognition technologies.
Oviatt  investigated the possibilities of improvements on recognition technology using multimodal interfaces and proposed the concept of mutual disambiguation that decreases error rates of recognition by complementary use of multiple modalities.
Goto et al  developed several speech-interface functions that use nonverbal information in speech input.
Hindus and Schmandt  discussed the utilities of unobtrusive capture of voice interactions in everyday work environments.
They tried to ease the mental resistance of users in using speech recognition by encoding voice commands in computers in socially acceptable conversations.
We take a similar approach, but our goal is assisting the user's handwriting and not giving commands to the computers.
Kaiser  and Feng  explored typed text entry methods with multimodal recognitions of speech and pen.
We also combine speech and pen but it is designed to reduce the burden of manual writing with minimum overhead.
One of the most important features is that it is relatively tolerant with recognition errors because recognition works only as an auxiliary support, not as the main interaction method as per the typed text entry.
Digital writing for real time presentations has been discussed mainly in the context of electronic whiteboard systems.
Early systems, such as Xerox Liveboard  and the Tivoli system  are mainly designed for small group meetings.
They provide various interfaces to organize the instructor's handwriting on the board.
Some recent systems are more specifically designed for large-scale classroom presentations .
Most systems emphasize the integration of digital writing with pre-authored presentation slides.
Some systems are designed to support note-taking.
The Audio Notebook , Dynomite  and many other systems record the instructor's speech and associate it with the handwritten notes.
The user can quickly play the audio track by specifying the corresponding handwritten note.
In these cases the speech is not converted into text but is simply recorded as audio data.
Livenotes  and StuPad  allow listeners to share the slides on their tablet PCs and discuss issues with other listeners by collaborative note-taking in the shared space.
When using mobile devices that only have smaller keyboards or stylus pens, text entry is not so easy and fast.
The user may have to type many keys to produce a word , or may have to work on tedious handwriting recognition and correction processes.
To improve input efficiency, several approaches have been investigated.
One is to improve recognition accuracy by designing robust gestures for the alphabet such as Graffiti.
Another approach is to provide efficient software keyboards .
SHARK  is a combination of the above two approaches.
Predictive text entry is yet another solution.
The user can select predictions in a list and paste them instead of entering all the characters .
One of the common arguments aimed at predictive text entry of the English language is whether it is really faster or not.
However, we can not simply deduce the same for other languages such as Chinese and Japanese.
In these Asian languages thousands of characters are used on a daily basis and each character consists of many strokes.
A common input method for them consists of two phases.
First, the user inputs phonograms .
Second, the user converts them to ideograms  by selecting a candidate from a list.
The user can not finish entering words without some interactions with the system because in general there are many ideogram sequences whose pronunciations are the same .
The effectiveness of predictive text entry in these Asian languages is demonstrated by the fact that almost all cell phones available in these Asian countries support predictive text-input methods, and users regularly make use of them.
In predictive handwriting, input predictions are effective only when the total time cost for selecting predictions is less than for writing.
Suppose the system could always provide the correct predictions.
In this simplest case, input predictions are time-effective under the following condition:
However, the real system does not always provide the correct predictions.
Then the user is forced to look at the list and confirm that there is no appropriate candidate and return to write manually.
The total time for this action is as follows:
This paper proposes predictive handwriting, which is an extension of predictive text entry to digital writing.
In predictive handwriting, the user manually writes characters stroke by stroke using a pen and sometimes word predictions are selected .
Masui  established an effective predictive input method for typed text without quantitative justification of its necessity because it is obvious for Asian languages.
On the other hand, it is not so obvious whether predictive handwriting is actually preferable because the properties of handwriting are different from those of typed text entry.
This section describes a user study we performed to address this concern and to collect basic data for designing the system.
Our goal here is to investigate the users' behavior towards handwriting and selection in Japanese writing, but this is a little complicated because many parameters are involved such as number of strokes and number of candidates.
Therefore, we first propose a simple practical model that incorporates these parameters and establish hypotheses using the model.
We then estimate parameters of the model in the study and examine the hypotheses.
Although our current focus is on predictive handwriting of the Japanese language, we expect that the result is also applicable to other Asian languages that use complicated characters, such as Chinese.
D is a kind of discriminant that tells us the theoretical advantage of handwriting.
It is expected to be faster to write when D is positive and vice versa.
Namely, writing tends to occur in some cases when D is positive.
In the user study, we first obtain a simple estimation of H and S for the calculation of D. Then we examine the hypotheses using the D values.
We observed that the user tends to keep typing rather than selecting from a list in English text entry.
We also observed that the user tends to select words rather than manually typing everything in Chinese and Japanese text entries.
From these observations we can imagine that there must be a certain critical point where the user switches the strategies from typing  to selecting.
They were asked to perform the following 3 tasks on a Tablet PC :  Handwriting task,  Selection task, and  Combined task.
In addition, the probability of whether the appropriate candidate appeared in the selection list was restricted to two simple cases {p=1.0, p=0.5}.
The probability p was notified to the participants beforehand to help them establish their strategy.
The total test-set consists of 132  words per participant.
The order of words in the test-sets was randomized.
The system first shows the target word, blank cells for writing the word, and the list of masked candidates .
If the user prefers to write the word, he simply starts writing in the cells.
If the user prefers to select from a list, he first taps the masked list and the system shows the actual candidates.
This allows us to separate the cases where the user decides to write manually without using selection and where the user wanted to select but ended up writing it because the target word was not in the list.
This task investigates the writing time in terms of total strokes and number of characters.
For each number of strokes, there are many words that consist of different numbers of characters.
Thus we chose three random words corresponding to the minimum, midrange and maximum number of characters.
The maximum number of characters were limited to less than or equal to 10.
In the end we constructed a 22-word test set per participant to write.
During the study, the system presents the words in random order to the participant and the participant writes each word in a designated writing space .
The writing space is divided into 1.6cm square cells.
Cursive writing was not allowed.
This task investigates the selection time in terms of the number of total characters and the number of candidates shown in the list.
The number of candidates was 1, 3, or 5.
The properties of the word set {number of strokes, number of characters} were the same as task .
All the false candidates are generated by randomly choosing words whose properties are the same as the target word.
In this way we constructed a 66-word  test set per participant.
During the study, participants selected the words in the lists by tapping appropriate candidates.
The order of words in the test sets shown was randomized.
The space for showing a candidate and selecting was a 1.6cm x 12cm rectangle.
Figure 2  shows a snapshot of the task.
This is not a strong fit.
However, we need some estimate for selection time to build the system.
Thus for engineering purposes this linear regression is adequate to construct an initial system implementation, but we expect that future work can identify a superior model.
A reason for the poor fit is large individual differences among users.
This can be addressed by adjusting parameters for individual users.
As for the number of characters, their effect was negligible and we did not use it in the model.
Writing long, complex Japanese words is burdensome and Japanese people are familiar with the process of selecting words from a list.
The participants' aggressive tendencies to select reflect this background.
However, this result might be slightly biased toward selecting because the participants did not need to compose sentences by themselves in this study.
If they actually write while composing sentences in their mind, they might prefer handwriting because it can cause cognitive overhead to examine the list.
Future studies should explore this issue further.
From the individual participant's point of view, D-value analysis for each participant revealed a diversity of decision strategies.
Figure 7 shows the two extreme cases of writing tendencies and selecting tendencies.
This result shows that it is important not to force the user to use predictions and to allow both strategies at any time.
Figure 6 shows the result of task  categorized by the D value.
The histogram above shows the number of the cases when the participants decided to write manually at certain D-value condition.
The histogram below shows the number of cases of selection.
D-value analysis is useful for suppressing relatively useless input predictions.
When the user wants predictions, the system first obtains many candidates from the database based on the user's recent input.
Suppose the number of the obtained candidates is c and the maximum number of the candidates in the prediction list displayed on a screen is m'.
At that time the system knows S and H for each candidate.
Finally we obtain estimated D values for each candidate.
If some of them are positive, the candidates are thought not to be worth providing in the sense of time-efficiency.
These candidates can be suppressed for achieving conservative predictive handwriting, which will be suitable for the user who prefers the handwriting option.
The upper half is the number of cases where handwriting was used and the lower half is that where selection was used.
D is a theoretical metric that estimates the relative advantage of handwriting considering the number of strokes and the number of candidates.
The user study in the previous section shows that predictive writing can be useful for the Japanese language.
Based on this result, we designed a prototype predictive handwriting tool called "speech-pen" to support digital writing in a class or presentation.
It recognizes the instructor's speech and handwriting and provides predictions for further writing to the instructor.
This section describes the details of the system.
The instructor can keep writing when he is not interested in the predictions or when the prediction results are incorrect .
If the instructor finds a desired text in the predictions, he can paste it in the board with a single gesture.
The text is presented in a font that imitates his own handwriting.
Figure 8 shows the system configuration of the current speech-pen system.
A microphone is attached to the instructor to record his voice.
The instructor gives a presentation by writing materials on a large digital surface or tablet PC connected to a projector.
The audience also takes notes individually on their tablet PCs.
The instructor's speech is recognized by a speech recognition server.
The recognition results are distributed to all the users  over the network.
The current prototype supports the Japanese language only while some examples in this paper are in English.
Figure 9 illustrates how the speech-pen system works from the user's point of view.
Suppose we are in a lecture.
The instructor writes on an electric whiteboard while speaking freely .
When he pauses writing for a moment or presses a button explicitly, the system displays some predictions that are likely to be written next based on the result of the speech and handwriting recognition.
Multiple prediction results appear around the user's latest writing .
Each prediction result corresponds to the recognition result of an utterance in past speech, or a word in the user's customized dictionary.
The retrieved prediction result is displayed as a collection of multiple sub-candidates  as shown in Figure 10.
It is a visualization of probabilistic recognition results sorted in the order of likelihood.
This interface was originally designed as a method to correct recognition errors for speech-to-text systems .
In real-time operation it is superior to zooming interfaces such as  in which the user traverses a vast area of candidates.
The latest predictions are shown when the user pauses before starting the next stroke .
We also decided to always show the latest result of speech recognition at the bottom of the screen because we found that the instructor often writes what he is speaking.
Oviatt  reported the existence of individual differences in the order of input modalities when multimodal interfaces are used.
Some people tend to speak and write sequentially, and some people prefer to speak and write simultaneously.
It might appear that the speech-pen system only supports the speaking-to-writing order.
However, the speech-pen system creates predictions using not only sentences that were uttered just before writing but also all sentences that appeared during the current and even past lectures.
Important words are often repeated during a lecture and over several lectures.
Therefore the speech-pen system provides support for both those who speak and write simultaneously, and those who speak after writing.
The result of the instructor's speech recognition is distributed to the audience as a shared ambient context.
It is used in order to generate prediction results for each member of the audience.
The system recognizes a member of the audience's handwriting and retrieves a text in the shared context that begins with the recognized word.
As is the case with the instructor, the audience can always ignore the predictions and continue with manual writing.
Figure 13 shows an example of writing by an instructor and a member of the audience, obtained in the user study.
This result shows that the system successfully supports a variety of individual writing by providing ambient support.
The current prototype system shares speech recognition results only.
Our future work is to implement a framework to share other forms of ambient context such as handwriting recognition results.
Sharing information on which prediction has been selected by the instructor and the audience would also be useful.
This section describes how the speech-pen system generates predictions from speech and handwriting input.
The basic idea is to show previous utterances that start with the recently written characters as predictions.
While the instructor is speaking, his voice is sent to the speech recognizer and the recognition result is stored in a database.
When the instructor starts to write, the handwriting recognizer recognizes the most recent writing.
Then the system searches the database using the result of handwriting recognition as a query, and shows the search results to the user as predictions .
When the prediction candidates are displayed, the user can either select a candidate and paste it on the board or discard the candidates and resume writing.
The selection is done by crossing , that is to say, the user draws a stroke over one of the prediction results, tracing the desired words in the list .
The selected text is pasted on the board with a font that mimics the user's own handwriting.
We currently use a commercial service to generate the customized font .
The font size is determined based on the size of recent writing.
The user can simply ignore all predictions and continue with manual writing when they are not useful.
As soon as the user starts writing the next character, the prediction disappears.
They also disappear when a certain period of time passes after the user finishes writing.
Unlike typed text entry, digital writing does not require the user to always convert handwriting to typed text.
Written characters persist as they are and the user can return to writing manually when the predictions are incorrect.
This is a significant feature of the speech-pen system that makes it possible to use error-prone speech and handwriting recognition technology in noisy environments.
The current implementation is distributed over a LAN for performance reasons.
The speech recognizer uses a largevocabulary continuous speech recognition engine  and it runs on a Linux workstation.
The handwriting recognizer and the user interface component use the Microsoft Tablet PC platform SDK and run on Tablet PCs.
In the speech-pen system, the speech recognizer always works in the background and recognizes the instructor's speech in real time.
It first detects the endpoints  of each utterance by using a standard technique that uses short-time energy and recognizes an utterance according to the language model that includes the system vocabulary .
Note that even up-to-date HMM-based speech recognizers require a system vocabulary consisting of all the target words.
The language model we use is built by learning Mainichi newspaper articles, which covered various general topics over a 10 year period.
What is important here is that we do not have to register all the terms that will be used in upcoming lectures: even if some terms cannot be recognized, the instructor can simply ignore those wrong predictions.
To improve the speech recognition accuracy however, it is recommended that domain specific terms related with upcoming lectures are registered in advance when available.
We think it is practical to prepare and register those terms because the instructor usually prepares the contents of lectures beforehand.
To prepare the terms, it is also possible to "recycle" speech and handwriting recognition results of the past lectures given by the instructor or other participants.
Those terms for the system vocabulary can also be used to improve the handwriting recognition accuracy and be shared by the audience.
The speech recognizer then generates a confusion network, which is the result of condensing intermediate hypotheses  of speech recognition.
Figure 10 showed a simple example of a graphically represented confusion network.
In general the internal word graph itself is too huge for users to understand in the case of largevocabulary continuous speech recognition.
With the confusion network, the user can easily understand competitive candidates  of recognition results and select the correct word sequence as shown in Figure 10.
The details of generating the confusion network and the evaluation of the recognition accuracy are described in .
The confusion network is then sent to the database and used as predictions for further writing.
The database is distributed to all the digital writing UI managers in the current implementation.
Given the result of speech recognition and handwriting recognition, the system generates predictions by combining these two.
The system first searches the speech recognition results using the handwriting recognition result as a query.
Then the result of the search is used for the predictive input suggestion.
In the following we describe how to select specific number of predictions by gradually expanding the query.
The system first takes the last character  as a query, and searches for the corresponding character in the confusion networks in the database.
If the system returns many matches, the system extends the search query by adding the second last character.
That is to say, the system searches for the same two-character sequence in the confusion network, which naturally reduces the number of matching results.
The system repeats this procedure until the search returns no result.
In the example in Figure 12, the system tries following queries in this order: "to", "want to", "we want to".
Finally, the system returns the result that matches the longest query.
It is possible that the search fails at the beginning, i.e.
In this case the system tries the next best candidate for the last character.
If it returns many results, the system extends the query backwards.
If it returns no result, it tries the third best candidate and so on.
In the example in Figure 12, the system tries "tu" and then "want tu" when "to" returns no result.
The search results obtained by the above process are sorted in order of likelihood, considering estimated D values.
Finally some of the best results are presented to the user .
This simple algorithm works relatively well in our experience, but there is clearly room for improvement.
Our future work will be to investigate various approaches for the search.
Our system allows the user to write freely on the blank canvas, i.e.
Therefore, the system first segments the strokes before recognizing them.
Figure 12 illustrates an example of a segmentation and recognition results.
We segment the strokes into characters and use it as a unit for handwriting recognition.
This is because Japanese characters consists of many strokes and can represent a semantic unit.
It would be better to use a word as a unit of segmentation for European languages.
The result of handwriting recognition is sent to the next step as a sequence of n-best lists.
We currently use a recognition engine of the Microsoft Tablet PC Platform SDK  and do not consider possible ambiguities in segmentation.
We performed a preliminary study in order to evaluate the speech-pen system and to obtain the test-users' feedback for further improvements.
Eight test-users  participated in the study as volunteers.
Each test-user played the role of either an instructor or a student once.
We chose "How to cook octopus dumplings " as the topic of the simulated lectures.
Our speech recognition engine was not customized for this specific topic.
We used an acoustic model and vocabulary that was built from canonical speech of newscasters and not designed for informal conversations.
We decided not to optimize the system for this specific test in order to show that our system is still useful with error-prone recognition.
Each session took approximately 10 minutes.
Figure 14 shows the support ratios of all test-users labeled A, B, C, D, E, F, G and H. Despite short training, the testusers benefited from the system's support to some extent .
We had expected that the support ratios for instructors would be lower than those of students because it might be difficult to write and speak at the same time.
However we did not observe such a significant tendency from the result of this small study.
In general, novelty effects might bias the test-users' behaviors toward using the predictions aggressively.
We need further detailed investigations for obtaining data in more natural setting.
The support ratio becomes 0 when all the strokes are written by hand and becomes 1 when all the strokes are generated by the system.
It is not our goal to obtain a perfect support ratiothe user basically writes manually and only occasionally uses the predictions.
Note that strokes other than text, such as bullets, marks, and drawings5 always drop the support ratio.
This definition is of course a rough approximation to measure the degree of the system's support as a first step.
It is more accurate to consider the cognitive load of the user, which is relatively difficult to measure in a natural environment.
Machine-generated text does not actually consist of strokes.
We counted the number of strokes necessary to replace them with manual writing.
5 Microsoft Tablet PC Platform SDK has a function to recognize whether what the user writes is text or drawings.
It can be useful for deciding whether to show predictions.
We interviewed the test-users after the test.
We first asked for the general impressions of the system.
All eight testusers answered that they had positive impressions of the system.
They especially found it attractive that the user can use the system only when he wants to, without being forced to do so.
We then asked them to compare push-to-talk recognition  and background recognition.
Six test-users preferred background recognition saying that explicit pushing is tedious, while two preferred the push-to-talk interface saying that recognizing all speech is wasteful because it contains stuttering and irrelevant remarks.
We finally asked them to give suggestions to improve the system and obtained comments such as the following:  The location to show prediction results needs to be improved.
This paper introduced predictive handwriting as a mean to facilitate the manual writing process on electronic boards.
We first showed that predictive writing can be effective at least under certain conditions such as writing long, complex Japanese text.
We then introduced a predictive-handwriting system called speech-pen which helps users to write by hand during presentations and lectures using speech recognition and handwriting recognition.
The system also allows a sharing of information for predictive handwriting among the instructor and the audience in the form of an ambient context.
A preliminary study showed the effectiveness of the system and we obtained the users' comments for further improvements about the UI design.
We would like to continue to investigate various issues such as: where to place the predictions, how many candidates to show, and how long the predictions should persist on the screen with/without the user's interaction.
It is also necessary to perform longitudinal studies in more realistic situations that require complex planning and composition with a more diverse age range of users.
Investigating the possibilities of predictive handwriting for other languages and applying the system to them will also be a promising research direction.
Comparing support ratios of Japanese/English contents, and multimodal/unimodal situations will reveal more about the nature of predictive handwriting.
