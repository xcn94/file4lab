Advances in electronics have brought the promise of wearable computers to near reality.
Previous research has investigated where computers can be located on the human body - critical for successful development and acceptance.
However, for a location to be truly useful, it needs to not only be accessible for interaction, socially acceptable, comfortable and sufficiently stable for electronics, but also effective at conveying information.
In this paper, we describe a study that evaluated reaction time performance to visual stimuli at seven different body locations.
Results indicate numerous and significant differences in the reaction time performance characteristics of these locations.
ACM Classification: H5.2 : User Interfaces.
Keywords: Wearable computing, smart clothes, reaction time, visual alerts, ambient information, design research.
Electronics continue to reduce in size and cost, offering tremendous potential to bring the power of computation to a wider audience and to more aspects of our lives.
Computers that once used to fill a room are now mobile, and soon might even be incorporated into our clothing.
This notion of wearable computing promises a highly integrated and personal information and communication infrastructure that travels with us .
However, unlike traditional computing systems, wearable devices do not require explicit periods of user attention - a user does not sit down in front of wearable devices like they would with a traditional computer.
Instead, the strength of wearable computing lies in its lightweight and spontaneous interaction.
A promising use for this class of technology is lightweight ambient information displays.
There are many types of information that we can now bring with us nearly anywhere.
Finding ways to usefully and reliably deliver this information, while still balancing the costs of attention demand and distraction, will be important to consider if the technology is to be successful.
The study described in this paper seeks to provide some basic information to guide this balance.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Copyright is held by the author/owner.
Wearable computers will need to notify users about, for example, new emails, upcoming meetings, changes in weather forecast, stock market fluctuations, excessive caloric intake, and other aspects of our lives they will be able to monitor directly or have access to.
One way to draw a wearer's attention is through vibrotactile stimulation, commonly employed in mobile devices to alert users .
Also popular, although not tied to any particular body location, are audio alerts.
Visual displays offer an alternative notification method .
However, there has been little research into their optimal body placement, despite being an unobtrusive, lightweight, and low-powered information delivery mechanism.
Furthermore, visual stimuli have the added benefit of being able to work alone or in concert with conventional methods, including auditory and vibrotactile alerts .
We believe the results presented in this paper can be used to inform the design of future wearable displays with or without particular urgency or time constraints.
An incoming phone call, for example, must be answered within perhaps 15 seconds.
Locating the corresponding alert at a place on the body that is rarely viewed will result in many missed calls.
On the other hand, consider the example of a notification indicating that six months has elapsed since your last dentist appointment.
This does not require a response with 15 seconds, or even potentially days.
Locating this alert in a highly visually salient location is clearly inappropriate.
This would unnecessarily interrupt the user from their present task or social engagement for an item of little immediate importance.
Thus, this research allows developers and researchers to best align their application with areas of the body that have the necessary attention demand and reaction time characteristics.
Applications that apply this information may be less disruptive and reduce overall information burden.
To support the study detailed below, we developed a distributed array of small sensor-displays .
These devices were attached to different parts of a participant's body, where they would initiate visual stimuli and capture reaction times.
The final design, employing a PIC microprocessor, measures 2.3x3cm and weighs 11g, including battery.
Being both small and wireless meant that participants' mobility was not restricted.
A velcro strap was used for the wrist if the participant did not wear long sleeves.
A red LED flashing at approximately 10Hz acted as the visual stimulus.
The LED was frosted to provide better, omni-directional light dispersion.
Users interacted with the device using a single, large, surface mounted button.
The device's logic is as follows:  Suspend for a random period between 2 and 16 minutes.
Begin tracking the elapsed time using an internal counter.
Continue flashing the LED until the user responds by pressing the button.
To prevent overwhelming the user with an incessant need to react, we had to strike an important balance between the number of devices and the frequency of their activation.
We ultimately selected seven locations per participant based on suggestions by Gemperle et al  .
Their investigations looked at issues such as device weight and size.
Although electronics have miniaturized considerably over the past decade, ameliorating these factors, many classes of devices are bounded by the size of their screens and input controls, which cannot be readily reduced .
However, devices that do not require traditional screens or input controls can be very small, especially if supported computationally via a PAN .
Gemperle's recommendations, however, also took into account accessibility  and aesthetics - two vital factors to consider regardless of device footprint.
Additionally, the locations we use encompass the most common placements for devices today - upper arm, wrist, and waist - as well as all of the locations used in .
To be comprehensive, we included four additional locations that represented the most significant, although unconventional, remaining areas - feet, legs, and torso.
Twenty-five participants  with a mean age of 23.3  were recruited using a recruiting website and posted flyers.
Each received $15 for their involvement.
To keep device  count down, we deployed sensors to only one side of the participant's body.
We compensated for experimental effects associated with differences in laterality   by balancing the side of the body we deployed our sensors to within right- and left-handedness groups .
Participants were given a brief explanation of how the devices work, and that they were to press the device button as quickly as possible once they noticed the light blinking.
Then, with the help of the participant, the devices were affixed in the specified locations.
Participants completed an exit survey at the end of the study period.
During the study period, participants were told to go about their normal routine.
We purposely avoided engaging participants in an artificial task, as this would have created a fictitious attentional scenario, both cognitively and visually .
We simultaneously hoped this flexibility would capture a variety of use contexts.
However, participants spent almost all  of the study period seated and working .
A positive byproduct of this behavior was that we were able to observe all but three participants during the study period.
This provided a considerably more contextualized and intimate source of information than the raw reaction time data, especially about how different locations perform in various postures and settings .
Finally, although this seated and working context is narrow, it is perhaps the most prevalent backdrop for device interaction and information exchange in the digital age, and is thus of significant interest and importance.
We found that the reaction times in our study roughly conform to an exponential distribution.
To derive our statistical measures, we took the log of reaction times, which transformed the data into a more normal distribution, allowing us to use paired, two-sided t-tests.
A closer inspection of the data revealed a slight bimodal distribution.
This appears to be caused not by two types of participants , but rather an effect within participants.
We suggest that this is a result of two distinct ways users react to visual alerts.
In particular, we believe the first peak  is caused by people noticing the device switch state , prompting them to react immediately.
However, if the wearer does not catch this initial change, their reaction time is roughly modeled by a log-normal distribution, with means between 32 and 128 seconds, depending on the location.
There was no significant difference  between leftand right-handed groups; we combine the results for brevity.
There was also no significant difference  in dominant vs. non-dominant body side placement, except in the waist location .
However, the data suggests dominant side placement outperforms non-dominant side placement by almost 40% on average.
We hope to investigate this effect more rigorously in future work.
Figure 2 displays the average reaction time for each body location across participants.
Table 1 contains p-values from a Bonferroni-corrected, all-pairs, two-sided t-test.
Figure 3 illustrates how these reaction times are distributed.
The cumulative reaction time charts can assist wearable computing designers in selecting body locations one can look up, for example, that 90% of reaction times for the arm location occurred within 64 seconds.
There were distinct differences in how participants reacted to visual stimuli in each of the seven locations.
In addition to strong variations in reaction time captured by our distributed sensor system , there were many interesting observations made regarding when and how different locations were particularly effective and ineffective.
Participants reacted not only fastest to the visual stimuli on the wrist, but also more consistently .
It outperformed all other locations , except the arm.
This result aligns well with field observations.
The wrist appeared to be most visually accessible during hand motor activities, such as writing, reading, typing, and conversational gesturing.
Because most of our subjects were engaged in one of the latter activities, wrists performed exceptionally well.
However, this performance might drop significantly in other contexts such as walking.
Nonetheless, wrists offer the unique opportunity to deliver alerts and information most saliently during activities when we are using our hands, which could be leveraged to great effect in activities like typing , building , and cooking.
With performance almost matching that of the wrist, it is curious why so few devices take advantage of this salient area for visual alerting.
It significantly outperformed the thigh, shoulder, waist and shoe locations .
Located on the exterior of the upper arm , our devices appeared to be just within most participants' peripheral vision, allowing for almost immediate reaction when the visual stimulus began.
Participants were 41% faster at reacting to stimuli when the device was placed on their nondominant side .
Unlike the wrist and arm, the brooch location seems to be located outside the wearer's peripheral vision.
From field observations, we learned that when participants were engaged in activities in which their head was level with the horizon , brooch-located visual stimuli often went unnoticed.
It was only when users tilted their heads down did the location seem to enter the visual field .
The brooch area, like the arm, benefited from being placed on the non-dominant body side, a performance gain of 36%.
The respectable performance of this location  is likely due to the high percentage of participants working on the laptops or reading books, both of which tend to orient the head downwards.
This characteristic could be useful, however, by allowing information to be pushed to the user when engaged in "heads down" activities, such as eating, reading, and drawing.
Meanwhile, "heads up" activities, such as socializing, walking, and driving, could benefit from reduced interruption.
Interestingly, this proximity did not guarantee the stimulus would fall within the wearer's field of view.
Observations suggest that natural panning or tilting of the head caused the location to periodically drift into the peripheral vision, allowing the device to catch the wearer's attention.
Changes in posture, which shift how clothes sit on the body, also contributed to this effect.
Lastly, some participants noted that the stimulus was occasionally visible as a reflection in their glasses.
Data collected from the reaction time sensors show the shoulder only significantly outperformed a single location, the shoe .
Although not statistically significant, it fares slightly better than thigh and the waist.
This is interesting as the waist is a popular location to situate mobile electronics, where they can be attached to the belt.
However backpack straps  could serve a similar function and offer improved reaction time.
The performance of this location suffered heavily due to occlusion by tables - it was only readily noticed when participants were leaning back.
Additionally, similar to the brooch position, the thigh is typically not visible when standing - the wearer must look down to see it.
However, it is possible that the thigh could be especially useful in contexts like TV viewing or seated socialization.
It is in these settings that users tend to be especially reclined, providing line of sight to the thigh and easy access to it.
The thigh, like other locations, benefited from being placed on the non-dominant side of the body, with average reaction times 45% faster - a difference of more than 28 seconds.
From our observations, this seemed entirely due to the fact that feet are either tucked under tables, hidden by knees when seated, or obscured by books or laptops .
The shoe also demonstrated the largest dominant vs. non-dominant placement effects, with means of 157 and 78 seconds respectively.
However, the poor visual accessibility of the shoe might be considered its strongest attribute.
Feet tend to be hidden when people are working , a context where interruption may be particularly expensive.
Information presented via the shoe is unlikely to catch the wearer's attention, reducing disruption.
It seems the shoe becomes most accessible when in an upright location, and in particular, when walking or running, when the feet tend to kick out in front, and may enter the peripheral vision.
Thus, shoes might be the ideal platform from which to deliver information when users are mobile between tasks .
Because of the significant variation in reaction time, information would likely be limited to types that are timeinsensitive .
This location was particularly sensitive to how participants were seated.
When leaning back, the waist was usually visually accessible.
However, participants leaning forward or sitting up against a work surface tended not to notice alerts.
In some cases, the area was completely occluded by the body  - one participant forgot about the position entirely.
Additionally, this position does not fall within the wearer's field of view when facing forward.
This was the only location that had a significant difference between dominant and non-dominant body side placement,  with reaction times of 83.8 and 48.1 seconds respectively.
The poor performance of this location, second only to the shoe, stands in contrast to its popularity for device placement .
Although the waist  is convenient for attaching mobile devices today, future smaller devices are likely to render this location less useful, especially considering its visual reaction time performance deficit.
Furthermore, this result clearly demonstrates why we are so reliant on auditory and vibrotactile alerts for devices worn in this area.
We have shown that there are significant differences in how visual alerts distributed on the human body capture our attention.
This reaction time performance is not only influenced by the innate properties of each location, such as physical distance or visual accessibility, but also outside factors, such as occlusion by furniture.
We believe these data and observations can inform the design of future onbody visual displays.
