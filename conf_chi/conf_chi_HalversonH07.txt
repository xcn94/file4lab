Visual search is an important part of human-computer interaction.
It is critical that we build theory about how people visually search displays in order to better support the users' visual capabilities and limitations in everyday tasks.
One way of building such theory is through computational cognitive modeling.
The ultimate promise for cognitive modeling in HCI it to provide the science base needed for predictive interface analysis tools.
This paper discusses computational cognitive modeling of the perceptual, strategic, and oculomotor processes people used in a visual search task.
This work refines and rounds out previously reported cognitive modeling and eye tracking analysis.
A revised "minimal model" of visual search is presented that explains a variety of eye movement data better than the original model.
The revised model uses a parsimonious strategy that is not tied to a particular visual structure or feature beyond the location of objects.
Three characteristics of the minimal strategy are discussed in detail.
One way to better understand the visual search processes people use, and why they use them, is with computational cognitive modeling.
Theory developed through cognitive modeling, as is done in this research, is essential for the development of automated interface analysis tools.
Interface designers can use such tools to evaluate visual layouts early in the design cycle before user testing.
Two tools that could benefit from a straightforward, minimal model of visual search are CogTool  and G2A .
There are many cognitive models of visual search that may one day converge to form a solid basis for the theory of visual search in HCI .
While these models are very useful, many such models are designed to explain the effects of particular visual structures or salient features.
The research reported here is motivated by the need to find a minimal model for goal-directed visual search that is not tied to a particular visual structure or feature saliency.
A minimal model of visual search is presented that explains a variety of eye movement better than previous research of the same task.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
This work builds on previous modeling and eye movement analysis of menu search.
Hornof  studied the visual search of layouts with and without a visual hierarchy and built computational cognitive models of the task.
Hornof and Halverson  replicated the study to evaluate the eye movement strategies predicted by the models and found that while the models predicted the search time and a fair amount of the visual search behavior, some critical aspects of the visual search behavior  were not well predicted.
A goal of the current research is to improve the original models by accounting for more eye movement data found in the follow-up study.
Figure 1 shows the task relevant to the current research .
Sixteen participants searched four different screen layouts for a precued target.
Each layout contained one, two, four, or six groups.
Each group contained five objects.
The groups always appeared at the same locations on the screen.
One-group layouts used group A. Two-group layouts used groups A and B. Fourgroup layouts used groups A through D.
The minimal model was derived iteratively by making gradual improvements to the model based on eye movement data.
At each step in the model's development, a substrategy was added or a perceptual parameter was changed to increase the model's fidelity.
In the original models, the simulated eyes moved down the first column of text, then down the second column, and then down the third.
Furthermore, the eyes jumped over a carefully controlled number of items with each eye movement.
The model accounted for the reaction time and a fair number of eye movement measures, considering that the model was built without eye movement data to guide its development.
However, the model's strategy is somewhat tuned to aspects of this one visual task and layout.
The model directly controls the direction and amplitude of eye movements.
This direct control, while providing a good fit to the reaction time data, does an unsatisfactory job of explaining people's visual scanpaths.
The original model did a better job of predicting the frequency and number of fixations, but there is room for substantial improvement.
A goal of this research is to improve the accuracy with which the model explains people's visual search strategies, while at the same time maintaining a minimal model that does not directly control the scanpaths based on the visual structure of the layout or visual properties of the layout items.
A potential criticism of the task modeled here is that it lacks ecological validity and any change to the task may invalidate the resulting model.
We acknowledge this concern but point out that the model captures fundamental human perceptual-motor processes, capabilities and constraints that will be common across a wide range of ecologically valid, real-world tasks such as air-traffic control.
Common processes and constraints include error in object recognition, biases towards shorter saccades, and fixation duration control.
The resulting model is useful for predicting visual search in HCI.
The model contains a visual search strategy that is not tied to a particular visual structure or saliency of a feature beyond the location of the visual objects.
A text feature is used to determine if the target is found, but does not guide search.
The development of the model and the integration of the three key characteristics are discussed next.
This research proposes three characteristics of a minimal model of visual search:  Eye movements tend to go to nearby objects,  fixated objects are not always identified, and  eye movements start after the fixated objects are identified.
These characteristics are motivated by previous research and eye movement data, and are introduced to the model here in a step by step manner.
We propose that any applied model of visual search should include at least these three characteristics, and furthermore that much visual search behavior can be explained by the integration and interaction of these three characteristics.
The cognitive models described in this study were built using the EPIC  cognitive architecture .
EPIC captures human perceptual, cognitive, and motor processing constraints in a computational framework that is used to build cognitive models.
EPIC simulates ocular-motor processing, including the ballistic eye movements known as saccades and the fixations during which the eyes are stationary and information is perceived.
Visual properties of objects are available at varying eccentricities and timing.
The basic job of the human visual search process is to decide which objects to fixate.
Though a completely random search strategy is very useful for predicting the mean layout search time, people do not search completely randomly.
Instead, people enjoy the many benefits of moving to objects that are relatively nearby rather than across the layout.
Saccade destinations tend to be based on proximity to the center of fixation .
In the current research, rather than searching randomly or following a prescribed search order , a strategy was used that selects saccade destinations with the least eccentricity.
To account for variability in the human saccade distances, noise is added to the model's process of selecting the next saccade destination as follows:  After each saccade, the eccentricity property  of all objects is updated based on the new eye position.
This scaling factor is individually sampled for each object.
The standard deviation of the fluctuation factor was determined by varying the fluctuation factor to find the best fit of the mean saccade distance.
This suggests that the participants may occasionally fail to recognize the target, even though they eventually complete the trial correctly.
Previous modeling research  suggests that people do occasionally fail to recognize fixated text.
The minimal model was modified to include a text recoding failure rate.
The parameter represents the probability that the text property of an object will not be encoded.
The text recoding failure rate parameter was used in the current work for two reasons.
First, to explore ways to account for the observation that participants missed the target occasionally.
Second, if the current modeling predicts observed eye movement data with a failure rate similar to that used in the previous modeling, this would not only support the use of the parameter here but also suggest a default value for the parameter in future modeling.
The text recoding failure rate was initially set to 10%, the value used in .
This failure rate was changed by 1% increments until the model predicted the mean number of fixations per trial.
A value of 9% provided the best fit for the number of fixations per trial.
As shown in Figure 4, the current model predicts the number of fixations per trial very well, with an AAE of 4.2%.
This is an improvement over the original model  and the current model with no text recoding failure rate.
The decreased error and the similarity between the bestfitting text recoding failure rate found here and the rate found in past research provides support for the use of the text recoding failure rate parameter.
Again, we are maintaining a minimal model in that this improvement to the model does not require layout-specific information.
Future research will need to address the possibility of encoding failure rates for non-text stimuli.
This strategy also does a good job of predicting the observed scanpaths.
Figure 3 shows the three most frequently observed scanpaths, and how the current model predicts the observed scanpath frequencies better than does the original model.
This "nearby with noise" strategy used in the minimal model has a couple of benefits for predicting visual search compared to models tied to particular visual structures or saliency of visual features.
First, only the location of the layout objects if required.
This is beneficial if other properties in the layout are unknown or difficult to extract.
Second, this search strategy can be used when the visual saliency alone cannot predict visual search, as is the case with goal-directed search .
Unlike the original model , this minimal model does not require a predefined notion of how the eyes will move through the layout to predict the observed scanpaths.
One goal of the current research was to produce a model that accounts for multiple eye movement measures.
Although a model that moves the eyes to nearby items accounts for the observed scanpaths, improvements were required that accounted for the observed number of fixations per trial.
Fixations per trial observed , predicted by the original model , predicted by the current model with 0% encoding failure , and predicted by the current model with 9% encoding failure .
Various strategies have been proposed for how long people fixate items .
The two basic competing theories are  preprogramming, in which fixation durations are directly controlled by the search strategy, and  process-monitoring, in which fixation durations are determined by the time required to perceive the fixated stimuli.
The minimal model utilizes a processmonitoring strategy, which requires fewer production rules and parameters than required by a preprogramming strategy.
In the model, saccades are initiated after objects in the fovea are identified.
Once the simulated eyes reach their destination, the strategy waits until the text property of the fixated objects is available.
While waiting, the strategy starts the process of deciding where the eyes will go next.
As shown in Figure 5, the current model predicts the fixation durations very well, with an AAE of 4.6%.
This is an improvement over the original model  that had an AAE of 26.5%.
The use of a process-monitoring model for determining fixation durations predicts the observed data very well.
These principles are:  Eye movements tend to go to nearby objects,  fixated objects are not always identified, and  eye movements start after the fixated objects are identified.
This minimal model does a better job of accounting for the observed visual search behavior than a previous model of the same task that was not informed by eye movement data.
The minimal visual search model discussed here will be useful to further research in predicting and understanding user behavior in HCI.
Such a model could be used in future cognitive modeling as a base on which to build more robust models of visual search.
Further, predictive tools like CogTool  could incorporate a similar model for predicting users' visual search behavior.
Theory developed through cognitive modeling such as the work presented here is essential for the development of predictive, automated interface analysis tools that allow designers to evaluate their visual layouts early in the design cycle before user testing is feasible.
A minimal model of visual search accounts for a variety of eye movement data, from fixation duration to the most common scanpaths.
