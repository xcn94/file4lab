Categorization of online videos is often treated as a tag suggestion task; tags can be generated by individuals or by machine classification.
In this paper, we suggest categorization can be determined socially, based on people's interactions around media content without recourse to metadata that are intrinsic to the media object itself.
This work bridges the gap between the human perception of genre and automatic categorization of genre in classifying online videos.
We present findings from two internet surveys and from follow-up interviews where we address how people determine genre classification for videos and how social framing of video content can alter the perception and categorization of that content.
From these findings, we train a Naive Bayes classifier to predict genre categories.
The trained classifier achieved 82% accuracy using only social action data, without the use of content or media-specific metadata.
We conclude with implications on how we categorize and organize media online as well as what our findings mean for designing and building future tools and interaction experiences.
Although clearly a joke in itself, Homer's perception of Police Academy illustrates how the social consumption of media can alter the way content is perceived and categorized.
Media content that we believe to fit a particular genre is both constituted by, and constitutive of, the changing social contexts in which that content is produced, shared and consumed ; genres are socially constructed.
This article presents a study of genre categorization and demonstrates how the analysis of social consumption and sharing behaviors can reveal the nature and characteristics of online video content.
Genre engenders recognizable patterns of social action and we can infer the genre of a video by looking at what people do with that content.
Genre categories have both epistemological and functional dimensions; they are first and foremost ways of organizing and defining content, but also ways of organizing social actions .
For instance, the category Comedy helps us identify and expect a particular kind of video content, such as a humorous narrative and funny characters.
At the same time, Comedy also presents content that specifies a type of individual behavior and social interaction that would be different, or even inappropriate, if one were to watch a Documentary.
Besides the dual nature of genres, they can also be understood as "socially constituted systems"  that shape the context and social activity surrounding them.
Genres are social constructs which specify particular interaction patterns and social activity, which in turn, determine how we define and interpret those genre categories .
Current automated tag-based classification techniques ignore these aspects of social construction.
In this paper, we focus on the categorization of videos that are socially shared on the Internet.
The online social sharing of media can take place asynchronously, through social media websites like YouTube, or synchronously, with services like http:// justin.tv or Yahoo!
In addition to simply sharing videos, people interact around the video by leaving comments, rating content or, in the case of synchronous sharing, by chatting.
These interactions leave traces which are either explicit, publicly viewable annotations in the form of comments and ratings, or are implict, logged usage data; both can be mined to reveal patterns of interaction and engagement with the video content.
More importantly, the data resulting from social actions around the video content can also provide insights in to the nature, characteristics and genre classification of the video content itself.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In this paper, we address people's understanding of genre categories from a social construction perspective.
For the purposes of our analysis we focus on Comedy as a genre.
We develop a model of video genre classification using data captured from the social consumption and sharing of online videos.
The predictive model is built from 5 features sampled from a dataset of 9,364 videos taken from 2,188 sharing sessions.
The model classifies videos by genre at an accuracy greater than 82%.
In the following sections, we review the online video categorization task from a human and computational perspective.
To gain further insight into the human perspective of comedy, we present findings from two online surveys and eleven 1-hour, semi-structured interviews with a sample of the survey participants.
The findings from the qualitative exploration are used to inform the training features used for automated genre prediction.
Finally, we conclude by suggesting what this model means for current recommendation systems as well as for the interaction design of future systems.
For example, overlaid commentary can transform a serious event into something comedic.
The cult TV series Mystery Science Theater 3000 ran from 1988 to 1999 based entirely on this premise, the main protagonists providing a running commentary over a series of science fiction B-movies and transforming what had initially been created as "serious" content into comedy.
Given social action can thus transform the reception of a media event, we can conclude that without social context, categorization is a difficult task to do accurately, and that representing content by classification must account for contextual and social uses.
Bowker and Star explain this difficulty, "people  routinely conflate formal and informal, prototypical and Aristotelian aspects of classification.
There is no such thing as an unambiguous uniform classification system."
Online videos are no exception; they can be categorized on several levels such as, the semantic or narrative content of the video, the inherent properties of the video itself like its format, by stylistic features like mood, and so on .
Methods used to classify the genre of web videos have traditionally focused on the video's content or metadata.
Classification of online videos is computationally treated as tag suggestion from a pre-defined set .
However, we argue the task of genre classification can be made more accurate via an examination of the social interaction surrounding online video--in effect, how the video is discussed and interpreted.
How a video is consumed, interacted with, and commented on is indicative of the nature of its content.
Online tools for content viewing support many forms of social interaction  such as link sharing/forwarding, ratings, "liking" as a public statement of endorsement, explicit recommendation, commenting, synchronous chatting while watching, and so on.
In synchronous video sharing contexts, people also start/stop videos to review key moments and discuss them.
These activities leave digital traces in the form of server logs and large-scale databases .
These digital traces are used to assess the popularity of content, and to create promotional mechanisms  to further raise the profile of the content for recommendation.
Activity data are also mined, aggregated and analyzed to develop models of user consumption patterns.
As well as understanding people's consumption practices around content, these activity data can be used to categorize the content itself.
Crane and Sornette  identified "signatures" in the metadata of YouTube videos in order to identify "quality" content.
However, while actions on or around a media object by individuals have been explored to classify content, social actions have not been explored.
In our work we analyze patterns of activity between people over or around that media object as a means of classifying content.
Historically, in computing and the social media, categorization is a collaborative filtering task for tag suggestion.
Computationally, finding a category for a video can be the equivalent of suggesting the single best tag for that video.
Automatic tags and tag suggestions are often content-based either from the video's meta-data or the visual media's content .
Recently, Zhang, Zhang, and Tang  proposed a hybrid method based on video meta-data and social-graph distance.
Similar web-graph systems have been proposed for image classification .
These richer tag sets have been shown to improve the performance of web video categorization .
Similar approaches use tag metadata co-occurrence using machine learning  or term-ranking techniques .
From the start of tagging's popularity, social navigation, or recommendation, has been an ongoing research investigation .
Likely the most publicized example is the NetFlix Prize contest.
From a dataset of 100 million movie ratings, the open contest challenged people to find a 10% improvement over the current state of the art.
The solutions used a variety of information types, from ratings data to the movie rental's ZIP-code location to the renter's age; no content was used.
In general, the actual video, image, or audio signal/content is believed to be unreliable for recommendation.
Instead, ML researchers look for explicit distinct actions, like 5-star rating scores, to find similarity  and categorization.
For example, Yew and Shamma  trained a Naive Bayes classifier to predict YouTube categories to an accuracy of 75.5% using only three features of YouTube Metadata: video duration, view count, and 5-star rating.
They attribute the rating's dominance to "explicit social action" being a higher fidelity signal than implicitly aggregated usage data.
From our findings in this study, we model "implicit social sharing behaviors" as the construction of genre.
Zync4  which lets people host viewing parties with their broadcast TV content.
We acquired a 24-hour sample of the Zync event log for Christmas Day 2009.
Zync allows two people in an instantmessage session to watch a video together; the video's playback stays in sync across both participants and both participants share playback control.
This provides a set of watched videos from YouTube as well as conversational activity rich enough to train a classifier.
The dataset records several features: anonymous user id hashes, session start/stop events, the session duration, the number of play commands, the number of pause commands, the number of scrubs , and the number of chat lines typed as a character and word count.
For the chat lines, the dataset contained no actual text content, only the aggregate count of characters and words.
The only text that is collected is video URLs and emoticons.
Each item/activity collected is a line in the dataset which records the time of the event and the playback time on the video .
In total, the dataset contained 2,188 dyadic sessions that shared a sum of 9,364 YouTube videos.
Not all sessions appeared complete and were missing the start/stop events, was missing a participant, or other critical information; these sessions were discarded.
Some videos were no longer available on YouTube due to copyright infringement or owner deletion; these videos and their respective sessions were also discarded.
The final test sample consisted of 1,732 videos with valid metadata.
The data collected from YouTube consisted of a video identifier, the video's title, its published date, its description, the genre category the uploader used, the tags the video was labelled with, the video's duration and the 5star rating score it attained.
Of these data, we only use the video's genre/category, of which YouTube provides a constrained list of 18 genre categories.
The YouTube video's category is specified by the person who uploaded the video, which is required at the time of upload.
For this article, it functions as the predictive category for our surveys and classifier and allows us to investigate if there's a match between the uploaded category and the social actions.
Within our sample of 1,732 videos and associated sessions, not all categories were well represented.
However, the category distribution was unevenly distributed.
To ensure we had enough data to accurately perform this study, we retained only the Top 5 video categories for our investigation; this discarded 13 categories, 11 of which contained fewer than 20 videos of that category watched.
Is there a correlation between genre and watching behavior?
If so, could watching behavior be more effective than 5-star ratings when used for categorization?
This paper aims at identifying this connection as guided by two primary research questions:  How do people socially consume, perceive, and categorize videos ?
This work employs a mixed-method approach to explore the issues of categorization of shared online videos, answering RQ1 qualitatively, which in turn should advise a quantitative model to answer RQ2.
This approach consists of three phases combining multiple data collection and analysis methods.
The first phase conducts two surveys, a pilot survey with 43 respondents administered internally and a larger survey conducted via Mechanical Turk with 69 valid responses, in order to assess the difficulties associated with categorizing online videos by humans.
In phase two, the surveys are followed up with 11 interviews from the phase one survey respondents.
The interviews involved questions about the respondents' mental processes when categorizing online videos, in particular videos that they labeled as "comedy."
Finally, we use the findings of the surveys and the interviews to develop a classifier for categorizing video genres using only metadata from conversational and interaction activity.
The main aim of adopting a machine learning classifier is to devise a method of genre classification based on how we watch videos, and not what is in them.
The results from all three phases will then be used to explicate on the issue of why the categorization task, especially for comedy videos, is so difficult.
Each method complements the others and allows for a stronger generalization of genre categorization from various human-computer interaction perspectives .
For this study, we need to investigate how people judge the category of a set of videos and compare that judgement to a classifier.
Since we will be training the classifier on conversation activity, the metadata that arises from crawling a site like YouTube is insufficient.
In both surveys we presented each respondent with a set of 20 videos, selected from our corpus of retrieved videos.
The primary task in both surveys is to view each video and carry out a categorization task.
In the pilot, we assess how individuals categorize shared online videos from websites like YouTube.
The online survey involved presenting each respondent with one video at a time randomly from our video corpus .
The respondent would watch or scan through the video and then make a judgement about the genre category of the video by selecting one of the Top 5 sampled genre categories: Music, Film, Comedy, Entertainment and People and leave an optional comment.
An agreement between the survey respondent's genre selection and the video uploader's original genre designation signified an accurate judgement.
The survey call was posted to 3 internal mailing lists within our organization.
A total of 43 valid and complete responses were gathered from this survey.
Respondents were able to correctly categorize 60% of the videos with the same label as the original uploader's categorization.
Looking at Comedy specifically, the success rate was 50%.
The comments left by the respondents on the incorrectly labeled comedy videos surfaced a lack of distinction between what is comedy and what the respondent believed was funny.
One respondent illustrated this confusion in their comment as, "Comedy is the category.
The respondents knew what was funny to them, but were less readily able to classify videos based on whether it belongs to the Comedy category.
The pilot survey's results prompted us to ask, why are people so bad at classifying Comedy genre videos from YouTube?
We designed a second survey to address responses to what is funny and what is comedy.
First, we performed a content analysis of the videos which surfaced the following mediatypes: cartoons, stand-up comedy, music videos, foreign film clips, and video blogs.
In the second survey, each survey respondent was also presented a set of 20 videos at random.
However, this time the survey presented each respondent with the same set of 20 videos.
The videos were selected as a distribution of 10 Comedy videos and 10 Non-Comedy; the latter encompassed videos from the 4 other categories.
Additionally, the second survey asked a different set of questions .
This second survey asked "Comedy or Not" and "Funny or Not," which differs from our first survey which asked for an explict category label.
Discussions with the pilot survey respondents indicated that category selection is dependent on the number of choices and more choices result in wider distribution of selections: i.e., consensus goes down.
The second survey was administered externally using the Crowdflower crowdsourcing service6 to recruit respondents from Amazon's Mechanical Turk7 .
6 Crowdflower assists with "crowdsourcing" respondents for tasks from several online "labor on demand" pools, see http:// crowdflower.com/  7 Amazon's Mechanical Turk service is an crowdsourcing marketplace that enables users to recruit large numbers of workers for tasks that computers do not do well.
We followed the suggestions of Kittur et.
Crowdflower's service pulls from a pool of trusted turkers as respondents for a survey.
Our survey gathered 103 responses of which 69 of them were valid and complete responses.
For the questions "Is this funny" and "Is this a comedy?," we test for inter-rater reliability using Krippendorf's alpha  in order to determine if there was a consensus amongst the respondents about which videos were funny and which videos should be classified as a comedy.
Similar to our finding in the pilot survey, respondents showed little agreement about Comedy videos or if they were funny .
Higher reliability scores for the noncomedy videos highlight more agreement, something is obviously funny or not funny .
The inter-rater reliability test provides only a general insight about how the videos were perceived and classified.
For a more detailed understanding, we conducted a text analysis of the keywords and comments solicited for each video.
Table 1 lists the frequency of the keywords "comedy" and "funny" provided by the respondents for each video.
Not all the comedy videos  were tagged as "comedy" by the survey respondents.
In particular, videos 8 and 9 are more frequently tagged as "funny" rather than "comedy".
To better understand why these two videos are tagged more frequently as "funny," we reviewed the content of both these videos.
These two popular, viral videos share the following characteristics: both have generated millions of views, are short, are easy to understand, and are humorous and "cute".
Video 8 is Laughing Baby8 , a video where a swedish baby boy bursts into giggles whenever his father makes the "bing!"
Video 9 is Sneezing Panda9 , it depicts a tiny infant panda, who is sleeping at the feet of its mother, suddenly sneezes about halfway into the video giving his mother a huge fright.
For our respondents, both these videos exemplify videos that are "funny," but don't necessarily fit the definition of a comedy genre video.
Some of the comments provided by the survey respondents about these two videos include: This is very funny video of a baby laughing.
Not sure it should be categorized as a comedy.
It is not comedy because the actions were not specifically done to make us laugh.
So what exactly are the characteristics of a comedy video?
To better understand what our respondents typically view as comedy, we carried out a word association analysis on the tags provided for each video.
Specifically, we transformed all the tag/keywords responses into a term-document matrix in order to find terms that correlate with the keywords "comedy," essentially the term-frequency portion of a tf * idf model .
The results of the word association analysis provide us with some insight into how the respondents characterize videos tagged with the keyword "comedy".
Table 2 shows the strong association between the keyword "comedy" with descriptions of comedic structure or stylistic features such as the presence of audience laughter  or if the video depicts the performance of a stand-up comedy routine.
The same word association analysis for the keyword "funny" was not as meaningful as the analysis of "comedy".
The analysis retrieved a total of 109 terms associated with the keyword "funny".
One reason why comedy videos are hard to classify is because people have very set structural and stylistic norms they look for--such as the presence of a laugh track or a standup comedy routine.
However, a cursory scan of the comedy category on YouTube will reveal that most, if not the majority, of the videos there do not conform to this form of stylistic classification.
In fact, many highly popular and viral videos, such as Laughing baby and Sneezing panda, fall between genre categories and are often difficult to place within existing categorization schemes.
Without a priori knowledge of the video's uploaded intentionality, what are some of the other cues that indicate a video is a comedy?
According to two interview participants: If I ever hear the laughter track, it doesn't even have to be funny, it's intended to be Comedy.
A reason for this is because stylistic features such as audience laughter relate back to the earlier notion of intentionality--background laughter signals to the viewer that the content is meant to be funny and should be categorized as Comedy.
These are social indicators of intended comedy.
Apart from some of these contextual cues, Comedy, as a genre, shares little else in common.
Our interview subjects had particular difficulty in categorizing videos that were labelled as comedy and often disagreed with the classification selected by the original uploader for these videos.
In fact, some of the interview subjects have found that genre categories are sometimes used as prescriptions for how to appropriately interact with the content being presented: They  are uploading things and categorizing it as "Comedy" because they are proposing that there's something funny in it.
Even though the content itself may not be "Comedy".
And so, if you don't watch to the end, you won't pick up the intended funny part.
In the view of some of our interview subjects, one approach towards identifying whether a shared video is a comedy or not, is by studying the social actions and interactions surrounding the video: There's a context that you need to have for something to be identified as comedy.
But if other people are interacting with it in a way that makes me believe that it's funny.
Same thing for the wedding dance.
The above quote echoes the illustration from The Simpsons cited at the start of this paper.
For Participant 5, and for Homer, the interactional context surrounding a video/movie,
We followed-up the surveys with 11 interviews from the survey respondents.
Namely we wanted to find their rationale and mental process for classifying and distinguishing between videos that belong to the comedy genre and videos that are funny.
The interview questions and process involved having the interviewees explain their responses in the survey by showing them specific videos from their survey.
One of the main goals of the interviews is to inform the selection of metadata features to use in the next stage of our project.
All the interview participants were contacted via email and interviews were conducted face-to-face whenever possible.
The interviews highlighted the fact that the task of classifying online videos is much harder than the classification of movies.
According to Participant 5: Films in general tend to fit categories more narrowly.
I think on YouTube the range of possibilities for the content of the videos is much less constrained.
Cause it might literally be a segment from a film or it could something shot on a cheap digital camera.
Especially so with the Comedy genre, where the format of the content aids with the understanding of whether something was meant to be funny or not.
For instance, it would be much easier to categorize a video as a comedy if the content were presented as a short film.
If that same content were to be presented as in the format of a home movie video-clip, it would not be so easy to conclude that the video was a comedy because the home movie format is more ambiguous with regards to intentionality.
The issue of intentionality is paramount for determining whether a video is to be classified as comedy: Comedy videos are harder to classify.
Was the intention of the video to make me laugh?
An excerpt from The Office or standup would be "Comedy".
It gets hazier when the intention of the clip isn't to make you laugh--but the content is funny.
This issue relates to why videos like Laughing baby and Sneezing panda are hard to catego-
Genres are not just organizational tools, but also underpin social behaviors that surround the content of the shared videos.
Towards our automated classification method, we will need to look for metadata that similarly highlights the behaviors and activities of the users surrounding shared videos to utilize as features for the classifier.
In our case, the metadata from the Zync tool, detailed in the next section, consists of logs of video sharing sessions where users can chat, and at the same time, control the playback of a video.
The interaction between the pair of users and their activity controlling the video constitutes as the social and interactional context that can be used to indicate the nature and characteristics of the video content.
It is important to note that not all social actions are indicative of a video's genre.
Social media websites afford a variety of ways to interact, communicate and participate around the sharing of media content.
Indiscriminate use of metadata surrounding socially shared videos for automated classification may not yield meaningful results.
So we need to identify metadata that is representative of a person's relationship with the content and with others.
As Participant 6 noted, utilizing metadata from individuals who are strongly related to each other, or who resemble each other, in some way will yield meaningful cues about the content of a video.
When consuming a video, I trust other consumers more than I trust the producer.
So I would be interested in what other people who think like me, say about this video.
Probably out of all the users on YouTube, people who look like certain niches have affinities to certain videos.
Therefore, one important use of social metadata is that it can provide contextually and socially appropriate information to determine how the perception of a video's content by a particular group of individuals.
Another indicator of the viewer's relationship to a video can be found in the commentary and surrounding discussion.
Sending a photo of your dog and expecting them to comment on it is one thing.
Sending a 15min video of your dog and expecting them to comment on it.
Because of this, metadata derived from these contextual comments and discussions can provide a more robust signal than 5-star rating data that is typically used in contemporary ML approaches.
Comments and conversational activity surrounding a video not only requires a greater commitment from the user, but is also more indicative of their opinion and relationship to the video content and other people.
Given the comments provided by the interviewees, our automated method of video classification will similarly utilize social metadata that is reflective of the social and interactional activity surrounding shared videos.
In particular, we pay close attention to the use of social metadata that captures the conversational and interactional activity between the users and the shared video.
Having elaborated the issues our participants raised, we now turn to automated classification of genre.
From the surveys and interviews findings, and congruent with current ML research, content alone is insufficient for genre classification.
Traditionally, asynchronous annotation data, such as 5-star ratings, produce the meaningful signals for classification, but do not fully account for the social construction of genre in practice.
Activity data such how many times the video was watched in a single viewing session, the amount of conversational exchange  and use of video controls  signal how the video functions within the social interaction between people.
For training a classifier, we aggregate all the events in each session into a feature vector.
In our dataset of 1,580 video sessions, 80 sessions contained more than one video.
These sessions are split into separate sessions.
For example, if Bob and Mary have a 12 minute session where they shared a Music video for 8 minutes then a News video for 4 minutes, we create two unique sessions one 8 minute and one 4 minute.
This ensures every session has one video to classify.
This brings 1,660 sessions in total after multi-video sessions have been subdivided.
Each session yielded a large amount of data from feature use.
For the purposes of this research, our focus was on social actions, specifically social interaction and shared control features.
Data, such as load counts, emoticons, the video's event timestamps, and the event time were not considered.
Social interaction features included for the classification were session duration, the number of play commands, the number of pause commands, the number of scrubs , and the number of chat lines typed as a character and word count.
Our goal is to predict categories accurately from social behavior using as little data possible.
The final feature vector consists of two parts: the activity counts from the session's inviter and the activity counts of the receiver.
Each video's feature vector is an aggregated count of activities from each person in the dyadic social exchange.
The original YouTube tube video category, provided by the uploader, is used as the predictive category class variable for the independent feature vector.
In effect, we are training the classifier to match non-content, conversation activity patterns to the source video's uploaded category.
For comparison, people in our pilot study predicted categories at 60.9%.
In Yew and Shamma's YouTube study, they reported significant improvements from nominal factoring of features which are resultant of "explicit social action," in their case, the 5star ratings .
In their study, nominal factoring of the ratings bucketed numerical, interval data into discrete unordered bins .
This conversion from an interval to factors improved the classifier's training and overall performance.
The Zync data does not represent explicit social annotation; it is implicit behavior observed from using a tool.
Yew and Shamma's findings demonstrated little to no performance gain from the nominal factoring of implicit usage data from YouTube.
However, Zync's data comes from two people sharing and conversing around the media, thus socially constructing its genre.
As we hypothesized these data to be purposeful and definitional with regards to genre construction, we believe it to be equally predictive and explicit as Yew and Shamma defined.
After nominal factorization, the Naive Bayes classifier predicted category genre with an accuracy of 82.34% , a performance increase of 30%.
A full list of prediction accuracies by training set size is found on in Table 3.
Category prediction based on implicit conversational data increases accuracy by 6.8% when compared to Yew and Shamma's  YouTube dataset findings with an 80% training set.
Furthermore, our model trained on a 60% sample  predicts with the same accuracy as Yew and Shamma's model trained on an 80% corpus .
Using nominally factored conversational data for classification shows a 6.8% increase in accuracy over traditional, asynchronous social media metadata and a 21.4% increase over human judgement.
The chance prediction was based on random guess based on the corpus distribution of categories.
Bayes predictions based on 80% training sample size.
YouTube predictions reported are from Yew and Shamma .
For example, cues like background laughter functioned as signaling mechanisms, indicating that a video is a Comedy--even if for the viewer, the content was not deemed to be that funny.
Given our finding that people can articulate what factors make something a Comedy , we posed the question: why did our respondents fare so badly in classifying comedy videos in our surveys?
One explanation is that many of the videos shared on sites like YouTube do not, in fact, conform to the more culturally accepted conventions and characteristics of the Comedy genre.
An informal review of videos classified as Comedy on YouTube revealed why there was far more agreement in our survey results about whether a video was funny or not than whether the video fitted in the category Comedy.
People were much more likely to agree that something was funny, even if the original creator had not intended it to be funny.
For example, in the course of the study, two specific videos, Laughing baby and Sneezing panda, emerged from our data as examples of videos where there was some ambiguity between the intentions of the video producer and the user who had uploaded the video.
These are videos that people find funny but that were not intentionally created to be comedic; people therefore did not think they necessarily belong in the category Comedy.
To be categorized as Comedy requires as assumption of comedic intention on the part of the creator of the video itself.
In our results and findings we have addressed two research questions: RQ1 to better understand what individual and social processes people use to categorize online videos as Comedy, and RQ2, to develop an improved method for automatically classifying online videos, taking into account social and contextual features.
Triangulating between the results from the three phases of our study, we find that people find it hard to classify videos within the category `Comedy'.
It is clear that what is or is not considered comedic is subjective, contextual and somewhat culturally specific.
Unlike genre categorizations for other media channels , socially shared online videos are more diverse and varied in their form and content.
The videos in our sample range from edited excerpts from longer films, to music videos, personal home movies, and video blogs.
This variance in styles and formats makes it much harder for indi-
So why do individual uploaders classify their videos as Comedy?
One explanation is the use of genres as a signaling mechanism for hoped-for audience orientation to the content.
By categorizing a video clip as comedy, uploaders signal to potential audiences the appropriate or desired way to experience and interact around the content.
A video clip that does not appear to be a comedy, but that has funny elements within it for the audience to enjoy, may be classified as Comedy, signaling that it is intended to be taken as such.
The uploader of the video is using the genre classification to shape the social experience, shaping how the media content is perceived and classified.
This is something that occurs during conversations or through gestural or postural cues when we watch things together, in a co-present setting.
Table 5 shows a list of the social multimedia features which generate social and interactional traces for modeling.
Our approach also suggests diversity in classification.
Not everything that is Comedy should be primarily ranked as Comedy for everyone.
When building recommendation systems and new interaction experiences, there is no one-sizefits-all solution.
Sharing media, not only can, but should and indeed will transform how we perceive, understand and ultimately categorize it.
Therefore there is a strong role for systems that fluidly emerge categories for classification on the basis of social action and interaction.
Genres are more than just static organizational retrieval tools.
Our results counter the view of genres that suggests "things come in well-defined kinds, that the kinds are characterized by shared properties, and that there is one right taxonomy of the kinds."
Our findings support the view that genres like Comedy are constantly evolving, shaped by the behavior of others and by our interactions with others around and through media content.
These social interactions in turn affect our social behaviors and activities with and around that content.
Viral videos like Sneezing panda were not intentionally created to be comedic.
But having been constituted as funny, framed and shared with labels like the Comedy classification to indicate they should be read as such, drive them to be seen as humorous, and shape their trajectory through social networks as they spread and humorous memes.
Even if the original intention of a video clip is not meant to be comedy, how it is viewed, interacted with, and shared shapes its classification and expands its genre.
Our study supports the view that genres are social constructs, contextually defined and evolving out of people's pragmatic interactions and activities.
Therefore, for automated classification, implicit interaction which represents how media can transform context, should be utilized.
This work contributes to a developing understanding of the experience of sharing/consuming media content online.
In this article, we have shown how genre, specifically Comedy, can be modeled predictably when features are drawn from a qualitative understanding of media classification.
We have demonstrated that there is more to genre and classification than just flat or hierarchical organization as embodied in post-hoc tagging by people or machines.
Genre in practice is as much about how something is shared and communicated as how it fits into a pre-existing categorical schema, or genre, emerges from how we share and communicate around media.
Our classifier out performs previous solutions underscoring the importance of social action and interaction in categorization, but also suggests we have a uncovered a viable method for classifying online video for more effective organization and retrieval.
With this approach, we can help people find things which others have found to be funny rather that things that have been prescriptively tagged as such.
Addressing RQ2, we have found that a predictive model of genre identification can be built by using only implicit social traces that result from synchronous sharing, without using video content, metadata, or annotations.
This means that social behavioral traces may be stronger and thus more predictive signals on which to train predictive models.
This is a departure from the current ML approach, where explicit annotations are considered to be the most powerful features on which to train classification systems.
Our results indicate that conversational activity features prove to be very effec-
Cesar, P., Geerts, D., and Chorianopoulos, K. Social Interactive Television: Immersive Shared Experiences and Perspectives.
5. Cooper, M. M. The ecology of writing.
Crane, R., and Sornette, D. Viral, quality, and junk videos on youtube: Separating content from noise in an information-rich environment.
Making inferences in mixed methods: The rules of integration.
H olbling, G., Thalhammer, A., and Kosch, H. Content-based tag generation to enable a tag-based collaborative tv-recommendation system.
Kittur, A., Chi, E. H., and Suh, B. Crowdsourcing user studies with mechanical turk.
In CHI '08: Proceeding of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, ACM , 453-456.
Krippendorff, K. Content analysis: An introduction to its methodology.
Lakoff, G. Women, Fire, and Dangerous Things.
University Of Chicago Press, April 1990.
Liu, Y., Shamma, D. A., Shafton, P., and Yang, J. Zync: the design of sychnronized video sharing.
In DUX 2007: Proceeding of the 3rd conference on Designing for User Experience .
Mahajan, D., and Slaney, M. Image classification using the web graph.
In Proceedings of the International Conference on Multi-Media, ACM .
Miller, C. Genre as social action.
Naaman, M., and Nair, R. Zonetag's collaborative tag suggestions: What is this person doing in my phone?
Recent trends in video analysis: A taxonomy of video classification problems.
In In Proceedings of the International Conference on Internet and Multimedia Systems and Applications, IASTED , 348-354.
