Larger interactive tabletops are becoming commercially available.
However, their costs currently are fairly prohibitive for the everyday consumer market.
This is likely to change in years to come, as the cost of the hardware falls.
Currently however, these technologies have only really emerged in a few niche markets.
As interactive tabletops become more available, we can easily posit a world where people will want to procure such novel technologies for their homes.
But how will real users respond to and learn to interact with such technologies, when they are placed in their homes?
In this paper we present one of the first deployments of a multi-touch tabletop device in a domestic setting.
Our overall goal is to investigate the potential for surface computing in the home.
Specifically, in this paper we seek to observe what people`s interactions, perceptions and experiences are of such novel computing technologies and interfaces, as a means to further inform the design space.
Given that these devices provide such a revolutionary shift in form of interaction, will people be able to interact with them in the ways we intend?
This paper describes a field study of an interactive surface deployed in three family homes.
The tabletop technology provides a central place where digital content, such as photos, can be easily archived, managed and viewed.
The tabletop affords multi-touch input, allowing digital content to be sorted, triaged and interacted with using one or twohanded interactions.
A physics-based simulation adds dynamics to digital content, providing users with rich ways of interacting that borrows from the real-world.
The field study is one of the first of a surface computer within a domestic environment.
Our goal is to uncover people`s interactions, appropriations, perceptions and experiences with such technologies, exploring the potential barriers to use.
Given these devices provide such a revolutionary shift in interaction, will people be able to engage with them in everyday life in the ways we intend?
In answering this question, we hope to deepen our understanding of the design of such systems for home and consumer domains.
Although interactive surfaces and multi-touch are an established area of HCI research - dating back over two decades  - it is only recently that these technologies have become the focus of much media attention.
The internet is abuzz with talk of new products, and research in this area is growing apace, reflecting this enthusiasm.
In many ways this is understandable, given that these technologies change or even revolutionize the ways we can interact with computers.
The natural style of interaction afforded by the direct multi-touch displays, the form-factor, and their collaborative nature, are the often-cited appeals of such technologies.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To explore this question, we have built and evaluated an interactive tabletop, in-the-field with three families, for a month at a time.
The vision-based tabletop system, shown in Figure 1, was built from the ground up, in-house.
We have developed a media management application for these devices, which provides a central place for users to manage and archive their digital photos .
Although by no means feature-rich, this application has instead been designed to be rich in the interactions it affords,
The application therefore supports multi-touch input, allowing digital content to be sorted, triaged and interacted with using one or two-handed interactions.
A physics-based simulation adds dynamics to digital content, providing users with rich ways of interacting that borrows from the real-world.
Previous work  on this system reported on the more sociological aspects of the deployment, describing how the technology integrated, and at many times disrupted, social and organisational practices in the home.
In this prior work the technology was very much used as a probe to enrich our understandings of the home, ideas around family and practices around digital media management.
In this paper we describe our findings that pertain more towards the interactions that our surface computer offered, in particular around the multi-touch and physics-enabled techniques to manipulate the virtual.
We were particularly interested in understanding whether this interaction paradigm shift, away from the desktop, would work outside of the lab.
Specifically, with our deployment we wanted to investigate the following questions in detail:  Can households naturally learn to use multi-touch gestures, and do bimanual or multitouch patterns of use evolve over time?
How does a physics-based, hands-on interactional model affect the ways such a technology was used?
And does such a model actually allow users to develop their own interaction strategies?
Can we design a system that successfully blends the virtual and physical?
In particular how do we maintain the benefits of the virtual when designing an interface that closely resembles the ways in which we manipulate objects in the real world?
While the answers to these questions are specific to the device we have constructed, we believe that exploring these issues will have generalizable implications for informing the design and development of future surface and tabletop systems for home environments.
As we conclude, these questions and our observations during the field deployment open up an interesting discussion around the notion of intuitive or natural user interfaces in general.
However, real-world deployments are now beginning to emerge, including knowledge workers` use of large tablet displays , use of tabletops within museums , and other public and shared spaces , including schools  and universities .
Although not a tabletop, CityWall  studies the use of a single multi-touch wall within a public space for a week, revealing many findings with regard to the social phenomena surrounding such novel technologies.
We wish to extend this literature by considering the use of tabletops and surface computing in the home.
Rather than considering tangible tabletops, we wish to more closely understand the adoption of direct multi-touch tabletops within the domestic domain, and further also evaluate the more advanced techniques emerging from the tabletop literature, such as physics-based interactions .
In doing so, we wish to ground our work in the field, focusing on in-situ rather than lab-based studies.
Ultimately if we wish to have such technologies widely adopted in the real world, we need to better understand how people will interact with them as part of their everyday lives.
For this deployment we purposefully kept the features of the system basic, using the field study itself as a means for eliciting more advanced user requirements for future iterations of our tabletop media management system.
Given our dual purpose of also learning more about surface computing in-the-field, we decided to design a system that was interaction-rich, even if not feature-rich.
Therefore we aimed to provide users with hands-on and intuitive multitouch manipulation of virtual objects, supporting a variety of direct one and two-handed interactions.
We chose to add physical behaviors to virtual objects and use physical gestures wherever possible for interaction.
Our aim was to imitate the openness and ease with which we manipulate photos in the real world.
Much work has been done recently in the area of physics on tabletops , and we felt this was critical to include in the design and study of the system, as it is clearly a relevant technology for surface computing.
We were keen to understand whether users would see such features as a novelty, which wore off, or whether they would find them integral to system use.
Lab studies have uncovered many affordances of tabletop systems.
Much of this work considers tabletop use in the context of co-located collaboration .
These have led to guidelines for the development of tabletops .
Although studies and interaction techniques are common in the tabletop literature, real world applications have been less forthcoming.
Photo manipulation and organisation has become one popular application area .
We chose a common Frustrated Total Internal Reflection  based technique for multi-touch sensing .
The display screen was constructed using an acrylic sheet, covered with a thin sheet of silicon rubber and a layer of drafting film, and finally edge-lit with Infrared  LEDs for touch sensing.
A projector was mounted inside the unit in order to back project onto the surface.
A camera was similarly positioned inside to image the display surface.
These images were processed on an internally stored PC, using custom GPU-based computer vision techniques.
This allowed us to process multiple touch contacts at 60Hz, with little latency, giving an acceptable rate of interaction.
The projection gave a display size of 47cm x 35.5cm , which a single person or two people could comfort ably interact around.
Due to the anticipated usages and placement of the device, the display was set at waist-level allowing people to stand rather than sit to interact.
We were careful to set the height of the surface to support ease of use, without undue strain, when standing.
To fit with the goal of making a device for the home, we were careful in the physical design to try and hide much of the technology from the user.
The cameras, projector and connected PC were all encased.
A wooden top with an extended space to the side and shelves allowed devices, ornaments and other objects to be kept by the device.
A digital camera and dock was placed on one of these shelves to allow people to take and upload photos.
As we model multi-touch input, the user is free to use multiple fingers, and one or two hands to gather multiple objects simultaneously, pile them, separate two piles out using a sweeping gesture and so forth .
This behaviour is unscripted, in the sense that users are free to define their own ways of interacting within the bounds of the physics simulation.
This approach of leveraging physics is clearly inspired by prior work such as Bumptop  - although the original Bumptop system only supported interaction with one point of contact.
This is one of the core novel aspects of the technology that we wanted to evaluate with households.
Of course a literal translation of the physical world sometimes limits the qualities of the digital.
Therefore in certain instances we supported scripted gestures, such as a multifingered gesture for scaling virtual objects.
Content could be automatically uploaded to the table using either a digital camera docking station or other USB memory storage device.
The system tracks files that it currently contains in order to prevent multiple downloads of the same picture.
Once uploaded, content is automatically arranged in piles within the 3D world, and can be spread out to access individual objects by a flick of the hand.
Inspired by fieldwork around real photos in the home  we use the metaphor of virtual shoeboxes for containment.
This is an easily identifiable real-world analogy, and provides a loose way to partition the media on the tabletop.
Boxes can be moved and rotated using similar interactions used for photos.
Box lids can be opened and closed using a pinch gesture.
Once opened, applying multiple contacts off center, as shown in Figure 2 right, spills box contents.
To make the process of getting objects into boxes as straightforward as possible, an open box carries a magnetic force, which essentially attracts any colliding objects into the box.
The application running on our multi-touch tabletop is built using a 3D graphics engine .
We leverage the metaphor of peering inside the tabletop to reveal a 3D virtual world confined by four virtual walls, which correspond to the bounds of the tabletop display.
The virtual camera is set so that you are looking at the 3D scene from a birds-eye view .
Objects such as photos are controlled using a gaming physics engine .
We model multi-touch input in this world using a technique described in , which approximates each finger touching the surface to a series of virtual objects that can interact with others in the 3D scene.
Clutter was a concern in early UI versions.
As boxes were created less virtual floor space was available for working with digital photos and scans.
After iterative testing we decided to separate storage and interaction spaces into two virtual floors`.
The analogy we use is to have boxes stored in the basement - or at the bottom of the tabletop - from where they can be brought to the top level, to be spilled out, loosely arranged, displayed and subsequently tidied up and stored back in the basement .
So far we have illustrated how users interact with objects already in the system.
Interactions such as manipulating photos or boxes can be modeled quite nicely using physicsenabled multi-touch.
However, other core features for our application did not easily map to such a literal gesture manipulation paradigm.
In order to expose these features to our users we reverted to a collection of virtual buttons along the top edge of the screen.
Functionality included the creation of new, empty boxes and also their deletion once no longer needed.
Further icons are provided for users to move between the two floors.
Retrieving boxes from the lower storage level  was achieved by touching a particular box and the icon to move between floors.
The system also provides an easy means for labeling boxes  with ink - by switching to annotation mode using a virtual button.
In this mode users use their fingers to scribble on boxes and draw onto photographs.
We also decided to introduce a set of shortcuts allowing users to rapidly arrange objects an ordered grid in order to get a quick overview over many photos at once.
A slideshow mode then allows users to rapidly view and cycle through these photos up-close, by repositioning the virtual camera.
In terms of spending time in the field with families, we wanted to get a balance between capturing enough data and ensuring the families had time on their own with the system .
During the field trial all families had at least some members go on short breaks, frequently returning with new content for the tabletop.
At the end of the deployment we also collected system logs of device use and copies of all content uploaded and conducted an exit interview.
Our families were recruited by word of mouth and email advertisement and all came from the local area.
We recruited families with the requirement that they did not have children under the age of 6 , and that they had previous exposure to using digital photography.
Participants were rewarded for participation by being able to keep the digital camera and docking station that was provided with the tabletop.
Some users had prior experience of using multi-touch technology, such as iPhone use.
Household A: consisted of a recently married couple in their early 30`s, he was a biostatistician and she was an economist at a local university.
Both had a post-graduate level of education.
Household B: consisted of a married couple , with a six year old son.
The father worked in medical research and the mother in biotech.
Both had a postgraduate level of education.
During the trial for 2 weeks the family had their maternal grandmother staying to look after the little boy during his summer holidays from school.
Household C: consisted of a married couple  with two teenage daughters .
The father worked in software development and the Mother was a housewife.
The father had a graduate degree, the mother high school level education.
This family had maternal grandparents staying for much of the trial.
It is a common strategy for evaluations of user interfaces to take place in the lab and to have a formal experimental structure.
Our research goals were instead concerned with understanding the practicalities of everyday interaction with multi-touch and physics-enabled surfaces in-the-wild.
As such, an observational, in situ analysis of user interaction in the home was more suitable for our purposes.
Accordingly, we conducted an in-depth field study of our tabletop prototype in three different family homes, each deployment lasting for about a month.
Given time constraints, we built and deployed three tabletop units in parallel, one for each family.
This coupled with the technical complexity of the system - comprising of custom hardware, GPU processing, and a 3D physics-based UI - made for an extremely demanding deployment.
Similar systems deployed in the field have only supported a single system deployment , but we felt this was important to help generalise our results.
We officially visited each family once per week for several hours at a time, but also took opportunities to capture field data during times when we were called out to fix technical problems.
This data consisted of a written diary left with each family and video and audio recordings both of the participants directly interacting with the UI and whilst being interviewed about their use and experiences of the system.
This usually involved capturing footage for extended periods, where multiple family members closely interacted with the device to show us the media they had stored.
Logged data of system use was collated and summarized after the field trial had ended.
The video data of interviews and usage data of the system were then closely analyzed by the research team.
Transcriptions of interesting moments of interaction and reflection were transcribed and from this, deeper themes explored.
All three households chose to have their device situated in their living room.
Households B and C kept their systems running continuously, while Household A turned it on only when they intended to use it for uploading materials or displaying photos to friends, family and other guests.
This was due mainly to concerns about the possible energy consumption of the unit.
Due to the time taken to install and config-
Inevitably, as with most prototype systems, there were occasional technical problems.
Due to the fact that the system was vision-based, adverse lighting conditions affected the touch sensing.
However, because we rendered feedback in the UI when touch-points were sensed , households became very adept at detecting when rogue touch points were appearing on the surface, and would devise workarounds.
For example, Household B moved the orientation of light sources in the house, and Household C drew curtains at times when the surface was being used under direct sunlight.
By exposing the users to some of this technical maintenance we found they were able to devise fairly sophisticated means to work around these limitations on their own.
Despite these problems, we observed substantial use of the tabletop during deployment, as summarized in Table1.
This illustrates how diverse and fairly complex chains of hand gestures could be devised for simply moving virtual objects.
We observed how the wife distinguished and switched fluidly between precise selection of a single small target using her index finger, and more coarse selection and movement using multiple fingers of a single hand.
Likewise in the same session, the husband goes on to open a box using one hands` index and middle finger  and then close it using the index fingers of both hands , and rotate a photo bimanually , whilst the wife rotated a photo using a single finger placed in the corner  and finally both rotated using a more typical thumb and forefinger pinch .
Although this open-ended set of hand gestures may feel complex to master, these were in fact lightweight for users to achieve, and we began to observe these throughout the trial.
They felt at ease experimenting with new techniques, and also switching between techniques gracefully.
We often observed sequential interaction with objects as shown in , where users interacted with objects one at a time.
However, even with these single object interactions, we observed large use of the second hand, which was often used alongside the first to divide the labor between hands .
This again contrasts with .
That being said we also observed many instances of interaction with multiple objects .
These were often for more coarse movement of objects, such as gathering up, piling or rummaging through photos.
Here we noted about half the participants would not directly touch on photos, but instead would use more of a collision model, bringing their fingers in on the sides of objects to either push them away or more commonly bring then towards themselves.
Although not conclusive, this is interesting as the model does not directly translate to drag-anddrop metaphors where movement is achieved by direct selection, but instead suggests a model that derives more from physical interaction.
Returning to Household A, we see later in the session the wife sweeping multiple photos to one side using multiple fingers of one hand, while she searches for a specific photo to show us , she then introduces her second hand to speed this process up and again divide up the labor .
In the remainder of this paper we focus on investigating the users` perceptions of the UI, their uses of multi -touch, physics and 3D, and in general, how they interacted with this novel surface computing technology.
We avoid findings pertaining to the media management features of the system .
We were extremely pleased with the speed in which users picked up the gestural vocabulary for interacting, even when training had been kept to a minimum.
Manipulation of photos and boxes in particular were rapidly discovered.
On the first session most users were able to move, scale, rotate, flick and pile photos.
Flicking was particularly evident and users formed strategies for flicking multiple objects into boxes rapidly.
Basic interaction with boxes was also straightforward for users, such as moving, opening, closing, rotating and hovering up content.
Throughout the study we observed many one-handed multitouch interactions, and much bimanual interaction.
Users switched effortlessly between using one and two hands.
The great degree of bimanual contrasts with some lab studies of photo work on multi-touch tabletops .
Users also developed markedly different hand gestures to achieve the same UI actions.
This is perhaps best illustrated by Household A.
In Figure 4, we show just some of the hand gestures utilised by both husband and wife during a 20 minute period of sustained interaction .
We also logged whether during each minute, the user were interacting predominately with one, two or more fingers .
This gives a rough sense of how many fingers were typically used during interactions with the tabletop.
Note that we did not count contacts appearing and disappearing rapidly, which did not move, or did not lead to any UI actions, as these may have been falsely triggered by adverse lighting.
We also did a simple distance measure between contacts to again roughly predict when contacts could not physically belong to same hand, suggesting bimanual use.
One consideration here is that when users are touching the virtual box, they are not directly interacting with it.
Even if the tabletop gives the impression of directness, there is an intermediary - the computer - so reverting back to the simplest form of interaction, a single finger, allows users to make better sense of the actions of the computer in response to their input.
In general people appreciated the hands-on nature of the system and the physics-based interactions.
It gave a sense of familiarity when interacting digitally, and at the same time distinguished it from standard computing interfaces.
Users had a sense that they were interacting with something new and different.
Table 2: More detailed usage statistics.
Column two shows total interaction time per family, recorded when 1 or more finger was active.
Fourth, fifth and sixth estimates the percentage of this time that 1, 2 or more than 2 contacts were sensed.
The last column estimates percentage of overall bimanual use.
Overall our logs, video data and interviews demonstrate multi-touch gestures and bimanual use was plentiful.
This was interesting given that the size of the display is a little smaller than other tabletops and we had felt occlusion caused by two hands could be an issue.
Already through the field deployment we were seeing results that contrast with lab-based studies of tabletop photo work .
The data also highlights single finger use.
This in some ways is to be expected, particularly when moving a single object or clicking buttons.
However, when we began to unpack other moments when and why users switched to a single finger, we found a subtle, but yet critical difference between how users interacted with objects in the virtual and the physical.
In the real-world, we often use two hands to provide finer motor control .
In the digital however, more hands or fingers does not necessarily equate to more control.
Indeed users often have the opposite way of perceiving fine control, we often observed users reverting to single touch to carry out fine-grained interactions, for example to position an object precisely.
This reverting to a single finger was also the case when physics-based interactions led to unpredictable results in the UI.
For example, when boxes filled up they would become more unstable suggesting they may accidently tip over.
Under these conditions using multiple fingers leads to more force to hold the box in place, so in terms of control more fingers are better than one.
However, we observed that if the box did show signs of tipping, the users would in fact do the opposite - removing all fingers from the box and using a single finger to carefully move it.
This actually made the box less stable, and led to tipping and user frustration.
This is perhaps best illustrated again through example - the elderly grandfather in Household C walking us through his photo collection on the tabletop.
This participant openly admitted he had little if no computer expertise.
At first during his interactions he struggled and needed instruction from his daughter to recover his box from the virtual basement.
He continually asked "What button do I press?"
However, once the box was on the photo-level, he managed with little instruction to open the box with a pinch gesture, get the contents out of the box, scale and show each photo to us, and rescale them back using bimanual input, before finally flicking each photo back in the box and closing the lid, as highlighted in Figure 5.
Whilst interacting, he was fully engaged with us, describing the story behind each photo.
It is interesting that the only time he stalled was interacting with the icons at the top of the screen.
When interviewed , he alluded to what qualities of this interface he found beneficial:
Karen: He  doesn't really like computers but as soon as he was here, he was up here  Karen: I was saying wasn't I mum, that if he'd seen that there was a co mputer based thing behind it then the block would have come up and he wouldn't have done it Granddad: Yeah because it's touch init, it's like er fun.
Interviewer 1: do you like the fact that you can do that?
Granddad: Yeah I'm a hands-on kind of person, I've always worked with my hands, everything I do is with me hands, up here  is a little bit slower than me hands.
The granddad clearly sees a computer as requiring a technical mind, something that requires expertise and a mindset he will never have, but instead this device allows him to engage and express himself with his hands.
It is interesting that as the daughter points out - there is a PC hidden in the tabletop - but because of the tangible quality of the direct touch and the physics, the granddad feels qualitatively differently about this device.
He was confident enough to in-
Here the participants clearly identify the PC with work and the workplace, whereas this device has a more playful or appliance-like quality that seems more suited for the home.
As we have seen, the perception of this device as not a PC seems due to a combination of factors: its hands -on interaction, the illusion of physics, and its aesthetic qualities.
However, as we will go on to discuss, all of this can also create a tension when considering what the device is for, in particular, the tension between work and play.
Excerpt 3 -  Carol: And the whole physics-I like!
It's just that interaction that you don't get.
I guess you could put it in a normal computer but it's just a bit more funky, more real.
Excerpt 4 -  Victor: The physics of this stuff is really cool.
I mean it feels really like it's a real object.
A feature that I, now that I'm playing with it, would like to have, is a full hand.
Again the physics gives the interaction a real or tangible quality.
In excerpt 4, the participant is enticed to play and experiment with the system, and quickly discovers new unanticipated interactions are feasible.
In excerpt 3, the participant hints at how the physics adds further distance between this device and a regular PC.
She goes on to say:
Excerpt 5 -  Carol: The things have a space have a feeling, you are not dealing with files you're dealing with things, I think that has the edge, it is very different it does transform the way you think about these things because you have to think twice to realise that it is a file.
Here the non-deterministic nature of the physics added a touch of unpredictability or spontaneity to the interaction.
Indeed, we saw users play on this spontaneity further by arranging the photos into an aligned grid , and then proceed to shuffle the photos around slightly before starting a slideshow, as shown in Figure 6.
The physics by its very nature added non-determinism to the interaction, which was suitable for playful interactions.
However, there were times that the interactions were too literal and almost too real.
For example, participants would get frustrated by moments when carefully arranged and sorted boxes would accidently spill due to adverse collisions.
Another example is that participants complained about the need to tidy away photos after displaying them or tidy up the basement when it became cluttered.
One couple went to extreme cases, when they wanted to ignore the ensuing clutter in the digital by turning off the display.
Excerpt 8 -  Carol: Whenever it was messy yes it was like oh my god not only have I got to clean up my house on a Saturday but now I have to clean up the flippin desktop.
Excerpt 9 -  Karen: I'm normally tidying it up after people have used it, but then I have a problem with symmetry.
This further alludes to how the physics and the real-world metaphors, coupled with the tabletop hardware allowed the families to further shift from the idea that they were interacting with a normal computer and desktop metaphors.
This seemed critical for all the families, but why?
Again Household B provides some answers:
Excerpt 6 -  Carol: I think I have watched my pictures more with this thing.
Mike: because you have it always there and available Interviewer 2: Is that a good thing or a bad thing?
Carol: I think it is good, and if I have to go to the computer to look at pictures I will probably end up doing something related to work.
Carol: Because the keyboard and cables ... it's ugly.
Other frustrations were caused by the effort required by the physics interfaces to do such simple things as tipping over the boxes.
Many users struggled to do this gesture efficiently, instead opting to use the arrange in a grid button to pull the photos automatically out of the box.
Here some of the problem was down to the lack of 3D input on the surface.
However, with the interactive surface the input is bound to a 2D plane, which must somehow be mapped onto the 3D world - inevitably the DOFs in interacting are lost.
The analogy is like trying to interact with small physical boxes on a desk but only being able to push or nudge them with fingertips, instinctively we wish to pick the boxes up and interact with them using all 6DOFs.
This clearly highlights an important design trade-off as to when to create an interaction that borrows from the realworld, and when to instead offload to the virtual.
Being too literal in the translation of the digital tabletop can lead us to missing some of the benefits and efficiency of the virtual .
In this case we saw that, while aspects of physics helped create playful interactions, the physical work and unpredictability this entailed added to users` perceptions that this was mainly a device for play rather than one for productivity and efficiency.
Instead, participants got into the practice of interacting on their own to, for example, interacting to arrange photos before calling others over to view the show, rather than directly touch the surface.
This contention in collaboration has begun to be uncovered in the tabletop literature , but it really was paramount in the homes we studied.
Participants were indeed almost hesitant to use the system at the same time:
These issues surrounding the literalness of the UI also play out in the finite space that users had in the virtual 3D world.
This coupled with the fact that the space was shared by all family members, children and adult alike meant there was a real battle for space.
This is perhaps best illustrated in the following vignette:
Excerpt 10 -  Mike: It's like he  would have his own area with permission only to open boxes to yeah mess about but he cannot change for example where the pictures go in this box or in that box.
Interviewer 1: Would he be able to open your boxes?
Mike: Yeah I guess so but with permission, he cannot modify anything he cannot write on certain pictures for example or certain boxes things like that.
Carol: As long as he has his own space I don't think he'll go and try to mess up ours.
Excerpt 11 -  Luke: what I found fun was playing around with all of the pictures and letting mum and dad tidy them up.
Part of this could be where the tabletop was placed, or perhaps the form-factor, in particular the limits of the screen size.
However, it is interesting that participants comfortably interacted side-by-side during our deployment visit, and were happy to interact bimanually on their own at the tabletop.
Instead it seemed that many of the social norms of the real-world were simply ignored in the digital domain.
People would be happy taking objects out of each other`s hands, competing for screen real-estate, and fighting over objects such as photos and boxes.
This clearly was done in a playful way, but participants did mediate access to the device to avoid even these playful conflicts.
So to return to the question raised in the title of our paper, were people at home with surface computing?
And more specifically did they engage with this novel form of interaction in the ways we intended?
When it comes to interacting with such devices, we had assumed that the open nature of interaction supported by a combination of multi-touch and physics would make the system intuitive to use.
And indeed this was partly borne out: Both young and old were clearly engaged with the system, and in certain cases with little existing computing expertise.
We found that a number of factors contributed to this.
One set of factors had to do with the interaction model, and the use of virtual physics, which led to more open interactions allowing users to more readily experiment and develop their own strategies for interaction.
These became extremely elaborate and sophisticated as the trial progressed, allowing users to switch between many different strategies for interaction.
We also saw clear evidence of the ways in which households fluidly adopted multi-finger and twohanded methods of interaction, as the situation demanded.
Accordingly, we would suggest it is important for designers not to overly prescribe a single interaction model.
Bimanual interaction of the kind we observed has not previously been observed in shorter-term studies e.g.
Importantly, this ad hoc and fluid use of bimanual gesture contrasts with prior work such as .
This vignette demonstrates the contention in the household over the shared space.
Because the system was democratic, the son had equal rights to explore the space, open boxes and so forth.
He used this space more as a play area.
He would upload photos of his toys, make a mess  and not even need to tidy up.
In some ways, this was the only space in the home that he had such freedoms so he made the most of this.
Although our system was designed to be interaction-rich rather than feature-rich we still had to design a user interface that occasionally had to go beyond the literal to enable certain features.
Whenever possible we opted for gestures that at least borrowed from the real world such as two fingers to resize images.
However, metaphorical gestures such as the pinch to zoom are not always a feasible choice, especially when exposing functionalities that are not possible in the physical such as copy and paste.
These functionalities are often abstract concepts therefore we usually revert to non-literal metaphors to expose them to the user .
If we strive to inform the design of realworld applications based on physics-enabled interactions we need to derive ways to enable all three types of interactions: the literal, the metaphorical and the abstract.
For example, Figure 7  shows how Luke uses the mouse to move around the virtual, something he quotes as being "hard work" with touch.
He also uses the mouse to interact with the buttons on top of the screen as this aided his reach and accuracy.
However, for zooming he understands he needs two touches, and proceeds to touch a single finger down, and use the mouse cursor creating the necessary second touch point Figure 7 .
He then almost exclusively switches to manipulating objects - the boxes and photos - by touch Figure 7 .
Our field-study uncovered interesting tensions arising from the interplay between real-world and desktop-based metaphors.
Despite problems caused by mixing these concepts it is both that makes surface computing a compelling experience.
Our deployment has highlighted the need to develop a sophisticated understanding of how to design user interfaces that that seamlessly blend digital and physical interaction metaphors, optimising interplay between both worlds.
Through our study we have also begun to look more critically at a  term that has become synonymous with surface computing - natural - suggesting an interface akin to real-world interaction .
However, from our study, we have begun to question what this really means from the perspective of everyday users.
One example comes in tipping the box, where the most natural interaction - to simply pick the box up and spill out the contents - was unavailable due to lack of 3D input.
This made what at first appears a natural gesture, tipping a box by direct touch, a wholly unnatural one, which was a poor imitation of the real-world.
Users opted instead to perform a simple button press to perform this action.
Another example is when users fell back to interacting with a single finger, particularly when the physics-simulation meant virtual objects were behaving unexpectedly.
Here even though we tried to provide an illusion of interacting directly with a real object, users clearly saw that their interactions were being mediated and processed - by the computer.
To make better sense of the computers actions, they would interact cautiously and limit their interactions to a single finger, and observe how the system reacted.
Although almost an implicit action, users were simply limiting input to the computer to better make sense of the output.
We conclude by drawing out our main findings and highlighting their impact on the design of tabletop interfaces in the home.
We observed regular single finger, multiple fingers and bimanual hand configurations during the study.
Users implicitly transitioned between different hand and fingerbased configurations, and we feel that it is important for designers not to overly prescribe a single model.
Bimanual interactions of this kind were not observed in shorter term studies e.g.
This is a crucial finding that goes against prior work such as .
Many different factors impacted user hand configurations.
For example, when users perceived that a virtual physics object needed to be stabilized or controlled, they switched to using a single finger .
The fidelity of the task also resulted in different hand poses.
Users switched from single and multi-finger for finely controlling a single object, to using two hands for coarse multi-object interactions.
Another under explored factor was that of the confidence the user has in the system.
For example, if the system failed to respond in a predictable way, the user typically reduced the number of fingers interacting.
Hand-based interactions and physics allowed users to learn the system in ways they would not have been able to with a regular computer.
They saw it as a hybrid virtual/digital world.
Beyond a regular PC, but had some reservations due to the fact that it was still technology`.
We want to use one final rare and yet remarkable example, which we include here in our discussion, rather than our findings section, to inspire us to think more about how the lines between desktop and real-world metaphors can be blurred.
During the last few days of deploying the tabletop, Luke discovered the presence of a mouse plugged into the back of the device.
This can lead to confusion and interesting findings.
Some of the physics interactions made for hard work for the user, and the virtual space was limited due to the 3D physics.
This shows that simply replicating the real in the virtual is not without its downsides, and ultimately these metaphors should be used liberally and not always literally.
There is a huge body of work on tabletop systems for collaboration.
However, in practice we saw little examples of collaboration.
People genuinely avoided simultaneous use and would often become agitate each other interacting side by side.
We often think that NUI means touch based input.
For children in our study natural meant the mouse and double click.
Evidence from the study really demonstrates that natural` is an overloaded and often confused term.
We have evaluated a novel multi-touch and physics enabled interactive tabletop in three real family homes, providing a grounded understanding of tabletop UI design in domestic spaces.
In doing this we have explored how such devices can be used in everyday homes and with regular users.
By capturing a rich data set regarding peoples` interactions and perceptions of such devices the study has assessed the value of multi-touch, physics and surface computing, thinking deeply about the design of such devices.
It has allowed us to see where the emulation of physics fits well, and equally where it may hinder the kinds of things that families want to accomplish.
This raises a design challenge of building an interface for a system which supports both work and play, and which at once can incorporate affordances of both the physical and digital world in ways that optimize both.
Keepin' it real: pushing the desktop metaphor with physics, piles and the pen.
Tabletop sharing of digital photographs for the elderly.
Buxton, B., Multi-touch systems that I know and love, http://www.billbuxton.com/multitouchOverview.html 4.
Y., Asymmetric division of labor in human skilled bimanual action: The kinetic chain as a model.
Low-cost multi-touch sensing through frustrated total internal reflection.
