Both datasets were analyzed using MDS.
In both papers averaged models were employed, without much consideration for the underlying remaining diversity.
Approaches such as the Repertory Grid typically emphasize the idiosyncratic nature of perception and evaluation of objects.
In other words, individuals perceive interactive products, such as websites, through different, individual "templates".
This in turn leads to a certain amount of diversity in obtained attributes and attribute ratings.
Some people may use entirely different attributes to evaluate a website, whereas others may use the same attributes but apply them differently.
An idiosyncratic approach embraces this diversity and treats it as valuable information.
However, there is also a problem inherent in this approach.
As an analyst, one is confronted with as many idiosyncratic views as participants.
Views may overlap or even contradict one another; it is in any way complicated to systematically explore this diversity.
In practice, the consequence is either an idiosyncratic analysis with a "narrative" summarization  or the use of average models.
Averaging, however, treats diversity among participants as error and thereby contradicts the basic idea of the underlying approaches.
This paper argues against averaging as a common practice in the interpersonal analysis of subjective judgments.
More precisely, we suggest a quantitative, exploratory MDS procedure to identify homogeneous sub-models, thereby reducing the number of different views to be considered while gaining a deeper insight than an averaging approach by accounting for more and of greater semantic diversity in attributes.
It will be demonstrated that even single subjects can handle more than one view on a set of stimuli.
We will show that by averaging interesting views are overlooked due to majorization bias.
In this paper we argue against averaging as a common practice in the analysis of subjective attribute judgments, both across and within subjects.
Previous work has raised awareness of the diversity between individuals' perceptions.
In this paper it will furthermore become apparent that such diversity can also exist within a single individual, in the sense that different attribute judgments from a subject may reveal different, complementary, views.
A MultiDimensional Scaling approach that accounts for the diverse views on a set of stimuli is proposed and its added value is illustrated using published data.
We will illustrate that the averaging analysis provides insight to only 1/6th of the total number of attributes in the example dataset.
The proposed approach accounts for more than double the information obtained from the average model, and provides richer and semantically diverse views on the set of stimuli.
Subjective measures for assessing the quality of interactive products has always been of interest to the field of HumanComputer Interaction  .
However, with the recently increased interest in User Experience , personal attribute judgments are becoming more and more used.
A number of multivariate techniques such as Factor Analysis , Multidimensional Scaling   and Structural Equation Modelling   are employed traditionally for exploring the relations between different obtained attributes.
For instance, Schenkman and Jonsson 
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The development and validation of questionnaires that operationalize new constructs has been the common practice, both in HCI and in many other fields.
See, for instance, the ongoing discussion on the role of aesthetics in the acceptance of interactive systems .
This traditional approach to the analysis of subjective judgments has however two limitations.
The first limitation lies in the way in which it is being applied in the field.
The development of a questionnaire is often described as a three-step process: item  generation, scale development, and reliability assessment .
We consider current approaches to item generation often to be rather superficial.
Items are often generated purely on the basis of prior literature and brainstorming .
At best, constructs may be driven by psychological theories; this however introduces other shortcomings, especially in new domains, as constructs that are not supported by theory, will evidently be neglected.
In rare cases, where user studies are employed in the item generation process, they are mostly restricted within one research group, often involving a limited set of products and contexts of use.
A by-product of this emphasis on the latter two stages of the questionnaire development process is a limited reporting on the exact procedure and intermediate results of item generation, thus making it difficult for researchers to expand and further validate an existing questionnaire.
In other words, there is a natural force to undermine the qualitative part in the process of developing a new questionnaire.
Instead, we argue, that item generation should be at the core of researching and reporting when first attempts to measure new constructs are being made.
The second limitation is that of assuming homogeneity in the ways that different individuals perceive and evaluate products.
Previous research has raised awareness of the diversity between individuals' perceptions .
For instance, different individuals may form different evaluative judgments even while having no disagreement on the perceived quality of the product, e.g.
In extreme cases, individuals might even use entirely different attributes to evaluate a product, reflecting the qualities they consider important for the specific product being evaluated.
One might assume a certain hierarchical structure on the importance of different qualities, that is universal across different individuals, such as that proposed by Jordan  on the relative importance of functionality, ease-of-use and pleasure.
While this might hold true to a certain extent, empirical findings have shown this hierarchy to be modulated by a number of contextual aspects such as the user's motivational orientation , and time of ownership .
To some extent, one could even wonder whether rating a product on quality dimensions that are imposed by the experimenter is always a meaningful activity for the participant, for example when the participant does not consider a quality dimension as relevant for the specific product.
An alternative approach to posing predefined questionnaires to participants lies in a combination of structured interviewing, that aims at eliciting the attributes that are personally meaningful for each individual, with a subsequent rating process performed on the attributes that were elicited during the interview.
Many different interview approaches have been proposed in the fields of Constructivist and Economic Psychology.
Comparing the different techniques is not the focus of this paper; see  for more information on this.
While this paper illustrates the analysis procedure using Repertory Grid data, it may also be well applied to data derived from any of the other attribute elicitation techniques.
Construct Theory  which suggests that people form idiosyncratic interpretations of reality based on a number of dichotomous variables, referred to as personal constructs or attributes.
A personal construct is a bi-polar similaritydifference judgment.
For example, when we meet a new person we might form a construct friendly-distant to interpret her character.
In this process we perform two judgments: one of similarity and one of dissimilarity.
Both judgments are done in comparison to reference points: people that we regard as friendly or distant.
To elicit the idiosyncratic attributes of each individual, the RGT employs a technique called Triading, where the participant is presented with three products and is asked to "think of a property or quality that makes two of the products alike and discriminates them from the third" .
This can be repeated for all possible combinations of products and until no new attribute arise.
The result is a list of attributes that the specific individual uses to differentiate among a set of products.
The attributes may then be employed in rating scales, typically Semantic Differentials , and each participant rates the set of products on his own elicited attributes.
Participants' ratings are subsequently analyzed with exploratory techniques such as Principal Components Analysis  and Multi-Dimensional Scaling .
With the recently increased interest in user experience , the RGT has become popular in the field of HCI.
Hassenzahl and colleagues employed the RGT to evaluate the outcome of parallel design  and analyze the perceived character of websites .
Fallman  elicited users' experiences with mobile technology devices, while Boyd Davis and Carini  explored player's experience of fun in video games.
Karapanos & Martens  explored the differences between designers' and users' perceptions on a set of user authentication techniques for multi-user printers, while Hertzum et al.
It, thus, becomes evident that an increasing number of researchers in HCI, emphasize the idiosyncratic nature of subjective judgments on the quality of interactive products.
To our knowledge, however, all RGT approaches up to date have been employing averaging techniques for the quantitative analysis of personal attribute judgments .
We believe this to be due to a lack of more advanced techniques that can account for diversity in users' subjective judgments, eventually undermining the core motivation for the RGT and other personal attribute elicitation methods.
In the remainder of the paper, we suggest a quantitative, exploratory MDS procedure to account for the diverse views that one or more individuals may have on a set of products.
It will be demonstrated that even single subjects can handle more than one view on a set of stimuli.
We will show that by averaging interesting views are overlooked due to majorization bias.
The data for the present analysis was taken from Heidecker and Hassenzahl's  study of individuals' perceptions of eight university websites.
The study was part of a larger project aiming at understanding how the Technical University of Darmstadt  is perceived in comparison to other regional competitors.
Ten individuals, all students at TUD, participated in the study.
The eight university websites were presented to participants in the form of color A4 screenshots of the main page.
Participants were then asked to rate the websites on their own elicited attributes, using 7-point Semantic Differential scales.
The resulting data set consisted of a total of 118 attributes  on which ratings for the eight different stimuli were elicited.
More specifically, MDS looks for a Kdimensional configuration for the stimuli such that the coordinates of the stimuli in the configuration space along different axes can be monotonically related to the observed attribute ratings of the participants .
Figure 1 illustrates a 2D MDS configuration with two stimuli and two attributes.
The relative positions of the stimuli on a given attribute axis reflect subjects' ratings for the stimuli on this attribute.
For instance, website j can be perceived as being both more legible and colorful than website i.
An important motivation for MDS is the principle of homogeneity of perception which states that attribute judgments from different participants are related and thus can be represented in a common configuration space .
This view, although it holds in perceptual judgments, has recently been challenged in more cognitive judgments where the quality dimensions of interactive products are assessed .
These attributes are analyzed to form the first model, i.e.
In the second step, the attributes that displayed the least fit to the average model are grouped and used to attempt a second model.
By selecting the least-fit attributes, instead of all remaining attributes, we promoted the diversity between the two models.
The same goodness-of-fit criteria are applied for the second model to select the attributes that are retained.
The starting point of the proposed approach is that of identifying the different views that each participant has on the set of products.
In this step, an average model is attempted for each participant.
However, attributes that are not adequately predicted by the average model  are removed and used in deriving a second model, i.e.
Once the diverse views of all individuals have been identified, the similarity among them is assessed and views are clustered into groups of increased homogeneity.
A final set of diverse configurations is formed by grouping the similar views, which are then used to model the attributes from all participants.
In identifying the different views that an individual might hold, one tries to model the individual's perceptions in one or more non-trivial K-dimensional models, each explaining adequately a part of his/her attribute judgments.
The maximum dimensionality K is limited by the number of degrees of freedom in the data, but may also be set a priori by the data analyst.
For the example data set considered below the dimensionality was fixed to K=2 so that different visualizations can be easily presented on paper.
Note that models of degree higher than 2 need multiple 2D views to be assessed anyhow.
However, in this latter case, the views are different 2D projections of a shared multi-dimensional configuration.
The 2D views that we will present in this paper, on the other hand, can be independent.
A two-step procedure is proposed to establish whether zero, one or two models with dimension K=2 can adequately model the attribute scores of a single observer.
In the first step, all attributes of a participant are modeled together, as is common practice in MDS .
We suggest a combined goodness of fit criterion.
First, for an adequately predicted attribute, a substantial amount of its variance should be accounted for by the model.
This proportion of explained variance is the R2 statistic .
A threshold R2>0.5 was set, implying that only attributes are retained for which at least 50% of their variance is accounted for by the model.
A limitation of this criterion is that it is insensitive to the range of the ratings for the different stimuli on a given attribute.
To account for this limitation, we combine it with a second criterion.
This second criterion is a modification of a measure originally proposed by Draper and Smith .
It is the ratio of the maximum range of the predicted scores for attribute k divided by the standard deviation k of the estimation error  in the attribute scores .
The obvious limitation of the second measure is its sensitivity to outlier scores.
However, in single-stimulus scales such as the semantic differential scales, these outlier scores may actually be very valuable, since they point at the stimuli that most strongly influence the existence of the attribute scale in the first place.
When using more sensitive scales such as paired comparisons , one might consider adopting the modified measure  that averages across differences in predictions.
Draper and Smith  proposed a minimum ratio value of four, meaning that any attribute predic-
Predictors with a ratio value above ten are considered to be excellent.
We decided to use an acceptable ratio of six for the data analysis reported in Table 1.
We are aware of the fact that the criteria that we introduce for assigning attributes to models may come across as somewhat arbitrary.
The main objective of this paper is to illustrate that multiple views can provide richer modeling of heterogeneous data than a single  view.
It is hence not particularly crucial at this stage whether or not our strategy for partitioning the attributes is optimal.
Finding more optimal ways of partitioning attributes is an issue that can be addressed in more depth after the usefulness of having multiple views is firmly established.
A more optimal partitioning strategy will only help to strengthen our claim.
Out of the all the attributes that were not adequately predicted, attributes  displayed the least fit by model 1, i.e., Rk<4.
These were used to derive a second model.
Out of them, only attributes  turned out to be adequately predicted by model 2, using the same goodness of fit criteria as used in model 1.
Figure 2 illustrates the different insights that the two diverse views bring.
One can note that the two views highlight semantically different attributes.
Each attribute is visualized as an arrow, i.e.
The length of each arrow highlight's the strength of the attribute, reflecting the variance in the attributes ratings for the different stimuli; on some attributes all websites might be rated as 4 or 5 on a 7-point scale, while others might make strong differentiations between sites, i.e.
The first view provides overall three different insights.
Table 3 illustrates the analysis process on the attribute judgments of a single subject.
A first  model was attempted on all attributes of the subject.
Attributes  were adequately predicted by the average model, using the two criteria that were discussed before, i.e.
Model 1 was then derived by optimizing the average model only for the attributes that were adequately predicted by the average model.
Note that the R2 and Rk values are identical  for Model 1 and the average model.
This implies that when removing the attributes that are not adequately predicted , the 2D configuration space  displays virtually no change.
In other words, the attributes that were removed  had no contribution to the configuration space.
Thus, the information contained in these attributes is not modeled when attempting an averaging analysis and therefore it is lost.
This shows that diversity is prevalent.
Half of the participants even hold two different views, explaining subgroups of attributes.
All together, 13 different views emerged from the ten individuals.
These views may partly overlap, which motivated us to group similar views and identify the major diverse of this user group.
In grouping the diverse views one has to derive a distance measure that reflects the degree of dissimilarity between configurations.
Each configuration can be regarded as a NxK matrix, where N is the number of stimuli and K the number of dimensions of the configuration space.
The distance between configurations Xn and Xm can be calculated using the MATFIT procedure, developed by Ramsay .
MATFIT seeks for a transformation matrix M that minimizes the distance measure:   An arbitrary KxK transformation matrix M was applied.
The procedure was repeated with the matrices in reverse order as a means to calculating both distances: with Xn as independent and Xm as dependent, and vice versa.
The resulting distances were visualized in three dimensions using the program XGms .
A hierarchical  clustering algorithm was applied to the 3-D configuration .
Figure 3 represents a 2-D perspective on the 3-D configuration of individual models.
Note that the distances in this 2D perspective do not necessarily reflect the true distances in 3D, which is why one should rely on the lines that visualize the clusters .
Participant 7 and 8 are excluded, because no individual model could be fitted.
In case of two fitting models per participant  the first model is denoted as a, the second as b.
This may be induced by the websites but may also reflect prior beliefs of the individual.
Second, the websites of the universities of Munchen, Aachen, Karlsruhe and Heidelberg have a more professional layout as opposed to the remaining four which have a more playful one.
Last, the subjects perceive this same group of websites as legible as opposed to the remaining four in the upper part of the figure that are perceived as having no clear structure.
The second view partly provides overlapping information , but also gives three new insights.
First, the website of the University of Heidelberg is differentiated from all others by having a less colorful layout.
Second, the Universities of Darmstadt, Aachen and Karlsruhe are differentiated as universities that provide primarily technical studies, as opposed to the universities of Mainz, Mannheim and Frankfurt that are referred to as universities of average quality, and third, as opposed to the university to Heidelberg that is perceived as a university offering primarily medical studies.
Note that an attribute may range from being purely descriptive, i.e.
This enables the researcher to gain a better understanding of the inferences individuals make as they form evaluative judgments of products.
Table 1 summarizes the results of the above analysis for all ten participants.
For two of the ten participants , no substantial agreement between their attribute judgments is observed, i.e., no satisfactory MDS-model can be derived.
This implies that they either have as many different views as their attribute judgments, or more likely, that their ratings are too noisy to be analyzed in a meaningful way.
For another three participants  only one satisfactory model is determined, which accounts for roughly half of their attributes .
Three clusters of models emerged.
The complementary models  for these five participants appear to be quite dissimilar as illustrated in figure 3 by the fact that they belong to different clusters.
These clusters represent homogenous views, which can subsequently be mapped out.
Thus, by accounting for diversity, even with our clearly sub-optimal procedure, we account for more than double the number of attributes than in the case of the average model.
Table 5 illustrates the goodness of fit of the average and the three diverse models for the 38 in total attributes resulting from models 1 to 3.
But, does this increase in the goodness of fit of the model outweigh the increase in model complexity, i.e.
One of the most widely used criteria for model selection is the Akaike Information Criterion   which is a function of the log likelihood value reflecting the goodness of fit of the model and the M degrees of freedom in the model reflecting its complexity:
In the last phase we establish a final set of configurations that represent the major diverse views across all subjects and all attributes, on the set of stimuli.
Views that belong in the same cluster are analyzed together and a shared MDS configuration is sought.
Attributes that are not adequately predicted by the model are eliminated with the same criteria as in phase 1.
The resulting `averaged' views are then used for modeling the attributes from all participants.
Attributes are allowed to exist in more than one configuration if they are adequately explained by all of them.
When attributes in the same semantic category are not significantly different , they are grouped.
Attributes that cannot be grouped  are eliminated since no evidence exists that they contain reliable information.
We will address this question in three ways.
Firstly, we will illustrate that the average model predicts less than half of the attributes predicted by the three diverse models together .
Secondly, we will illustrate that, for the attributes that are predicted by the three diverse models, these models provide a better fit than the average model, as demonstrated by the amount of explained variance in the attribute data and the values of the well established Akaike Information Criterion  for model selection.
Thirdly, by exploring the resulting views, we will illustrate that the diverse models, not only account for more attributes and with a better fit, but that they also result in semantically richer insights, i.e., introduce more semantically different attributes.
Surprisingly enough, the average model could only predict 1/6th of all the attributes from the ten participants, i.e.
This means, that when deriving an average configuration to understand how individuals distinguish between these websites, only 1/6th of the attributes are taken into account.
This is illustrated by the high correlation between the two resulting configurations , the one derived using all 118 attributes and the one derived using only the 18 attributes that are well predicted.
Thus, the consequence of averaging is that we account only for 1/6th of the information available.
Burnham and Anderson  proposed a set of heuristics when comparing the AIC values of two models.
In our case i = 354 >> 10, providing significant evidence that the diverse models, despite the increase in the model complexity, perform better than the average model.
Figure 4 illustrates the insights gained by the average and the three diverse models that are derived from the views corresponding to clusters 1 through 3.
A significant overlap exists between models 1 and 3 , while model 2 provides a completely different view.
The average model, although it accounts for more attributes than each of the diverse models, fails to predict semantically similar attributes.
The websites of the university of Munchen and Aachen are differentiated from the remaining ones as web-
These two attributes are present also in two of the three diverse models, model 1 and model 3.
Model 1 further differentiates the websites of Aachen and Munchen as having a "graphical layout", the website of the university of Aachen is mainly differentiated from all others as a website that does not "refer to student life".
On the contrary, model 2 provides a different insight.
It reveals that the websites of the Universities of Mannheim, Frankfurt and Mainz put "less emphasis on achievement".
The set of websites can also be split in two groups based on the amount of information that they provide to the user.
Employing predefined questionnaires has been the common practice in the study of subjective judgments in HCI.
In this paper we highlighted a limitation inherent in this approach, that of assuming homogeneity across individuals perceptions.
Individuals may disagree on the perceived quality of a given product, or may even infer the overall value of a product on a different basis.
Relevant attributes may on the other hand be missing from the list of pre-defined attributes.
Approaches such as the Repertory Grid Technique typically emphasize the idiosyncratic nature of perception and evaluation of stimuli.
Individuals rate stimuli only on attributes that were elicited when they were asked to qualitatively differentiate between stimuli.
However, there is also a problem inherent in this approach.
As an analyst, one is confronted with as many idiosyncratic views as participants.
Views may overlap or even contradict one another; it is in any way complicated to systematically explore this diversity.
In practice, the consequence is either an idiosyncratic analysis with a "narrative" summarization or the use of average models.
Averaging, however, treats diversity among participants as error and thereby contradicts the basic idea of the underlying approaches.
In this paper we argued against averaging in the analysis of personal attribute judgments.
We illustrated that when using averaging only 1/6th of the attributes in our study, i.e.
A new MDS procedure that can better account for diversity in judgments was developed and its added value was illustrated through the reanalysis of published data.
The analysis resulted in three diverse views on the data which were directly compared to the average view that is the common practice in RGT studies.
We further illustrated that diversity exists not only across different individuals, but also within a single individual, in the sense that different attribute judgments of a subject may reveal different, complementary, views.
At any point in time individuals can have different, seemingly conflicting views.
For instance, individuals may regard one car as beautiful, but at the same time expensive.
Thus, being able to understand individuals' conflicting views is crucial for understanding how individuals infer the overall value of a product.
The proposed approach is a first step towards more exploratory procedures in the analysis of subjective judgments.
It is thus not free of limitations.
Firstly, the procedure that is currently used to assign attributes to different models is based on heuristics and not on an explicit optimization criterion.
Developing a more structured  approach is clearly one of our objectives for the future.
Once a set of latent attributes is however established, one could explore the relations among them and establish potential theoretical models of the product domain.
In this sense, one could perform exploratory path analysis which can be of significant value when limited theory exists in the field.
With this paper, we strongly advocate the view that the analysis of quality judgments of interactive products should not stop on a group level, but must be extended to the relations between the attribute judgments within an individual.
The Repertory Grid combined with the suggested technique to analyze the resulting quantitative data is an important step towards the adequate account of homogeneity and especially diversity in individual quality judgments.
Al-Azzawi, A., Frohlich, D., and Wilson, M., Beauty constructs for MP3 players.
Bech-Larsen, T. and Nielsen, N.A., A comparison of five elicitation techniques for elicitation of attributes of low involvement products.
Journal of Economic Psychology, 1999.
Breivik, E. and Supphellen, M., Elicitation of product attributes in an evaluation context: A comparison of three elicitation techniques.
Journal of Economic Psychology, 2003.
Collins, A.M. and Loftus, E.F., A spreading-activation theory of semantic processing.
Egger, F.N., " Trust me, I'm an online vendor": towards a model of trust for e-commerce system design.
Fallman, D., In Romance with the Materials of Mobile Interaction: A Phenomenological Approach to the Design of Mobile Information Technology.
11.Frokjaer, E., Hertzum, M., and Hornbaek, K., Measuring usability: are effectiveness, efficiency, and satisfaction really correlated?, in Proceedings of the SIGCHI conference on Human factors in computing systems.
12.Green, P.E., Carmone Jr., F.J., and Smith, S.M., Multidimensional Scaling, Concepts and Applications.
13.Hassenzahl, M. and Wessler, R., Capturing design space from a user perspective: The Repertory Grid Technique revisited.
International Journal of Human-Computer Interaction, 2000.
14.Hassenzahl, M. and Trautmann, T., Analysis of web sites with the repertory grid technique, in CHI '01 2001, ACM Press: Seattle, Washington.
15.Hassenzahl, M., Character Grid: a Simple Repertory Grid Technique for Web Site Analysis and Evaluation, in Human Factors and Web Development.
Lawrence Erlbaum, J. Ratner, Editor.
16.Hassenzahl, M., The interplay of beauty, goodness, and usability in interactive products.
17.Hassenzahl, M. and Sandweg, N., From mental effort to perceived usability: transforming experiences into summary assessments, in CHI '04 extended abstracts on Human factors in computing systems.
18.Hassenzahl, M. and Tractinsky, N., User experience - a research agenda.
Behaviour & Information Technology, 2006.
19.Hassenzahl, M., Aesthetics in interactive products: correlates and consequences of beauty, in Product Experience, Elsevier, Amsterdam, H.N.J.
Schifferstein and P. Hekkert, Editors.
20.Hassenzahl, M. and Ullrich, D., To do or not to do: Differences in user experience and retrospective judgments depending on the presence or absence of instrumental goals.
21.Heidecker, S. and Hassenzahl, M., Eine gruppenspezifische Repertory Grid Analyse der wahrgenommenen Attraktivitat von Universitatswebsites, in Mensch & Computer 2007: Konferenz fur interative und kooperative Medien, T.Gross, Editor.
22.Hinkin, T.R., A Review of Scale Development Practices in the Study of Organizations.
23.Jordan, P.W., Designing Pleasurable Products: An Introduction to New Human Factors.
24.Karapanos, E. and Martens, J.-B., Characterizing the Diversity in Users' Perceptions, in Human-Computer Interaction - INTERACT 2007.
25.Karapanos, E., Hassenzahl, M., and Martens, J.-B., User experience over time, in CHI '08 extended abstracts on Human factors in computing systems.
26.Karapanos, E. and Martens, J.-B., The quantitative side of the Repertory Grid Technique: some concerns, in in the proceedings of the workshop Now Let's Do It in Practice: User Experience Evaluation Methods in Product Development, Human factors in computing systems, CHI'08.
Exploring diversity in users' inferences in CHI'08 extended abstracts on Human factors in computing systems.
28.Karapanos, E., Zimmerman, J., Forlizzi, J., and Martens, J.-B., User Experience Over Time: An initial framework, in Proceedings of the Twenty-Seventh Annual SIGCHI Conference on Human Factors in Computing Systems CHI '09.
Keeping in touch with the family: home and away with the ASTRA awareness system.
30.Martens, J.-B., Image technology design: A perceptual approach.
2003, Boston: Kluwer Academic Publisher.
1957, Urbana, IL: University of Illinois Press.
32.Ramsay, J.O., MATFIT: A Fortran Subroutine for Comparing Two Matriced in a Subspace.
33.Sarstedt, M., A review of recent approaches for capturing heterogeneity in partial least squares path modelling.
Journal of Modelling in Management, 2008.
Behaviour & Information Technology, 2000.
36.van Kleef, E., van Trijp, H.C.M., and Luning, P., Consumer research in the early stages of new product development: a critical review of methods and techniques.
Food Quality and Preference, 2005.
