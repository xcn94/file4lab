Mountain View, CA, USA yangli@acm.org Redmond, WA, USA kenh@microsoft.com look at and press a relatively small target, will suffer performance degradation in non-ideal environments, while also potentially degrading performance of other concurrent tasks.
This implies that for some mobile tasks, soft buttons could be augmented with other techniques that demand less attention.
In this regard, direct touch gestures offer some possible advantages.
Some direct-touch gestures have the potential to be articulated eyes-free or with reduced visual monitoring, making them more resilient to distraction.
Gestures can be committed to muscle memory, which helps users focus on their task , and it is possible to articulate some gestures with one hand.
Gestures also require no dedicated screen space, which is a limited resource on mobile phones.
Even if gestures can be performed quickly and accurately with reduced attentional load in mobile environments, reliance on visual feedback is still a problem.
Audio may be sufficient for some tasks, even complex tasks such as scheduling meetings .
In other cases, feedback on a heads-up display may be sufficient, e.g.
Some cases may require little feedback, such as a gesture that is mapped to call a specific person.
In many situations, a quick glance may be all that is needed, e.g.
Thus, we believe there is significant value in reducing the attentional burden required by soft buttons.
Thus, the goal of this paper is to explore the design space of touch gestures on mobile devices, and evaluate these designs in multiple mobile environments that induce varying levels of situational impairment on users.
We induce situational impairment as two factors: motor activity and distraction level.
For motor activity, we examine sitting and walking.
For distraction level, we examine no distraction, a light situationalawareness distraction, and an attention-saturating distraction.
We explore two factors of gesture design: moding and gesture type.
Since touch often scrolls or pans by default, the user needs a robust method to enter gesture mode.
We explore several ways to indicate mode, including crossing through the screen bezel to integrate mode selection with the articulation of the gesture itself.
We also analyze two prevalent types of gestures: mark and free-form path gestures.
Thus this paper yields insights on a number of fundamental questions regarding touch gestures on mobile phones: - Which performs best in common mobile environments with varying levels of motor activity and distraction: soft buttons or gestures?
Direct-touch interaction on mobile phones revolves around screens that compete for visual attention with users realworld tasks and activities.
This paper investigates the impact of these situational impairments on touch-screen interaction.
We probe several design factors for touch-screen gestures, under various levels of environmental demands on attention, in comparison to the status-quo approach of soft buttons.
We find that in the presence of environmental distractions, gestures can offer significant performance gains and reduced attentional load, while performing as well as soft buttons when the users attention is focused on the phone.
In fact, the speed and accuracy of bezel gestures did not appear to be significantly affected by environment, and some gestures could be articulated eyes-free, with one hand.
Bezel-initiated gestures offered the fastest performance, and mark-based gestures were the most accurate.
Bezel-initiated marks therefore may offer a promising approach for mobile touch-screen interaction that is less demanding of the users attention.
Touch screens are growing rapidly in popularity as an input method for smart phones and other mobile devices .
These devices have few, if any, hard buttons; instead soft buttons are the dominant command invocation paradigm on commercial touch-based phones, and are effective when the user is seated and focusing directly on the phone, with no distractions .
However, users rely on mobile devices while sitting, walking, driving, and in diverse environments with various distraction levels .
When the user is in a non-ideal environment, such as walking through an airport looking for a currency exchange, she must navigate, maintain awareness of her location and avoid obstacles.
These situational impairments  are environmental factors that inhibit the ability to perform tasks on a device.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Can gestures be articulated efficiently one-handed?
Among the factors and conditions we consider here, what is the fastest combination of moding techniques and gesture type for mobile phones?
Would users like to have both soft buttons and gestures?
Our analyses of these questions contribute a fundamental body of knowledge about touch screen gestures on mobile phones that has not been adequately addressed by prior work.
A field study of two types of service workers concluded, as one of three design recommendations, that executing actions should not demand high visual attention .
In contrast, we focus on smaller mobile phones, using touch gestures - which necessitates different techniques and experimental design appropriate for mobile devices.
In addition,  asked users to wear the phone on the hip; while typical for media-players, we believe this is not representative of the common phone-usage scenarios in which the user holds a phone, which is our focus.
From this inspiration, we extend this work by exploring moding type, gesture type, distraction type, and motor activity, and by examining task performance with finer granularity, attentional load and eye gaze.
Lee and Zhai evaluated the hard and soft button performance on mobile devices with multiple types of feedback and input methods .
A study of one-handed soft button use found users could select small targets reliably and that walking did not affect speed or accuracy .
Marking menus  help users move from novice to expert by showing a spatially arranged hierarchical menu that uses compass-aligned marks; the menu is shown on a delay so that after a time users can simply perform the marks from memory.
This design inspires the mark-based gestures evaluated in this paper.
However, this requires a "transient mode": if the user has input 2 strokes and may input a third, the system must wait for some time threshold for a third mark.
We were concerned this might tempt users to look at the screen to ensure their marks had registered as intended; we thus used single-stroke marks, although we note that accuracy could be improved with simple marks.
A tilt-based marking menu  was explored on mobile phones, but task times were high - a 2-segment gesture required a mean of 3.9 - 4.7 s.
We examine four factors: moding technique, gesture type, users motor activity, and distraction level of the environment.
Below we discuss each factor in more detail.
This is not surprising since using both hands incurs the opportunity cost of the additional hand, and greater effort.
Given these advantages, we chose to focus on the single-hand scenario, and so all our techniques are designed to work well with a single hand.
Learnability of gestures is also an issue;  found it outperformed keyboard shortcuts.
Approachability is also a problem; it has been shown, however, that users of a GestureBar disclosure mechanism can use gestural UIs without prior training or experience .
Therefore we focus on expert performance, since such an approach could be adapted to disclose the gestures and moding techniques discussed here.
Technique Pressure Evaluation Rationale Li et al.
In addition, this variable is not available on standard capacitive touch screens.
Some devices, such as the Blackberry Storm , let the user bear down on the touch surface: exceeding a certain pressure threshold causes the surface to "click" in, much like a mouse button.
The click is used for picking, while swiping without clicking is used for scrolling.
This leaves dragging while the screen is clicked in, which has the same discomfort issue as pressure.
This technique therefore did not seem promising for touch.
Since chording cannot be done one-handed on the screen, we did not implement this approach.
Not tested for direct command invocation, as touch-based phones have few, if any, hard buttons.
Bezel Swipe  is a technique for scrolling and multiselection on mobile devices which defines sub-regions of the bezel for different command modes; the user can then cross these regions to enter a mode.
Manual Deskterity  uses a similar approach to create objects.
This work forms the basis of the bezel moding technique presented in this paper.
BlindSight  lets users perform interactive tasks on phones eyes-free while on a call, using physical buttons and audio cues, while  showed that gestures can be learned faster than keyboard shortcuts on desktop PCs.
This section summarizes the gesture-moding techniques explored.
Vibrotactile/audio feedback was not provided, as selection of command buttons on phones such as the Android/iPhone does not give feedback; also consistent with .
Since our study focuses on single-handed mobile devices use, we could not transfer this bimanual technique directly.
In pilot testing with 2 users performing the main experiment , we tested using a button on the side of the phone that could be pressed with the index finger while holding the phone.
However, users found this awkward since they had to brace the phone against their palm to counteract the force of the button press; they also had a strong tendency to look down at the phone "to make sure they didnt drop it."
We then tested switching modes by soft button; however, in pilot testing with 2 users they often missed the button, even after we made it quite large ; we also noted users often looked to ensure they hit the button.
The 14x14 mm button is made of a rubberized material that feels quite different from the glass phone bezel.
The hard button requires force to depress, creating a mechanical "click."
Without looking, users can rest and feel their finger on the button before pressing, and be confident that they have clicked it.
Once the button is clicked, users can draw a single-stroke gesture anywhere on the screen, at any scale.
On contact-up, it is recognized.
The user does not have to hold the button, but rather presses and releases it to enter gesture mode; the next stroke is treated as a gesture.
In pilot testing, users were able to comfortably reach the button, one-handed.
The user swipes through a bezel of the screen, setting gesture mode, continues by drawing a gesture, and finally releases contact to execute the command.
In a sense, bezel gestures combine a crossing interface  that has large crossing targets with a gestural interface.
On contact up, the stroke input is fed to the gesture recognizer, and the appropriate action is registered.
This heuristic worked quite well in pilot testing, and is the same used in .
The tactile feedback of touching the bezel confirms to users that they are contacting the bezel without having to look.
Bezel gestures include information on which of the 4 bezels was contacted , e.g.
The buttons are similar to and slightly larger than the numeric keypad soft buttons in the Android 2.1 Phone application .
Though smaller than an average finger, these buttons are typical in size of many smart phones, and we observed in pilot testing are easily pressed with the thumb, consistent with   .
Each button is white with a black single-letter label.
In most real-world toolbars and menus, commands are not listed in alphabetical order; mirroring this, we randomly chose 12 letters and assigned one to each button in random order .
We chose this toolbar/menu layout rather than a numeric keypad to model how most commands are selected on mobile phones, and also to mirror the unfamiliarity in button layout by the unfamiliarity of specific gestures in the gestural techniques .
As an example, in the iPhone web browser, while unrelated other than both are browser functions, the Switch Tabs and Bookmarks button are adjacent.
We paired the moding techniques with 2 types of path-based gestures frequent in the literature: rectilinear, mark-based gestures and free-form gestures.
We separate these two types here to better understand their performance characteristics; in a real application, both types could be used.
Gesture type also affects what type of recognizer can be used, which in turn may affect performance.
Although free-form path gestures are more complex than marks, our goal in testing was to identify design tradeoffs in a mobile context relative to the mark gestures, since they are used frequently in the literature.
As with soft buttons , we used a gesture set of size 12.
Mark-based gestures are simple and thus quick to execute and potentially tolerant of imprecision due to rapid execution.
To maximize recognition tolerance, we limit the mark-based gestures explored to only axis-aligned marks.
Our gesture set is comprised of the 12 marks shown in Fig.
We implemented a simple recognizer that examined the starting/ending point and bounding box of the gesture, combined with simple thresholds and heuristics to identify the gestures.
We added 3 gestures representative of the complexity used in prior studies, such as , so that the gesture set as a whole was distinct as to be robustly recognized.
Recognition was implemented using the built-in templatebased Android 2.1 OS recognizer , similar at a high level to .
We therefore chose 0.91 m/sec as the target speed.
Each of the moding techniques was crossed with the gesture types .
Hard-button-initiated gestures could be drawn anywhere.
Bezel path gestures could be drawn from any bezel.
Bezel mark gestures had to start from a specific bezel corresponding to the direction of the marks first segment .
We eliminated standing as in pilot testing with 2 users performing the experiment  we observed no major change in performance from sitting.
We felt using a mobile device while running is rarer than these other activities, and so eliminated it.
Perhaps one of the most common motor activities while using a mobile device is sitting: e.g., at the office, in a meeting, on a bus, while driving, etc.
We used 3 distraction levels, with varying attentional load.
This distraction level represents scenarios in which a user concentrates solely on operating the phone.
Command-invocation tasks were displayed on the phone screen in the bottom-left corner.
Thus, no eye movement is needed as the task and UI are displayed together.
This distraction level represents scenarios in which users must maintain some awareness of their environment, e.g.
We based the design of this task on successive sensory inspection sustained attention tasks .
Phone tasks were displayed on one half of the monitor, the distractor task displayed on the other.
A red circle appeared; users were asked to press and hold the spacebar of a keyboard with their non-dominant hand while the circle was visible.
The circle randomly appeared for 1 to 8 whole seconds, then disappeared for 1 to 8 whole seconds.
The relatively slow-changing but unpredictable nature of this task meant users spent most of their time watching the circle, but could glance down quickly at the phone with relative ease.
We implemented an attentionsaturating dual-task framework  in which users performed a continuous attention-saturating task  while simultaneously performing additional tasks on the phone.
The demand placed on attention by the phone tasks can then be measured as a drop in performance in the AST.
Like the situational awareness task, the AST was shown on one half of the external monitor with command tasks on the other.
As in , the goal of the attention-saturating task was to keep a moving circle centered on a fixed crosshair.
The user controlled the circle using an elastic tether connected to the mouse cursor, controlled via a desktop mouse.
This potential confounding factor is mitigated by the fact that performance on the AST task was normalized relative to baseline data , all users were right-handed, and the AST task was performed using the left hand for all techniques.
Eye gaze data was recorded using a video camera placed at a fixed distance recorded the users face including eye movements ; the videos were analyzed offline to the frame level to determine eye movement start and stop times.
The distance between phone and display was large enough to very reliably identify eye movements.
We did not fully cross motor activity and distraction because we found that some combinations were not representative of common scenarios .
Sitting+AST approximates an attention saturating experience, perhaps similar to driving; while talking or texting when driving is unsafe , users do issue commands on their phones while driving, e.g.
We did not explore Walking+AST since a user would not likely want to use a phone during such a scenario.
We also did not consider Walking+No Distraction as we believe that most users always maintain some level of environment awareness when walking and using a phone.
We advertised widely to get a sample of participants with diverse backgrounds and computers expertise; the average self-rated computer expertise was 5.06 on a 7point Likert scale where 4 was "Intermediate" and 7 was "Expert"; 4 users reported using a touch device regularly.
We used Motorola DROID smart phones running the Android 2.1 update 1 OS.
The phone communicated with a laptop  via network, outputting to a 17" XGA monitor for displaying the primary and distractor tasks where appropriate.
Participants held the phone in their right hand.
We used 2 phones, one with the added gesture hard button and one unmodified ; the weight difference between the 2 phones was negligible, and the button assembly did not affect how users held the phone.
Experimental software was Java-based, with one ap-
Our goal in this study was to measure expert performance, and so we sought to simulate an environment in which the user had memorized the gestures in question.
This approach is similar to , in which the users were asked to draw rectilinear marking menu gestures based on descriptions of the form "NE", which inherently indicate the sequence of marks required .
However, for our study, such acronyms do not include all necessary information, such as which bezel to start from, etc.
Feedback  was displayed on the screen  after each performance.
We used a repeated-measures within-participants experimental design.
After a pre-questionnaire, participants were introduced to the study; read a description of the first commandinvocation technique and shown a 40-second demonstration video.
Users were given a short demonstration and then asked to complete a series of tasks using the technique in the first environment: 2 training blocks followed by 6 measured blocks; each block contained each of the 12 commands once.
We felt 12 was a sufficient number of commands to be representative of a typical mobile application.
For environments involving a distractor task, participants were given 3 minutes to become accustomed to the distractor task in isolation before using the technique concurrently.
User in the indirect  environment.
Users were asked to hold the phone below the striped line .
User in the walking + discrete distraction environment.
Users were asked to hold the phone below the striped line .
Heads-up stimulus for discrete distractor task + markbased gestures , stimulus for attention saturating task + free-form gestures .
Condition order was counterbalanced with randomization.
After completing 5 techniques and before moving on to the next environment users had a 7-minute break.
Dependent variables included completion time, mode errors, command errors, and baseline and concurrent distractor task performance.
Cameras recorded users eye movements in context.
Participants were paid $15 per hour plus $0.25 for every 100 ms faster than 2,000 ms they completed tasks on average  for a given technique/environment combination while maintaining less than a 10% error rate average .
Users were notified when their average passed a 100-ms threshold and the accuracy was sufficiently high, with current average speed displayed in fractional seconds.
We implemented this performance-based compensation system to keep participants motivated, after observing in pilot testing that some participants lost interest in the tasks partway through the experiment.
Once complete, users filled-out a post-questionnaire.
We report mean completion time results in Fig.
Post-hoc comparisons were performed using 2-tailed t-tests, with Holms sequential Bonferroni adjustment  for multiple comparisons.
We began by looking at mean completion time by technique averaged across environments.
While this gives equal weight to each environment, which may not be representative of usage habits, it provides an overview of technique performance.
There was no significant performance difference between soft buttons and hard button marks mean of 1262 ms .
To explore the surprising result that bezel marks outperformed soft buttons, we made additional comparisons focused on each of the 4 environments.
In direct, without distractions, bezel marks had a mean completion time of 1022 ms, 6.82% lower than soft buttons 1097 ms.
There was no significant difference between bezel marks and soft buttons , surprising given that the user was not distracted and concentrated directly on the phone.
It is notable that bezel marks and soft buttons performed similarly in direct, and that with various distraction types, bezel marks significantly outperformed soft buttons in each case.
Effect of Environment on Performance.
We conducted further tests to see how the two best-performing techniques, bezel marks and soft buttons, were affected by environment.
We also examine bezel paths .
For soft buttons, there was a significant difference in performance between the direct and indirect environments  of 15.84%.
Interestingly, there was no significant difference for soft buttons between indirect and walk or indirect and AST .
This suggests that in terms of performance, the biggest effect for soft buttons came from not looking directly at the phone at all times.
For bezel marks, on the other hand, there was no significant difference in performance between direct and indirect, indirect and walk or indirect and AST , suggesting bezel marks were unaffected by not looking directly at the phone.
For walk and AST, bezel paths remained within 3.27% of indirect performance.
This was surprising given that vibration or distractions could be expected to affect free-form gesture performance.
The same post-hoc multiple comparisons adjustment procedure was used as above.
As above, to get a sense of the overall performance, we examine mean technique accuracy across environments .
There was no significant difference between hard button marks and bezel marks in accuracy, or between bez el marks and soft buttons .
Of the two moding approaches-bezel and hard button-there was 9.63% greater mode-in errors for hard buttons than for bezel; however, this was not significant .
From direct to indirect, soft button accuracy decreased by 2.88%, but this was not significant.
Differences in soft button accuracy between indirect/walk and indirect/AST were not significant .
Differences in mean accuracy for bezel marks between indirect/walk and indirect/AST were less than 1.16%.
Similarly, hard button marks performed consistently across environments; differences in accuracy between direct/indirect and between indirect/walk and AST, were less than 1.28%.
For bezel paths, from direct to indirect, accuracy decreased by 4.35%, but was not significant; from indirect to walk, the difference in accuracy was only 1.23%, and from indirect to AST the accuracy decreased by 3.45% but was not significant .
For hard button paths, from direct to indirect,
Hard button marks, bezel marks, and soft buttons were the most accurate.
For direct, accuracy differed by at most 0.8% and for indirect, walk and AST, at most 3.2%.
No significant differences were found for environment .
The path gesture-based techniques were consistently the least accurate.
This general trend continued, with hard button paths having 6.6% lower accuracy for indirect, 6.0% for walking, and 10.4% for AST.
Very similar trends were seen for bezel paths.
For walking, a similar pattern held, with the gesture techniques having mean performances within 3.3% of one another.
Soft buttons had the highest  mean normalized distance from the target , while bezel marks had the lowest mean , significantly lower than soft buttons  - Fig.
Hard button marks and bezel paths were within 9.7% of bezel marks.
Finally, hard button paths had a mean distance of 1.9, but this was not significantly different from bezel marks .
The relatively high normalized scores are likely caused by the continuous administration of command invocation tasks, which likely served as a greater distraction than occasional invocation.
Interestingly, there was no significant difference for glances between any of the gestural techniques .
The mean percentage of tasks requiring looks across environment for soft buttons was 98.8%; in contrast, the mean for gestural techniques was 3.5%.
Since users were free to look, it is clear that looking was required for soft buttons but essentially not needed for gestural techniques.
Looking was expensive, requiring 522 ms to finish looking down for soft buttons .
For direct, a majority  chose soft buttons as the preferred technique for performance; 4 chose hard button marks and 3 chose bezel marks.
For least preferred, 10 chose hard button paths, with the rest evenly distributed.
For indirect, 0 now preferred soft buttons; 7 of 15 users chose bezel marks, 7 chose hard button marks, and 1 chose bezel paths.
For their least preferred technique, 7 now chose soft buttons.
7 users wrote unprompted that a gesture-based technique allowed them to not have to look at the phone to perform commands: e.g.
This trend continued for walking, again 0 users preferred soft buttons; 10 users chose bezel marks, 4 chose hard button marks, and 1 chose bezel paths.
Finally, it further continued for AST, 9 users chose bezel marks and 6 chose hard button marks.
Despite the vibration of walking, users appeared to still prefer gestural techniques; " easy to draw rectilinear shapes while not looking at the phone."
For overall preferred technique, 7 chose bezel marks, 6 chose hard button marks, and 1 chose soft buttons.
This is a surprising result given the ubiquity of soft buttons.
When asked for their least favorite overall, free-form path gesture techniques dominated; 9 chose hard button paths and 4 chose bezel paths.
Users commented on the relative ease of making rectilinear marks over free-form paths; one user wrote, "simple straight lines and right angle turns are easier to make with high tolerance of error."
2 users felt pressing the hard button was an "extra step," but this did not appear to be universal.
225 ms; we attribute this primarily to user reaction time.
In addition, it is also notable that gesturing consistently begins sooner for bezel moding than hard-buttons.
It is also notable that users were able to consistently begin pressing the hard button before the bezel mode became engaged; however, given the cost of then repositioning the hand, users completed the gestures after the bezel conditions.
The gesture error rates of the path gesture techniques were higher than 10%, so an independent, single-blind human analysis of the recorded performances was done to test if the recognizer malfunctioned.
5% of path gestures performed were randomly sampled and then human-recognized with a specific "recipe" set up a priori2.
Comparison of the software to the human recognition found 3.5% false positives and 3.3% false negatives.
As even for humans 100% agreement is difficult to achieve, we feel this discrepancy is acceptable.
Specifically, we hypothesize that it is the nature of the thumb that makes it difficult to accurately perform gestures one-handed - indeed, accuracy rates likely would be higher with two-handed input.
We believe bezel marks were successful because the recognizer can be quite robust given the simple nature of the gestures.
We believe also that had participants performed the gestures more slowly they would have had a lower error rate.
We also note that the error rates for gestures were typically within 700 basis points of the soft button rates.
It is possible that with a different type of recognizer, performance could be improved.
The improvements were seen primarily for the gestural techniques.
After extended training , gesture performance could improve.
We believe this threat to validity is mitigated by the fact that the learning effect for soft buttons appeared to be limited, and gestural learning did not appear to be of sufficient magnitude to indicate the performance measured was not representative.
It is important to discuss how representative user performance was.
We feel that for this context, it is less likely, for many users, to fully memorize the location of toolbar button/menu items, the way they would for say QWERTY, and instead may perform a visual search coupled with adjacency cues from memory.
Looking at the soft button learning effect, this was minimal during the experiment, as expected.
We note mean soft-button speed recorded for direct  was less than other studies, such as approximately 500 ms from .
We attribute the difference to differences in methodology, e.g.
Which performs best: soft buttons or gestures?
Surprisingly, when users were looking directly at the phone there was no significant difference in completion time between soft buttons and bezel marks, probably because targets were not highlighted but needed to be identified from a letter caption.
We believe this is realistic, however, since soft buttons for executing commands typically have either a text or icon label.
In addition, bezel marks significantly outperformed soft buttons for the other three environments .
We hypothesize from the eye-gaze data that most of the performance benefit for gestures came from not having to look down at the phone.
How is attentional load affected by gestures?
Performance on the discrete distractor task and the AST were both significantly improved for bezel marks over soft buttons.
Hard button marks and bezel paths both performed similarly to bezel marks.
This result is excellent, showing that gestures can reduce users attentional load over soft buttons, even though gestures involve reproducing geometry with some degree of accuracy and soft buttons can be performed in ~1.2 s. Can gestures be made eyes-free in the above environments with sufficient accuracy?
The eye-gaze data indicates that gestures can be made eyes-free, requiring looking at the phone just 3.5% of the time.
In contrast, soft buttons required looking 98.8% of the time.
Can gestures be done one-handed effectively?
What is the fastest tested combination of moding technique and gesture type?
Bezel marks appear to be the fastest combination of moding techniques and gesture type, followed by hard button marks.
Overall, bezel marks were significantly faster than hard button marks across environments; they had no significant difference in accuracy.
Design Recommendations R1 R2 Gestural shortcuts/alternatives should be provided for soft button commands.
Users should be able to assign gestures to common action sequences, e.g.
The system could potentially identify such interaction patterns and automatically assign gestures to them.
Mark-based gestures are faster and more accurate than free-form gestures in all the mobile environments tested, so they should be used instead of free-form path gestures unless 2D operands are required.
We recommend bezel moding for design purposes as bezel marks have nearly identical performance to hard button marks; however, users preferred bezel marks.
For space-critical applications, gestures could be used to save screen real estate.
Because moded gestures are unlikely to be triggered by accident, they could be used to unlock the phone and execute a command, thus eliminating an extra step.
Bezel marks were preferred , and had the advantage of not requiring an additional button on the phone.
Interestingly, free-form path gestures performed significantly worse in both speed and accuracy.
Users made 168% more errors with free-form path gestures than with mark-based gestures.
Subsequent analysis of the gestures drawn indicates that virtually all these errors were due to user error, not a recognizer bug.
We hypothesize that it is harder to perform gestures accurately one-handed with the thumb than with two hands or a stylus, which was the root cause of the errors.
Users also preferred mark-based gestures to path-based.
It is notable that the specific free-form gestures used may affect performance.
However, we believe they are representative of prior research systems , and sufficiently unique to create a robustly recognizable set.
How do various mobile environments affect the speed and accuracy of gestures?
It was surprising that specific gestural conditions appeared to be unaffected by environment in speed or accuracy.
Bezel marks and bezel paths, for example had no significant difference from direct to indirect, indirect to walk, or indirect to AST.
Soft buttons, in contrast, were affected by environment, and had a significant increase in completion time from direct to indirect , after which completion time remained fairly steady.
The accuracy of soft buttons was unaffected by environment.
Interestingly, there was no significant difference in accuracy between soft buttons and bezel marks.
Would users like to have both soft buttons and gestures?
It was surprising that for direct usage, only half the users preferred soft buttons.
Furthermore, for the other 3 environments, 0 users chose soft buttons.
Bezel marks were most-preferred for walking and AST, and tied with hard button marks for indirect.
Users commented almost unanimously that gestures would be useful when not devoting full attention to the phone.
On gesture type, 9 commented it was difficult to perform freeform gestures; one user saying "too much thumb motion" was required, while users mentioned that rectilinear gestures helped them focus their attention on the circle centering task.
Two users felt pre ssing the hardware button slowed performance or felt like an "extra step."
A majority of users commented that it was easy to locate the hardware button and bezels without looking, while difficult to press soft buttons.
When asked if they would like more than one technique for different situations, 13 of 15 users chose more than one technique; just one chose soft buttons only.
Based on this and the quantitative performance results, we hypothesize that gesture-shortcut alternatives to command buttons on smart phones would improve performance and reduce attentional load.
Table 4  outlines our design recommendations.
Beyond visual comparison against a crib sheet of gestures, the verifier marked a gesture as unknown if >20% geometry was missing, >20% new geometry was added, if a feature was rotated/skewed by >50, or if a feature was scaled up or down by >60%.
The verifier was unaware of the software recognizers ratings.
The controlled, lab-based nature of our study limits the generality of the results.
Although we simulated several types of distractions and motor activities, these may not be fully representative of ecologically valid situations.
Prior experience  with touch screen devices may have affected performance; the results may not generalize to other populations.
Our compensation approach may limit the generality of results.
However, as remarked above, we found this was essential to prevent users from becoming "bored" and thus underperforming in speed/accuracy given the large number of trials.
We do not believe it sacrificed accuracy, since users were rewarded for both speed and accuracy, and because our soft button accuracy of 93.9% is close to the approx.
Freeform path gestures may have mnemonic advantages given their greater uniqueness.
Gestures are always available, whereas soft buttons must either consume screen space as a toolbar, or use an invocation procedure.
In this paper we explored the effect of mobile environments, modeled as motor activity and distraction level, on moded direct touch gestures.
The results of a formal experiment indicates direct touch gestures can produce on-par performance and accuracy with soft buttons when the user is focused on the phone, and improved performance, and reduced attentional load in the presence of environmental distractions.
Our results further indicate that bezel gestures did not appear to be affected by environment in speed or accuracy, and the gestures tested can be done effectively eyes free, with one hand.
We found bezel-initiated gestures to be fastest, and most-preferred by users.
We also found mark-based gestures were faster and more accurate to perform, and were preferred by users to free-form path gestures.
We believe, therefore, that bezel-initiated, mark-based gestural shortcuts should be provided for soft button commands on mobile devices.
We wish to thank Andries van Dam for his advice and insight, Alice Liu for her illustrations, and Robert Zeleznik, Zachary Kahn, Zachary Davis, Jeff Coady, and Max Salvas for their assistance.
This material is based upon work supported under a National Science Foundation Graduate Research Fellowship and by a Google Research Award.
Touch-Screen Shipments Expected to Reach 833 Million by 2013.
Brewster, S. Overcoming the lack of screen space on mobile computers.
When computers fade: Pervasive computing and situationally-induced impairments and disabilities.
Kurtenbach, G. The Design and Evaluation of Marking Menus.
Department of Computer Science, University of Toronto.
Li, K., Baudisch, P., and Hinckley, K. Blindsight: eyes-free access to mobile phones.
Designing a direct manipulation HUD interface for in-vehicle infotainment.
Experimental analysis of mode switching techniques in pen-based user interfaces.
Pirhonen, A., Brewster, S., and Holguin, C. Gestural and audio metaphors as a means of control for mobile devices.
Lee, S. and Zhai, S. The performance of touch screen soft buttons.
Perry, K. and Hourcade, J.
Evaluating one handed thumb tapping on mobile touchscreen devices.
How do people tap when walking?
An empirical investigation of nomadic data entry.
Getting Off the Treadmill: Evaluating Walking User Interfaces for Mobile Devices in Public Spaces.
Roth, V. and Turner, T. Bezel swipe: conflict-free scrolling and multiple selection on mobile touch screen devices.
Manual deskterity: an exploration of simultaneous pen + touch direct input.
MacKay, B., Dearman, D., Inkpen, K., and Watters, C. Walk 'n scroll: a comparison of software-based navigation techniques for different levels of mobility.
Appert, C. and Zhai, S. Using strokes as command shortcuts: cognitive benefits and toolkit support.
Zhao, S. and Balakrishnan, R. Simple vs. compound mark hierarchical marking menus.
A motion-based marking menu system.
Understanding One Handed Use of Mobile Devices.
In Handbook of Research on User Interface Design and Evaluation for Mobile Technology.
GestureBar: improving the approachability of gesture-based interfaces.
Accot, J. and Zhai, S. More than dotting the i's - foundations for crossing-based interfaces.
The Measure of Man and Woman.
Whitney Library of Design, New York, 1993.
Perry, K. B. and Hourcade, J. P. Evaluating One Handed Thumb Tapping on Mobile Touchscreen Devices.
Target size study for one-handed thumb use on small touchscreen devices.
ThumbSpace: Generalized One-Handed Input for Touchscreen-Based Mobile Devices.
Kurtenbach, G. and Buxton, W. The limits of expert performance using hierarchic marking menus.
Zeleznik, R., Bragdon, A., Liu, C., and Forsberg, A. Lineogrammer: creating diagrams by drawing.
Li, Y. Protractor: A Fast and Accurate Gesture Recognizer.
Field Studies of Pedestrian Walking Speed and Start-Up Time.
Engineering Psychology and Human Performance.
Harrison, C. and Hudson, S. Providing dynamically changeable physical buttons on a visual display.
Iqbal, S., Ju, Y., and Horvitz, E. Cars, calls, and cognition: investigating driving and divided attention.
Holm, S. A Simple Sequentially Rejective Multiple Test Procedure.
Pen-based interaction techniques for organizing material on an electronic whiteboard.
InkSeine: In Situ Search for Active Note Taking.
PapierCraft: A Gesture-Based Command System for Interactive Paper.
Gestures without Libraries, Toolkits or Training: A $1 Recognizer for User Interface Prototypes.
Lepinski, G., Grossman, T., and Fitzmaurice, G. The design and evaluation of multitouch marking menus.
