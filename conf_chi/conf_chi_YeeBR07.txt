The use of embodied agents, defined as visual human-like representations accompanying a computer interface, is becoming prevalent in applications ranging from educational software to advertisements.
In the current work, we assimilate previous empirical studies which compare interfaces with visually embodied agents to interfaces without agents, both using an informal, descriptive technique based on experimental results  as well as a formal statistical meta-analysis .
Results revealed significantly larger effect sizes when analyzing subjective responses  than when analyzing behavioral responses such as task performance and memory.
Furthermore, the effects of adding an agent to an interface are larger than the effects of animating an agent to behave more realistically.
However, the overall effect sizes were quite small .
We discuss the implications for both designers building interfaces as well as social scientists designing experiments to evaluate those interfaces.
Over the past decade, much empirical research has been dedicated towards examining the extent to which these embodied agents improve an interaction with an interface, beginning with an early landmark paper by Walker, Sproull, and Subramani .
Although many researchers have examined the presence and the type of embodied agents, there is little consensus as to whether or not the presence of visual agents improves a user's experience with an interface, and if so by what degree.
An early attempt to provide an overview of the literature was offered by Dehn and van Mulken , who descriptively examined eight independent studies in terms of the variables manipulated and the outcome measures.
They reported mixed results: adding an embodied agent to an interface made the experience more entertaining according to survey measures, but made no difference in the ratings of the quality of the agent.
In terms of behavioral responses, the few studies available at the time showed no consistent differences in a user's performance between the two types of interfaces.
The goal of the current paper is to use both informal  as well as formal  techniques to summarize the previous work which adds embodied agents to humancomputer interfaces.
Scholars examining human-computer interaction have often used meta-analyses to assimilate the research.
Previous papers presented at CHI have used both informal, descriptive techniques to explain the literature  and more formal statistical techniques as well .
This latter paper examined 39 studies that compared interviews either administered in person or via computer, and demonstrated that more personal disclosure occurred in front of a computer than in front of a live person.
Moreover, by examining the effects over time, their analyses traced the design changes in interfaces and provided practical implications for building new applications.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The current work seeks to provide a thorough review of the literature which has examined embodied agents as interface agents and to provide mathematical summaries of the extent to which embodied agents improve interfaces under various circumstances.
There is much controversy about what constitutes realism in an interface .
Clearly there are many dimensions on which an interface agent can be considered real--it can behave realistically through animations, it can be highly photographically realistic via computer graphics, or alternatively it can be highly humanlike .
In our meta-analysis, we were primarily interested in the presence of visual human representations, and the realism level of those representations.
In the current work, we excluded a small number of studies that specifically examined non-human interface agents such as animals .
Consequently, the current operationalization of realism is specifically defined as being more realistic on either the behavioral or photographic dimensions of realism .
Unfortunately, we did not have a high enough number of papers to conduct systematic comparisons of these two types of realism, because researchers most often manipulated both simultaneously.
Thus, we were primarily interested in the concept of realism rather than anthropomorphism .
There are reasons to assume that adding an embodied agent will improve an interaction with a user.
For example, Takeuchi and Naito  point out the embodied agent may draw the attention of the user and make him or her more engaged.
However, the danger of this addition is that it may distract the user from the very task on which the agent is supposedly aiding.
Clearly the design goal when adding an interface agent is to make the visual representation improve task performance, but there is no clear blueprint for creating embodied agents that keep users focused on the content of the interface.
In the current work we examined a large number of studies to determine whether, overall, embodied agents tend to augment, distract, or have no effect on task performance.
First of all, the inclusion of any visual representation seems to improve task performance when compared with not having a visual representation at all.
And secondly, animated agents with higher realism seem to lead to higher task performance than agents with lower realism.
Research in this area has primarily employed two kinds of task performance measures - subjective measures and behavioral measures.
Over the past decade, some researchers have criticized the validity of subjective measures, particularly questionnaires that are meant to measure the effectiveness of embodied agents and virtual humans.
For example, Slater  has shown that even meaningless questions can produce seemingly valid and reliable results when used to describe a virtual experience.
In other studies , large differences in behavioral measures, such as the amount of mutual gaze a person maintained with a virtual representation of another, were not reflected in self-report surveys of social presence.
And finally, subjective measures may provide an accessible outlet for what psychologists term demand characteristics, the phenomenon of experimental subjects conforming to or obstructing the study hypotheses, especially when research objectives become apparent to participants.
In other words, there is reason to believe that subjective measures and performance measures may produce very different results, even in the same study.
Thus, we could restate the abovementioned observed effects as more specific hypotheses: H1a.
The inclusion of any visual representation will lead to higher behavioral task performance measures than having no visual representation.
The inclusion of any visual representation will lead to higher subjective task performance measures than having no visual representation.
Animated agents with higher realism will lead to higher behavioral task performance measures than agents with lower realism.
Animated agents with higher realism will lead to higher subjective task performance measures than agents with lower realism.
The availability of both subjective and behavioral measures also allows us to explore whether subjective measures produced different results than behavioral measures, however, we did not have specific predictions for how the two may differ.
Thus, one research question was: RQ1.
Given the observed mismatches between subjective and behavioral measures in the literature, are there overall differences in effect sizes between these two kinds of dependent variables?
And finally, technology has changed rapidly over the past decade and social norms may differ between different cultures as new technologies are introduced.
Thus, we were also interested in whether study results were significantly influenced by their year of publication, location of study, or the equipment used.
Does year of publication have any impact on the findings of the study?
Does the location where the research was conducted have any impact on the findings of the study?
Do studies conducted in immersive virtual reality produce different results from those conducted in a desktop setting?
Of the 106 articles, 61 were discarded because they were outside the scope of the study , were theoretical articles with no empirical data, or were qualitative studies without quantitative measures.
Of the 46 papers remaining, 25 provided enough data to be included in the formal metaanalysis , while the other 17 did not provide enough statistics  to be included.
Although this appears to be a low number of studies, previous meta-analytic studies in computer-mediated communication have also tended to be based on only about a dozen useable studies .
With regard to study location, 13 were conducted in the US or Canada, 9 were performed in Europe, and the remaining 3 were conducted in Asia.
And finally, with regard to equipment used, 17 were conducted on desktop equipment, 6 were conducted using immersive virtual reality, and the remaining 2 were conducted on a large projected screen.
The studies considered for inclusion in this analysis were culled from bibliographic indexes related to the fields of psychology, computer-mediated communication , and virtual reality.
These included Expanded Academic ASAP, Google Scholar, Google keyword, PsycInfo, PsycArticles Fulltext Search, InterDok, ProQuest, and SearchPlus.
In this initial pass, articles that appeared to report an experimental study of anthropomorphism, embodied agents, or agent realism were collected and reviewed.
Sources were only considered if they were published in a peer-reviewed journal or in published conference proceedings.
This ensured a basic level of methodological and data integrity in the pool of included studies.
On the other hand, this potentially leads to a bias towards studies that showed results that were significantly different from the null hypothesis.
We will return to this issue again in the discussion section.
The literature review yielded 106 studies.
Several selection criteria were then applied.
First, an article was included only if it was an experimental study that manipulated the variables of interest and contained clear reports of quantitative data relating to the outcome of different conditions.
Thus, purely qualitative studies involving openended self-reports or observational user studies without quantitative coding schemes or dependent variables were removed.
In many cases, articles described experimental studies involving dependent variables, but did not report the statistics needed for the formal meta-analysis.
For example, a study might report the ANOVA F-value for the outcome of three conditions without reporting the means and standard deviations of the individual conditions.
In these cases, it would not be possible to generate an effect size value if we needed to compare one of the three conditions against another.
We discuss the details of the necessary statistics in more detail in the next section.
For each experimental study that clearly measured dependent variables but did not report specific statistics in the article,
To generate the necessary effect size tabulations in order to test our hypotheses, we tabulated several possible effect sizes for each paper depending on the available conditions.
First, we tabulated the results of performance data separately from the results of subjective data.
Performance data might include time to task completion, accuracy measures, or similar behavioral measures.
Subjective data, on the other hand, was any measure that was based on selfreport or survey data.
Second, we tabulated effect sizes based on two kinds of comparisons between conditions.
We wanted to be able to look at the effect of no representation against any degree of representation independently from the effect of low against high realism.
Also note that for studies which reported more than one experiment, effect sizes were calculated for each experiment separately and each experiment counted as its own case for the meta-analysis.
This tabulation process might be clearer with an example.
During the task, participants are timed for performance.
After the task, they are asked to judge the friendliness and appeal of the agent on a survey.
In this example, to generate the effect size of the performance differential of no representation against any degree of representation, we would collapse the cartoon and photorealistic face condition and compare the averaged score of the performance times against the score of the textual condition.
The other three possible tabulations are derived accordingly.
Of course, in many studies, the available conditions do not allow the generation of the effect sizes from all four comparison conditions.
For each study, we computed as many of the four comparison conditions as was possible.
We calculated these effect sizes based on formulas described in the widely referenced work by Rosenthal .
The effect size variable r is a measure of the impact that a manipulation has on a dependent variable.
Squaring this variable to get r2 shows the amount of variance in the dependent measure that can be accounted for by the manipulation.
For example, an r2 of .15 means that 15% of the variance in the dependent measure can be explained by a manipulation in realism .
For each study, we calculated an effect size r value for each possible comparison related to our hypotheses.
In a study with only two relevant conditions , it is possible to derive the r value from a t value along with the degrees of freedom.
If the t value is not reported, the availability of the means and standard deviations along with the number of cases in each condition would allow the derivation of the t value, and thus the r value.
In a study with multiple conditions, it is not possible to generate a relevant r value from an ANOVA F value because the omnibus F value doesn't test the specific comparisons of interest to us.
Thus, in these cases, we used the means and standard deviations, if reported, to generate the r value via a t value.
In many cases, a study may contain multiple dependent variables of interest.
For example, a study may use a variety of performance measures.
In these cases, we first calculated the r values independently for each measure.
As described in Rosenthal , the averaging of r values must be performed via a z transformation because the r distribution does not follow a normal distribution.
Thus, the individual r values are transformed into z values, averaged, and then the averaged z value is transformed back into the new aggregate r value.
The sign of each r value describes the positive or negative effect of the comparison and this was kept constant throughout the meta-analysis.
Thus, a positive r value signifies a positive increase in performance or subjective rating when comparing no representation against some form of representation, or low against high realism, while a negative r value signifies a decrease.
Finally, once all the effect sizes for each study had been calculated, we calculated an overall effect size for each of the four comparison conditions by converting the r values into z values.
We then averaged the z values after weighting them by the sample size of the study.
The averaged z value was then converted back into the overall r value.
The significance of an effect size is independent of the final r value.
For example, a collection of large sample studies may yield a highly significant, but small r value.
To tabulate the overall significance value, we converted the t values of each relevant comparison to a z value because the z distribution is normal.
We then calculated a significance z value for each effect size calculated as described in the above section .
In studies where multiple measures were used, we derived each z value from each t value individually before averaging the overall z value for that particular comparison.
For the aggregate significance level for each of the four comparison conditions, we used the Stouffer method , summing the z values for a given comparison condition , and then dividing by the square-root of the number of studies in that condition.
The results of the effect size and significance value aggregation are listed in Appendix A for each individual study and the overall values.
The overall effect sizes of the four comparison conditions ranged from -.04 to .14.
While three of the four comparison conditions were highly significant at p levels of less than .05, the comparison of high-low realism using performance measures was not significant, with p = .14.
We were also interested in whether the effect sizes in the studies varied as a function of other factors, such as whether a subjective or behavioral measure was used.
To this end, we carried out a series of contrasts on the set of effect sizes based on other factors.
In cases where a study had effect sizes from several comparison conditions, the effect sizes were first averaged according to the factors of interest.
Thus, each study only contributed at most once to each factor.
First, we compared the effect sizes based on subjective measures against those based on performance measures.
In other words, participants indicated larger differences via subjective selfreport than were observed via performance measures.
A comparison of these two groups of effect sizes revealed that the effect sizes from yes-no comparisons  were significantly larger than those from the high-low comparison , z = 2.43, p = .02.
We also compared the effect sizes of studies that were conducted on a desktop computer against those that were conducted in an immersive virtual reality environment.
The effect sizes of studies conducted in immersive virtual reality  was approaching significance on being larger than those conducted on a desktop computer , z = 1.85, p = .06.
We also compared studies published before the year 2000 and those afterwards as a crude measure of whether technological advances have impacted the results of studies in the area.
And finally, we conducted a series of contrasts to compare the effect sizes of studies conducted in North America, Europe, and Asia.
Again, we did not find a significant difference, p's > .10.
In this case, we would indicate that the high realism condition produced more positive results.
In Appendix B, we list these descriptive results from the available studies.
These descriptive results are striking in that they predominantly show that interface agents have positive results on users across all the comparison conditions implying more consistent and stronger effect sizes than we found in the formal meta-analysis.
We will discuss this disparity in greater detail in the next section.
The main comparison conditions in the formal metaanalysis produced several consistent findings.
First, the presence of a representation produced more positive social interactions than not having a representation.
This effect was found in studies that used both subjective and behavioral measures.
Secondly, human-like representations with higher realism produced more positive social interactions than representations with lower realism; however, this effect was only found when subjective measures were used.
Behavioral measures did not reveal a significant difference between representations of low and high realism.
In addition, we found several interesting differences via contrasts between studies with different features.
For example, effect sizes tended to be larger when subjective measures were used than when behavioral measures were used.
There are several potential explanations for this.
It may be because subjective measures are more sensitive than performance measures.
For example, task performance may be a less sensitive measure of attitudes towards an agent than a direct survey item.
However, the opposite has also been shown.
Differences in behaviors are sometimes undetected by direct survey items , thus the difference we found may also be driven by demand characteristics.
Participants interacting with an animated character  may suppose that the researcher is expecting a high appraisal.
Unfortunately, the data from our meta-analysis is unable to tease out these potential explanations.
It was also interesting that the effect sizes in the yes-no comparisons were larger than the effect sizes in the lowhigh comparisons.
This difference suggests that while the presence of a face is better than no face at all, the quality of the face matters much less.
One limitation to interpreting this difference , however, is the potential of confounding variables in digital renditions of human faces.
In other words, it is quite possible that animating highly realistic faces inherently allows for residual attributes of the faces that are negative--for example making 3D human faces may produce gestures and animations that appear unnatural or disturbing .
To provide a more thorough, albeit less rigorous, aggregation of the studies we found during the literature review, we decided to revisit the original data in the set of 46 selected papers and tabulate trends descriptively even if they did not provide the values necessary to make statistical comparisons.
The lack of two kinds of data prevented us from making the statistical comparisons for the formal meta-analysis.
For example, let us assume a data set that provided four pairs of dependent measures in one of our four comparison conditions.
In many cases, these four measures might vary in scale and also standard deviation.
Unless the paper reported standard deviations for those measures, it would be impossible to accurately aggregate them.
In some cases, papers did not even report the underlying range of scale points a measure was based on.
To work around these limitations, we devised a crude comparison technique.
For each pair of dependent measures in each of our comparison conditions, we simply counted the number of times each condition  had the better score.
We would then label the result with whichever condition had more tallies.
So in our example, consider the example in which we were working with the comparison of low versus high realism for subjective measures.
Thus, the difference we found may be magnified if our analyses could reflect this unexplored potential confound of further defining anthropomorphism or realism in this area of research.
Comparing the results from the formal meta-analysis to the descriptive informal table is illuminating.
When assimilating the general findings, more than three-quarters of the published papers in our sample indicated that interface agents have positive effects on users.
Given such an overwhelming majority, one might expect this to be an extremely large and consistent effect.
However, when conducting the formal meta-analysis, we see that the manipulation with the largest effects  accounts for less than three percent of all the variance across the studies.
Consequently, one must be cautious when generalizing from a large number of published studies without taking into account the effect size.
In other words, while most studies have found that interface agents have positive effects on task performance, these effects are overall actually quite small.
While we gave our best effort at being exhaustive in our literature review, it is of course highly likely that we failed to find and include other relevant studies in this area.
On the other hand, the studies in our meta-analysis are probably representative of the large majority of studies in this area.
A common critique of meta-analyses is the file drawer problem.
In fields where significant differences from hypothesized nulls are favored by journals, it is assumed that a great deal of null results go unpublished.
The fail-safe n is a measure of the number of such non-significant studies that would be needed to make our finding non-significant.
In our case, when examining the ratio of studies which found significant results, we computed the lowest fail-safe n to be 100 in any of the four comparison conditions.
Thus, this implies that our findings are likely to be stable given that the fail-safe n is about four times the number of published studies we were able to find.
Another potential concern with meta-analyses is that they combine studies with widely varying tasks and dependent measures, and thus it is not clear what aggregating them actually means.
Also note that the studies included are actually moderately similar - all of them ask participants to interact with an agent while some measure of task performance is being tracked.
In our meta-analysis, we were careful to examine methodological differences  that we felt might impact results across studies.
There are implications for designers and researchers that derive from our meta-analysis.
For designers, the metaanalysis makes it clear that a visual representation of an agent leads to more positive social interaction than not having a visual representation.
On the other hand, it appears that the realism of the embodied agent may matter very little.
For researchers, the differences between subjective and performance measures highlights the danger of interpreting results from only one type of measure.
For example, it is surprising that subjective measures of the high-low realism conditions show a highly significant effect while the performance measures show no effect at all.
As we've mentioned before, this may be due to the lower sensitivity of performance measures or due to demand characteristics in lab experiments.
Future studies should be careful to include both types of measures to further our understanding of this mismatch.
The authors would like to thank Julie Farrell, Alice Siu, and Zi Lin for their help in acquiring and coding relevant papers.
The authors would also like to thank Cliff Nass for his guidance on this project and Henriette Van Vugt for providing helpful comments on an earlier draft of this paper.
The current work was partially supported by NSF grant 0527377.
Detailed descriptions of how the individual studies were coded can be found at http://vhil.stanford.edu.
Bailenson, J., Blasovich, J., Beall, A., and Loomis, J., Equilibrium Theory Revisited: Mutual Gaze and Personal Space in Virtual Environments.
Bailenson, J., Beall, A., and Blasovich, J., Gaze and Task Performance in Shared Virtual Environments.
The Journal of Visualization and Computer Animation, 2002.
Bailenson, J., Blasovich, J., Beall, A., and Loomis, J., Interpersonal Distance in Immersive Virtual Environments.
Personality and Social Psychology Bulletin, 2003.
Bailenson, J., Swinth, K., Hoyt, C., Persky, S., Dimov, A., and Blascovich, J., The independent and interactive effects of embodied agent appearance and behavior on self-report, cognitive, and behavioral markers of copresence in Immersive Virtual Environments.
Presence: Teleoperators and Virtual Environments, 2005.
Bailenson, J. and Yee, N., A Longitudinal Study of Task Performance, Head Movements, Subjective Report, Simulator Sickness, and Transformed Social Interaction in Collaborative Virtual Environments.
Presence: Teleoperators and Virtual Environments, 2006.
Bartneck, C., How Convincing is Mr. Data's Smile: Affective Expressions of Machines.
User Modeling and User-Adapted Interaction, 2001.
Bente, G., Kramer, N., Petersen, A., and Ruiter, J., Computer Animated Movement and Person Perception: Methodological Advances in Nonverbal Behavior Research.
Journal of Nonverbal Behavior, 2001.
Beun, R-J., de Vos, E., and Witterman, C., Embodied conversational agents: Effects on memory performance and anthropomorphisation, in IVA 2003, Lecture Notes in Computer Science 2792, T. Rist, Editor.
Blascovich, J., Loomis, J., Beall, A., Swinth, K., Hoyt, C., and Bailenson, J., Immersive virtual environment technology as a methodological tool for social psychology.
The Role of Expectations in Human-Computer Interaction.
Burgoon, J., Bengtsson, B., Bonito, J., Ramirez, A.J., and Dunbar, N. Designing Interfaces to Maximize the Quality of Collaborative Work.
Burgoon, J., Bonito, J., Bengtsson, B., Cederberg, C., Lundeberg, M., and Allspach, L., Interactivity in Human-Computer Interaction: A Study of Credibility, Understanding, and Influence.
Computers in Human Behavior, 2000.
Cassell, J., Nudge Nudge Wink Wink: Elements of Face-to-Face Conversation for Embodied Conversational Agents, in Embodied Conversational Agents, J. Cassell, Editor.
Cassell, J., Towards a Model of Technology and Literacy Development: Story Listening Systems.
Journal of Applied Developmental Psychology, 2004.
Cowell, A.J., Increasing the Credibility of Anthropomorphic Computer Characters: The Effects of Manipulating Nonverbal Interaction Style and Demographic Emodiment.
2001, University of Central Florida: Orlando, Florida.
Dehn, D. and van Mulken, S., The impact of animated interface agents: a review of empirical research.
International Journal of Human Computer Studies, 2000.
Fabri, M., Moore, D., and Hobbs, D. Expressive Agents: Non-Verbal Communication in Collaborative Virtual Environments.
Gerhard, M., Moore, D., and Hobbs, D., Close Encounters of the Virtual Kind: Agents Simulating Copresence.
Guadagno, R., Blascovich, J., Bailenson, J., and McCall, C., Virtual Humans and Persuasion: The Effects of Agency and Behavioral Realism.
Gulz, A., Social Enrichment by Virtual Characters Differential Benefits.
Journal of Computer Assisted Learning, 2005.
Hess, T.J., Fuller, M.A., and Mathew, J., Involvement and Decision-Making Performance with a Decision Aid: The Influence of Social Multimedia, Gender, and Playfulness.
Journal of Management Information Systems, 2006.
Hongpaisanwiwat, C. and Lewis, M. Attentional Effect of Animated Character.
Hook, K., Persson, P., and Sjolinder, M., Evaluating Users' Experience of a Character-Enhanced Information Space.
Kiesler, S., Waters, K., and Sproull, L., A Prisoner's Dilemma Experiment on Cooperation with People and Human-Like Computers.
Journal of Personality and Social Psychology, 1996.
Koda, T. and Maes, P. Agents with Faces: The Effect of Personification of Agents.
Koda, T., User Reactions to Anthropmorphized Interfaces.
IEICE Transactions on Information and Systems, 2003.
Kramer, N. Social Communicative Effects of a Virtual Program Guide.
Kos, Greece: Springer-Verlag Berlin Heidelberg.
Lee, E. and Nass, C., Experimental Tests of Normative Group Influence and Representation Effects in Computer-Mediated Communication: When Interacting Via Computers Differs from Interacting with Computers.
Marti, S. and Schmandt, C. Physical Embodiments for Mobile Communication Agents.
McBreen, H. and Jack, M., Evaluating humanoid synthetic agents in E-retail applications.
McBreen, H. and Jack, M., Evaluating Humanoid Synthetic Agents in E-Retail Applications.
IEEE Transactions on Systems, Man, and Cybernetics - Part A: Stystems and Humans, 2001.
Moreno, R., Mayer, R., Spires, H., and Lester, J., The Case for Social Agency in Computer-Based Teaching: Do Students Learn More Deeply When They Interact With Animated Pedogogical Agents?
Mori, M., The Uncanny Valley.
Mosteller, F. and Bush, R., Selected quantitative techniques, in Handbook of social psychology: Vol.
Theory and method, G. Lindzey, Editor.
Moundridou, M. and Virvou, M., Evaluating the Persona Effect of an Interface Agent in an Intelligent Tutoring System.
Journal of Computer Assisted Learning, 2002.
Murano, P. Effectiveness of Mapping Human-Oriented Information to Feedback From a Software Interface.
Nass, C., Moon, Y., and Carney, P., Are respondents polite to computers?
Social desirability and direct responses to computers.
Journal of Applied Social Psychology, 1999.
Nowak, K. and Rauh, C., The influence of the avatar on online perceptions of anthropomorphism, androgyny, credibility, homophily, and attraction.
Journal of Computer-Mediated Communication, 2005.
Okonkwo, C. and Vassileva, J. Affective Pedagogical Agents and User Presuasion.
Oulasvirta, A. and Salovaara, A., A cognitive metaanalysis of design approaches to interruptions in intelligent environments.
Understanding the Effect of Life-Like Interface Agents Through Users' Eye Movements.
Prendinger, H., Mori, J., and Ishizuka, M., Using Human Physiology to Evaluate Subtle Expressivity of a Virtual Quizmaster in a Mathematical Game.
International Journal of Human-Computer Studies, 2005.
Qvarfordt, P., Jonsson, A., and Dahlback, N. The Role of Spoken Feedback in Experiencing Multimodal Interfaces as Human-like.
Rickenberg, R. and Reeves, B.
The Effects of Animated Characters on Anxiety, Task Performance, and Evaluations of User Interfaces.
Rosenthal, R., Meta-analytic procedures for social research.
Rosenthal, R. and DiMatteo, M., Meta-Analysis: Recent Developments in Quantitative Methods in Literature Reviews.
Annual Review of Psychology, 2001.
Schaumburg, H., Computers as Tools or as Social Actors?
International Journal of Cooperative Information Systems, 2001.
Slater, M., How colorful was your day?
Why questionnaires cannot assess presence in virtual environments.
Presence: Teleoperators and Virtual Environments, 2004.
Sproull, L., Subramani, R., Kiesler, S., Walker, J., and Waters, K., When the Interface Is a Face.
Takeuchi, A. and Naito, T., Situated Facial Displays: Towards Social Interaction.
Tomlinson, B., Yau, M., and Baumer, E. Embodied Mobile Agents.
The Persona Effect: How Substantial is It?
An Empiricial Study on the Trustworthiness of Life-Like Interface Agents.
Mahway, New Jersey: Lawrence Erlbaum Associates.
User engagement with task-related interface characters.
Interacting with Computers, in press.
Vertegaal, R. and Ding, Y.
Explaining Effects of Eye Gaze on Mediated Group Conversations: Amount or Synchronization.
Walker, J., Sproull, L., and Subramani, R., Using a Human Face in an Interface.
Walker, J., Sproull, L., and Subramani, R. Using a Human Face in an Interface.
Walther, J., Anderson, J., and Park, D., Inpersonal effects in computer-mediated interactions: A metaanalysis of social and antisocial communication.
Walther, J., Slovacek, C., and Tidwell, L., Is a Picture Worth a Thousand Words?
Photographic Images in Long-Term and Short-Term Computer-Mediated Communication.
Weisband, S. and Kiesler, S., Self-disclosure on computer forms: Meta-analysis and implications.
Wexelblat, A., Don't Make that Face: A Report on Anthropomorphizing an Interface.
Zanbaka, C., Goolkasian, P., and Hodges, L., Can a virtual cat persuade you?
The role of gender and realism in speaker persuasiveness.
