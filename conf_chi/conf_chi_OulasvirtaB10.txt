Most interactive tasks engage more than one of the user's exteroceptive senses and are therefore multimodal.
In real-world situations with multitasking and distractions, the key aspect of multimodality is not which modalities can be allocated to the interactive task but which are free to be allocated to something else.
We present the multimodal flexibility index , calculated from changes in users' performance induced by blocking of sensory modalities.
A high score indicates that the highest level of performance is achievable regardless of the modalities available and, conversely, a low score that performance will be severely hampered unless all modalities are allocated to the task.
Various derivatives describe unimodal and bimodal effects.
Results from a case study  illustrate how an interface that is superior to others in absolute terms is the worst from the multimodal flexibility perspective.
We discuss the suitability of MFI for evaluation of interactive prototypes.
In the development of "multimodal interfaces," researchers have traditionally devoted effort to the question of how to orchestrate sensorimotor capacities optimally for interaction with an interface.
Multimodality as viewed in this context could be termed "intra-interface multimodality."
In this paper, we turn the question upside down: what modalities are available to be allocated to tasks other than the current one the user is engaged in?
This question, of "extrainterface multimodality," is a timely one, particularly in the area of mobile HCI .
For example, if, during writing of a text message, something happens that causes distraction or reallocation of a sensory modality-- e.g., someone asks for directions, a cyclist suddenly approaches, or it is so cold that the fingers start freezing-- will you still be able to finish the message without significant costs to performance?
Our initial motivation to study this issue came from the casual observation that two interfaces that nominally involve the same sensory modalities may be very different in how well they allow modalities to be employed simultaneously for something else.
To address this, we operationalize the multimodal flexibility index  of a task as the average of performance changes measured over conditions in which the sensory modalities to be studied are blocked.
The magnitude of change in user performance caused by blocking is a quantitative indicator of a task's "dependency" on the blocked modality.
Intuitively, MFI denotes user ability to reach high performance despite modality withdrawals.
The index will be 1 when the highest level of performance is reached in all blocking conditions.
If the blocking of one modality decreases performance, the index also decreases.
Its value will be 0 if performance in all blocking conditions is at floor level.
This corresponds to the situation wherein the user must stop everything else and allocate modalities to the task, or cannot operate the system because a modality is not available.
Almost all human-computer interaction  situations engage more than one human sense and can therefore be considered multimodal.
In any given situation, some modalities are not engaged at all, some are but only more passively, and others must be actively deployed if the sensorimotor capacities are to be positioned optimally for action and feedback with the interface.
Even the seemingly simple act of pressing a button on a mobile device in fact involves coordination of multiple modalities :
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The method can be applied with two to an arbitrarily large number of modalities, though four is the practical maximum.
To study the method's usefulness and practical implementation, we carried out an experiment comparing three input interfaces for mobile devices  that nominally engage the same set of senses.
The results confirmed our initial observation and indicated that the interface that was the best in absolute performance was the worst from the multimodal flexibility viewpoint.
Derivative indices allowed us to analyze the situation in more detail, considering possible causes for the differences observed.
We conclude the paper by discussing the method's limitations and potential.
On the positive side, the method 1.
3. captures a wide range of outcomes in a single study offers a precise meaning for multimodal flexibility is reasonably cost-efficient--in our study, running one subject took about an hour and the blockings could be administered with inexpensive materials enables practitioners, who may not have time to work with theories to predict the outcomes of complex situations , to get quick feedback.
The logic of blocking is to emulate the situation wherein a resource is fully or partially allocated away or inoperable for some other reason, such as physiological changes.
This idea is rooted in cognitive psychological studies of cooperation and interference among cognitive faculties.
While the Gestalt psychologists had already employed the methods of subtraction and residues, an important systemization of the techniques was given in Posner's  book Chronometric Explorations of Mind, where he analyzed the time course of human information processing pathways.
Numerous papers have been published that utilize blocking to study bimodal performance.
Illustrating a typical blocking study, to examine the cooperation of modalities in object recognition , subjects were given a wooden object in one hand and asked to state whether its shape is the same as that of a slot that was touched with the other hand.
This was done in three conditions--visually only, haptically only, and bimodally with both--and it was found that bimodally performance was better than unimodally.
MFI, too, is based on observations of changes that are compared to non-manipulated  baselines.
The defining difference from psychology is why this is done: we are interested not in understanding the modalities themselves but in emulating the situation seen when real-world demands render modalities unable to be allocated fully.
We discuss opportunities for future work and good practices that can mitigate the most probable problems in applying the method.
Our current recommendation is to utilize the method in a comparative setting before committing to a particular design solution, and before one conducts more expensive field studies or embarks on cognitive modeling.
This method therefore complements the others available by offering developers a way to get quick feedback in early stages of design.
Similar ideas have been explored in many domains of HCI.
Six main categories of methods can be recognized.
The first two  are empirical and follow a logic similar to MFI's, the next two  aim at direct simulation of real-world situations, and the final ones  are analytical.
Visual occlusion : Visual occlusion involves temporary occlusion of the visual field--e.g., a driver must press a button to get a brief  glance at the road; the situations in which this is done reflect vision-critical moments of driving.
The method partly shares with MFI the idea of blocking human senses.
Secondary tasks that overload specific cognitive faculties: In this paradigm, an individual performs two tasks simultaneously, one of which is the main task and the other a task designed to selectively knock out or distract a particular cognitive subsystem.
Performance is compared to single-tasking baselines.
Typical secondary tasks include random number generation to overload the central executive, repetition of meaningless syllables to distract the auditory loop, and imagery tasks to distract the visual capacities of cognition.
The main task is argued to involve a particular cognitive subsystem if the corresponding overloading task significantly degrades performance.
In HCI, the term "multimodality" often refers to novel input techniques or communication channels .
In this paper, modality refers to human exteroceptive senses.
Because the method assumes no theory of what those modalities are or how they operate, we seek no further definition either-- although we do take a stance in the study.
Within this scope, we are interested in how freely the user can allocate a modality or its capacities away from the interactive task.
Technically, MFI is a method of residues: "Subduct from any phenomenon such part as is known by previous inductions to be the effect of certain antecedents, and the residue of the phenomenon is the effect of the remaining antecedents" .
The index is calculated on the basis of data collected from blockings of all combinations of the modalities under scrutiny.
Dual-tasking with natural secondary tasks, or laboratory analogues of them: This allows directly testing the consequences of a particular secondary task for target performance .
The difference from methods 1 and 2 is that they look at modality involvement irrespective of what the particular secondary task is; their results are supposed to generalize to any secondary task that recruits a particular resource.
Changing the modality of information display/access: In this extensively utilized paradigm, the human exteroceptive sense as such is not affected, only the output or input signal of the user interface  .
For example, Liu  compared visual-only, aural-only, and multimodal display of information during driving, and Burke et al.
In a variant of this method, multimodal support is removed entirely or changed adaptively .
In this paradigm, what is varied is the modalities that are simultaneously stimulated by the UI, while allocation of sensory modalities is not affected.
When the UI is the only source of stimulation of a particular sense, the manipulation is effectively the same as in the MFI method.
Task analysis: One may not need experiments to understand the roles that senses play in interactive tasks.
Oviatt  proposed using task analysis to reveal points where users are more likely to interact multimodally, and Baber and Mellor  proposed using critical path analysis for identifying constraints to cooperation of senses in a task.
Simple analysis can also be done with the Multiple Resource Theory of Wickens , which allows prediction of dual-task interference on the basis of four variables: stages , sensory modalities , codes , and channels .
If these methods are applicable in a given case and yield valid predictions, they may obviate the need to carry out experiments.
However, while we cannot claim that these methods could not have predicted the results of our study, we suspect that a difficulty would arise in the fact that the three interfaces we tested in our study are very similar from pre-empirical perspectives.
Cognitive modeling: Cognitive models such as ACT-R  and EPIC  have provided the richest description of multimodal flexibility, in the sense that they make empirically testable predictions of performance variables and shed light on the underlying processes.
Cognitive models can also be used for exploratory purposes, through charting of the space of possible interaction strategies for an interface with the related tradeoffs .
In view of the sophistication of cognitive models, why would one choose any other method?
First, cognitive models are criticized for having steep learning curves--they require expertise in model architectures, human cognition, and programming.
Second, present-day models may be insufficient for novel HCI situations.
The off-the-shelf models do not cover all aspects of modalities, for example, and the number of models of task domains available is limited.
Third, a practitioner applying these models may unwillingly have to take stances to controversial theoretical debates--such as whether interference arises from central bottleneck limitations or from graded capacity-sharing .
For practitioners, empirical methods have a role in providing initial understanding of a novel situation before one embarks on theory construction and modeling.
The calculation of MFI is based on performance scores obtained over all combinations of blockings.
Let us indicate with the numbers 1, 2, ..., n each modality under scrutiny and with M the set of modalities we are interested in; for example, when n=3, M={1,2,3}.
Now, the set of blocking conditions B we need for an experiment is B = P \ {M}; in other words, B is the power set of M, P, from which the none-blocked condition is removed.
With the none-blocked baseline, the total number of conditions needed for an experiment is 2n.
For example, were we interested in audition  and vision , we would need to run an experiment with four conditions: O, a, v, av .
Both none-blocked and all-blocked conditions are necessary for MFI.
For calculation of the index, the performance scores obtained are first normalized per interface for one subject by dividing every score of that user in a blocking condition by the none-blocked score .
By implication, the means in the none-blocked conditions are always 1 and the other normalized scores range from 0 to 1, where 0 is the floor level that indicates zero performance or failure in the task.
We use the notation sb to denote the performance score observed in blocking condition b.
For example, sav indicates a score recorded when modalities a and v were both available  while others  were blocked.
Now, the MFI is the mean of the scores in B, as follows.
The interpretation of this result is that, on average, reallocating a modality degraded performance to 23% of the maximum.
This formulation has intuitively appealing properties.
First, the index ranges from 0 to 1.
Second, the index is not determined by absolute performance .
Third, statistical testing can be performed on MFI.
Fourth, MFI generalizes to any n. Since five modalities already yields 25=32 conditions, any more than four  is impractical.
MFI delivers only a single number to describe a complex pattern, and one is likely to need further indices to detail the situation.
Below, we define a few indices that tie in with existing work in the field of multimodal performance.
There are examples of these six outcomes in the literature.
For example, there are numerous examples of intersensory facilitation .
In the above definitions, synergistic and additive effects can be considered special cases of the complementary effect.
The additive effect may be seen in cases where the user can fully allocate a supportive function to one modality but nothing else without simultaneously hampering the performance of the other modality.
Interchangeability may occur, when a task can be performed with either of two modalities but simultaneous attention to both modalities is not possible or does not improve performance.
If the interface worked perfectly, interchangeability could be observed.
The dominance effect has been suggested to apply for many tasks--for example, dominance of vision in driving .
Mutual distraction  occurs when the addition of a modality hampers performance.
One can, for example, consider the situation wherein poorly designed spatial sounds distract use of vision to the extent that blocking of audition improves performance.
This could also result in cases in which an attention shift in one modality causes a shift in another modality .
While the formula looks complex, the idea is simple.
Dm is interpreted as the average decrease in performance caused by the blocking of a modality.
One could calculate an index of independence by subtracting Dm from 1.
We can make one further demarcation based on Dm: We call an interactive task m-dependent, if Dm > 0.5.
In other words, the removal of m yields a drop of 50% in performance .
Using the subtraction method to estimate the importance of a modality is not a new idea in HCI .
Our contribution is to provide a general formulation for dependency and place it in the context of multimodal flexibility.
Our formulation requires the experiment to include the all-blocked condition, which, for example, Jacko et al.
Moreover, as was discussed in the Background section, our aim is not to estimate optimal modalities for feedback but to assess the robustness of performance under conditions where some modalities are not  available.
Our preliminary ideas are listed in Table 1.
Develop a dependent variable for performance of the main task that is reliable and sensitive.
Ensure comparable conditions, particularly the modalities and interface solutions in different blocking conditions.
The rest of the steps follow standard experimental procedures, with the following precautions: 5.
Decide on the level of statistical power desired and calculate the required sample size.
Design pre-trial instructions and practice so as to ensure that performance under blocking conditions does not overly reflect the novelty of the situation.
After running a pilot, execute the experiment.
After preprocessing the data to address outliers and missing data, normalize the scores and calculate the MFI and derivatives.
We chose mobile text input as the study domain, because mobile interaction is known to involve much multitasking  and there are efforts to develop interfaces that allow the user to better allocate modalities .
We decided to compare three input interfaces that nominally engage the same sensory modalities : 1.
Touchpad-QWERTY: The full-QWERTY touchpad of the Nokia XpressMusic 5800 Physical-ITU12: The ITU E.161 12-key telephone keypad of the Nokia E75 Physical-QWERTY: The full-QWERTY keyboard of the Nokia E75.
The collection of data for MFI takes place in an "analogy experiment": the task is carried out in as natural conditions as possible but with no external distractions.
Because comparison across tasks and interfaces is problematic , we recommend designing the study as a within-subjects comparative experiment.
Decide on the modalities that will be blocked.
Identification of candidates could be based on user observations or analytical work.
For implementing this blocking, we explored alternatives from local anesthesia to surgical gloves and silicone-covered fingertips used by clockmakers.
However, we ended developing a thin plastic layer attached to the keypads that prevents the user from feeling button releases and the edges of buttons .
While this blocking is imperfect, we were interested not in the absolute performance degradation it caused but in comparing the three interfaces.
Vision for locating the buttons and coordinating hand movements and for feedback on key presses from the display was blocked with cardboard that occluded the mobile device but did not occlude the line of sight to the task stimuli on a laptop screen .
Auditory feedback for button releases and the phone's key-press sounds was blocked by ear protection and by turning off auditory feedback from the device.
With the XpressMusic 5800, we used the touchpad-QWERTY interface with horizontal layout.
Levels of tactile feedback and key-press sounds were set to "high."
With the E75, the physical QWERTY keyboard and ITU keypad were used with loud key-press sounds.
The default audio and tactile feedback of these devices were used.
Predictive text entry was not allowed.
To block the vision, a piece of cardboard was placed under the subject's chin.
The subject was still able to maintain a natural sitting position in the chair.
The keyboard of the computer was covered with cardboard so that the subject could not check the QWERTY layout from it.
Hearing was blocked by turning the key-press sounds off, and hearing protectors  were employed to eliminate the feedback of the natural mechanical sounds.
We used a thin layer of plastic on the keypads that effectively blocked most tactile feedback; in the case of the 5800, also the tactile feedback feature was turned off.
We printed the keypads' layout and placed the letter-labeled printout on the plastic layer .
One caveat is that the outer edges of the device could still be felt, although individual keys could not.
Another was that the 5800 gives visual feedback on the keyboard: a key flashes when it is pressed.
The plastic layer occluded this feedback.
Twelve students were recruited for the study from a local technical university.
Their mean age was 22.8, with an age range of 21 to 26 years , and the sample was roughly even in gender terms .
As for usage experience, 11 were currently using an ITU keypad, seven with predictive text entry and four without.
One subject was using a physical QWERTY keyboard but was also experienced in using an ITU keypad.
Two subjects reported that they send fewer than 10 text messages per month, five reported 10-50, four between 50 and 100, and one over 100.
The task was made as similar as possible to that of writing a text message; real words and sentences were used.
The task was to type words as correctly as possible for 30 seconds.
For every task, five sentences were presented on the computer screen at the same time.
The phrases used were from a set of 500 sentences translated into Finnish , the subjects' native language, from the original set by MacKenzie and Soukoreff .
No special characters, punctuation marks, uppercase letters, or umlauts were used.
Because 30 seconds is too long for memory-based transcription, the sentences remained visible for the duration of the task.
Therefore, the task can be considered to involve copying rather than text generation.
The copying involved in the task potentially presents a form of multimodal task that differs from text generation: The user has to read the separate computer screen as well as attend to the mobile device.
The experimental design was an eight-by-three within-subjects design with blocking combinations as the first factor and input interface as the second.
In total, there were eight modality conditions: O, a, t, v, at, av, tv, and atv, with two trials performed in each.
Every subject thus completed 48 trials.
The order of the two factors was counterbalanced, by reversing for blockings and by rotating for interfaces.
With our decision to keep the none-blocked and all-blocked conditions at the end of a trial, the design yielded a minimum of n=12.
In the end, the placement of the all-blocked condition at the end was a slight mistake: Despite our attempts to minimize learning effects, users'
The experimental design and sample size were planned such that a small-to-medium effect size of 0.4 could be reliably captured for MFI and for D-values, with the aim of a power of 95%.
However, for the individual cells of the design, effect sizes would be lower--"medium," or about 0.6 to 0.7--because the comparisons would be based on fewer samples per user.
The subjects were trained to use each keypad via a three-task training set.
They were instructed to write the words as correctly as possible and to separate words and sentences with space characters.
Correction  was forbidden, to minimize variance due to strategic differences and to ensure comparability of blocking conditions.
Before every blocking combination, the subject had a chance to practice with the blocking.
When the subject was ready, the moderator made the set of sentences visible.
After 30 seconds, a red indicator flashed to mark the end of the time.
All trials were videotaped with a recorder placed on a 1.5 m tripod one meter to the right of the subject.
Figure 4 shows the situation.
With the data subjected to a planned comparison , ITU12 was shown to be distinct from touchpad-QWERTY  but not from physical-QWERTY , and the two QWERTY interfaces were not statistically different from each other .
As the performance variable we chose 80% correct words transcribed in 30 seconds, with the idea that 80% correct text messages would still be mostly understandable for the receiver.
Moreover, because of blocking of feedback  in vision-blocking conditions, 100% correct was not realistic.
Similar to the Levenshtein metric , the figure was calculated by subtracting the number of letter deletions, insertions, and reversals from each word's length and dividing the result by the presented word's length.
The D-values obtained are presented in Figure 5.
An ANOVA was run with interface and modality as factors, showing a significant interaction effect, F=6.8, p < .01.
All three interfaces were vision-dependent , but the ITU12 interface showed this effect the least.
A probable explanation is that the fingers get lost in the middle parts of the QWERTY keyboard when it cannot be seen, while the ITU12 layout is so simple that one can always infer the buttons on which the fingers are resting.
Audition in general was not influential, and adding the other modalities did not change performance.
Curiously, Da was negative for ITU12, which indicates that hearing auditory feedback decreased performance.
Some users were startled by auditory feedback  that they were not used to.
It may also be that the feedback latency is not optimal.
Regardless, the effect was small.
Hence, stv > st+sv, which constitutes a case of the two modalities being synergistic .
This makes sense, because the two modalities aid each other in the task of localizing the position of fingers on the keys and together enable a better "micro-strategy" : vision can be used to monitor feedback on the display and release the fingers to move toward the next buttons without the need to wait for button release.
The definition of MFI captures a wide range of phenomena that characterize multimodal flexibility beyond a single number, however.
For example, the text input study shows an interesting crossover: While ITU12 was the worst interface in terms of absolute performance, it was less hampered by the blockings we administered, within its performance range.
Its absolute performance was better than other interfaces' when vision was blocked.
The source of this advantage was the use of tactition to compensate for lack of vision.
Consequently, ITU12's performance was more dependent on tactition than the others'.
Tactition and vision were found to operate synergistically, boosting performance beyond the sum of the two single-modality conditions.
By contrast, auditory feedback was not successful: It did not increase performance when vision or tactition was blocked.
However, the interfaces may not have optimally used auditory feedback.
A fair criticism can be raised that blocking an exteroceptive sense is crude.
First, blocking a sense does not reflect the requirements of typical secondary tasks and environmental conditions, not all secondary tasks require full and uninterrupted allocation.
For example, one glance at the speedometer takes under 1.0 s and is enough to inform of driving speed.
Second, blocking in our study was an all-ornone business and did not leave room for the strategies users apply in allocating their modalities.
For example, switching back and forth among channels  is not possible and we would not be able to observe still finer "micro-strategies" .
An exception is tactition, which we blocked in a graded fashion--the thin plastic layer did not prevent feeling the edges of the device.
MFI can accommodate finer-grained manipulations , but the effects of "bandwidth" allowed by a blocking is a topic for future study.
Moreover, future work should address how the results obtained generalize to real-world tasks where modalities can be allocated in whole or in various combinations.
A third and related problem is that blocking may not reflect the real bottlenecks of multitasking, such as interference between tasks utilizing the same processing resources or codes .
These problems are real but can be addressed by two means: knowing when not to use MFI and choosing blockings carefully.
The method best suits the analysis of those modalities that are heavily competed for and can be allocated away or blocked for long periods of uninterrupted time.
One must accept that, for example, central interference is not addressed by the method.
It is noteworthy that blocking a sensory transduction channel from interaction restricts the usefulness of the method.
Because of this characteristic, the method does not suit the study of "intra-interface" multimodality--that is,
Because all subjects save one were experienced with ITU12 and physical-QWERTY, prior experience was not a predictive factor for these two interfaces.
By contrast, only five  had prior touchpad-QWERTY experience, and these users were, on average, more flexible with this interface than others were .
However, this difference was not statistically reliable .
Frequency of phone use  was not a reliable predictor either.
One curious finding was that the only heavy  user had the best mean MFI with all three interfaces  and a very high MFI for ITU12 , which he used as his main interface.
In human factors research, a legend was passed on from one paper to another for almost two decades.
According to the legend, 90% of all information used in driving is visual .
Later on, making such estimations was criticized as meaningless and operationally impossible .
Have we attempted something similar here, to propose quantification for the importance of a modality?
The answer is both "yes" and "no."
Had we done an MFI study of driving, we would have most likely found a very high Dv, but the interpretation would be very different.
This is one of the main interests of multimodal interface developers.
MFI data tell nothing of whether the modalities are used in a cascading  or concurrent  fashion in commanding the interface.
What the indices do indicate is which sensory modalities are available for something else--an "extra-interface" aspect of multimodality.
These discussions and our experiences from the study are summed up in a list of ten recommendations in Table 3.
Use the method when you suspect that a modality is important but do not know how important.
The indices answer the "how much" question, though they tell little about the why and what.
The method does not suit the study of "focal" modalities, whose blocking would take performance to the floor.
Understanding the relationship between possible blockings and the sensory systems is a topic for future study.
The choice of blockings is critical.
One should include in the study only modalities that are effective and competed for or challenged in real-world conditions.
No information is gained from studying a modality that can always be allocated or is completely passive.
Including ineffective blockings will boost the index and convey a falsely optimistic view.
Inspect data for instances wherein the blocking of a modality has improved performance.
The indices assume that performance decreases as a result of blocking.
This assumption may not always hold, especially where the UI's support for a modality is so poorly designed that blocking the corresponding sensory modality helps the user to achieve better performance.
Inspect the indices in light of absolute performance.
A high index can be an artifact of performance being at the floor level, which compresses the variability of performance and thereby increases the index.
Also, an exceptionally good/bad mean in one condition may pull the index up or push it down in relation to others.
Remember that the indices treat all modalities equally.
If blockings is not equally distractive, as in our study, where blocking of tactition was only partial in comparison to vision and audition, comparisons of absolute D-values is not recommended.
If there are a priori reasons for favoring a modality , scores can be weighted.
Understand that the indices are contingent on the particular task and the users' skill levels therein.
Inspect individual variation in the indices, for example, by examining such variables as skill, prior experience, and exposure.
The use of the indices in different types of tasks--closed-loop tasks , open-loop tasks, alarms, more complex cognitive activities, etc.--is a question for future research.
Avoid comparison of indices obtained in a different task, with different blockings, or with different dependent variables.
Mix in additional methods such as think-aloud, interviews, and video analysis to obtain qualitative understanding of the events that underlie the indices.
Interpret the indices as indicators of how flexible the user is for allocating modalities elsewhere.
Optimal dual-tasking in a concrete situation will be contingent on factors not visible from these indices.
Pursue other means for further study of the role of a modality in multitasking.
Most HCI situations engage more than one of the human sensory modalities.
We have presented and empirically investigated a generalization of the modality-blocking methodology in order to quantify an important aspect of multimodal user performance: how dependent the user's performance is on modalities being fully allocated to the task.
The method complements existing methods by providing a precise way of assessing this aspect of multimodality for a given interactive task in a way that allows comparisons of interface solutions.
We have presented an example study in mobile text input and discussed the limitations of the method, concluding that this method may be best suited to early-stage evaluations of interface solutions.
Future work should address the generalizability of indices to real-world HCI.
This work was funded by the Tekes project Theseus and by the Emil Aaltonen Foundation.
We thank Ville Nurmi, Miikka Miettinen, Tuomo Kujala, Lingyi Ma, Pertti Saariluoma, Tero Jokela, Saija Lemmela, Kimmo Rantala, Jari Laarni, Eve Hoggan, Poika Isokoski, Miika Silfverberg, Celine Coutrix, Heikki Summala, Johannes Tarkiainen, and Shamsi Iqbal for their help and comments.
A manual and a sheet for calculations are available at http://www.hiit.fi/mfi.
