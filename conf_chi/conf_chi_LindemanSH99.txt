This paper reports empirical results from a study into the use of 2D widgets in 3D immersive virtual environments.
Several researchers have proposed the use of 2D interaction techniques in 3D environments, however little empirical work has been done to test the usability of such approaches.
We present the results of two experiments conducted on low-level 2D manipulation tasks within an immersive virtual environment.
We empirically show that the addition of passive-haptic feedback for use in precise UI manipulation tasks can significantly increase user performance.
Furthermore, users prefer interfaces that provide a physical surface, and that allow them to work with interface widgets in the same visual field of view as the objects they are modifying.
The introduction of Virtual Environment  systems into mainstream computing has not been as rapid as researchers first projected.
Indeed, outside of the entertainment industry, most VE systems in use today remain in research labs and universities.
This paper presents empirical results from experiments designed to shed some light on effective user interface  techniques for Immersive Virtual Environments .
Permission to make digital or hard topics of all or part ot`this work i'ol personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute to lists.
In order to support symbolic interaction in 3-sp;ace, some IVE applications have abandoneddesktop interface devices for more freeform interface methods.
The latter use either the user's finger or some sort of laser-pointer, combined with a physical button-click, to manipulate widgets.
With these types of interfaces, however, it is difficult to perform precise movements, such as dragging a slider to a specified location, or selecting from a pick list.
Feiner et al describe an approach for using 2D windows in 3D worlds .
The system they describe is implemented for an augmented reality system, however the idea can be applied to immersive environments as well.
Feiner et al identify three different types of windows, differentiated by what the window is fixed to.
World-fixed windows  have an absolute, fixed position in the VE.
As the user moves or looks around, the world-fixed windows go out of, or come into, view, as if they were fixed in space.The second type.of window is a view-fixed window .
These windows move along with the user as they look around within the VE.
They remain at a fixed location, relative to the user's viewpoint, and may be suitable for manipulating systemwide attributes, such as the rendering method to use for objects .
The third type of window is an object-fixed window .
Each object-fixed window is fixed, relative to a specific object in the VE.
If the object moves, the window moves along with it.
These may be used to display and manipulate object attributes, such as to display the current velocity of an airplane, or to turn on a virtual lamp.
Deering uses hybrid 2D/3D menu widgets organized in a disk layout .
When invoked, the menu pops up in a fixed position relative to the current position of the tip of the wand.
Similarly, Sowizral  and Wloka et al  use menus that pop-up in the same location relative to a 6-DOF mouse, then use the mouse buttons to cycle through menu entries.
Each of these methods, however, provides limited user precision because of a lack of physical support for manipulations.
To counter this, some researchers have introduced the use of "pen-and-tablet" interfaces .
These approaches register an object-fixed window with a physical prop held in the non-dominant hand.
We call these handheld windows.
Users interact with them using either a finger, or a stylus held in the dominant hand.
These interfaces combine the power of 2D window interfaces with the necessary freedom provided by 3D interfaces.
There are many advantages to these approaches.
First, hand-held windows move along with the user, so they are always within reach.
Second, they do not clutter the user's view, unless explicitly moved there by the user.
Hand-held windows also take advantage of the proprioceptive sense, because they reside close to the non-dominant hand.
Finally, some systems using hand-held windows have incorporated a lightweight, physical surface that the user carries around, increasing precision .
Storing the physical surface when not in use can be an issue with these systems, and increased arm fatigue may degrade performance during prolonged use.
Most of the previous work in this field has called for detailed study into how these interfaces can most effectively be designed to enhance user performance.
Papers We have designed a number of empirical studies of user performance and preference on tasks which focus on these basic motions.
The results of two of these studies are presented here, and can be used to suggest how designers can develop general IVE interfaces that allow users to work efficiently.
Recent work in designing interfaces for immersive virtual environments attempts to apply 2D techniques to 3D worlds.
However, there is a dearth of empirical study into how best to implement these interfaces; indeed, most designs seem to arise horn simple intuition.
As has been done for desktop systems, we need to rigorously explore the different characteristics that make up these interfaces, in order to elicit optimal user performance.
Our work hopes to define and compare the characteristics that may be used to improve IVE interfaces.
The Haptic Augmented Reality Paddle  system is a testbed we have designed to take advantage of bimanual interaction, proprioception, and passive-haptic feedback .
This system allows us to compare many characteristics that may be helpful for IVE user interfaces.
The HARP system uses a 2D window, called the work surface, for displaying interface widgets.
The user selects widgets using the index finger of the dominant hand, as in , or a stylus, as in .
Finally, the work surface can be registered with a physical surface , or not .
Unlike others, our system does not provide support for a specific application, but rather serves as a testbed for comparing low-level interaction tasks.
Our research attempts to provide some guidelines for designers of IVE interfaces.
In order to better study UI interaction techniques, we can decompose user interaction into basic motions, using what Shneiderman calls Widget-Level decomposition .
This approach looks at the widgets that are defined in the system, and bases decomposition on their manipulation.
The testbed we have designed provides 2D widgets for testing typical UI tasks, such as drag-and-drop and button presses.
We define  two distinct types of actions bed on these widgets: discrete  actions and continuous  actions.
Discrete actions involve ballistic selection operations, such as clicking a toolbar icon, double clicking a filename, or positioning an input cursor.
Continuous actions include dragging sliders, using drag-and-drop to move a file, or accessing a cascading pull-down menu.
This section describes the experimental design used in the hrst empirical studies conducted with the HARP system testbed.
We use quantitative measures of proficiency, such as mean task completion time and mean accuracy, as well as qualitative measures, such as user preference, to compare the interfaces.
Two experiments, one involving an open-loop task, and one involving a closed-loop task, were administered.
In the interest of space, we present them together.
These experiments were designed using a 2 x 2 withinsubjects approach, with each axis representing one independent variable.
The first independent variable was whether the technique used hand-held  or world-fixed  windows, The second independent variable was the presence  or absence  of passive haptic feedback.
Four different interaction techniques  were implemented which combine these two independent variables into a 2 x 2 matrix, as shown in Table 1.
Each quadrant is defined as: HP = Hand-Held Window, with Passive-Haptics.
WP = World-Fixed Window, with Passive-Haptics.
HN = Hand-Held Window, No Haptics.
WN = World-Fixed Window, No Haptics.
For the HP treatment, subjects held a paddle-like object in the non-dominant hand , with the work surface defined to be the face of the paddle.
The rectangular work surface measured 23cm x 17cm .
The paddle handle radius was 2.8cm, and the handle length was 12.5cm.
Subjects could hold the paddle in any position that felt comfortable, but that allowed them to accomplish the tasks quickly and accurately.
Subjects were presented with a visual avatar of the paddle that matched exactly the physical paddle in dimension .
For the WP treatment, a panel with the same dimensions as the work surface of the HP treatment was mounted on a rigid floorstanding mounting frame in front of the dominant-hand side of the body of the subject.
The panel was mounted on a rigid Styrofoam box attached to the surface of the mounting frame.
Before the experiment began, each subject was asked at which height the panel should be mounted, and this remained fixed for the duration of the experiment.
Each subject was free to move the chair to a comfortable location before each task.
For the HN treatment, the subjects held only the handle of the paddle in the nondominant hand , while being presented with a full paddle avatar.
Again, subjects were free to hold the paddle in any position that allowed them to work quickly and accurately.
The WN treatment was exactly the same as WP, except that there was no physical panel mounted in front of the subject.
Using a diagram-balanced Latin squares approach, four different orderings of the treatments were defined, and subjects were assigned at random to one of the four orderings.
We had each subject perform 20 trials on two separate tasks for each treatment.
Four different random orderings for the 20 trials were used.
The subjects were seated during the entire experiment.
Each subject performed two tasks  using the treatments.
Task one was a docking task.
Subjects were presented with a colored shape on the work surface, and had to slide it to a black outline of the same shape in a different location on the work surface, and then release it .
Subjects could repeatedly adjust the location of the shape until they were satisfied with its proximity to the outline shape, and then move on to the next trial by pressing a "Continue" button, displayed in the center at the lower edge of the work surface.
This task was designed to test the component UI action of "Drag-and-Drop," which is a continuous task.
The trials were a mix.
If the target location for one trial was close to the start position of the next trial, and subjects were not required to begin the trial at a home position, then they could acquire the shape for the next trial more quickly than for trials where the target and start position for successive trials were further apart.
Finally, this gave a clear cut event which signaled the end of one trial and the start of the next, which is necessary for timing purposes.
The second task was a shape selection task.
For this task, a signpost was displayed in the VE , upon which one shape was chosen at random to be displayed.
For the right-handed subjects, the signpost was positioned in front and to the left of the subject.
For the left-handed subjects, it was positioned in front and to the right of the subject.
In addition, four shapes were arranged horizontally on the work surface, one of which matched the shape and color of the one on the signpost.
The subject had to select the shape that matched the one on the signpost, and then press the "Continue" button to move on to the next trial.
The subject could change the selection before moving to the next trial.
This task was designed to test the component UI action of "Button Press," which is a discrete task.
Five different shapes were used for these experiments: a circle, a square, a diamond, a triangle, and a five-pointed star.
In addition, each shape could appear in any one of three colors: red, green, or blue.
The bounding box used for intersection testing was the same for all shapes, so the only difference was their shape in the VE; each one was as easy to select as every other one.
Subjects selected shapes simply by moving the fingertip of their dominant-hand index finger to intersect the shape.
A shape was released by moving the finger away from the shape, so that the fingertip no longer intersected it.
For movable shapes , this required the subject to lift  the fingertip so that it no longer intersected the virtual work surface, as moving the finger tip along the plane of the work surface translated the shape along with the fingertip.
For immovable objects , the subjects were free to move the fingertip in any direction in order to release the object.
Once the fingertip left the bounding box of the shape, the shape was considered released.
The HARP software was running on a two-processor SiliconGraphics  Onyx workstation equipped with a R8000 processors, 64 megabytes of RAM, and 4 megabytes of texture RAM.
Because of a lack of audio support on the Onyx, audio feedback software  was run on an SGI Indy workstation, and communicated with the HARP system over Ethernet.
The video came from the Onyx, while the audio came from the Indy.
We used a Virtual I/O i-glasses HMD to display the video and audio, with a Logitech ultrasonic tracker mounted on the front to track 6-DOF head motion.
For the index-finger and paddle, we used an Ascension Flock-of-Birds magnetic tracker.
The mounting stand for the panel was constructed using only wood and PVC tubing, so as to avoid introducing noise to the magnetic trackers.
The work space was calibrated once, and the computed values were used for all subsequent runs of the software.
All the software ran in one Unix thread.
A minimum of 11 frames per second  and a maximum of 16 FPS were maintained throughout the tests, with the average being 14 FPS.
First, this provided a clear distinction for when the trial was over.
Subjects had to actively signal that they were through with the trial, so mistakes could be avoided because they could make adjustments before continuing on to the next trial.
A total of 32 unpaid subjects were selected on a first-come, first-served basis, in response to a call for subjects.
Most of the subjects were college students , either undergraduate  or graduate .
The rest  were not students.
The mean age of the subjects was 27 years, 5 months.
In all, 30 of the subjects reported they used a computer with a mouse at least 10 hours per week, with 22 reporting computer usage exceeding 30 hours per week.
Three subjects reported that they used their left hand for writing.
15 of the subjects were female and 17 were male.
19 subjects said they had experienced some kind of "Virtual Reality" before.
Each subject passed a test for colorblindness.
15 subjects reported having suffered from motion sickness at some time in their lives, when asked prior to the experiment.
At the beginning of the first task, the subject was instructed to move their dominant hand into the field of view, and that they would see the hand avatar .
After moving their hand around for a few moments to get used to the mapping of hand movements to avatar movements, for the H treatments they were then asked to hold out their non-dominant hand, into which the paddle was placed, and they were given a few moments to get used to its movement.
For the W treatments, it was pointed out that the panel in front of them was the panel that had been described in the introduction.
Each subject signed an "Informed Consent for Human Subjects" form, and was given a copy to keep.
Before beginning the actual experiment, demographic information was collected.
The user was then fitted with the dominanthand index finger tracker, and asked to adjust it so that it fit snugly.
The user then chose between two different heights for the mounting position of the world-fixed work surface.
Six subjects chose to use the higher mounting location of the panel  and 26 chose the lower position .
The subjects were free to move the chair forward or back during the experiment.
The chair surface was 46cm from the floor.
Each subject was read a general introduction to the experiment, explaining what the user would see in the virtual environment, which techniques they could use to manipulate the shapes in the environment, how the paddle and dominant-hand avatars mimicked the motions of the subject's hands, and how the HMD worked.
After fitting the subject with the HMD, the software was started, the visuals would appear, and the audio emitted two sounds.
The subjects were asked if they heard the sounds at the start of each task.
To help subjects orient themselves, they were asked to look at certain virtual objects placed in specific locations within the VE.
Subjects were told mat if they turned their head to the left, they should see a blue cube, and the same for the green cone to the right.
The subjects' location within the VE was such that they were in the center of a horizontal plane, texture-mapped with a beige, repeating pattern.
Above the subject was a sky plane, which was texture-mapped with a blue sky and clouds.
The subject was told to look up to see the blue sky, and to look down to see the patterned ground.
The work surface displayed the message, `To begin the first trial, press the "Begin" button.'
Subjects were asked to press the "Begin" button on the work surface by touching it with their finger.
Five practice trials were then given, during which subjects were read a verbal description of the task they had to perform within the IVE.
Each subject was coached as to how best to manipulate the shapes for each specific treatment.
After the practice trials, the subject was asked to take a brief rest, and was told that when ready, 20 more trials would be given, and would be scored in terms of both time and accuracy.
It was made clear to the subjects that neither time nor accuracy was more important than the other, and that they should try to strike a balance between the two.
Trial time for both tasks was measured as the total time between successive presses of the "Continue" button.
Accuracy for the docking task was measured by how close the center of the shape was placed to the center of the target position, and for the selection task, accuracy was simply whether the correct shape was selected from among the four choices.
After each treatment, the HMD was removed, the paddle was taken away , and the subject was allowed to relax as long as they wanted to before beginning the next treatment.
In addition to visual and haptic feedback, the HARP system provided other cues for the subject, regardlessof treatment.
Fist, the tip of the index finger of the dominant-hand avatar was colored yellow .
Second, in order to simulate a shadow of the dominant hand, a red drop-cursor, which followed the movement of the fingertip in relation to the plane of the paddle surface, was displayed on the work surface .
The location of the drop-cursor was determined by dropping a perpendicular from the fingertip to the work surface, and drawing the cursor centered at that location.
When the fingertip was not in the spacedirectly in front of the work surface, no cursor was displayed.
To help the subjects gaugewhen the fingertip was intersecting UI widgets, each widget becamehighlighted, and an audible CLICK!
When the user released the widget, it returned to its normal color, and a different UNCLICK!
In order to produce an overall measure of subject preference for the four treatments, we have computed a composite value Tom the qualitative data.
This measureis computed by averaging each of the Likert values from the four questions posed after each treatment.
Because "positive" responsesfor the four characteristics were given higher numbers, on a scale between one and five, the average of the ease-of-use,arm fatigue, eye fatigue, and motion sickness questions gives us an overall measure of preference.
A score of 1 would signify a lower preference than a score of 5.
Table 2 shows the mean values for each question, as well as the composite value, for all four treatments.
Qualitative data was collected for each treatment using a questionnaire.
Four questions, arranged on Likert scales, were administered to gather data on perceived ease-of-use, arm fatigue, eye fatigue, and motion sickness,respectively.
The questionnaire was administered after each treatment.
Quantitative data was collected by the software for each trial of each task.
This data varied for the two tasks.
For the docking task, the start position, target position, and final position of the shapeswere recorded.
In addition, the total trial time and the number of times the subject selected and releasedthe shapefor each trial was recorded.
For the selection task, the total trial time, number of selections made for each trial, the correct answer, and the answer given by the subject were recorded.
Figure 7 shows a box plot of the trial times by treatment, where the boxes represent the middle 50% of the values, the thick line represents the median, and the whiskers represent lines to the highest and lowest values.
A comparison of the means tells us that subjects had particular manipulation problems with hand-held windows when no passive-haptic feedback was present.
On the selection task, which required looking around the IVE, subjects performed 21% faster using hand-fixed as opposedto world-fixed windows.
Thesequantitative findings are in line with our qualitative results.
Users prefer interfaces that allow them to work efficiently and effectively.
The use of passive-haptic feedback,coupled with a hand-held device, can greatly aid interaction in immersive virtual environments.
During our analysis, we found that some learning effects were present.
Specifically, for the docking task, HN and WN trial times improved ,and accuracy increased over time, while HP and WP stayed fairly constant.
All treatmentsshowed a general improvement trend over time on the selection task.
Our results show that the addition of passive-haptic feedback for use in precise UI manipulation tasks can significantly increase user performance.
In addition, users prefer interfaces that provide a physical surface, and that allow them to work with UI widgets in the same visual field of view as the objects they are modifying.
In our work, we have tried to provide data to help IVE designers produce interfaces that allow users to perform real work.
Becauseof the complexity of user interaction in IVES, much work still needs to be done.
We have shown that the HARP testbed is an effective environment for performing future studies.
We will look at ways of improving non-haptic interfaces for those systemswhere it is impractical to provide passive-haptic feedback.
Possible modifications include the use of 3D representations of widgets instead of 2D representations, and the imposition of simulated physical surface constraints by clamping user movement to the virtual surface of the paddle.
Also, we would like to explore other component interaction techniques, such as cascading menus, within the HARP testbed.
