Analysis is a key part of conducting usability evaluations, yet rarely systematically studied.
Thus, we lack direction on how to do research on supporting practitioners' analysis and lose an opportunity for practitioners to learn from each other.
We have surveyed 155 usability practitioners on the analysis in their latest usability evaluation.
Analysis is typically flexible and light-weight.
At the same time, practitioners see a need to strengthen reliability in evaluation.
Redesign is closely integrated with analysis; more than half of the respondents provide visual redesign suggestions in their evaluation deliverables.
Analysis support from academic research, including tools, forms and structured formats, does not seem to have direct impact on analysis practice.
We provide six recommendations for future research to better support analysis.
At the same time, introductory texts provide only high-level descriptions of analysis in usability evaluation.
Texts on usability testing , as well as on heuristic evaluation , lack detail on how to do analysis.
The lack of knowledge on analysis implies that researchers who develop methods and tools to support analysis in usability evaluation will find it hard to meet practitioners' needs.
In turn, this may impede the transfer of methods and tools from research to practice   and lead to a gap between research and practice .
This paper presents a survey of usability practitioners' analysis in their latest usability evaluation.
We present data on how analysis is supported, how usability problems are identified, and how evaluation and redesign are integrated.
Usability researchers may benefit by improved knowledge on what practitioners need from future methods and tools for analysis.
Also, we hope that practitioners can learn from insights into the state of the practice, and use these in their professional development.
Analysis in usability evaluation is challenging.
Analysis is the process by which observations of users or inspections of interfaces are turned into prioritized, coherent descriptions of usability problems, including descriptions of causes, implications, and potential solutions .
Doing analysis is described as "the ultimate detective work" , involving multiple data sources , and highly dependent on practitioners' knowledge and expertise .
Despite these well-known challenges in analysis, there is a surprising lack of research on analysis practices.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In spite of the lack of research-based knowledge of analysis practices, in the last two decades research has been conducted to develop methods and tools for analysis in usability evaluation.
Analysis process improvements have been suggested both as light-weight approaches, such as instant data analysis upon conclusion of usability testing sessions , and as formal procedures and structures, such as the user action framework   and the framework for structured usability problems extraction  .
Concern has been voiced regarding the usability and learnability of the latter .
Problem description formats have been developed to facilitate usability problem merging, either as templates , guidelines , or integrated into software tools .
Research-based analysis procedures and problem description formats include guidelines or strategies for problem identification.
These strategies are mainly taskoriented, using task performance as the key indicator for usability problems.
However, user behavior and verbalization are acknowledged as separate data sources to complement task performance data.
Tools for problem identification and consolidation have been developed in the research communities as well as in industries.
Research-based tools include software tools such as the Usability Problem Inspector , and simple conceptual tools such as Skov and Stage's tool to support problem identification and prioritization .
Commercial video analysis software such as Morae  includes facilities for usability problem description and grouping.
Howarth, SmithJackson and Hartson provided evidence that novice analysts may benefit from having structured formats for problem description included in such software .
To the best of our knowledge, only one prior study concern analysis practice in usability evaluation .
The study was presented as a pilot, and was based on interviews with 11 usability practitioners.
It concluded that  analysis is informal and pragmatic with little use of structured formats,  professional experience is the most important analysis resource while resources like standards and guidelines are less important,  redesign suggestions seem to be an integrated part of analysis, and  collaboration on analysis is frequent, but mainly used to help identify more usability problems and redesign suggestions, rather than to reduce evaluator bias.
Other studies related to analysis practices are Boren and Ramey's  and Norgaard and Hornbaek's  observational studies of usability practitioners during thinkaloud testing sessions.
These studies, however, offer only indirect data on analysis  or no data at all .
Previous research has provided methods and tools, knowledge on collaboration benefits, and perspectives on the relationship between evaluation and design.
However, we lack knowledge about analysis practices and, consequently, knowledge about how the research-based methods and tools support such practices.
Our research questions aim to generate such knowledge.
The main question is: What is the state of practice of analysis in usability evaluation?
Based on this question and the related work, four subquestions are asked: Q1.
How are usability problems identified?
How do usability practitioners collaborate in analysis?
How is redesign integrated into the evaluation process?
As analysis practices are expected to vary substantially among practitioners, it is important to get a broad sample of respondents.
Consequently, we employed a web-based questionnaire survey.
We regarded this methodological approach as adequate given that this study was to complement the existing knowledge generated through observation  and qualitative interviews .
Our choice of method is in line with previous survey studies of usability practices .
The survey respondents were required to be usability practitioners who had conducted a usability evaluation within the last six months.
The research literature clearly shows the importance of collaboration between usability practitioners in evaluation.
The use of multiple evaluators in problem identification and analysis has been found useful for improving the thoroughness  and reliability  of problem identification; that is increasing the likelihood that all real problems are found and improving consistency in analysis.
To serve the intended purpose, collaboration typically happens among two or more usability practitioners who perform independent analysis of the same data set .
There is an ongoing debate in usability research on the relation between evaluation and design .
Some suggest an integration of evaluation and redesign, as in Rapid Iterative Testing and Evaluation , where fixes to usability problems with "an obvious cause and an obvious solution" are encouraged as part of the usability evaluation procedure , or by including ideas or redesign suggestions in the findings from a usability evaluation .
Others argue against such integration of evaluation and redesign.
It is held that the findings from a usability evaluation should only drive the next iteration in the development process, requiring a "grounding of  in business goals, usage contexts, design principles, and design constraints" prior to considering redesign solutions .
Our knowledge of general usability evaluation practices is extensive.
In particular, survey studies provide information on evaluation method use and preferences among practitioners .
Both usability testing and inspection are widely applied, and informal usability inspection methods  are used more frequently than inspection methods with higher degree of formality, such as cognitive walkthrough.
The respondents were recruited through several channels; invitations were distributed via local SIGCHI chapters , local UPA chapters , the UTEST mailing list , the European COST project TwinTide , and fliers at CHI 2011 .
As incentives to participate in the survey, all respondents leaving their email address could get early access to the project results and were included in a lottery for a $250 Amazon gift card.
There were two versions of the questionnaire; one for respondents reporting on usability testing and one for usability inspection.
Both versions were piloted with nonauthor respondents.
The first question of the questionnaire served to direct the respondents towards the Usability testing or the Usability inspection version of the questionnaire.
The respondents were explicitly asked to report on their latest usability evaluation, and they were repeatedly reminded to focus on this latest evaluation throughout the questionnaire.
The questionnaire on usability testing consisted of 32 questions and that on usability inspection consisted of 29.
Of these, 26 questions overlapped.
The questions concerned background data , perceived challenges, perceived impact, analysis support , usability problem identification strategy, collaboration, redesign, and prioritization of findings.
Questions on note taking forms, tools, problem identification strategies, usability inspection methods, as well as the respondents' educational background, were free text items.
The other questions were fixed response items.
To avoid bias related to the order of response categories, fixed response alternatives were randomized for each question where the order bias was judged to be a threat.
Questions and response categories are quoted throughout the result section in italic font.
The questionnaires are accessible at http://tinyurl.com/7uegbwa.
Due to the space limitation, this paper only presents findings on analysis support, problem identification strategies, collaboration and redesign.
Respondents were informed that data collection was fully anonymous so as to avoid bias.
Respondents who chose to leave their email address to receive the report on the study findings and participate in the lottery did so using an independent form; the e-mail addresses could not be connected to the questionnaire data.
Quantitative analyses were descriptive, due to the exploratory aim of the study.
Qualitative analyses of free text answers were conducted as thematic analysis .
Itemized free text answers related to Q2  were coded by two independent analysts to check agreement; free marginal kappa coefficients were in the range of .70 - .78 indicating adequate agreement .
Results are presented separately for usability testing and inspection throughout the paper except for those results where the difference between these two groups is less than 5 percentage points.
In those cases the results for all the respondents are combined to compress the description.
It could also have been relevant to break up results according to respondent background variables, such as work context or educational background.
This, however, is not done in order to reduce complexity.
In total, 224 people attempted the questionnaire.
The responses from 155 of these were included in the analysis.
Sixty-nine were excluded for  providing no response in any of free text fields in the questionnaire ,  nonsense responses in the free text fields ,  no response beyond the second page  of the questionnaire , and  not having conducted a usability evaluation in the last 6 months ; the latter group left the questionnaire after the first question.
The majority of the respondents  had started their latest usability evaluation within the last two months.
The respondents' typical work context and educational background are presented in Table 1 and Table 2.
The respondents worked in 21 countries: 55% in Europe, 36% in the US, 4.5% in Latin America and 4.5% in other parts of the world.
In Europe, respondents worked in 14 countries.
Three countries had remarkably large numbers of respondents: Switzerland and Czech Republic each had 15% of the respondents, and Poland had 10% of the respondents.
The skew towards these three countries is likely the result of particularly effective dissemination of the survey locally.
A check of responses from these countries showed no noteworthy difference to the others and so all responses were retained.
The third and the last group reflect the trend described by Rosenbaum  towards the adoption of a broader spectrum of methods in evaluation.
Thirty-four respondents provided free-text descriptions of their inspection method.
The descriptions of the methods from the literature  indicate flexible use.
For heuristic evaluation, three respondents described combinations of heuristic evaluation and other methods, and one described an inspection process including ad-hoc construction of heuristics.
For cognitive walkthrough, only two of five respondents clearly presented tasks as the starting point of the analysis, and one of these respondents also described a combined walkthrough and heuristic evaluation approach.
Similarly, the descriptions of the other inspection methods  indicated variation and flexible combination of elements from different methods.
Respondents detailing their Informal expert review mentioned "walkthrough", "team discussion", "comparison study", "checklist", and "business field study".
The respondents with No particular method / mix of methods described combined approaches such as heuristics plus walkthrough, heuristics plus best practice, and heuristics plus some form of expert review.
For usability testing the median time spent on preparing, conducting and reporting the test was 48 working hours , for usability inspection the median time spent was 24 working hours .
Most of the respondents claimed that these were typical time constraints .
The majority of the evaluations were conducted on mature systems or prototypes.
However, early phase evaluations were also fairly frequent.
Details are given in Table 3.
112 respondents reported a usability test as their latest usability evaluation.
Usability testing was most typically conducted with 8 users .
The most frequently used measure was Task completion , followed by Satisfaction measures , Error rate  and Task time .
Eighteen respondents reported Other measures, but the free-text explanations did not show any pattern.
Video recordings were made by 73% of the respondents.
Half of the respondents also reviewed the video recordings after the evaluation, either in part  or in full .
Besides, 43% of the respondents reported that key findings of the test were summarized immediately following each test participant, whereas 24% reported that such summarizing was done at the end of the day.
About half of these summarizing exercises were done with observers.
14% reported no summarizing; neither after each test participant nor at the end of the day.
43 respondents reported a usability inspection as their latest usability evaluation.
Notes were used during analysis by all the respondents.
Fewer than half reported using any kind of form to structure note taking.
For this question, multiple answers were allowed.
The respondents who used forms for note taking were asked to describe these in free text.
We subsequently coded the text through thematic analysis.
The reported types of structuring principles are presented in Table 4.
In addition, seven usability-testing respondents reported to include scales for task performance; four to include fields for notes on task time.
Six participants reported use of the note functionality in their video analysis software.
Tools used for problem description and analysis were reported as free text answers.
Spread sheets, text editors and presentation tools were frequently mentioned.
More specialized tools are presented in Table 5.
Structured formats for usability problem description were used by the majority of the respondents .
The structured formats used were, however, mainly home grown: 55% of the respondents reported that The problems were described according to my/our own format.
Only 4% reported that The problems were described according to a structured format described in standards or the literature.
This latter finding is highly interesting, given the attention and persistent effort ascribed to structured formats in usability research .
The remaining 41% reported that The problems were described in plain prose.
No noteworthy difference was found between usability testing and inspection respondents.
The respondents were asked which resources they had used during analysis.
Details are presented in Table 6.
Free text elaborations of the Other category  indicated two additional resource categories: Knowledge and skills from particular domains   and input from clients .
Participating users were a frequently consulted resource for usability testing respondents ; 80% of the respondents reported that they had asked test participants for opinions on either Usability problems , Redesign suggestions , or Other issues .
Observers, which could include clients or colleagues, were used as a resource by 59% of the usability testing respondents , by asking their opinion on Usability problems , Redesign suggestions , or Other issues .
None of the Other issues was reported more than once.
As we did not ask the respondents whether observers were present during the tests, it is likely that some did not report to seek observer opinion simply because no observer was present.
Rubin and Chisnell described the identification of the sources of user error, that is, the usability problems, as "the ultimate detective work" .
Knowing how practitioners perform this detective work is critical to understand analysis.
The respondents were asked for A brief description of how you decided something being a usability problem.
Ninety-three usability testing respondents provided descriptions of their problem identification strategy; from these, 214 items were extracted for thematic analysis.
Two main strategies were related to  observed consequences for task performance and  the users' responses.
Observed consequences for task performance were reported in 32% of the items.
Task completion was, not surprisingly, most frequently reported , followed by Other consequences for task performance , and Task completion time .
Other consequences for task performance included "user error", "deviations from the expected path", and "users misjudging their status as completed / not completed".
The users' responses, irrespective of task performance, were reported as indicative of usability problems in 19% of the items.
Observations included Emotional responses such as `boredom', `confusion', `frustration', `impatience' and `reduced engagement' , and Behavioral responses such as `hesitation', `seeking help', `struggling', `giving up', and `discomfort' .
Fourteen items concerned Verbal responses collected through think aloud.
The remaining items concerned strategies related to Frequency and severity , Causal explanations , Usability expertise , and Other approaches , such as the use of rating scales, and web analytics.
One item was coded as incomprehensible.
Collaboration with fellow usability practitioners was reported by most respondents ; only 26% reported No collaboration.
The reported types of collaboration are presented in Table 7.
Collaboration purposes were typically reported as Improve reliability / avoid that the findings were biased by personal perspective , Generate better redesign suggestions , and Quality assurance .
This practitioner awareness of reliability is in marked contrast to previous research indicating a lack of practitioner attention to reliability as a rationale for deploying multiple evaluators .
The respondents characterized their evaluation deliverable with respect to the inclusion of redesign suggestions and usability problems .
Redesign suggestions were included in the deliverables of nearly all the respondents.
Half of the respondents described their deliverable as A set of redesign suggestions either in response to a set of usability problems or in part motivated from usability problems.
Details are presented in Table 8.
The respondents were divided on whether or not all usability problems were identified prior to generating redesign suggestions .
49% reported that First all usability problems were identified, then the redesign suggestions were made.
46% reported that some  or all  redesign suggestions were generated immediately when a usability problem was identified.
Thirty-two usability inspection respondents answered this question; 61 items were extracted for thematic analysis.
When describing strategies for problem identification in usability inspection, the respondents typically referred to the usability expertise involved in the inspection .
Expertise included professional experience / expert knowledge and mindset , and the use of resources such as heuristics, standards, specifications, checklists, best practices, design patterns, and research literature .
Consequences for task performance  were mentioned more often than consequences for the users' responses , but much more rarely than for the usability testing respondents reported above.
The remaining items targeted Causal explanations  and Other approaches , such as benchmarking comparison and web analytics.
Four items discussed general issues or were coded Incomprehensible.
One key finding of the survey concerns the flexibility of method use.
A variety of methods is employed across process phases and project types, and a range of adaptations and method combinations is reported.
Web analytics and tools for drawing and prototyping are flexibly integrated in usability evaluation, in particular in usability inspection.
Appropriation and adaptation is also evident in the use of analysis resources.
Heuristics or guidelines, Design patterns, and Standards are almost as frequently used in usability testing as in inspection.
This somewhat surprising finding indicates the value of general knowledge resources, not only to compensate for lack of user-based empirical data, but also to understand and organize such data.
The prevalence of seeking test participants' opinions on usability problems and redesign suggestions also seem to represent a pragmatic exploitation of the available resources, in this case the knowledge of the test participant.
As data from usability testing are challenging to interpret, test participants' knowledge is valuable to explain observations and support analysis.
The observed appropriation and adaptation indicates that practitioners use existing methods and resources as components to be combined in response to the requirements of a given evaluation.
These findings support practitioners  and researchers  who argue that we need to support combining and adapting methods better.
No noteworthy differences between usability testing and inspection respondents were detected.
The respondents also answered how they Reached the redesign suggestions that were included in the deliverable.
Redesign suggestion were typically provided in response to usability problems.
However, more than one-third reported to reach redesign suggestions even though no usability problem had been observed.
Details are presented in Table 9.
The respondents were also asked how they present redesign suggestions in the deliverable.
More than half of the respondents provided visual redesign suggestions such as UI-digital mock-ups, sketching, and graphical elements / code .
The others provided redesign suggestions as oral presentation only, plain text only, or text supported by screen shots.
Oral presentations were frequent, but only six respondents reported this as their only means of redesign presentation.
The reported uses of notes, tools, and problem description formats indicate that practitioners tend to use commercial or home-grown support, rather than support from usability research.
Possibly, this preference for commercially available and home-grown support may be attributed to the fact that tools developed by the research community tend to be complex  and difficult to learn.
Complexity is not compatible with the time demands of the practical evaluation context; when a usability test is to be completed in 48 hours, and an inspection in 24, supporting tools clearly need to be light weight.
For new analysis support to be successful, it will have to fit the fast-paced analysis context.
Their suggestions on  low awareness of reliability and  collaboration mainly motivated by an intent to increase the pool of problems are not corroborated.
A surprising finding was the prevalence of group discussions in analysis for both usability testing and inspection.
In usability inspection, group discussion following individual assessments is a recommended practice .
However, group analysis of usability testing data is rarely emphasized in the literature.
For usability testing, two ways of problem identification in analysis were identified:  Issues that compromise task performance and  issues identified from users' responses.
In the research literature, data on users' responses are typically used to complement task performance data.
However, the importance given to the users' responses may also reflect the emphasis on user experience in HCI in the last decade  where user experience may be compromised even though task performance in a usability test is found satisfactory.
For usability inspection, problem identification typically is grounded in general usability knowledge, rather than in speculations on users' task performance or responses.
Interestingly, this expertise-based approach to problem identification is also clearly present in usability testing: 38% of the usability testing respondents reported to make redesign suggestions even though no usability problem had been observed.
It seems as if problem identification practices from usability inspection are being used also in usability testing.
This is in line with the argument that practitioners view methods as components to be combined and appropriated as needed.
The results clearly support the current trend within usability research to strengthen the integration of evaluation and redesign.
An evaluation deliverable without redesign suggestions is reported by a minority of respondents only, and almost half of the respondents provide redesign suggestions immediately upon problem identification for at least some of the usability problems.
The research community seems to be overdue in providing practitioners with facilities to support redesign as part of the evaluation process.
Redesign suggestions can take many forms.
In the literature, change recommendations are typically perceived as textual, though exceptions exist ; the recommended approach to solving an identified usability problem is to present the solution in prose , possibly with the support of screenshots .
Quite surprisingly, then, we find that more than half of the respondents report visual presentations of redesign suggestions, such as sketches, mock-ups, and graphical elements.
Usability practitioners seem to have advanced further than both the current literature and the research field in their integration of redesign and evaluation, indicating a potential for transfer of knowledge from practice to research.
On basis of the above findings, we suggest six implications for future usability research.
The flexible use of evaluation methods and analysis support indicates a need to move from usability research on methods to research on method components, as was recently suggested within usability research .
Method components may, for example, concern task-scenario development, participant recruitment, problem identification, problem consolidation, and redesign.
Research is needed to establish a basic taxonomy of method components, identify the components in most need of strengthening, as well as provide empirical knowledge on their strengths and weaknesses for typical evaluation contexts.
The drive for efficient analysis is evident also in peercollaboration.
The most frequent form of collaboration is short discussions at the outset of analysis.
At the same time, there is substantial awareness on collaboration as a means to improve reliability.
Nearly half of the usability-testing respondents reported that improved reliability was their main purpose for peer-collaboration.
An unanswered question, however, is whether the collaboration done in analysis actually serves to improve reliability.
Research suggests that reliability is improved by independent analysis of the same data set , something that was reported by only one-fourth of the usability testing respondents.
The findings on purposes of peer-collaboration partially go against the findings of Folstad et al.
Research is needed on support for fast and effective analysis, following the lead of Kjeldskov, Skov and Stages' instant data analysis .
New approaches to analysis should involve analysis support already used by the practitioner, ideas include structuring forms for note taking to better support subsequent analysis, and improving posttask data collection of test participants' opinions.
Practitioners in our study reported only using commercially available software tools for analysis.
As a consequence, researchers should consider aligning their research with commercial software, for example, by developing plug-ins to existing software, as done by Howarth et al.
Forms and structured formats are widely used, but they are typically home-grown, rather than retrieved from the literature.
In consequence, researchers should consider supporting home-growing of simple analysis support, rather than to provide fixed forms and templates.
A good example of support for home-growing is Capra's guidelines for usability problem description, which suggest important description elements without presenting a strict template .
However, the questionnaire survey design is also a strength of the study as it enables data collection from a broad spectrum of usability practitioners.
This would not be practically possible with methods supporting data collection on the respondents' behavior.
Given the broad variation in current practices identified in this study, our choice of a survey approach to data collection appears supported.
Our findings paint a picture of usability evaluation where analysis practices evolves as practitioners adapt and appropriate tools and methods to the needs of their evaluation context.
The picture, however, is incomplete.
In particular, the findings of the current study should be complemented with observational studies and case studies of actual analysis.
We hold that usability research may provide valuable contributions to the future evolution of analysis practices.
For this to happen, however, we need to a much larger extent to inform our research efforts by the needs of usability practitioners.
This study was done as part of the R2D2 Networks project, supported by the Norwegian research council's VERDIKT programme, and by the COST Action TwinTide, IOC0904, supported by the European Commission.
Thanks to all who participated as respondents and to all who helped us distribute the invitations to participate.
The user action framework: a reliable foundation for usability engineering support tools.
Determining the effectiveness of the usability problem inspector: a theory-based model and tool for finding usability problems.
Use and Usefulness of HCI methods.
Results From an Exploratory Survey among Nordic HCI Practitioners.
Thinking aloud: Reconciling theory and practice.
How to bring HCI research and practice closer together?
Capra, M. Usability Problem Description and the Evaluator Effect in Usability Testing.
Group analysis is prevalent in usability testing as well as inspection.
Such analysis is not well understood for usability testing and its strengths and weaknesses, as well as guidelines for practice, need to be established.
Also, processes for group analysis could be established so as to improve reliability, for example, by presenting brief individual summaries prior to the group interaction - learning from the oscillation between individual analysis and group discussion recommended by Nielsen .
The integration of evaluation and design has been an important topic in the research literature of the last decade.
Even so, it seems as if usability practitioners are ahead of researchers in bridging the gap between evaluation and design.
Research is needed to refine emerging approaches among leading practitioners into sustainable practices.
In a recent doctoral thesis, Furniss argues for the need to study usability evaluation in context in order to facilitate transfer of research results to usability practice .
In the case of the integration of evaluation and design, research on evaluation in context is also needed in order to identify method innovations as they happen in practice.
The main limitation of the study is that it was conducted as a questionnaire survey: what respondents say may differ from what they do.
In particular, forgetfulness and a bias towards providing socially desirable responses pose threats to questionnaire studies.
Cockton, G., and Lavery, D. A Framework for Usability Problem Extraction, In Proc.
Cockton, G., Woolrych, A., and Hindmarch, M. Reconditioned Merchandise: Extended Structured Report Formats in Usability Inspection.
Cockton, G., Lavery, D. and Woolrych, A. Inspectionbased Evaluations.
A practical guide to usability testing.
Ezzy, D. Qualitative Analysis: Practice and Innovation.
Folstad, A., and Hornbaek, K. Work-domain knowledge in usability evaluation: Experiences with Cooperative Usability Testing.
Folstad, A., Law, E.L-C., Hornbaek, K. Analysis in Usability Evaluations: an Exploratory Study.
Furniss, D. Beyond Problem Identification: Valuing methods in a `system of usability practice'.
Gulliksen, J, Boivie, I., and Goransson, B. Usability professionals - current practices and future development.
Hassenzahl, M., and Tractinsky, N. User Experience - a research agenda.
Hertzum, M., and Jacobsen, N. E. The evaluator effect: A chilling fact about usability evaluation methods.
Hornbaek, K. Usability Evaluation as Idea Generation.
Hornbaek, K., and Frokjaer, E., Comparing usability problems and redesign proposals as input to practical systems development.
The Interplay between Usability Evaluation and User Interaction Design Introduction to Special Issue.
Howarth, J., Smith-Jackson, T., and Hartson, R. Supporting novice usability practitioners with usability engineering tools.
