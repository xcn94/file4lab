A person seeking someone else's attention is normally able to quickly assess how interruptible they are.
This assessment allows for behavior we perceive as natural, socially appropriate, or simply polite.
On the other hand, today's computer systems are almost entirely oblivious to the human world they operate in, and typically have no way to take into account the interruptibility of the user.
This paper presents a Wizard of Oz study exploring whether, and how, robust sensor-based predictions of interruptibility might be constructed, which sensors might be most useful to such predictions, and how simple such sensors might be.
The study simulates a range of possible sensors through human coding of audio and video recordings.
Experience sampling is used to simultaneously collect randomly distributed self-reports of interruptibility.
Based on these simulated sensors, we construct statistical models predicting human interruptibility and compare their predictions with the collected self-report data.
The results of these models, although covering a demographically limited sample, are very promising, with the overall accuracy of several models reaching about 78%.
Additionally, a model tuned to avoiding unwanted interruptions does so for 90% of its predictions, while retaining 75% overall accuracy.
Unfortunately, computer and communications systems cannot currently act in a similar fashion - they are almost entirely oblivious to the human context in which they operate and cannot assess whether "now is a bad time."
As a result, they operate the same way in essentially all situations, and do not act in ways that remain appropriate to the situation.
If left unchecked, current systems can easily disturb or annoy - consuming the valuable resource of human attention in a haphazard and inefficient fashion.
As a result, we often avoid building proactive systems - forcing our interfaces to be silent and passive until called upon.
If we could develop relatively robust estimators of interruptibility, we might enhance human-computer interaction and computer mediated communications in a number of ways - making people more efficient .
For example, we might build a "smart answering machine" which stopped our phone from ringing and diverted our other messaging traffic when we should not be interrupted.
We might also be able to build information displays that could balance an estimation of the importance of a piece of information against the attentional costs of delivering it.
This paper describes work exploring the feasibility of creating such an estimator by using sensors to drive models that predict human interruptibility.
This should be theoretically possible, since the equivalent assessments made by people are based on directly observable phenomena .
However, replicating this kind of rich human judgment in practice may be very challenging and might even be currently impossible.
The study described here seeks to assess the feasibility of this kind of automatic prediction based on sensor data.
Specifically it seeks answers to at least these five questions: * Can a practical sensor-driven model reliably predict human interruptibility?
As a part of our early socialization, human beings normally learn when it is appropriate to interrupt someone.
As adults, we can typically assess someone's interruptibility very quickly and with a minimum of effort.
For example, in the time it takes to walk past someone's open office door, we can often tell that we should not intrude on the person.
This assessment allows us to balance the benefits of an interruption with its cost.
Maintaining such a balance usually results in what is recognized as socially appropriate  behavior.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We might proceed by creating and deploying sensors, then testing their effectiveness with various forms of models.
However, given the large uncertainty surrounding these questions, it is almost inevitable that we would spend considerable effort to build sensors which in the end turned out to be ill-suited or sub-optimal for the task.
Instead, we have chosen a Wizard of Oz approach that allows us to simulate a wide range of plausible sensors, build multiple models based on data from these simulated sensors, and then test the effectiveness of different models and different combinations of sensors.
Specifically, this study is based on a long-term digital audio and video recording of the working environment of a subject.
These recordings were made during full working hours for 14-22 working days for each subject.
The recordings were then viewed by a person who coded for actions and situations that could plausibly be sensed.
For example, as detailed below, we recorded the number of people present, who was speaking, what task objects were being manipulated, whether the phone was off-hook, and other similar facts about the environment.
Overall, we recorded 602 hours of audio and video from the office environments of four subjects with similar job functions.
During the time that recordings were being made we employed experience sampling techniques   to elicit in-situ self-reports of their interruptibility.
Finally, a variety of machine learning techniques were used to create predictive statistical models that could use the simulated sensor data to predict the collected self-reports.
While currently limited to a fairly narrow demographic group, the results produced have been quite promising.
When predicting overall interruptibility or noninterruptibility of subjects , several models produced very similar results with a cross-subject accuracy of approximately 78% .
We were also surprised to discover that much of that predictive power of several of these models could be obtained using a single, relatively easy to build, sensor that indicates whether anyone in the space is talking.
A primary use for an interruptibility estimator will likely be to structure information delivery to avoid situations when the user does not wish to be interrupted.
In that setting it may be more important to avoid "incorrect" interruptions than it is to make sure not to miss opportunities for "correct" interruptions.
We have constructed a model tuned to avoiding unwanted interruptions that does so for 90% of its predictions .
Custom Coding Interface Showing a Typical View of a Subject's Office what technological interventions might be best to negotiate multiple and sometimes complex tasks .
HCI researchers have only begun to provide a more analytic and precise approach to understanding interruption, so as to design better context-aware systems.
Some researchers believe that understanding the context of interruptions cannot be handled successfully by machines, but instead machines must defer to users in an accessible and useful fashion .
Others, such as Horvitz , are optimistic that machine learning techniques can handle many of the predictions needed to present information appropriately.
Other researchers have worked towards design guidelines for coordinating interruption in HCI.
McFarlane tested four known methods for deciding when to interrupt people .
Although the results have implications for structuring appropriate interruption in HCI, no one method emerged as best in doing so.
O'Conaill completed an ethnographic study on the nature of interruptions in the workplace with implications for how to better design communication technology .
One major finding was that the recipients of interruptions often derive personal benefit from the interruption, often at the expense of the initiator.
This suggests that a blanket approach of suspending all interruptions may eliminate the benefit that recipients receive from being interrupted, and that an intelligent filtering approach, such as done by human assistants, would be useful.
Bellotti defines the components of context, design guidelines and human-salient details for realizing them .
Hudson poses the challenge of making interruptions more effective, since many view interruptions as a valuable part of getting work done .
The study of interruptions began with classic experiments in the 1920s showing that tasks that were interrupted were more likely to be recalled after a delay than tasks that were not interrupted .
In order to increase uniformity for this first experiment, we chose four subjects who are similar in terms of working environment and the types of tasks they perform.
Each serves in a high level staff position in our university, and has significant responsibilities for day to day administration of a large university department and/or graduate program.
Each subject has a private office with a closable door.
They typically respond to a significant number of "walk in" requests, and overall are frequently interrupted.
Each of these subjects almost always works with their door open, making them accessible to others most of the time.
For each subject we placed a PC with a large disk and an A/V capture and compression card connected to a small camera and a microphone in their office.
Each machine also had speakers for producing audio prompts, and a keyboard which allowed the user to temporarily disable recording if they felt their conversations were too sensitive to be recorded .
The PC did not include a visual display.
As illustrated in Figure 1, cameras with wide angle lenses were carefully positioned  so that the primary work area as well as the door were visible.
Data was captured in grayscale with a resolution of 320x240 pixels at about 6 frames per second, and 8-bit audio was recorded at 11 Khz.
Recording was performed from 7am to 6pm on workdays for 14-22 days for each subject.
We estimate that 300 hours  of compressed recording could be placed on the 80 Gb disks we used.
This recording setup worked well except in one case where a week's data was lost due to an undetected improper compression setting that caused the disk to fill up prematurely.
For this subject we collected an additional 10 days of data at a later date.
Overall we recorded 602 hours from the subject's offices.
Subjects were given an audio prompt to provide a selfreport of interruptiblity at random but controlled intervals, averaging two prompts per hour.
In order to try to minimize the disturbance caused by our prompts, we chose to ask only one question, and used a five point scale so that the subject could respond in a minimally disruptive way - holding up some number of fingers on one hand .
Specifically, subjects were asked to "rate your current interruptibility on a scale from one to five, with one being most interruptible."
We collected data for a total of 672 prompts when the subject was present.
While willingness to be interrupted is clearly dependent not only on the state of the person, but also on the nature of the interruption, we made this study more tractable by choosing to only look at the state of the interruptee.
To facilitate processing of the recorded audio and video, we constructed specialized software for playback and coding of the data.
Both the overall operation of the software and the items coded for were iterated based on coding and analysis of the first subject .
Multiple coders - students hired on an hourly basis - were employed, and began their work being trained for consistency with the other coders before performing coding that was retained.
For cross-validation between coders we performed duplicate coding for a randomly selected 5% of the data and found 93.4% agreement at the granularity of 15 second intervals.
To minimize coding time, we have initially only coded the five minutes prior to each sample point for a total of 56 hours of coded data.
The final coder's interface, shown in Figure 1, presented recordings in 15 second sequences.
A series of buttons  were provided to indicate the occurrence of events within that segment.
At the coders' option, a sequence could be played at normal or double speed.
After each segment, the coder pressed a key to go to the next segment, or could back up and see the segment again.
Overall, we found this setup a very good compromise which allowed coders to operate at speeds near or even better than real-time in the most common cases where very little activity was apparent, but also allowed them to control pacing so that they did not fall behind or lose data when complex actions were occurring.
To speed up processing, multiple passes over the recordings were made, starting with whether the occupant was present during each prompt.
This information was then used to optimize subsequent passes.
For example, after coding how many people were present, no sequences of an empty room were shown, and passes which coded information about the activities of guests automatically skipped all sequences when only the occupant was present.
With these optimizations, we are now able to code data at a rate of between three and four minutes of coding time per minute of processed recording.
In 54 of the 672 samples  the subject was present but did not respond to the self-report prompt.
We examined these cases individually and determined that in the vast majority of them, the subject was either on the phone or engaged in conversation with a guest.
Based on empirical results from the literature, we expected these activities to be highly correlated with non-interruptibility .
Further, in testing we found that removing these samples from the data had very little effect on the accuracy of the final predictions.
As a result, to make analysis and model building simpler we placed these samples in the "least interruptible" category for purposes of model building.
For guests present, sitting, standing, talking, or touching, we also counted: * The number of such guests at the sampling point .
Overall, we obtained observation values corresponding to a set of 128 direct or derived simulated sensors.
Of these, 30 were occurrence counts and 98 were based on binary events.
Of the binary event sensors, 8 never occurred in our data, 14 occurred with fewer than 1% of samples , and 20 occurred with fewer than 2% of samples .
None of the occurrence count sensors had all counted values occurring less 2% of the time.
Overall Distribution of Interruptibility Self-Reports The overall distribution of self-report responses is shown in Table 1 with the aggregate distribution illustrated in Figure 2.
Although there are clear differences between the subjects, we can see that a substantial portion of the reports  indicated the least-interruptible condition.
In coding from recorded data we logged the following 23 events or situations to act as simulated sensors: Occupant related: * Occupant presence.
Environment: * Door open or closed.
These simulated sensors  were chosen because we a priori believed they might be correlated with interruptibility, a sensor could plausibly be built to detect them, and they could be readily observed in our recordings.
Based on the directly recorded information, we also derived a number of variant sensors that captured recency and density effects.
Based on the literature in this area, we would expect that the strongest indicators of non-interruptibility would be those related to social and task engagement .
In particular, interruptions are undesirable when someone is speaking .
Further, speaking on the telephone is particularly unfavorable for interruption.
This may be because the subtle negotiation of an interruption that often occurs in person via eye contact, or other nonverbal cues, cannot include the remote party to the conversation.
While we knew in advance the general type of activities that needed to be detected to produce a good prediction, it was still unclear exactly which specific sensors would be most useful .
To try to understand this, we analyzed the predictive power of individual simulated sensors using an information gain metric .
In addition to the power of particular sensors, we can also see that the shorter term binary sensors  generally tend to be more predictive than the binary sensors working over a five minute period.
However, the five minute density sensors  seem to have roughly the same power as the short term sensors.
Most of the sensors having very low information gain scores occurred too infrequently to provide any predictive power.
The only potentially surprising sensor among the bottom 30 scores is the Desk  sensor at rank 101 out of 128.
Information gain statistics only consider the predictive power of features in isolation and do not take into account the overlapping nature of our sensors.
In the next section we will also consider an approach to analysis of predictive power based on constructing models with more and more sensors and measuring the results of adding sensors on the accuracy of the model.
Features with Top 20 Information Gain Scores In simple terms, an information gain metric works by sorting a set of observations with respect to the values of a single feature within the observation.
This effectively removes the entropy associated with variations in that feature.
The entropy of the resulting ordered data set is then estimated.
The entropy estimates from different sortings can then be compared to determine the relative amount of entropy removed, hence the relative information content of each feature.
Note that the absolute value of an information gain metric is not of particular interest, only the relative values between features.
Also, information gain indicates potential usefulness in prediction, but does not show directly whether the feature indicates interruptibility or non-interruptibility.
It is also important to note that many of our simulated sensors are, by design, inherently overlapping.
For example, Talk  will always be true when Talk  is true.
In addition, there are some less obvious overlaps such as the fact that the Telephone  sensors will almost always imply the corresponding Talk  sensors, and that Guest Talk  will be quite correlated with Talk , since long monologs by either the occupant or guest would be expected to be fairly rare.
Information gain statistics allow us to consider multiple overlapping sensors and provide a way to estimate which will be most predictive if we need to choose between them.
Table 2 presents the information gain ordering for the top 20 features.
Here we can see that the occupant talk and telephone sensors clearly rise to the top in predictive power - holding eight of the top nine positions.
After talk and telephone, occupant movement sensors Sit  and Stand  show the next highest predictive power.
These are followed a little further down by Sit  and Stand .
Taken together, the sit and stand sensors might be interpreted as being positive and negative indicators, respectively, of engagement with office tasks that are typically done in a seated position.
The next highest indicator is the number of guests talking , which clearly indicates social engagement.
This is followed by several indicators of computer use - Keyboard  and Monitor .
In order to explore the question of whether predictive models can be constructed at all, as well as how predictive they might be made, we employed a number of well known machine learning algorithms to construct several different forms of predictive model.
To make this work simpler and less ambiguous we first considered the binary decision problem of predicting whether or not the user would indicate "least interruptible".
We will call these two states "interruptible" and "not-interruptible."
This split was motivated in part by the expected uses of the predictor in avoiding the most harmful interruptions.
In addition, anecdotal evidence suggests people often have strong feelings about particular times being "obviously notinterruptible," but often have more ambivalent attitudes towards "partially interruptible" times.
This seems to be at least hinted at in the bimodal distribution of self-report values, and would also argue for such a split.
After considering this binary problem we also took the most promising modeling approach and explored other variations.
For the binary decision problem we constructed models using decision trees , naive Bayesian predictors, support vector machines , and AdaBoost with decision stumps .
All of these models were constructed using widely available, open source software packages .
For this data set there is a base accuracy rate of 68.0% .
For the model evaluations shown in Table 3, we used a standard cross-validation approach involving multiple trials of model construction.
In each trial we randomly selected 90% of the data for training, and used the resulting model to predict the remaining 10%.
The numbers reported here are sums from 10 such trials.
Decision trees are perhaps the simplest of the techniques.
The decision trees we used are constructed by first selecting the binary test  > 0" which most usefully splits the data into two parts.
Decision trees are then recursively constructed for those subsets.
Leaves of the resulting tree are then assigned predicted values.
One drawback of decision trees is that, after many subdivisions, each leaf may represent only a small number of samples and hence may be susceptible to noise in the data.
As a result, one does not normally build decision trees as deeply as possible, but instead applies certain stopping criteria.
In our case we used the C4.5 decision tree package  with 10 trials and a minimum branch size of 15.
Table 3a gives the results from our decision tree model.
Here, correct predictions appear on the diagonal, and incorrect predictions appear off the diagonal .
Incorrect predictions come in two forms, which we will call "incorrect interruptions"  where "interruptible" is incorrectly predicted , and "incorrect delays"  where "non-interruptible" is incorrectly predicted .
We would expect decision trees to work well for this problem because there is a strong and unambiguous feature  that provides a very good initial split.
In fact as shown in the rest of Table 3, the 78.1% accuracy provided by decision trees is the best result across the modeling techniques.
In addition to decision trees we also tried creating models based on naive Bayesian predictors, support vector machines, and AdaBoost with decision stumps.
The results from these four techniques are presented in Table 3b-d.
These results are all similar and there is no statistically significant difference between the largest and smallest.
Since results from a variety of unrelated approaches produce very similar results, we feel this clearly shows that predictive models can be constructed, and we are quite hopeful that robust models with results in the 75-80% accuracy range can be driven from real sensors.
Since decision trees represent, in some sense, the simplest of the models and also produced the best results, we used them to explore several additional variations.
All the results reported thus far have been for predictions across all subjects.
This is a preferable approach because it offers the hope that general models could be constructed without an extensive individual training period.
However, it might be possible to produce better predictions by tailoring models to one specific person.
To explore this, we constructed four decision tree models isolated to the data from each individual.
While we would expect these models to perform better, in fact they did not in most cases.
This lack of improvement is likely due to the effects of having substantially less training data.
As a result, it is difficult to draw conclusions about how well personalized models might work with more extensive individual training data.
We next revisited the decision to produce predictions of "least interruptive" vs. something else.
To do this, we considered whether a threshold value of three rather than four might produce better results.
However, this instead reduced accuracy to 69.6% .
Finally, we looked at whether better results might be obtained via a five-way decision problem rather than a binary decision - in other words, whether there was an advantage to attempting to directly predict the one-to-five interruptibility value.
For a multi-way decision problem we were able to use a more sophisticated technique: decision trees with error correction codes .
Table 4 presents the results of this model.
Since this multi-way problem is substantially harder than the binary problem, overall accuracy  is substantially lower than for the binary problems.
Note that while it is possible to reduce inappropriate interruptions to as low as 2% of total predictions without stopping all interruptions, this causes overall accuracy to drop to 40% which may be unacceptably low.
Results for Decision Tree with Error Correction Codes However, we can compare this model more directly if its results are mapped onto the same decision problem as the previous predictors.
This is done by considering all 1-4 predictions to match any 1-4 actuals , as illustrated in Figure 3.
In this case, the overall accuracy is 74.9%.
While this is not an improvement, it is important to note that this model has two potential advantages.
First, this model allows the decision problem to be changed by the user at run-time without changing the model.
In particular, the user may set how conservative they would like the system to be in choosing to interrupt by selecting a threshold value between 0 and 5 .
The second major advantage concerns the distribution of incorrect results.
In this model, the percentage of incorrect interruptions as defined in the binary problem is only 10.4% of the total predictions.
If we assume that incorrect delays are preferable to incorrect interruptions in our final application, this could be a substantial advantage and worth the loss of 3% overall accuracy.
In addition to the information gain metric described above, we can also examine predictive power of sensors by looking at the effect of sensors on the accuracy of the models themselves.
To do this, we constructed a series of decision tree models constrained by the number of different sensors they could employ and measured the accuracy of each model.
Accuracy  as Features are Added to Decision Tree Models Figure 5 presents a graph showing this effect.
The important thing to note here is that the first few sensors  have a very large impact which accounts for most of the prediction, then the remaining sensors provide mixed results, eventually adding only a few percentage points to the overall accuracy.
Note that the sensors added here are chosen by the decision tree algorithm.
This is done on the basis of an information gain metric.
However, the analysis performed is more sophisticated than the independent information gain scores presented earlier, in that it accounts for the overlapping effects of previously chosen sensors.
Each point in this graph represents the average of several decision trees, and so does not necessarily represent a specific sensor being added.
This indicates that a relatively small number of sensors can be used to attain most of the predictive result.
In order to further test this, we constructed a final model using sensors chosen based on their ease of implementation.
These included a new combined "anyone talking sensor" , as well as telephone , keyboard , mouse , and time of day .
As indicated in Table 5 the accuracy of this model falls within the range of results found in main models in Table 3.
Thus we are lead to believe that robust results should be attainable from practical, relatively easy to implement sensors.
While the study described here only considers a particular category of office worker, and we cannot yet tell how well the results might translate to other demographics, its results are still quite promising.
We have demonstrated that sensor-based estimators of human interruptibility are possible, that robust sensors operating in the 75-80% accuracy range might be constructed using several different types of models, that speech detectors are the most promising sensor for this problem, and that overall a relatively simple set of sensors can probably be employed to achieve good results.
There are many areas for future work in this line of research.
First it will be important to expand the study done here to different demographic groups  in order to understand how robust the results might be across the population.
We would also like to compare the predictions made by our model with the performance of humans estimating interruptibility.
There are also many additional opportunities for analysis of this data.
For example, the analysis done thus far has concentrated almost exclusively on questions relating to construction of predictive models.
There is another set of interesting questions related to understanding human behavior that we have only partially touched on here.
We would also like to do an in depth review of the misclassifications made by our models to see if there are discernible patterns which could be used to improve the models, and to systematically explore the effects of sensor errors on predictions.
Finally, based on these promising results in a Wizard of Oz setting, we hope to be able to construct working systems with real sensors, and create new interactive applications that use them.
This work was funded in part by DARPA and by the National Science Foundation under grants IIS-01215603, IIS-0205219, IIS-9980013 and the second author's NSF Graduate Research Fellowship.
We would like to acknowledge the employees that recorded simulated sensor values: Ben Davies, Rick Ebert, Rucha Humnabadkar, Becky Kaplan, Matt Mowczko, and Long Pan.
