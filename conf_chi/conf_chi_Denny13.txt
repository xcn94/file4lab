Badge-based achievement systems are being used increasingly to drive user participation and engagement across a variety of platforms and contexts.
Despite positive anecdotal reports, there is currently little empirical evidence to support their efficacy in particular domains.
With the recent rapid growth of tools for online learning, an interesting open question for educators is the extent to which badges can positively impact student participation.
In this paper, we report on a large-scale  randomized, controlled experiment measuring the impact of incorporating a badge-based achievement system within an online learning tool.
We discover a highly significant positive effect on the quantity of students' contributions, without a corresponding reduction in their quality, as well as on the period of time over which students engaged with the tool.
Students enjoyed being able to earn badges, and indicated a strong preference for having them available in the user interface.
The term "gamification" was coined  to describe the general approach of using elements typically seen in games, such as badges, in non-game applications.
Despite the growing utilization of badges, there is little empirical evidence to support their effectiveness at motivating and engaging users.
Intuitively, the functionality and user-base of an application are important factors in determining appropriate and effective badges, therefore systematic research is needed to explore their effects across a range of domains.
Researchers continue to highlight the importance of ongoing work in this area, and explicitly call for further investigation .
We answer this call by conducting a large-scale study that provides empirical evidence of the positive effect that badge-based achievements have on user participation.
Given the rapid growth in the use of educational technologies, we focus specifically on examining student interactions with an online learning tool and we discuss the implications of our work for designers of similar systems.
We begin by presenting the tool used in our experiment and then describing the implementation of its badge system.
The PeerWise tool is a freely-available, award-winning  platform used by more than 300 institutions around the world .
Students using PeerWise create their own exam-style questions relevant to the course they are studying and share these via a central repository with other members of the class.
This repository then becomes a practice resource with which students can answer and critique their peers' questions.
This activity aligns with a "contributing student pedagogy" , a recent pedagogical approach in which students take responsibility for creating and moderating learning resources and producing peer feedback.
Such approaches are attractive to educators as students become actively involved in the learning process.
However, motivating students is a key challenge facing instructors , and is of great importance for activities such as this where learning resources are studentgenerated.
We conducted an experiment over a period of four weeks involving a large class of more than 1000 students using PeerWise as they would in a typical semester.
The initial emergence of virtual achievements, sometimes referred to as badges or trophies, awarded to users for completing certain tasks was primarily focused around games.
The first large-scale successful implementation of badges to online games was in 2005, when "achievements" were added to the then 3 year old Xbox Live platform .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The other half of the class did not have access to the badges in the user interface, but in all other respects were treated identically.
We found the presence of the badges had a clear positive effect on both the number of questions answered and the number of distinct days that students engaged with the tool, yet had no effect on the number of questions authored by students.
Students reported enjoying being able to earn badges, and indicated a strong preference for having them available in the interface.
The authors note that as non-game applications begin to make greater use of achievement systems, there is a need for further research into their design, use and effects.
Current research into the impact of badges in non-game applications has focused on investigating user perceptions rather than measuring the effect on participation.
An early example of this work was conducted by Montola et al.
The 8 week field trial involved 20 participants using mobile phones to take pictures and record audio clips that were automatically uploaded by an application on the phone to the Nokia Image Space service .
Individual achievements were awarded for uploading various numbers of photos, sound clips and scenes , as well as the first upload of each type of content.
Participant interviews indicated that the achievements triggered some friendly competition, however a drawback of the application was that achievements were not visible within the mobile client.
Participants did not receive explicit feedback or notification of earned achievements on the device, and were restricted to viewing this information on the web at a later time.
The authors conclude that such immediate feedback is a necessity for an effective achievement system.
The authors also noted that achievements in their system appeared to have particular value in assisting users to learn about the features of the application.
This is an example of the "instruction" role that badges play in many social systems, as identified by Antin and Churchill  in their review of the social psychological functions of badges in social media contexts.
Badges serving this "instruction" function help new users learn about the types of activities that are possible in a system as well as those that are highly valued and embody the system's social norms.
A more recent study by Fitz-Walter et al.
The application leveraged the GPS and QR-code scanning functionality of smartphones, and achievements were awarded when students physically visited certain parts of their campus and answered questions relating to University services.
Several of the achievements, such as those for attending a certain number of events on campus, served the "goal setting" role as described by Antin and Churchill .
Goal setting is known to be an effective motivator, and achievements of this type set a target for users that they are challenged to meet.
Almost all students reported that the achievements helped motivate them to explore their campus.
We begin by describing the PeerWise environment and giving an overview of the badge system being investigated in our study.
PeerWise is an online repository of student-generated multiple-choice questions .
MCQs consist of a question stem  along with a set of  alternative answers, only one of which is correct.
Creating effective MCQs, particularly designing plausible alternatives, is time-consuming and challenging and requires very good content knowledge .
Despite this, MCQs are an attractive option for summative assessment in large classes due to the fact that student responses can be automatically graded.
The challenge of constructing good MCQs can be an advantage if students take the role of question authors.
Nicol argues that by being involved in the process of constructing their own questions, rather than simply answering instructor generated questions, students can develop self-regulating skills which are crucial for life-long learning .
In addition, research in some disciplines has shown that authoring questions is an effective learning technique.
Foos  investigated the effect of student-written questions on exam performance, and randomly selected students who created questions as a study technique prior to an exam performed significantly better than those who did not.
In addition, they need to provide an explanation for the correct answer to their question in their own words.
This self-explanation step is a key part of the authoring process, and is supported by research demonstrating the benefits of student self-explanation  and producing explanations for others .
The repository of MCQs developed by a class then becomes a practice resource.
Students can answer as many questions as are available and can provide feedback to one another in the form of comments.
Students are able to moderate the resource by assigning ratings to individual questions and voting on the most helpful comments.
The higher quality questions and comments then become more visible to other students in the class.
Figure 1 shows the main menu of the PeerWise interface from a student's perspective.
This menu organizes all of the questions in the repository into three categories: those that have been authored by the student, those that the student has previously answered and all the remaining questions which have not yet been answered.
Prior to the introduction of the badge system, PeerWise already included some game-like elements.
For example, each student is shown their PeerWise "score" , which is calculated by an algorithm that rewards students for authoring, answering and evaluating questions.
Rewarding students with points for their actions may help to foster friendly competition which can be a strong motivator .
However, one criticism of scoring systems in nongame applications is that certain users may attempt "gaming" of their score by focusing on repetition of non-useful actions which yield points .
In the context of educational environments, "gaming" behavior can detract from the learning task and is associated with poor learning outcomes, yet is frequently observed .
The PeerWise scoring algorithm attempts to discourage gaming behavior by rewarding students only when other students endorse their contributions.
Implementation of the badge system in PeerWise took place independently of this study.
The process began by identifying the core activities that students engage with, and then defining badges for those that instructors wished to encourage.
The two key activities are authoring questions and answering questions, and more than half of the badges defined targeted these two activities.
In addition to the number of questions answered, students should be encouraged to test their knowledge frequently as the benefits, particularly with respect to long-term retention, are well known .
Several badges were therefore defined to reward students for returning at regular intervals to practice answering questions and to review and reinforce what they have learned.
Table 1 gives a complete list of the 22 badges made available in PeerWise along with a description of the requirements for earning each.
The badges are organized into three categories: "basic", "standard" and "elite", roughly corresponding to the difficulty required to earn them.
For example, "basic" badges are primarily designed to highlight the core functionality of the PeerWise tool and can be earned simply for discovering and using different features such as replying to a comment or submitting a rating.
In this sense, they satisfy the "instruction" role as defined by Antin and Churchill .
Previous research highlighted the necessity of notifying users when new achievements are reached .
When a student earns a badge in PeerWise for the first time, a notification message appears on the main menu as illustrated in Figure 2.
The notification message includes a link to the "Badges" page which lists all of the available badges highlighting those that the student has currently earned.
This page also displays how many students have earned each of the badges, and the rank in class of the current student's total badge tally, providing them an indication of their relative accomplishments.
An example of the "Badges" page is shown in Figure 3.
Once the student has viewed this page, the notification message disappears until any subsequent badges are earned.
The "Badges" page can be viewed at any time by the student, even in the absence of a notification message, by following the link labeled "View my badges" at the bottom of the main menu.
This link is visible at the bottom of Figure 2, where it appears in the middle of the row of three icons.
Students satisfying these requirements were awarded 1.5% credit towards their final grade in the course.
This credit was determined by the course instructor and was consistent with how PeerWise had been used in previous offerings of the POPLHLTH111 course.
A mid-term test, contributing 15% to the final grade, was held towards the end of this period on Wednesday 28th March.
Earning course credit is a very strong motivator for almost all students and accounts for much of the activity within the system.
In this study, we are looking specifically for effects that can be attributed to the presence of the PeerWise badges.
In this study, we collected data from authentic student use of the PeerWise tool in a large undergraduate course, POPLHLTH111, taught at The University of Auckland in 2012.
This course introduces students to frameworks and tools for understanding and controlling the impact of disease in populations.
We have two primary reasons for selecting the POPLHLTH111 course for this research.
Firstly, it is a particularly large course with more than 1000 students, giving our analyses good statistical power.
Secondly, PeerWise has been used in POPLHLTH111 since 2008, prior to the introduction of the badge system, and thus is a regular and expected part of the course for all students.
We begin this section with a description of the requirements for students to participate with PeerWise, as determined by the course instructor.
We then present our study design and research question, and describe a survey conducted towards the end of the study to investigate student perceptions of the badges and the tool.
For the purposes of our study, each student was allocated at random to one of two groups.
To ensure balanced group sizes, the allocation was performed by generating a random sequence of the integers between 1 and 1031 inclusive , and using this sequence to partition the class list into two near-equal sized groups.
7 badges, 3 are awarded for answering questions on distinct days.
Our specific research question is as follows: Research question: To what extent does the inclusion of badges  within PeerWise have an impact on the number of questions a student authors, the number of questions a student answers, and the number of distinct days that a student engages with the tool?
A survey was conducted online to investigate the perceptions students had regarding the learning value of the PeerWise activity and, for students in the "badges on" group, the motivational effect of the badges.
The survey was available from the 26th March, two days before the mid-term test, to the 1st April, immediately following the conclusion of the data collection.
Students in both groups were asked to rate the learning value of both authoring and answering questions on PeerWise, as well as provide gender information.
In addition, students in the "badges on" group were asked to provide feedback on the extent to which the badges motivated them and indicate their prior experience with badges in the context of online games.
Table 2 lists the 7 survey questions shown to students in the "badges on" group, only the first two of which were shown to students in the "badges off" group.
Responses to each question were collected on a standard 5-point Likert scale .
On the 5th March, when all students began using PeerWise for the first time, badges were enabled in the user interface only for students in the "badges on" group.
Students in this group were able to earn badges, receive notifications for every new badge earned, and see the "View my badges" link at the bottom of the main menu  which linked to the "Badges" page.
Students in the "badges off" group were not able to earn badges -- they did not see the "View my badges" link on the main menu , did not receive notifications, and were not able to view the "Badges" page.
All other features of the tool and requirements for using PeerWise were identical for all students.
Figure 4 shows a schematic of the first 6 weeks of the POPLHLTH111 course during which this study was conducted.
In this study, we set out to investigate whether incorporating a badge system into PeerWise would result in a higher level of participation from students.
We defined three measures of participation, the first two of which, the number of questions authored per student and the number of answers submitted per student, reflect the core functionality of the tool.
The third measure was the number of distinct days that a student uses the tool.
To encourage regular practice, as opposed to completing all required contributions in one sitting, several badges are awarded only when a student interacts with the tool on a number of different days, as well as on a number of consecutive days.
The rightmost column of Table 1 highlights the badges that are most closely related to our measures of interest.
In particular, there are 5 badges awarded for question authoring and 7 awarded for answering questions.
We observed an excellent rate of participation amongst students, most likely attributed to the course credit associated with the activity.
Of the 516 "badges on" students and the 515 "badges off" students, only 12 and 15 respectively did not participate with PeerWise.
Therefore, 504 "badges on" and 500 "badges off" students either authored or answered at least one question on PeerWise, leading to a 97% participation rate.
Over the four weeks, a total of 2620 questions were authored and 95685 answers were submitted by all students.
The histogram in Figure 6 shows the frequency with which students in the "badges on" group earned various numbers of badges.
80% of students earned more than half of the 22 badges that were available, with only 1 student earning all of the badges.
Considering only students who were active, only 3 earned fewer than 7 of the 22 badges.
To visualize when these differences occur over the period, Figure 7 plots the total number of answers submitted per day by students in each group.
There appear to be two "peaks" in the chart, the most prominent of which  corresponds to the day of the mid-term test.
The sharp increase in activity immediately prior to the mid-term test is due to students voluntarily using the tool to practice, which is a pattern consistent with reports of the use of PeerWise in other disciplines .
On 25 of the 26 days in this study, students in the "badges on" group submitted more answers than students in the "badges off" group.
Table 3 gives a summary of the number of questions and answers submitted, as well as the number of distinct days of activity, for students in each group.
A student is considered to be active on a given day if they either author or answer at least one question.
Totals, averages, medians and standard deviations are given, as is the percentage of all answers submitted by students in each group that were "correct".
An answer is deemed to be correct if it matches the answer that the author of the question suggested was correct.
This is very often, but not always, the case .
Finally, there appears to be a difference in the number of distinct days that students in each group were active.
The average number of distinct days of activity is 13% higher for the "badges on" group, however this does not translate to a difference in the median values.
The data, particularly the number of answers submitted, are heavily skewed as in both groups the most active 10% of students submit approximately one third of all of the answers.
For example, the 50 most active students in the "badges on" group submitted 18763 answers  compared to 14881 answers  submitted by the 50 most active students in the "badges off" group.
To determine the significance of the differences between the groups, we performed a series of Mann-Whitney tests .
The distribution of the number of answers submitted per student did differ significantly between the two groups , with the median number of answers submitted by the "badges on" group being 68 compared with 60 by the "badges off" group.
Several things are apparent in this data.
Firstly, despite 5 badges being associated with question authoring, there appears to be no difference between the number of questions authored by students in the "badges on" and "badges off" groups.
To the level of significance shown in the table, both the average and median number of questions authored by students in each group is identical.
Secondly, there appears to be a large difference between the number of questions answered by students in each group.
The total number of questions answered by the "badges on" group over this period represents a 22% increase over the number of questions answered by the "badges off" group.
Finally, the distribution of the number of distinct days of activity differed significantly between the two groups , with the average number of active days for the "badges on" group being 7.01 compared with 6.21 for the "badges off" group.
One potential concern for the course instructors is that the increase in the number of answers submitted by students in the "badges on" group may have been offset by a corresponding reduction in their quality.
For example, perhaps these students were more likely to randomly make selections when answering questions.
It appears that this was not the case, as there is no significant difference between the two groups in terms of the correctness of the answers they submitted .
This data provides evidence that the presence of the badges within the PeerWise interface caused an increase in the number of answers submitted by students without a corresponding change in the proportion that were correct.
The badges also caused an increase in the number of days for which students were active, however the effect size is quite small and equates to an average of less than one day.
The badges did not have any impact on the number of questions authored by students.
Figure 10 summarizes all student responses to the survey.
Each survey question is represented by a horizontal bar, centered at "neutral", with segments proportionate in width to the number of responses of the corresponding type.
A bar that is generally positioned further to the right indicates a more positive response to the corresponding question.
Note that in the Figure, the survey questions have been edited for length - the full questions can be seen in Table 2.
Figures 8 and 9 offer alternative ways of visualizing the differences between the groups.
In each case, the shaded bars represent the "badges on" group.
Students in the "badges off" group outnumber students in the "badges on" group when answering no more than 50 questions or being active on no more than 6 distinct days .
However, when looking at students who answered more than 50 questions or who were active on more than 6 distinct days , "badges on" students tend to outnumber "badges off" students.
Students in both groups were positive regarding the learning value of the exercise, however there is no statistical difference between the group responses to Q1 or Q2.
Students indicated that authoring their own questions was most beneficial to their learning with 83% of all students in agreement and only 2.5% in disagreement with statement Q2.
However, answering questions created by their classmates was also rated as valuable, with 80% in agreement and fewer than 5% in disagreement with statement Q1.
Students in the "badges on" group, who responded to the survey questions regarding the badges, indicated that being able to earn them was enjoyable with more than 60% of responses in agreement and just 12.5% in disagreement with statement Q3.
These voluntary contributions suggest that students already find some intrinsic value in answering questions, as even in the "badges off" group more than four times the required number of answers were submitted.
That the badges appear to have an impact on this activity is consistent with Rajat Paharia's observation that the "entity being gamified needs to have some intrinsic value already" .
Not all students are motivated by badges to the same extent.
One way to gauge a student's interest in the badges is to examine the number of times they view the "Badges" page shown in the screenshot of Figure 3.
When a student earns a new badge, they are notified of this by a message that appears on the main menu.
This notification message prompts the student to view the "Badges" page, and the message disappears once the "Badges" page is viewed.
If the student earns additional badges before visiting the "Badges" page, the notification message simply changes to state how many new badges have been earned.
Figure 12 plots, for every student in the "badges on" group, the number of badges they earned against the number of times they viewed the "Badges" page.
While most students did not view the Badges page many times, a small number of students were clearly drawn to it, with 10 students viewing the page more than 40 times.
Of all enrolled students, 65.7% were female.
The percentage of female survey respondents was 64.9% which is representative of the class as a whole .
Responses by male students to most questions  did not differ from those of female students.
Figure 11 compares the responses of all "badges on" students by gender.
Identical questions are paired and the question number is suffixed with  for male responses and  for female responses.
Male students had significantly more experience playing online games  than females , with 14% of male students "Strongly-agreeing" with statement Q5 compared to just 5% of female students.
However, this had no impact on the reported "enjoyment" of the badges in PeerWise, as the distribution of male and female responses to Q3 did not differ significantly.
Our data suggests that the badges had a positive motivational effect, leading to an increase in both the number of answers submitted and the number of distinct days for which students were active.
However, the badges did not have an impact on the number of questions that were authored.
One explanation for this is that the authoring requirements in this course were low  and most students do not typically author more questions than they are required.
As a result, there is very little variability in the number of questions authored across all students in the class.
This phenomenon, of students preferring to answer rather than author questions voluntarily, has been previously reported  and is likely due to the greater effort and time required to author questions.
Also evident in Figure 12 is the fact that many students viewed the "Badges" page without being notified of having earned a badge.
This may indicate an interest in learning which badges are available and within reach, or simply reflect curiosity regarding how many other students have earned badges.
The maximum number of distinct notification messages any student can receive is equal to the number of badges they earn.
In practice, a student is likely to receive fewer notification messages than the number of badges they earn, as would be the case if they earn more than one badge prior to visiting the "Badges" page.
However, even using the number of badges earned as a conservative upper bound on the number of notifications received, 43 students viewed the "Badges" page at least once without being prompted and are represented by the data points lying above the straight line.
Only 49 students never visited the "Badges" page.
As we know all 504 students in the "badges on" group who were active earned at least one badge, this indicates that approximately 1 in 10 students simply ignored the badges altogether.
Finally, although the badges did have an effect on participation, they did not impact student perceptions of the learning value of the exercise.
The two groups were equally positive on the benefits of both authoring and answering questions.
As discussed, a possible explanation is that answering questions is a more familiar activity for students and one in which they immediately see value.
The implication for designers of similar systems is that defining badges that reward students for actions in which they already see value may have the greatest impact.
Our data also shows that across each measurable action, student activity either increased or was unchanged by the presence of the badges.
For example, both the number of questions authored, and the proportion of answers submitted that were correct were identical for students in both groups.
Reinforced by the very positive attitudes our survey revealed, the implication is that utilizing badges in similar contexts is a low-risk proposition for educators as they do not appear to negatively affect student participation.
In this section, we consider possible threats to the validity of our results.
Students in the "badges on" group were selected at random from more than 1000 students.
There were no other differences between the treatment of students in each group, and so in this course the presence of the badges appears to have caused the observed increase in student participation.
However, we need to be careful not to overgeneralize this result.
Firstly, POPLHLTH111 is a competitive course and students are typically very focused on achieving good grades and therefore tend to participate well with class activities.
The students in this experiment may not be representative of another cohort for a different class.
Also, the specific badges available in PeerWise were defined prior to our study and it is possible that a different set of badges may be more or less effective than the current set.
The requirements that an instructor places on students may also have an impact.
For example, students in POPLHLTH111 were required to author 1 and answer 20 questions for 1.5% course credit.
Simply meeting these minimum requirements would have at least earned students the first two "Basic" badges, and potentially an additional 3 badges if all questions were answered correctly.
Most students enrolled in POPLHLTH111 study a common set of subjects.
In the semester that this study took place, the vast majority of POPLHLTH111 students were concurrently enrolled in a first-year Biology course for which PeerWise was also being used.
At the request of the Biology instructor, the badges were enabled for all students for the duration of the course.
While it was not compulsory for students to engage with PeerWise in the Biology course, around three-quarters of all students did participate voluntarily.
It therefore must be noted that many students in the "badges off" group in our study would have experienced the PeerWise badges for this other course.
However, we do not believe that having access to the badges for the Biology course would have greatly influenced student engagement with the POPLHLTH111 repository.
Badges are awarded based on participation with a specific course and it is very clear within the interface which course is being interacted with at any one time.
This paper presents the first large-scale study providing empirical evidence of the impact of a badge-based achievement system within an online learning tool.
The badges had a significant positive effect on the number of questions answered and the number of distinct days that students were active with the tool, and did not lead to a reduction in the accuracy of student answers.
In addition, students enjoyed being rewarded with badges for their contributions and indicated a strong preference for having them in the interface.
Although increases did not exist across all measures of activity, such as the number of questions authored by students, no negative effects were observed.
Badges therefore can act as powerful motivators in educational contexts of this kind and may be integrated with little risk into similar environments.
Activity in PeerWise is currently anonymous, and so students who have earned the greatest number of badges cannot be identified by their peers.
As a result, the badges currently provide personal affirmation rather than status amongst peers.
An interesting area for future work will be exploring what it means to publicize these achievements more openly.
Allowing a student to display their badges to the rest of the class, or perhaps to a subset of friends who approve, may introduce a competitive element that further impacts engagement.
Future work will also explore why earning badges encouraged students to participate in certain ways, but not in others.
In general, the efficacy of a badge-based achievement system depends on many factors.
These include user demographics, the purpose of the tool, and the relevance of the badges for encouraging appropriate user behavior.
While this work has examined the implementation of one badge system and one particular set of badges in a specific domain, more work is needed to assess the impact of badges and achievements within a range of contexts.
Aleven, V., and Koedinger, K. An effective metacognitive strategy -- learning by doing and explaining with a computer-based cognitive tutor.
Antin, J., and Churchill, E. Badges in social media : A social psychological perspective.
Baker, R., Walonoski, J., Heffernan, N., Roll, I., Corbett, A., and Koedinger, K. Why students engage in "gaming the system" behavior in interactive learning environments.
Bottomley, S., and Denny, P. A participatory learning approach to biochemistry using student authored and evaluated multiple-choice questions.
Burguillo, J. C. Using game theory and competition-based learning to stimulate student motivation and performance.
Chi, M., Bassok, M., Lewis, M., Reimann, P., and Glaser, R. Self-explanations: How students study and use examples in learning to solve problems.
Chi, M., Leeuw, N., Chiu, M.-H., and LaVancher, C. Eliciting self-explanations improves understanding.
The PeerWise system of student contributed assessment questions.
In Proceedings of the tenth conference on Australasian computing education - Volume 78, ACE '08, Australian Computer Society, Inc. , 69-74.
Student use of the PeerWise system.
Deterding, S. Gamification: designing for motivation.
Deterding, S., Khaled, R., Nacke, L., and Dixon, D. Gamification: Toward a definition.
Electronic Entertainment Design and Research.
EEDAR study shows more achievements in games leads to higher review scores, increased sales.
Farzan, R., DiMicco, J. M., Millen, D. R., Brownholtz, B., Geyer, W., and Dugan, C. When the experiment is over: Deploying an incentive system to all the users.
In Proceedings of the Symposium on Persuasive Technology, In conjunction with the AISB 2008 Convention .
