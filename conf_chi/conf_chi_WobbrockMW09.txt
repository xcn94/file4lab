Many surface computing prototypes have employed gestures created by system designers.
Although such gestures are appropriate for early investigations, they are not necessarily reflective of user behavior.
We present an approach to designing tabletop gestures that relies on eliciting gestures from non-technical users by first portraying the effect of a gesture, and then asking users to perform its cause.
In all, 1080 gestures from 20 participants were logged, analyzed, and paired with think-aloud data for 27 commands performed with 1 and 2 hands.
Our findings indicate that users rarely care about the number of fingers they employ, that one hand is preferred to two, that desktop idioms strongly influence users' mental models, and that some commands elicit little gestural agreement, suggesting the need for on-screen widgets.
We also present a complete user-defined gesture set, quantitative agreement scores, implications for surface technology, and a taxonomy of surface gestures.
Our results will help designers create better gesture sets informed by user behavior.
Recently, researchers in human-computer interaction have been exploring interactive tabletops for use by individuals  and groups , as part of multi-display environments , and for fun and entertainment .
A key challenge of surface computing is that traditional input using the keyboard, mouse, and mouse-based widgets is no longer preferable; instead, interactive surfaces are typically controlled via multi-touch freehand gestures.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To date, most surface gestures have been defined by system designers, who personally employ them or teach them to user-testers .
Despite skillful design, this results in somewhat arbitrary gesture sets whose members may be chosen out of concern for reliable recognition .
Although this criterion is important for early prototypes, it is not useful for determining which gestures match those that would be chosen by users.
It is therefore timely to consider the types of surface gestures people make without regard for recognition or technical concerns.
What kinds of gestures do non-technical users make?
In users' minds, what are the important characteristics of such gestures?
Does number of fingers matter like it does in many designer-defined gesture sets?
How consistently are gestures employed by different users for the same commands?
Although designers may organize their gestures in a principled, logical fashion, user behavior is rarely so systematic.
As McNeill  writes in his laborious study of human discursive gesture, "Indeed, the important thing about gestures is that they are not fixed.
They are free and reveal the idiosyncratic imagery of thought" .
To investigate these idiosyncrasies, we employ a guessability study methodology  that presents the effects of gestures to participants and elicits the causes meant to invoke them.
By using a think-aloud protocol and video analysis, we obtain rich qualitative data that illuminates users' mental models.
By using custom software with detailed logging on a Microsoft Surface prototype, we obtain quantitative measures regarding gesture timing, activity, and preferences.
Moreover, we explicitly recruited non-technical people without prior experience using touch screens , expecting that they would behave with and reason about interactive tabletops differently than designers and system builders.
This work contributes the following to surface computing research:  a quantitative and qualitative characterization of user-defined surface gestures, including a taxonomy,  a user-defined gesture set,  insight into users' mental models when making surface gestures, and  an understanding of implications for surface computing technology and user interface design.
Our results will help designers create better gestures informed by user behavior.
Some prior work has directly employed users to define input systems, as we do here.
Incorporating users in the design process is not new, and is most evident in participatory design .
Our approach of prompting users with referents, or effects of an action, and having them perform signs, or causes of those actions, was used by Good et al.
It was also used by Wobbrock et al.
A limited study similar to the current one was conducted by Epps et al.
They found that the use of an index finger was the most common gesture, but acknowledged that their Windows-based prompts may have biased participants to simply emulate the mouse.
Similarly, the gestures from the Charade system  were influenced by observations of presenters' natural hand movements.
Other work has employed a Wizard of Oz approach.
They found that gestures were used for executing simple, direct, physical commands, while speech was used for high level or abstract commands.
Robbe  followed this work with additional studies comparing unconstrained and constrained speech input, finding that constraints improved participants' speed and reduced the complexity of their expressions.
Beringer  elicited gestures in a multimodal application, finding that most gestures involved pointing with an arbitrary number of fingers--a finding we reinforce here.
They asked users to generate gestures for accessing multiple projected displays, finding that people overwhelming used finger-pointing.
Efron  conducted one of the first studies of discursive human gesture resulting in five categories on which later taxonomies were built.
The categories were physiographics, kinetographics, ideographics, deictics, and batons.
The first two are lumped together as iconics in McNeill's classification .
McNeill also identifies metaphorics, deictics, and beats.
Because Efron's and McNeill's studies were based on human discourse, their categories have only limited applicability to interactive surface gestures.
Kendon  showed that gestures exist on a spectrum of formality and speech-dependency.
From least to most formal, the spectrum was: gesticulation, language-like gestures, pantomimes, emblems, and finally, sign languages.
Although surface gestures do not readily fit on this spectrum, they are a language of sorts, just as direct manipulation interfaces are known to exhibit linguistic properties .
Poggi  offers a typology of four dimensions along which gestures can differ: relationship to speech, spontaneity, mapping to meaning, and semantic content.
Rossini  gives an overview of gesture measurement, highlighting the movement and positional parameters relevant to gesture quantification.
Tang  analyzed people collaborating around a large drawing surface.
Gestures emerged as an important element for simulating operations, indicating areas of interest, and referring to other group members.
Tang noted actions and functions, i.e., behaviors and their effects, which are like the signs and referents in our guessability methodology .
Their classification uses seven dimensions.
These dimensions address groups of users and omit issues relevant to single-user gestures, which we cover here.
Some working tabletop systems have defined designermade gesture sets.
Wu and Balakrishnan  built RoomPlanner, a furniture layout application for the DiamondTouch , supporting gestures for rotation, menu access, object collection, and private viewing.
Some prototypes have employed novel architectures.
Rekimoto  created SmartSkin, which supports gestures made on a table or slightly above.
Physical gestures for panning, scaling, rotating and "lifting" objects were defined.
Finally, some systems have separated horizontal touch surfaces from vertical displays.
The system distinguished among 1-, 2-, 3-, and 5-finger gestures, a feature our current findings suggest may be problematic for users.
Moscovich and Hughes  defined three multi-finger cursors to enable gestural control of desktop objects.
Each participant saw the effect of a gesture  and was asked to perform the gesture he or she thought would cause that effect .
In linguistic terms, the effect of a gesture is the referent to which the gestural sign refers .
Twenty-seven referents were presented, and gestures were elicited for 1 and 2 hands.
The system did not attempt to recognize users' gestures, but did track and log all hand contact with the table.
Participants used the think-aloud protocol and were videotaped.
They also supplied subjective preference ratings.
The final user-defined gesture set was developed in light of the agreement participants exhibited in choosing gestures for each command .
The more participants that used the same gesture for a given command, the more likely that gesture would be assigned to that command.
In the end, our user-defined gesture set emerged as a surprisingly consistent collection founded on actual user behavior.
Conceivably, one could design a system in which all commands were executed with gestures, but this would be difficult to learn .
So what is the right number of gestures to employ?
For which commands do users tend to guess the same gestures?
If we are to choose a mix of gestures and widgets, how should they be assigned?
To answer these questions, we presented the effects of 27 commands  to 20 participants, and then asked them to invent corresponding gestures .
Some were conceptually straightforward, others more complex.
The three authors independently rated each referent's conceptual complexity before participants made gestures.
Table 1 shows the referents and ratings.
A human's use of an interactive computer system comprises a user-computer dialogue , a conversation mediated by a language of inputs and outputs.
As in any dialogue, feedback is essential to conducting this conversation.
When something is misunderstood between humans, it may be rephrased.
The same is true for user-computer dialogues.
Feedback, or lack thereof, either endorses or deters a user's action, causing the user to revise his or her mental model and possibly take a new action.
In developing a user-defined gesture set, we did not want the vicissitudes of gesture recognition to influence users' behavior.
Hence, we sought to remove the gulf of execution  from the dialogue, creating, in essence, a monologue in which the user's behavior is always acceptable.
This enables us to observe users' unrevised behavior, and drive system design to accommodate it.
Another reason for examining users' unrevised behavior is that interactive tabletops may be used in public spaces, where the importance of immediate usability is high.
In view of this, we developed a user-defined gesture set by having 20 non-technical participants perform gestures on a Microsoft Surface prototype .
To avoid bias , no elements specific to Windows or the Macintosh were shown.
Similarly, no specific application domain was assumed.
Twenty paid participants volunteered for the study.
No participant had used an interactive tabletop, Apple iPhone, or similar.
All were recruited from the general public and were not computer scientists or user interface designers.
Participant occupations included restaurant host, musician, author, steelworker, and public affairs consultant.
Hand pose is held in one location.
Hand pose changes in one location.
Hand pose is held as hand moves.
Hand pose changes as hand moves.
Static pose with one finger.
Static pose & path with one finger.
Gesture visually depicts a symbol.
Gesture acts physically on objects.
Location can ignore world features.
Response occurs after the user acts.
Response occurs while the user acts.
After the voice finished, our software animated a field of objects moving from left to right.
After the animation, the software showed the objects as they were before the panning effect, and waited for the user to perform a gesture.
The Surface vision system watched participants' hands from beneath the table and reported contact information to our software.
All contacts were logged as ovals having millisecond timestamps.
These logs were then parsed by our software to compute trial-level measures.
Participants' hands were also videotaped from four angles.
In addition, two authors observed each session and took detailed notes, particularly concerning the think-aloud data.
The authors manually classified each gesture along four dimensions: form, nature, binding, and flow.
Within each dimension are multiple categories, shown in Table 2.
The scope of the form dimension is within one hand.
It is applied separately to each hand in a 2-hand gesture.
Onepoint touch and one-point path are special cases of static pose and static pose and path, respectively.
These are worth distinguishing because of their similarity to mouse actions.
A gesture is still considered a one-point touch or path even if the user casually touches with more than one finger at the same point, as our participants often did.
We investigated such cases during debriefing, finding that users' mental models of such gestures involved only one contact point.
In the nature dimension, symbolic gestures are visual depictions.
Examples are tracing a caret  to perform insert, or forming the O.K.
Physical gestures should ostensibly have the same effect on a table with physical objects.
Metaphorical gestures occur when a gesture acts on, with, or like something else.
Examples are tracing a finger in a circle to simulate a "scroll ring," using two fingers to "walk" across the screen, pretending the hand is a magnifying glass, swiping as if to turn a book page, or just tapping an imaginary button.
Of course, the gesture itself usually is not enough to reveal its metaphorical nature; the answer lies in the user's mental model.
Finally, abstract gestures have no symbolic, physical, or metaphorical connection to their referents.
The mapping is arbitrary, which does not necessarily mean it is poor.
Triple-tapping an object to delete it, for example, would be an abstract gesture.
In the binding dimension, object-centric gestures only require information about the object they affect or produce.
An example is pinching two fingers together on top of an object for shrink.
Our software randomly presented 27 referents  to participants.
For each referent, participants performed a 1hand and a 2-hand gesture while thinking aloud, and then indicated whether they preferred 1 or 2 hands.
After each gesture, participants were shown two 7-point Likert scales concerning gesture goodness and ease.
Of these, 6 were discarded due to participant confusion.
As noted in related work, gesture classifications have been developed for human discursive gesture , multimodal gestures with speech , cooperative gestures , and pen gestures .
However, no work has established a taxonomy of surface gestures based on user behavior to capture and describe the gesture design space.
After all 20 participants had provided gestures for each referent for one and two hands, we grouped the gestures within each referent such that each group held identical gestures.
Group size was then used to compute an agreement score A that reflects, in a single number, the degree of consensus among participants.
World-independent gestures require no information about the world, and generally can occur anywhere.
We include in this category gestures that can occur anywhere except on temporary objects that are not world features.
Finally, mixed dependencies occur for gestures that are worldindependent in one respect but world-dependent or objectcentric in another.
This sometimes occurs for 2-hand gestures, where one hand acts on an object and the other hand acts anywhere.
A gesture's flow is discrete if the gesture is performed, delimited, recognized, and responded to as an event.
An example is tracing a question mark 
Flow is continuous if ongoing recognition is required, such as during most of our participants' resize gestures.
Discrete and continuous gestures have been previously noted .
1, r is a referent in the set of all referents R, Pr is the set of proposed gestures for referent r, and Pi is a subset of identical gestures from Pr.
As an example, consider agreement for move a little  and select single .
Both had four groups of identical gestures.
Agreement for our study is graphed in Figure 3.
Referents' conceptual complexities  correlated significantly and inversely with their agreement , as more complex referents elicited lesser gestural agreement.
We found that our taxonomy adequately describes even widely differing gestures made by our users.
Figure 2 shows for each dimension the percentage of gestures made within each category for all gestures in our study.
An interesting question is how the conceptual complexity of referents  affected gesture nature .
Thus, simpler commands more often resulted in physical gestures, while more complex commands resulted in metaphorical or symbolic gestures.
The user-defined gesture set was developed by taking the largest groups of identical gestures for each referent and assigning those groups' gestures to the referent.
However, where the same gesture was used to perform different commands, a conflict occurred because one gesture cannot result in different outcomes.
To resolve this, the referent with the largest group won the gesture.
Our resulting userdefined gesture set  is conflict-free and covers 57.0% of all gestures proposed.
Twenty-two of 27 referents from Table 1 were assigned dedicated gestures, and the two move referents were combined.
Four referents were not assigned gestures: insert, maximize, task switch, and close.
For the first two, the action most participants took comprised more primitive gestures: insert used dragging, and maximize used enlarging.
For the second two, participants relied on imaginary widgets; a common gesture was not feasible.
For example, most participants performed task switch by tapping an imaginary taskbar button, and close by tapping an imaginary button in the top-right corner of an open view.
Immediately after performing each gesture, participants rated it on two Likert scales.
The first read, "The gesture I picked is a good match for its intended purpose."
The second read, "The gesture I picked is easy to perform."
Both scales solicited ordinal responses from 1 = strongly disagree to 7 = strongly agree.
Gestures that were members of larger groups of identical gestures for a given referent had significantly higher goodness ratings =34.10, p<.0001, indicating that popularity does, in fact, identify better gestures over worse ones.
This finding goes a long way to validating this user-driven approach to gesture design.
Referents' conceptual complexities  correlated significantly and inversely with participants' average gesture goodness ratings .
The more complex referents were more likely to elicit gestures rated poor.
The simpler referents elicited gestures rated 5.6 on average, while more complex referents elicited gestures rated 4.9.
Referents' conceptual complexities did not correlate significantly with average ratings of gesture ease.
Generally, as planning time increased, goodness ratings decreased, suggesting that good gestures were those most quickly apparent to participants.
Planning time did not affect perceptions of gesture ease.
Surprisingly, gestures that took longer to perform were generally rated as easier, perhaps because they were smoother or less hasty.
Gestures rated as easy took about 3.4 seconds, while those rated as difficult took about 2.0 seconds.
These subjective findings are corroborated by objective counts of finger touch events , which may be considered rough measures of a gesture's activity or "energy."
Clearly, long lived gestures will have more touch events.
Gestures with the fewest touch events were rated as the hardest; those with about twice as many touch events were rated as easier.
Our user-defined set is useful, therefore, not just for what it contains, but also for what it omits.
Aliasing has been shown to dramatically increase input guessability .
In our user-defined set, ten referents are assigned 1 gesture, four referents have 2 gestures, three referents have 3 gestures, four referents have 4 gestures, and one referent has 5 gestures.
There are 48 gestures in the final set.
Gratifyingly, a high degree of consistency and symmetry exists in our user-defined set.
Dichotomous referents use reversible gestures, and the same gestures are reused for similar operations.
For example, enlarge, which can be accomplished with four distinct gestures, is performed on an object, but the same four gestures can be used for zoom in if performed on the background, or for open if performed on a container .
Flexibility exists insofar as the number of fingers rarely matters and the fingers, palms, or edges of the hands can often be used interchangeably.
In general, the more complex the referent, the more time participants took to begin articulating their gesture.
Simple referents took about 8 seconds of planning.
Complex referents took about 15 seconds.
Conceptual complexity did not, however, correlate significantly with gesture articulation time.
Overall, participants preferred 1-hand gestures for 25 of 27 referents , and were evenly divided for the other two.
No referents elicited gestures for which two hands were preferred overall.
Interestingly, the referents that elicited equal preference for 1- and 2-hands were insert and maximize, neither of which were included in the userdefined gesture set because they reused existing gestures.
Although participants' preferences for 1-hand gestures was strong, some 2-hand gestures had good agreement scores and nicely complemented their 1-hand counterparts.
Gestures depicted as using one finger could be performed with 1-3 fingers.
Gestures not depicted as occurring on top of an object are performed on the background region of the surface or full-screen object.
To save space, reversible gestures  have been depicted in only one direction.
Examples of dichotomous referents are shrink / enlarge, previous / next, zoom in / zoom out, and so on.
People generally employed reversible gestures for dichotomous referents, even though the study software did not present these referents together.
This user behavior is reflected in the final user-designed gesture set, where dichotomous referents use reversible gestures.
In all, about 72% of gestures were mouse-like one-point touches or paths.
In addition, some participants tapped an object first to select it, then gestured on top of the very same object, negating a key benefit of gestures that couples selection and action .
The close and task switch referents were accomplished using imaginary widgets located at objects' top-right and the screen's bottom, respectively.
Even with simple shapes, it was clear how deeply rooted the desktop is.
Some quotes reveal this: "Anything I can do that mimics Windows--that makes my life easier," "I'm falling back on the old things that I've learned," and "I'm a child of the mouse."
The rank order of referents according to conceptual complexity in Table 1 and the order of referents according to descending 1-hand agreement in Figure 3 are not identical.
Thus, participants and the authors did not always regard the same referents as "complex."
Participants often made simplifying assumptions.
One participant, upon being prompted to zoom in, said, "Oh, that's the same as enlarge."
Similar mental models emerged for enlarge and maximize, shrink and minimize, and pan and move.
This allows us to unify the gesture set and disambiguate the effects of gestures based on where they occur, e.g., whether the gesture lands on an object or on the background.
To our surprise, multiple participants conceived of a world beyond the edges of the table's projected screen.
For example, they dragged from off-screen onto the screen, treating it as the clipboard.
They also dragged to the offscreen area for delete and reject.
One participant conceived of different off-screen areas that meant different things: dragging off the top was delete, and dragging off the left was cut.
For paste, she made sure to drag in from the left side, purposefully trying to associate paste and cut.
Thirteen of 20 participants used varying numbers of fingers when acting on the surface.
Of these, only two said that the number of fingers actually mattered.
Four people said they often used more fingers for "larger objects," as if these objects required greater force.
One person used more fingers for "enlarging actions," the effects of which had something to do with increasing size .
Another person felt she used more fingers for commands that executed "a bigger job."
One participant said that he used more fingers "to ensure that I was pressing," indicating that to him, more fingers meant more reliable contact.
This may be, at least in part, due to the lack of feedback from the table when it was being touched.
Interestingly, two participants who regularly used onefinger touches felt that the system needed to distinguish among fingers.
For example, one participant tapped with his ring finger to call up a menu, reasoning that a ring-finger tap would be distinct from a tap with his index finger.
In general, it seemed that touches with 1-3 fingers were considered a "single point," and 5-finger touches or touches with the whole palm were something more.
Four fingers, however, constituted a "gray area" in this regard.
These findings disagree with many prior tabletop systems that have used designer-made gestures differentiated only on the basis of the number of fingers used .
We instructed participants to touch the table while gesturing.
Even so, some participants gestured in ways few tables could detect.
One participant placed a hand palm-up on the table and beckoned with her fingers to call for help.
Another participant put the edges of her hands in an "X" on the table such that the top hand was about 3" off the table's surface.
One user "lifted" an object with two hands, placing it on the clipboard.
Acting in the air, another participant applied "glue" to an object before pasting it.
Before the study began, the three authors independently designed their own gestures for the 27 referents shown in Table 1.
Although the authors are experts in humancomputer interaction, it was hypothesized that the "wisdom of crowds" would generate a better set than the authors.
Indeed, each author individually came up with only 43.5% of the user-defined gestures.
Even combined, the authors only covered 60.9% of the users' set.
This suggests that three experts cannot generate the scope of gestures that 20 participants can.
That said, 19.1% of each author's gestures were gestures never tried by any participant, which indicates that the authors are either thinking creatively or are hopelessly lost!
Either way, the benefit of incorporating users in the development of input systems is clear .
That our participatory approach would produce a coherent gesture set was not clear a priori; indeed, it reflects well on our methodology that the proposed gestures seem, in hindsight, to be sensible choices.
Additionally, the user-defined gesture set differs from sets proposed in the literature, for example, by allowing flexibility in the number of fingers that can be used, rather than binding specific numbers of fingers to specific actions .
Also, our user-defined gestures differ from prior surface systems by providing multiple gestures for the same commands, which enhances guessability .
Hit-testing within objects will be necessary for taking the right action.
However, whenever possible, demands for precise positioning should be avoided.
Only 2 of 14 participants for 2-hand enlarge resized along the diagonal; 12 people resized sideways, unconcerned that doing so would perform a non-uniform scale.
Similarly, only 1 of 5 used a diagonal "reverse pinch" to resize along the diagonal, while 4 of 5 resized in other orientations.
Gestures should not be distinguished by number of fingers.
People generally do not regard the number of fingers they use in the real world, except in skilled activities such as playing the piano, using a stenograph, or giving a massage.
Four fingers should serve as a boundary between a fewfinger single-point touch and a whole-hand touch.
Many of the gestures we witnessed had strong implications for surface recognition technology.
With the large number of physical gestures , for example, the idea of using a physics engine  rather than a traditional recognizer has support.
Seven participants, for example, expected intervening objects to move out of the way when dragging an object into their midst.
Four participants "threw" an object off-screen to delete or reject it.
However, given the abundance of symbolic, abstract, and metaphorical gestures, a physics engine alone will probably not suffice as an adequate recognizer for all surface gestures.
Although there are considerable practical challenges, tabletop systems may benefit from the ability to look down or sideways at users' hands, rather than just up.
Not only does this increase the range of possible gestures, but it provides robustness for users who forget to remain in contact with the surface at all times.
Of course, interactive systems that provide feedback will implicitly remind users to remain in contact with the table, but users' unaltered tendencies clearly suggest a use for off-table sensing.
Similarly, systems might employ a low-resolution sensing boundary beyond the high-resolution display area.
This would allow the detection of fingers dragging to or from off-screen.
Conveniently, these gestures have alternatives in the user-defined set for tables without a sensing boundary.
The current study removed the dialogue between user and system to gain insight into users' behavior without the inevitable bias and behavior change that comes from recognizer performance and feedback.
But there are drawbacks to this approach.
For instance, users could not change previous gestures after moving on to subsequent ones; perhaps users would have performed differently if they first saw all referents, and then picked gestures in an order of their choosing.
Application context could also impact users' choice of gestures, as could the larger contexts of organization and culture.
Our participants were all non-technical literate American adults; undoubtedly, children, Eastern, or uneducated participants would behave differently.
These issues are worthy of investigation, but are beyond the scope of the current work.
Thankfully, even with a lack of application context and upfront knowledge of all referents, participants still exhibited a substantial level of agreement in making their gestures, allowing us to create a coherent user-defined gesture set.
An important next step is to validate our user-defined gesture set.
Unlabeled video clips of the gestures can be shown to 20 new participants, along with clips of designers' gestures, to see if people can guess which gestures perform which commands.
After, the user-defined gesture set can be implemented with a vision-based gesture recognizer so that system performance and recognition rates can be measured.
Our study of users' gestures has implications for tabletop user interface design, too.
For example, Figure 2 indicates that agreement is low after the first seven referents along the x-axis.
This suggests that referents beyond this point may benefit from an on-screen widget as well as a gesture.
Moreover, enough participants acted on imaginary widgets that system designers might consider using widgets along with gestures for delete, zoom in, zoom out, accept, reject, menu access, and help.
Gesture reuse is important to increase learnability and memorability .
Our user-designed set emerged with reusable gestures for analogous operations, relying on the target of the gesture for disambiguation.
For example, splaying 5 fingers outward on an object will enlarge it, but doing so in the background will zoom in.
In our study, object boundaries mattered to participants.
We have presented a study of surface gestures leading to a user-defined gesture set based on participants' agreement over 1080 gestures.
Beyond reflecting user behavior, the user-defined set has properties that make it a good candidate for deployment in tabletop systems, such as ease of recognition, consistency, reversibility, and versatility through aliasing.
We also have presented a taxonomy of surface gestures useful for analyzing and characterizing gestures in surface computing.
In capturing gestures for this study, we have gained insight into the mental models of non-technical users and have translated these into implications for technology and design.
