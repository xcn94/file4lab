Every so often, it is useful to re-evaluate the standard set of techniques used within an approach and consider whether they provide researchers with the tools they need to answer the questions they are interested in, and whether other techniques would in fact serve the community better.
Such a debate is currently taking place in the field of psychology, as demonstrated by a recent special collection of papers within Perspectives on Psychological Science .
Critics of the traditional statistical inference method of significance testing argue that "it is time for researchers to consider foundational issues in inference" .
In fact this is a long-standing problem; Cohen noted already in 1994 that such criticisms have been made within psychology for forty years .
As a field of study that builds upon statistical methods used by psychologists, usability evaluation is subject to the same criticisms.
Indeed, a small number of HCI researchers have identified flaws in experimental design and statistical testing in usability studies.
In 1998 Gray and Salzman published an in-depth critique of five well known studies of usability evaluation methods, observing that weaknesses in experimental design  call into question the reliability of these findings .
More recently, Cairn's survey of inferential statistics in BCS HCI conferences and two leading HCI journals noted common problems in reporting of statistical results; failure to check assumptions about the data required by particular tests, over-testing and using inappropriate tests .
Dunlop and Baillie  aimed to raise awareness within the sub-field of mobile HCI of problems with statistical analysis techniques such as the use of null hypothesis testing in a binary way to approve results, abusing statistical tests, making illogical arguments as a result of tests, deriving inappropriate conclusions from nonsignificant results, and confusing the size of p-values with effect sizes.
CHI researchers typically use a significance testing approach to statistical analysis when testing hypotheses during usability evaluations.
However, the appropriateness of this approach is under increasing criticism, with statisticians, economists, and psychologists arguing against the use of routine interpretation of results using "canned" p values.
Three problems with current practice - the fallacy of the transposed conditional, a neglect of power, and the reluctance to interpret the size of effects - can lead us to build weak theories based on vaguely specified hypothesis, resulting in empirical studies which produce results that are of limited practical or scientific use.
Using publicly available data presented at CHI 2010  as an example we address each of the three concerns and promote consideration of the magnitude and actual importance of effects, as opposed to statistical significance, as the new criteria for evaluating CHI research.
A core strength of the CHI community is that members bring together expertise from a range of disciplines "as diverse as user interface design, human factors, computer science, psychology, engineering, graphics and industrial design, entertainment, and telecommunications" .
Correspondingly, the community has a rich set of design and evaluation practices at its disposal.
An important aspect of training new interaction designers is to teach them how to use different data gathering and analysis techniques "flexibly and in combination to avoid biases which are inherent in any one approach" .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In our own examination of the CHI 2011 proceedings we found that 35  papers report results from a t-test.
Of these 35 papers, six do not report any descriptive statistics, none report a standardized effect size, and only nine make an attempt to describe or interpret the effect size .
None of the papers related the magnitude of the effect to previous findings in the literature.
We believe this shows that the issues we address in this paper are relevant to the CHI community.
In this paper we delve deeper into the criticisms raised by earlier scholars by considering an underlying issue: should statistical significance testing be the "gold standard" for quantitative empirical work within our field?
In line with the discussion of Ziliak and McCloskey  we argue that there are fundamental flaws associated with sciences that are built primarily on the interpretation of pvalues.
In this paper we focus on three common problems: 1.
The fallacy of the transposed conditional - Researchers often wrongly interpret p-values as the probability of the null-hypothesis being true.
A lack of power - Researchers often pay little attention to null-results without being aware of the potential of their experimental set-up to reject the null when it is false.
Confusion between p-values and estimates of effects- Researchers often judge a small p-value as indicating a  important relationship.
This however is not generally correct.
In the remainder of this paper we will first conceptually explain the "traditional" statistics that most of HCI's quantitative results rely on.
Next, we will address each of the three errors and, using a running example show  how they arise, and  how they could be mitigated.
Our example is based on publically available simulated data previously published at the CHI 2010 conference .
We have chosen this data set to avoid singling out studies by other researchers for criticism, and to make it possible for the interested reader to download the data to study the examples for themselves.
The reader is referred to  and  for a discussion of these and related issues in the fields of economics and psychology.
The proportion of times the null is false but was accepted is called beta.
The power of an experiment  is the probability of detecting an effect given that the effect really exists in the population.
If it is sufficiently unlikely that the observed data was generated by a process that is adequately described by the null hypothesis, then the null is rejected and another, alternative, hypothesis is taken as truth.
Sufficiently unlikely is in most null hypothesis tests defined in terms of a ratio of signal and sampling error.
Higher t-values indicate that it's less likely that the sample mean  would be observed if indeed in the population mean was equal to u0.
Thus high t-values lead to low p -values: the probability of observing the current data given the null hypothesis.
Low p-values - those lower than .05 - would in turn drive most researchers to conclude that the null-hypothesis is not true, and thus some alternative hypothesis should be accepted.
The traditional approach to statistics within many scientific fields is to use significance testing.
In this familiar decision making procedure, the null hypothesis is compared to an alternative hypothesis and one or the other is rejected.
The great advantage of this decision-making procedure is that long-term error rates are known, and therefore can be controlled.
Researchers can control for both Type I and Type II errors.
Type I errors occur when the null hypothesis is rejected when it is actually true and can be controlled by specifying an alpha value  which specifies the level of significance under which the null hypothesis will be rejected.
This approach, fiercely promoted by Fisher in the 1930's , has become the gold standard in many disciplines including quantitative evaluations in HCI.
However, the approach is rather counter-intuitive; many researchers misinterpret the meaning of the p-value.
To illustrate this point Oakes posed a series of true/false questions regarding the interpretation of p-vales to seventy experienced researchers and discovered that only two had a sound understanding of the underlying concept of significance .
So what does a p-value actually mean?
Because p-values are based on the idea that probabilities are long run frequencies, they are properties of a collective of events rather than single event.
For guidance on setting the value for a prior expectation, consult  and .
Over the last decades this Bayesian approach to computing what we actually want to know, the probability that our hypothesis are true, has gained increasing attention among statisticians and researchers.
The Bayesian t-test allows researchers to compute the ratio of the likelihood of two competing hypothesis, for example the null hypothesis and an alternative hypothesis .
The resulting "Bayes Factor" values of greater than 1 indicate evidence for the null hypothesis, and values less than 1 give support for the alternative hypothesis.
Heuristics to judge the strength of support for the null and alternative hypotheses given by a range of Bayes Factor values are listed in .
The use of Bayesian statistics rather than traditional statistics has considerable practical ramifications.
That is, for studies which reported significant effects  70% of them had Bayes Factors indicating only anecdotal evidence in support of the alternative hypothesis.
Changing conventions about the statistical tests accepted within a community is a slow business.
However, it is no longer the case that the calculations themselves are difficult.
Using packages like {BayesFactorPCL}  researchers can now easily compute Bayes factors.
Full Bayesian analysis reaches beyond the Bayesian t-test and provides methods for model comparisons as well as test for different hypothesis.
In each case the quantity of interest, P, informs researchers about the decisions to make based on their collected data.
We do not advocate a shift from "canned" p-values to "canned" Bayes Factors - researchers' careful interpretations are still vital- but used appropriately, the Bayesian approach is a solution to the Fallacy of the Transposed conditional.
The false interpretation of the p-value by most researchers brings up the first problem with null-hypothesis testing.
Researchers often interpret the p-value to quantify the probability that the null hypothesis is true.
Under this misinterpretation the p-value would quantify P - the probability that the null hypothesis  is true, given the data  collected in the experiment.
However, the correct interpretation of the p-value is rather different: it quantifies P - the probability of the data given that H0 is true.
Researchers who state that it is very unlikely that the null hypothesis is true based on a low pvalue are attributing an incorrect meaning to the p-value.
It is easy to understand why this misconception is incorrect by the following example: consider the probability of being dead after being lynched, P.
Most would estimate this to be very high, say 0.99.
However, the mere observation of a dead person does not lead most people to believe that the corpse was lynched - after all, there are many possible ways to die which don't involve lynching.
P is  estimated to be rather small.
Thus, the probability of the null-hypothesis being true given the data depends on the probability of the data given the null-hypothesis, P, the prior probability of the nullhypothesis, P, and the probability of the data P.
The prior probability refers to the probability of the hypothesis before the current set of data is collected.
It can be seen that for the lynching example the prior probability is so low that the transposed conditional is also low.
The probability of the data, P, is often difficult to compute directly.
In the discrete case it is given by the sum of the probability of the data given all competing hypotheses.
However, in practice its computation is hardly necessary: If a researcher wants to decide between two competing hypotheses P will be the same in the computation of both P and P and thus merely acts as a normalizing constant.
This  dataset describes the potential outcomes of a usability evaluation of two different operating systems.
While the original dataset contains usability ratings - answers to the statement "The system was easy to use" on a seven-point scale - at two points in time, we focus only on the measurements obtained in the first time point.
The data describes the scores of participants on this question after using  Windows Vista, or  Apple Mac OS-X.
The dataset provides the obtained scores for three evaluations with different samples sizes: N=10, N=40, and N=200.
We believe this dataset - a straightforward rating presenting a comparison between two conditions  for differing sample sizes - provides a good numerical example to illustrate the problems raised in this paper.
Table 1 presents the mean score on the statement for the two different groups for each of the 3 sample sizes reported upon in .
The results for the N=200 case are statistically significant with a difference in means of 0.91 points on the seven-point scale.
This is not possible with traditional statistics but is of high importance because it enables us to distinguish between cases where the data is inconclusive  and cases where there is strong evidence regarding the null hypothesis .
The use of p-values enables researchers to control Type I errors - or the rejection of H0 while in fact it is true.
However, controlling Type II errors  through calculating the power of an experiment appears to be attended to less frequently .
The power of a statistical test is the long-term probability that a given test will find an effect assuming that one exists in the population.
Thus, power indicates whether your experimental setup is capable of detecting the effect that you wish to find.
The power is a function of sample size, population effect size and the significance criteria .
The standard accepted power within psychology is .80  which means that there would be 20%  chance that the researcher fails to reject the null hypothesis when it is false.
Reviews of the psychology literature reveal that the majority of published studies lack power, resulting in a confusing literature with apparently contradictory results .
In studies with low power, getting a null result is not particularly informative: it does not distinguish between the cases where the null is true and where the experimental setup did not detect the null.
The result obtained for N=40, a more typical case in many HCI studies, is however not straightforward.
The p-value of .126 would lead many to conclude that there is no significant difference between the usability of Windows Vista and that of Apple Mac OS-X based on the obtained ratings.
Often, the observed difference of 0.81 points would be neglected and the results not further discussed.
Researchers however can compute the actual probability of H0 given the data to further interpret their results.
Results from a Bayesian t-test give a value of 1.53 for the N=40 case.
This Bayes Factor describes the likelihood of the nullhypothesis compared to an alternative, non-informative, hypothesis.
In this case, the result would be interpreted as providing weak evidence in favor of the null hypothesis ; this evidence would be classified as only "anecdotal" by .
For the N=10 case the Bayes Factor is 1.69, leading to a similar conclusion.
For N=200 the Bayes Factor leads to a similar conclusion as the standard t-test: a Bayes Factor smaller than 0.01 provides strong evidence against the null hypothesis.
What can researchers do to address lack of power in their studies?
Maxwell  recommends that power calculations should be performed before the experiment is carried out, and that they should be reported as standard in empirical papers.
Cohen  gives some heuristics for required sample sizes for eight commonly used statistical tests, given the effect size that is deemed important or sought by the researcher.
Consider an example which might occur within usability studies: a researcher is comparing two versions of the same interface with a between subjects design using number of errors as a dependent variable.
For analysis using a two tailed independent samples t-test with alpha set at .05, with a power of .80 and attempting to detect a medium sized effect , the researcher should recruit 176 participants in each group.
Netx to Cohen's heuristics, software packages such as the {pwr} package in   can be used for more accurate results, or more complex designs.
Power calculations would at least make researchers aware of the problem, but what can be done to increase power if it is found to be low?
The most obvious way to increase power is to sample size.
Of course, this can be impractical fields, including HCI, but there are ways around example, Maxwell  suggests that in the psychology researchers could gain power by increase in many this.
Hansen and Collins discuss approaches to increasing power, which do not require an increase in sample size .
Although their recommendations are intended for epidemiologists, some are pertinent to HCI such as preventing attrition from studies, increasing the difference between groups by appropriately timing follow-up studies, and reducing variance within groups by using a more homogenous set of participants.
They also discuss the virtue of using more reliable and appropriate measurement instruments.
For example, in the context of HCI, this suggests the more widespread use of thoroughly validated standard attitudinal scales rather than researchers creating bespoke questionnaires specifically for a new study as is often current practice .
Such scales should be sensitive enough to capture differences between groups as advised in .
We will illustrate the often surprising lack of power in HCI experiments by following up on the results presented for the N=40 case comparing the usability ratings of Windows Vista and Apple Mac OS-X that we also used to demonstrate Bayes Factors.
Here we compute the power of the difference presented in Table 1.
The difference in means is 0.81, and the pooled variance is 2.2.
Given the between subjects design with 20 users each, this gives a power of 0.14.
The inverse of the power, 1-0.14 = 0.86 is the probability of making a type II error: a failure to reject the null when it is false.
Thus, given this experimental setup, and the estimated effect size, a researcher would not detect an effect this size even if it were actually present in the population in 86 out of a 100 similar experiments.
This low power for the given effect size and sample size again illustrates the point made in the Bayesian analysis: the evidence in favor of the null is only minor because the chances are good that the experimental set-up will fail to detect an effect.
To provide a better understanding the relationship between effect size, power, and the number of subjects, Figure 1 shows power as a function of N for three different effect sizes: 0.37 - the effect size estimated from the data reported in  - 0.6, and 0.8.
It is clear that both the effect size as well as the number of participants in a study have a large impact on the power of the study.
However, it can also be concluded that for small or moderate effect sizes, sample sizes larger than those typical in HCI are necessary.
Besides the often-erroneous interpretation of p-values and low power, the focus on null hypothesis significance testing in HCI has another severe consequence: qualitative questions about whether an effect exists are favored over quantitative questions relating to how much of an effect there is, and to whom it matters.
The latter can only be assessed by considering effect sizes and appropriate loss functions, and by interpreting these in a real world context.
A p-value smaller than .05 does not necessarily imply that the effect is important - it only informs us that the sampling error was small compared to the signal.
Especially for large data sets  low p-values are common but do not inform our search for scientific answers.
Only a numerical interpretation of the estimated effect can tell us whether a "significant" effect is indeed important to us and warrants further research or a theoretical explanation.
For example, if there was a significant difference in the time taken to learn two competing versions of a software package, but the size of the effect was only fifteen seconds, this would likely not have a very large practical impact.
Perhaps surprisingly, the flip side of the argument also holds: a high p-value does not imply that the effect under study was unimportant.
It only means that it was measured with a relatively high sampling error.
Compelling examples of this can be found in neighboring disciplines, and in the courtroom: the painkiller Vioxx was tested in a clinical trial against Naproxen, a general already-on-the-market painkiller.
During the trial one person died that was taking naproxen.
For Vioxx however, five people died.
The difference was not statistically significant, p >.05, and thus written off as unimportant.
The lawsuits against Vioxx in 2005 proved the researchers wrong: The real-life, and regrettably more powerful, test showed that Vioxx severely - although initially not significantly - raised risks of cardiovascular side effects.
If statistical significance is neither a sufficient nor a necessary criterion for importance, what good does it do?
Currently the "size-less stare" at p-values actually does a lot of harm .
In some fields, where historically researchers were trained in graphing their data and exploring the actual numerical values, means, and confidence intervals, this practice seems to be decreasing due to the fixation on pvalues .
In computing fields it is not clear that effect size reporting was ever common; Dunlop and Baillie have identified lack of effect size reporting in HCI as "dangerous"  and in the related field of software engineering experiments, a review of 92 experiments published between 1993 and 2002 shows that only 29% of the papers reported estimates of effects.
These mainly occurred in studies with small sample sizes where the p-value was close to .05.
We make a distinction here between standardized measures of effect size - like Cohens's D, eta squared, or the easily interpretable Common Language Effect Size  - and non-standardized measures of effect size.
The latter are dependent upon the scales by which variables are measured.
This latter property makes non-standardized measures of effect size less suitable for comparisons across experiments.
However, only non-standardized measures of effect size - estimates of actual differences in means or parameter estimates in regression models - can be used to assess the theoretical and practical importance of the quantitative findings of a study.
We believe that standardized effect size measurements can be useful in comparing results across studies.
Cohen for example has published useful heuristics for interpreting effect sizes as small, medium or large .
However, it would be of limited value if researchers replaced canned reporting of p-values with canned reporting of Cohen's d or other such statistics.
While standardized effect size measures overcome the confusion of importance and sample size as common for p-values, standardized effect sizes cannot, by themselves, be the only outcome of a quantitative experiment.
The important point is to consider what the estimations of effect mean in the context of previous work and what the practical and theoretical implications of an effect of that magnitude would be for users or designers.
To overcome the fixation on p-values instead of estimates of effects researchers should report their actual findings, and interpret the numerical estimates of their models or tests.
An effect size is "any statistic that quantifies the degree to which sample results diverge from the expectations specified in the null hypothesis" .
Effect sizes have three main uses : Firstly, a prediction of effect size is necessary when planning studies in conjunction with power, sample size and significance criteria .
Secondly, it enables researchers to interpret the practical significance of their results because it estimates the magnitude of an effect.
Thirdly, reports of standard measures of effect size enable researchers to compare the results from different studies and put their findings in the context of previous work in the literature.
The APA recommend that standard measures of effect sizes should be reported along with p-values ; they give complementary information.
In a study examining the reporting of 855 t-tests published in the psychology literature it was found that effect sizes and p-values were generally consistent, with large effect sizes corresponding to low p-values.
The consistency can in large part be explained by the relatively standardized sample sizes adapted by the field.
To highlight the importance of an inspection of parameter estimates  versus the  interpretation of p-values we present a set of hypothetical results obtained from a study similar to that used in the previous examples.
Suppose again the ratings of the usability of two different systems are compared.
However, this time we do not only compare Windows Vista en Apple Mac OS-X but we obtain ratings both by novices and by expert evaluators.
Table 2 presents the results for this new experiment.
The table presents the usability ratings  comparing Vista and OS-X  when these ratings are provided by novices and by expert evaluators.
Given the mean differences presented here, the pooled standard deviations, and the equal sample size, both of the effects - that of type operating system and type of evaluator - are statistically significant.
Thus, according to most researchers they are both important findings2.
However, for both theoretical as well as practical purposes it is feasible to evaluate the sizes of the effects of both the operating system as well as the user expertise on the usability ratings that are provided.
We have - hopefully - already convinced readers of the inadequacy of p-values to make these kinds of judgments.
Thus, researchers should not decide, based on the lower p-value of the operating system factor that this is the most important variable in eliciting critiques.
The third column of Table 2 presents Cohen's d for the two comparisons presented here.
Cohen's d is given by the ratio of the mean difference and the pooled standard deviation.
This makes the computation similar to that of the t-value with the only difference being the exclusion of N, the number of subjects, in the equation.
Given equal sample sizes in these two evaluations there is a direct relation between the t-value  and the value of Cohen's d. Now, should we decide that the operating system is the most important factor influencing people's usability ratings of their systems?
The value of Cohen's d is higher indicating a higher effect size than for the expertise.
However, we think that researchers, knowledgeable of the origin of Cohen's d, should look a bit further.
The actual mean difference `caused' by the expertise of the user as opposed to the type of operating system is far larger.
The difference between an expert user and a novice user is around 1.3 points on the 7 point scale, while that for the different operating systems is only 0.91 points.
However, the standard deviations indicate that the measures obtained for the different operating systems are more `consistent' - less spread out - than those obtained for the different user expertise levels.
It is up to the researcher to determine and motivate the conclusions drawn from a dataset like this.
However, in this scenario a large standard deviation for expertise is very plausible: actual expertise levels are not binary and thus there is heterogeneity within the expert and novice groups.
This argument does not hold for the type of operating system, hence its smaller standard deviation.
The actual mean difference however shows that - if assessed accurately - user expertise could potentially be a more important determinant of the usability ratings of a system.
We do not mean to imply that all HCI researchers neglect to discuss the size of their effects.
For instance in the domain relating to our worked example, a highly cited paper which considers the evaluator effect is very much concerned with the interpretation of effect magnitudes .
Within HCI more generally, a good example of a focus on quantitative estimates  can be found in the Fitt's law literature.
Fitt's law describes the quantitative speed accuracy trade-off associated with pointing.
The importance of quantitative evaluations when building a science is clear from the status of Fitt's law within HCI research: It presents the only paradigm which consistently fills up at least one session at CHI, and the results are replicated and extended upon frequently.
Presenting only p-values can lead to misleading results with unfortunate real life consequences .
The p-value often does not inform us about what we want to know, which is generally the probability of the hypothesis given the data.
Also, high p-values do not imply that the null is indeed true if the power is inadequate, and finally, sizes of effects should be more important than their associated sampling error.
We conclude with a more general criticism of the way theories are developed within HCI.
A hallmark of a good theory is that it is highly falsifiable.
It should make definite claims about the world, because the more claims it makes, the more opportunities there are to falsify it.
A major criticism of the traditional approach to statistics is that it encourages weak theorizing by proposing hypotheses which make vaguely specified claims about the world .
In specifying a null hypothesis, the researcher generally predicts no difference between conditions.
If this is rejected, the alternative hypothesis is accepted.
But the alternative hypothesis that matches this null hypothesis  is vague and underspecified.
It rules out only one point where the means are exactly the same across conditions.
Any other relationship between the variables could be true.
Seen in this light, the null hypothesis is intuitively almost always false, and so rejecting it isn't very informative.
A theory which predicts in advance the magnitude of an effect is more useful, and the consideration of estimated effects from the current study in the light of previous findings enables the researcher to contribute coherently to the existing body of work in a field.
Dunlop and Baillie  have argued that HCI does not generally attempt replication of previous work, a point which is confirmed in Bargas-Avila and Hornbaek's recent analysis of UX studies .
Yet, single studies cannot be taken as the basis for believing a scientific result to be true.
If we do not, then what purpose is served by conducting traditional statistical tests?
If we do, we are more likely to achieve our aims by adopting best practices for the planning, analysis and reporting of empirical studies.
Based on the convergence of advice from related disciplines, we offer the following initial recommendations for best practice.
We hope that future authors will add to these recommendations.
A more specific hypothesis yields more information when it is falsified than a vaguely specified hypothesis.
For this reason, bolder predictions predicting the direction and magnitude of effects would be beneficial rather than choosing the "safe" null that there is no difference between conditions.
For example, a researcher might hypothesize that a shopping website optimized for screen reading software would decrease the average time taken to buy an item by a visually impaired user by one minute over the original version of the website.
Such an hypothesis can be evaluated quantitatively, after which qualitative judgments about the importance of an effect this size can be discussed.
When planning an experiment, it is helpful to predict the size of the effect likely to be found, based on previous findings from related studies if possible.
In the above example about the interface for visually impaired users, the researcher could have calculated effect sizes from the descriptive statistics published in previous similar studies, or predicted them from theory or even estimated them from pilot tests in the lab.
Deciding on power, significance criterion , and effect size in advance enable the researcher to calculate the number of participants they require to detect an effect of practical or theoretical importance.
If there are practical difficulties in recruiting enough participants, research teams could consider collaborating for multi-site experiments.
Power can also be increased by careful choice of valid and appropriate measurement instruments.
It can be beneficial to use Bayesian analysis to calculate the probability of the hypothesis given the data instead of traditional significance testing.
This analysis method enables researchers to build on the body of knowledge in the field by incorporating previous results as prior probabilities.
We encourage researchers, reviewers, programme chairs and journal editors to work towards raising the standard of reporting statistical results in order that future researchers can use this information to inform their own hypothesis generation, effect size estimates and prior probabilities in Bayesian analysis.
The guidelines in the 6th edition of the APA publication manual  are helpful in this regard.
It would be useful for researchers without a strong statistical background if submission instructions for authors included clear guidance to help them enhance their analyses and conclusions.
And, last but not least, it is good practice to interpret the non-standardized sizes of the estimated effects.
If the predicted effect size was found in the example of the shopping web site for visually impaired users, what would this mean?
What practical difference would it make to the user experience for members of this target user group?
Would a time saving of one minute per transaction be worth the effort it would take to learn how to use the new layout?
Some questions of this sort are arguably best answered in consultation with users, emphasizing the need for triangulation between qualitative and quantitative data.
These changes to the best practice within a field will require effort, and may take many years to come to fruition.
But if we, as a community, value the tools offered to us by statistical methods, we should do our best to avoid known methodological flaws, and embrace the best practices which are emerging from our sister disciplines.
The benefits to HCI will be great in terms of generating a more coherent body of work thus enabling the field to advance more rapidly.
Publication Manual of the American Psychological Association, Sixth Edition .
Old wine in new bottles or novel challenges: a critical analysis of empirical studies of user experience.
In Proceedings of the 2011 annual conference on Human factors in computing systems .
An Essay towards Solving a Problem in the Doctrine of Chances.
Mr. Bayes, F. R. S. Communicated by Mr. Price, in a Letter to John Canton, A. M. F. R. S. Philosophical Transactions of the Royal Society of London, 53, 370-418.
HCI... not as it should be: inferential statistics in HCI research.
