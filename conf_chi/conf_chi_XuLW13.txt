Recognizing users' engagement state and intentions is a pressing task for computational agents to facilitate fluid conversations in situated interactions.
We investigate how to quantitatively evaluate high-level user engagement and intentions based on low-level visual cues, and how to design engagement-aware behaviors for the conversational agents to behave in a sociable manner.
Drawing on machine learning techniques, we propose two computational models to quantify users' attention saliency and engagement intentions.
Their performances are validated by a close match between the predicted values and the ground truth annotation data.
Next, we design a novel engagement-aware behavior model for the agent to adjust its direction of attention and manage the conversational floor based on the estimated users' engagement.
In a user study, we evaluated the agent's behaviors in a multiparty dialog scenario.
The results show that the agent's engagement-aware behaviors significantly improved the effectiveness of communication and positively affected users' experience.
When deployed in an unconstrained environment for interaction with multiple persons, the agent has to deal with more complicated situations, where participants may have diverse objectives and engagement state .
To deal with such complex situations, the agent should be able to perceive users' status, and generate appropriate responses, such as, choosing the right person to attend to and managing the speaking floors .
There are two interrelated issues in building engagementsensitive agents:  engagement evaluation, which refers to the estimation of how much a participant is involved in a conversation , and whether a participant has an intention to enter or leave a conversation , and  engagement-based behavior design, which refers to the design of agent behaviors that accommodate the engagement state and intentions of the users.
For the first issue, computational models have been built where the engagement state and intentions were inferred from certain verbal and nonverbal cues 
However, most of the evaluation models relied on a set of handcrafted heuristics to determine the engagement state, which was typically coded as binary values .
This makes it difficult for the system to determine the relative importance of multiple participants in the scene.
Moreover, detecting engagement intentions remains to be a tough challenge due to the ambiguity of the concept, the lack of measurable audiovisual cues, and the subtleties of personal, task, and social factors .
Conversational agents are expected to penetrate into the public place and undertake various tasks that involve interactions with multiple users.
Recognizing user engagement and intention is a pressing task for the agents to facilitate fluid interactions in situated situations .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
It serves as an important clue for the agent to prioritize the participants and make decisions on who should have the conversational floor.
This study tackles the engagement intentions in the specific scenario of information providing .
For the second issue, extant work has been restricted by the lack of quantitative evaluation of engagement to design appropriate agent behaviors in multiparty conversations.
In this research, we study  how to quantitatively evaluate high-level user engagement state and intentions based on low-level visual information, and  how to design engagement-aware behaviors for the conversational agents to behave in a sociable manner.
First, we captured humans' decisions in engagement estimation using a data-driven, machine learning approach.
We conducted a Wizard-of-Oz experiment, where a physically embodied agent provided guidance information to multiple persons.
The video of the interactions were annotated with the participants' engagement state, intentions, and attention saliency .
We built a linear regression model to evaluate the attention saliency, and a support vector machine  to detect the engagement intentions, each based on a set of carefully selected visual cues.
Second, we designed a new behavioral model that allowed the agent to perform engagement-aware activities - adjusting its direction of attention based on the participants' attention saliency and triggering floor-exchanges based on their engagement intentions.
Third, we tested the performance of the agent in a multiparty dialogue user study.
We manipulated the agent behaviors so that it worked in either engagement-aware or engagement-unaware mode.
We investigated how the engagement-aware behaviors influenced the effectiveness of communication , and the user experience .
It is found that engagement-aware behaviors effectively enhanced the effectiveness of communication and led to favorable user experience.
The design of the engagement-aware agent represented our contributions in two aspects.
First, we proposed a datadriven approach and machine learning models to quantitatively evaluate the high-level user engagement and attention saliency based on the low level visual cues.
The model was validated and proved reliable in evaluating the high-level engagement information.
Second, we designed a new behavior model for the agent to carry out engagementaware activities in the conversation with multiple persons.
The behavior model significantly enhanced the perceived intelligence and sociability of the agent.
Our work paved the way towards deploying advanced conversational systems in the open world.
Engagement is a fundamental issue in computer-mediated conversations.
It is defined as "the process by which two  participants establish, maintain, and end their perceived connection" .
Alternatively, it can be considered as "an emotional state linked to the participant's goal of receiving and elaborating new and potentially useful knowledge" .
In practice, it has various implications such as attention and interest .
These definitions afforded elegant solutions for engagement evaluation in a dyadic conversation, because one may use a simple binary representation of the engagement state  to trigger the system's actions.
However, it did not differentiate the degrees of engagement , when multiple persons are involved.
Such differences are partially captured by the role of the participants, known as addressees, side-participants, over-hearers, and eardroppers .
Along this line, some researchers developed methods to infer the participants' roles .
However, it still could not tell the relevancy and saliency of a participant when there are multiple addressees.
It is desirable to identify the relative importance of individuals in the scene so that the agent could attend to persons of higher importance .
While the perception of engagement is an essential ability of human beings, no computational system has been built to make human-like estimations of engagement, due to the limited computational intelligence of the systems.
In extant work, engagement has been evaluated based on both verbal and non-verbal cues.
Solutions based on the verbal  cues are restricted by the difficulties of detecting the start/finish, and the source and target of the utterance in an uncalibrated, noisy environment .
Hence, non-verbal cues, especially the visual information, have been utilized to build robust models for engagement evaluation .
The most important visual cue is probably the eye gaze, which serves as a useful indicator of the direction of attention .
Alternatively, the role of other parts of the body in signifying engagement has been investigated .
For instance, the head and body pose may provide cues of user attention and interest .
Users' roles and saliency were also estimated from the spatial information, such as, users' distance to the robot, and their relative position to the front of the robot .
Technology development in wearable sensors and biosignal analysis has made it possible to evaluate engagement from physiological signals, e.g., attention estimation based on the electroencephalography .
However, such systems are intrusive and not easy to be deployed in public spaces.
Engagement intention detection is a more complicated task and is less reported in the literature.
In fact, there has been much ambiguity about the implications of intention.
Alternatively, in an on-going conversation, an engagement intention is related to the motivations of turn-taking: an inclination to take the conversational floor  or to terminate the conversation.
The latter case is more precisely considered as a disengagement intention .
For purpose of clarity, we use E-intention to represent the intention to initiate or take the speaking floor, D-intention to refer to the disengagement intention, and engagement intentions to denote both types of intentions.
Currently, most work relied on gaze behaviors to detect engagement intentions .
Bohus and Horvitz  inferred engagement intentions from the actual engagement actions, which may not be a favorable logic - an engagement action is the result of intention, not vice versa.
They also built a predicative model to detect the willingness of a participant to start a conversation with the agent .
However, this model is not suitable for predicting the E-intention when the conversational floor has been established.
Nakano and Ishii  used the patterns of gaze shift, called 3-grams, as a predictor of Dintention.
However, the generalizability of the method may suffer from the limited number of such patterns based solely on the gaze direction.
It might be useful to adopt visual cues beyond the eye gaze.
It was found that the agent improved users' impression of the agent in terms of appropriateness of behaviors, smoothness of conversation, and perceived humanness .
In behavior design, many researchers manipulate the agents' eye gaze to convey the engagement information.
Sidner  investigated where the robot should direct its attention .
Bohus and Horvitz  endowed the virtual agent with the ability to direct the eye gaze towards the engaged persons.
Mutlu  made use of gaze cues to shape the role of users.
Other behavioral cues have been used as well.
For example, Szafir and Mutlu  designed the educational agent to engage participants using both vocal  and nonverbal cues .
Research in this area could drew on findings in sociological studies on human-human interactions .
Importantly, it will help the agent to determine when to initiate/stop a conversation, and when to take/release the conversational floor.
Yamazaki  made use of the transition relevant places to coordinate the timing of hand movement and utterance in a museum guide robot.
In a similar scenario, Yamazaki  identified several interaction patterns that might allow the agent to attract a user's attention.
However, the above work dealt with oneto-one interaction only.
We used a physically embodied receptionist agent called Olivia  that consisted of a vision head, a motion system, and a speech system .
The vision head had a stereo camera  which provided a pair of color and disparity images.
The motion system allowed for pan and tilt motion of the agent's head  and torso .
Thus, the agent could change its direction of attention by adjusting the head and body poses.
To elicit natural human behaviors in multiparty conversations, we hypothesized a scenario where the agent provided guidance information about an office building.
Multiple participants were involved in the interaction, each consulting on one of three issues, namely  the direction to a meeting room,  dining information, and  means of transportation to a downtown place.
A human operator controlled the agent motion  remotely, and selected from a list of predefined scripts for implementing the speech.
During the interaction, the operator was able to monitor the activities of the participants and the agent, so that he would take proper interactive actions.
We conducted the experiment with 23 participants in 9 sessions, each session having 2 to 4 persons.
The interaction procedure was recorded by both the agent's vision system and a high resolution digital video camera.
Next, the videos were analyzed using annotation software ELAN .
The videos were segmented into 0.5-second fragments.
Two human experts annotated the segments independently following common instructions.
The annotation was concerned with two aspects,  the participants' attention saliency, and  the existence of engagement intentions.
For the first aspect, the annotators were instructed to rate on a scale of 0-10 concerning "how much a participant deserves attention from the agent".
They were advised to consider the participants' engagement state, intentions, and any other clues that make the person relevant to the conversation.
We purposely did not give specific marking schemes.
Rather, we asked the annotators to assume the role of a receptionist and infer the participants' attention saliency.
For annotating engagement intentions, the annotators were instructed to mark a person in a segment as E-intention positive  if he/she wanted to start/enter a conversation or intended to take the speaking floor, provided that such a floor was being held by others.
For example, a participant might want to barge in an on-going conversation because he had something urgent to ask, or he might be confused about a subject matter and needed an immediate explanation.
Similarly, a person was considered as D-intention positive if he/she wanted to terminate a conversation or release the floor to the agent or another person.
This may happen when a participant needs to attend to other unexpected matters, or feels unease about the current conversation .
It should be noted that the scope of engagement intentions as dictated above was not all-encompassing, whereas it reflected the need for the agent to pay attention to specific persons, and act accordingly to enable smooth conversation.
Angle with respect to the front of agent body.
Probability that a person is Motion  moving about.
Degree to which upper body Upper-body directed towards the agent.
Pose  Face  User face detected in the frame.
User eye gaze directed towards Eye  the agent.
User speaking as evaluated from Speaking  the lip motion.
Facial 0: neutral, known; -1: scowl Expression  1: smile * Feature variable was significant at .001 level Angle 
Next, we checked the between-annotator consistency on three items.
The attention saliency was considered as a continuous variable, and the Pearson's correlation coefficient was =0.839 .
We computed the mean value of each annotation pair from two annotators, and scaled it down with a factor of 10.
So the attention saliency was in the range of 0-1.
For the engagement intentions, we computed the Cronbach's alpha of the E-intention  and Dintention , respectively.
Both of them were acceptable.
To improve consistency, the two annotators discussed all discrepancies and reached a final agreement.
We made use of eight visual features .
The technical detail on visual feature extraction is available in .
We computed the aggregated mean value for each feature in each 0.5-second segments for every participant, consistent with the annotations of the attention saliency.
We used these features to evaluate the attention saliency using a linear regression model.
Assuming that there are M persons in the scene, and j  is the index of a person.
DOA is computed from three features: upperbody pose , face , and gaze .
T is a time interval.
DOA   How long does the agent maintain its attention to Duration of the agent?
DOA  Speaking  Is the participant speaking?
Change of y denotes if the person starts to speak or stops speaking status 5 speaking.
Change of emotion  y7 denotes if the current facial expression is different from previous facial expression Distance  Raw distance value.
Let TP, TN, FP, FN denote the numbers of true positives, true negatives, false positives, and false negatives, respectively.
The precision and recall are defined as: Precision=TP/, Recall=TP/.
Based on our observation, the engagement intentions were better reflected by the change of visual cues following certain sequences rather than the temporal values of the features.
For example, when a participant wanted to enter a conversation , he/she usually changed the direction of attention from elsewhere to the agent, and there was a distance change because the participant moved or leaned towards the agent.
Without manually finding and coding the patterns from the visual cues, we resorted to the power of machine learning to identify the engagement intentions.
To do so, we computed a set of features that reflected the changes in multiple visual cues .
Xi   where C is the regression coefficient vector .
The variable `motion' was not significant in this model because most subjects did not move around during the interaction.
The model was able to predict the attention saliency with R2 = .661 and a standard error of e = .170, which showed excellent predictive power.
Engagement intention detection is a classification problem where the existence of E- or D-intention is to be determined based on the visual cues.
We adopted the Matlab LIBSVM toolbox  and used the leave-one-out cross-validation to test the system performance, i.e., data from four experiment sessions was used for training and that from the remaining session was used for validation.
The existence of engagement intentions  was rather rare in the data set.
In fact, there were only 98 occurrences of E-intention and 85 D-intention, amongst 6905 data records.
To deal with the imbalanced data, we used the precision-recall values for performance evaluation,
Two SVM models were built to detect E-intention and D-intention, respectively.
We fine-tuned the model parameters to achieve optimal performance.
Figure 3 shows the performance of several models according to the precision-recall curves.
We found that the optimal models could achieve 83% precision and 72% recall for E-intention, and 75% precision and 71% recall for D-intention.
These were reasonably good for detecting engagement intentions.
The performance of the linear regression model and the SVM models showed a close match between the computational results and human annotations, allowing us to use the computed attention saliency, and E- and D- intentions to modulate the agent's behaviors.
The elegance of the above model is that it uses attention saliency as a natural measure of the uses' relevance to the conversation, thus saving the trouble of building complicated models that define the reasoning logic based on many variables.
In principle, the agent directs its attention to the participant with the highest attention saliency.
The direction of the agent's attention is informed by its gaze direction and body orientation .
The agent may adjust its body orientation towards a participant when it starts a new conversation with that person.
Thereafter, it will maintain the body orientation until the current conversation session ends, or the attention saliency of another person is consistently higher than the current person for more than 15 seconds.
If the duration is less than 15 seconds, the agent will maintain the body orientation while using head-turn motion to control its gaze direction when necessary.
When the agent directs its gaze at a person, it will maintain the gaze for a duration of di, which value is generated as a normally distributed random number.
We adopted mean duration of 2 seconds and standard deviation of 1 second based on findings in .
During the time lapse of di, a set of video frames are captured, based on which a set of attention saliency values is computed.
The aggregated mean value of the set of attention saliency during di is denoted as  j .
A benchmark model is built where the agent adapts its direction of attention randomly without considering users' attention saliency and intentions.
In the random gaze model, for every time interval d, the agent randomly chooses from the set of M users, and directs its gaze at that person.
The interval d is generated as a random number with a normal distribution.
Because the amount of gaze may influence the joint arousal between conversational partners  and the perceived-affiliation and immediacy , the interval d was adjusted so that the frequency of gaze direction-change in the benchmark mode was equivalent to that in the engagement-aware mode.
To do so, we adjusted the centroid of d to for every experiment session where the agent worked in the engagement-unaware mode, such that the frequency of gaze-change was identical to the last experiment session when the agent was in the engagementaware mode.
Similarly, to benchmark with the behaviors where engagement intentions were accounted for, we adopted a random floor-change strategy where the agent facilitated floor shift at random intervals.
The frequency of the floor-change was in par with that in the last experiment session when the agent was in the engagement-aware mode.
It should be noted that more comprehensive models might be adopted for benchmarking.
Nevertheless, the random model was able to simulate the dynamics of multiparty conversations as did the proposed model.
The independent variable is the agent behavior, namely,  engagement-aware behavior , and  engagementunaware behavior .
The dependent variables include the effectiveness of the communication, and the user experience in the interaction.
Likeability/Favorability : 4-item measure of how friendly or likeable the agent was .
Satisfaction : 3-item measure of how satisfied the subjects were with the agent's service .
Manipulation check We checked two aspects of our manipulation of the agent behavior.
The appropriateness of agent gaze, i.e., whether the subjects felt the agent appropriately changed it gaze direction with respect to the status of the subjects.
Appropriateness of floor-change, i.e., if the subjects felt the agent perceived their need, and took or released the speaking floor properly.
The scenario of the human-agent interaction was identical to that in the data collection phase, i.e., the agent provided guidance information to multiple participants.
However, instead of controlled by a human operator, the agent worked autonomously to control its direction of attention and utterance.
The participants randomly chose one of three topics concerning the information consultation and were debriefed about the function of the agent.
After a subject finished the interaction, he/she answered a questionnaire which consisted of two parts,  the subjective evaluation of the agent's interactive behaviors, and  the recall of information provided by the agent .
H1: Engagement-aware behaviors augment user attention  and engagement , thereby improving intimacy , task performance , and smoothness of communication .
H2: Engagement-aware behaviors positively affect users' satisfaction  and their perceptions of the agent's humanness , intelligence , and likeability .
Further, assuming that the engagement-aware behaviors lead to higher likeability of the agent, based on the attraction-transformation model , there should be closer proximity between the agent and participants.
Thus, we predict that participants hold closer proximity  to the agent when it works in engagement-aware mode than in the engagement-unaware mode.
Objective measures There are four objective measures.
Attention : Measured as a subject's amount of gaze at the agent  when he/she was being addressed by the agent .
Intimacy : We estimated the intimacy between a subject and the agent based on the amount of eye contact, which was calculated as the percent of mutual gaze during an interaction .
The data was extracted from the annotations of the experiment videos.
Task performance : Measured as the recall of information: the number of correct answers that a subject got in 5 questions related to the information provided by the agent.
Proximity : We measured the proximity as the minimum physical distance between the agent and a participant .
Subjective measures There are six subjective measures, each evaluated on a 7-point Likert scale.
Engagement : 5-item measure of how engaged the subjects felt during the conversation .
Smoothness of conversation : 3-item measure of how smooth or natural the conversation was .
Humanness : 4-item measure of how much the subjects perceived the agent as a human .
We recruited 28 participants  who were divided into 10 groups .
Each experiment condition hosted equal numbers of groups and individuals.
Figure 4 shows a scene of the experiment involving three participants.
We analyzed the between-subject effect of agent behaviors using two sample t-test, with respect to manipulation checks, and objective and subjective measurements.
Thus, our manipulation of the agent behavior was effective.
The effectiveness of communication was evaluated by three objective measures  and two subjective measures .
Our first hypothesis predicted that the effectiveness of communication would be improved if the agent worked in the engagement-aware mode.
This hypothesis was partially supported.
The results of these measures are shown in Figure 5.
First, the participants maintained a higher level of attention to the agent which was measured as the amount of eye gaze directed towards the agent while they were listening to its explanations .
The average eye gaze towards the agent when the subjects were listening to the agent was 78.8% in the engagement-aware condition and 73.7% in the engagement-unaware condition.
Next, the agent behavior had a significant effect on the intimacy level, and the engagement-aware behavior induced more eye contact .
Both measures were positively affected by the engagementaware behavior.
However, the task performance  were identical in both conditions, i.e., there was no significant difference in the participants' recall of information .
This might have been caused by the novelty effect, i.e., the participants were interested in the agent's activities and features, and did not pay particular attention to the information provided by the agent.
Task performance may not be a good measurement of conversation quality in this scenario per se .
The agent behavior did not have a significant effect on the proximity , i.e., the physical distance between the agent and participants were identical in both conditions .
We will explain this result in the discussion section.
Our manipulation of the agent behavior with respect to the mode of engagement-awareness was effective based on the participants' positive evaluation of the agent's gaze behaviors and floor-management activities in comparison with the benchmark model.
This suggested that the evaluation of user attention saliency and engagement intentions was valid and contributed to the aptness of the agent behaviors.
By evaluating the high level engagement state and intentions, we were able to integrate the reasoning logic of humans in multiparty conversations, thus improving the perceptual capability of the system.
The objective and subjective measures of the agent behaviors showed encouraging results, i.e., engagementaware behaviors enhanced the effectiveness of communication and user experience.
In view of the effectiveness of communication, users were more attentive to the agent's talking and maintained higher level eye contact.
They also reported higher engagement levels and perceived the conversation to be more fluent.
Although the task performance was not affected by the agent's working mode, it can be explained by the novelty factor of the experiment, which involved a very brief conversation scenario .
Engagement-aware behaviors also had a positive effect on the evaluation of the agent's humanness, intelligence, likeability and overall user satisfaction.
These underscored the importance of being aware of the status of the users in a multiparty conversation environment.
In fact, even for the benchmark model where the agent adopted random gaze and a random floor-management strategy, the user experience was generally positive as was indicated by the scores that were larger than 3.5 on a scale of 1-7.
It seems that the simple action of being aware of the presence of multiple persons could lead to positive effects.
We found a positive effect of the agent's engagement-aware behavior on the user experience, except a marginal effect on the likeability measure .
Thus, hypothesis 2 was partially supported.
The lack of an effect of agent behavior on proximity turned out to be surprising.
According to the attractiontransformation model , the amount of gaze is expected to have a significant effect on the physical distance provided that the agent is perceived to be likeable.
Since the average likeability scores were quite high in both conditions , and there was more eye contact  in the engagement-aware condition, the proximity  should be closer in this condition.
However, we did not find a significant correlation between the engagement-awareness and proximity.
One possible reason was that the intimacy as perceived by the participants was in effect low in the short interaction procedure, irrespective of the experiment conditions.
This was exacerbated by the nature of the interaction, namely, information consultation, which did not leave much space for establishing intimate relationships and building rapport.
Thus, the effect on physical proximity was not evident.
For this reason, we did not evaluate the user's psychological distance to the agent .
We further developed behavior models that made use of the engagement information.
The engagement-aware behaviors could significantly improve the performance of the agent in multiparty conversations.
We believe the ability to understand the status and needs of multiple users as implemented in our system is an important step towards deploying embodied conversational agents in the public sphere.
Our system may be improved in several aspects.
First, we focused on the evaluation of engagement using visual cues, due to the lack of legacy computational methods.
We plan to add audio cues and other contextual information in the model, which may lead to better performance .
Second, for engagement state and intentions detection, we used the machine learning approaches to exploit the computational intelligence.
Alternatively, it would be useful to carry out detailed analysis and interpret the machine learned model as logical rules.
We intend to develop human readable rules in the future.
Third, non-parametric testing would be a good practice for evaluating the subjective scales in the experiment.
As future work, we will apply it for more comprehensive evaluations.
Finally, an intelligent agent should have the ability to both understand the users' engagement, and make decisions on its own level of engagement.
In the current system, the agent's engagement level is designed to match that of the respective users, through floor-management and gaze direction.
Future work can be carried out to investigate how the agent may adapt its engagement level through context-awareness and multiple interaction modalities, e.g., pace of utterance, arm gestures, facial expressions, visual aids, etc.
In multiparty conversations, evaluating engagement is a prerequisite for designing effective, sociable behaviors of a conversational agent.
We designed an engagement-aware agent that was capable of evaluating both engagement state and intentions of multiple users, and adapted its behaviors to address the user needs.
We demonstrated that visual information could be used to infer the high-level user engagement with good reliability.
