In this paper, we explore how to add pointing input capabilities to very small screen devices.
On first sight, touchscreens seem to allow for particular compactness, because they integrate input and screen into the same physical space.
The opposite is true, however, because the user's fingers occlude contents and prevent precision.
We argue that the key to touch-enabling very small devices is to use touch on the device backside.
In order to study this, we have created a 2.4" prototype device; we simulate screens smaller than that by masking the screen.
We present a user study in which participants completed a pointing task successfully across display sizes when using a back-of device interface.
The touchscreen-based control condition , in contrast, failed for screen diagonals below 1 inch.
We present four form factor concepts based on back-of-device interaction and provide design guidelines for extracted from a second user study.
Recently, we have seen a departure from devices using keypads and d-pads  towards devices using touchscreens.
Touch screens allow for efficient pointing input, and by eliminating the need for a keypad they allow for a comparably compact enclosure.
Intuitively, one might think that this would be a step forward towards achieving higher miniaturization .
Unfortunately, the opposite is true.
The selection point on touch screens is ambiguous, because of the size and softness of the user's fingertip.
Since the finger occludes the target area, users are required to target without visual feedback.
This fat finger problem  makes the selection of small targets difficult and error-prone.
Device designers address the problem most commonly by enlarging targets to approximately finger size.
Unfortunately, this typically results in devices even larger then the more traditional d-pad devices .
In this paper, we explore the question of how to provide very small screen devices with pointing input capabilities.
Pointing input is crucial for many mobile applications from the efficient selection of links in a web page to the ability to play interactive real-time games.
With very small we consider the following range.
At the larger end, we look at devices with screens diagonals around 2.5" : devices designed for use during physical activities as well as tangible screen devices .
At the smaller end, we look at the truly tiny screens used in accessories, such as smart watches  or electronic jewelry .
The latter can be as small as a fraction of an inch.
While diminishing screen size has many impacts on overall usability, e.g.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Back-of-device interaction  avoids interference between fingers and screen by keeping the user's hand on the back of the device, a space historically unused .
A pointer on screen informs users about the position of the finger on the back .
Apart from that, screen contents remain visible and occlusion-free.
In order to help users acquire small targets, researchers have proposed a variety of ways of enlarging targets, such as by zooming manually  or automatically .
Other techniques enlarge targets in motor space only, e.g., by snapping the pointer to a target.
Starburst  extends the concept to non-uniform target distributions.
Escape combines snap-to-target with marking .
A similar effect can be accomplished by enlarging the pointer instead  or by slowing the pointer down when high accuracy is required .
In this paper, we explore whether back-of-device interaction can enable pointing on very small touch devices.
Figure 1 and Figure 3 show four devices that we are envisioning and that use back-of-device input across different degrees of smallness.
These concepts inform the screen sizes we used in our user study.
In order to explore this design space, we have created a prototype device we call nanoTouch .
It features a 2.4" screen, like the clip-on design shown in Figure 3a.
We simulate screen sizes below 2.4" by masking the device.
We found that back-of device input continues to work for screen diagonals below 1 inch, while front-side input does not.
We report the results of a second user study in which we quantify task times and error rates of back-of-device interaction.
Based on these findings, we define design guidelines.
Several solutions have been proposed to overcome the fat finger problem.
Styli offer a precise tip and remove the user's hand from the target surface.
A downside of styli is that introduce an additional physical object that users need to retrieve before use .
Software techniques designed to address the fat finger problem shift finger and target away from each other.
Offset cursor  creates a software pointer a fixed distance above the finger's contact point and has therefore been referred to as a software version of a stylus .
Offset Cursor uses take-off selection  in which the target is selected at the point where the finger is lifted rather than where it first contacted the screen.
On the flipside, the input area needs to be extended beyond the screen in order to prevent part of the screen from becoming inaccessible.
On a screen measuring half an inch high, adding half an inch of space effectively doubles the device size .
Devices offering a touch pad on the device front are impacted in a similar way.
Users can operate shift like a regular touch screen.
If users require high precision, however, they can trigger a special precision mode by dwelling.
Shift then "escalates" and displays a "callout" mirroring the area currently hidden under the user's finger as shown in Figure 5b.
Since the callout is placed dynamically in dependence of the touch location, it can always be contained within the screen area and without additional device space.
Because of these benefits, we chose shift as the control condition in User Study 1.
Several researchers have proposed moving the interaction to the back of the device as means to eliminate occlusion.
BehindTouch  and BlindSight  place a 12-key pad on the backside of a mobile phone.
HybridTouch  allows users to scroll by performing drag gestures on a touch pad mounted on the backside of a PDA.
Wobbrock enabled EdgeWrite on the backside of a similar device .
Unlike HybridTouch, the touch surface in under the table interaction is operated in absolute mode, mapping every location on the back 1:1 to the directly opposing spot on the front.
This first generation of back-of-device interaction eliminated the occlusion of contents by the user's fingers.
In exchange, however, they caused the user's fingers to be occluded by the device.
While eyes-free use works fine for gestures , it turned out to cause large error rates when targeting .
LucidTouch  addresses the problem by introducing pseudo-transparency, a concept borrowed from augmented reality .
The device creates the illusion of a transparent device by overlaying a video image of the user's fingers , creating a visual effect similar to VideoDraw .
Users interact based on pointers images tracking with the user's fingertips.
This allows for precise manipulation independent of finger size.
A more recent system, LimpiDual Touch  creates a similar effect using physical seethrough.
The resulting device supports the same three aspects that distinguished the original LucidTouch from its predecessors, i.e., 1:1 absolute mapping between input and screen, three states , and a simplified version of pseudo-transparency.
Although we have explored some one-handed form factors, we designed the prototype primarily for bimanual use, as shown in Figure 1 and Figure 4.
In this section we give a brief overview of the device, which we used to run both user studies reported in this paper.
Lacking a camera, the new device cannot sense whether a hand is hovering over the device backside.
In order to still offer the required three states  we added pressure sensitivity to the pad and remapped the 3 states as shown in Figure 8: Touching the device results in tracking; users enter the dragging state by pressing.
Our initial plan was to use a lucidTouch device for our studies.
Pilot studies, however, indicated that its size and in particular screen and bezel thickness were impacting targeting.
This meant that study results obtained with this prototype would not necessarily transfer 1:1 to the space of very small devices.
We therefore redesigned the device, resulting in the nanoTouch prototype shown in Figure 4 & Figure 7.
In order to achieve the desired size, we made several simplifications compared to lucidTouch.
We decided that multi-touch would be less crucial for a device barely large enough to fit two fingers at the same time.
This pressure sensing mechanism consists of a ring of elastic silicone located under the edge of the touch pad.
It compresses under pressure and eventually establishes contact with a conductive path.
When pressed, a solenoid inside the pad provides a brief auditory and tactile feedback confirmation, similar to the sensation produced by a micro switch.
Earlier designs based on one or more micro switches had led to uneven clicking behavior.
In the default absolute mode, the touch pad maps 1:1 to the screen, operating like a  touch screen.
Applications expecting relative  input, such as the shooter game shown in Figure 9 run the device in relative mode.
This causes the device to function like a mirrored version of a track pad, as found in a standard notebook computer.
Figure 4 shows the default setting.
Different users can fit the overlay to their needs by tweaking posture, finger size, and skin color.
Several details reinforce the sensation of transparency: The user's finger is rendered in front of the background, but behind the translucent buttons, suggesting a stacking order.
Pressing is visualized by overlaying the user's fingertip with a white tip, suggesting that blood is being pressed out of the finger tip .
The bitmap image of the finger was taken while the finger was pressed against a glass pane.
A fake white reflection in the top left corner of the screen hints the existence of a glass surface in the device.
To allow users to trigger discrete functionality, we mounted two buttons at the bottom left corner of the device  as shown in Figure 10.
This placement allows the button to be operated using a rocking motion of the thumb of the non-dominant hand-- while holding the device.
Users can also hold down the buttons, which allows them to work as qualifier keys and thus to implement quasi modes .
We found this to work reliably and over extended periods of time.
We have also used thumb buttons as a more ergonomic  alternative to pressing the touchpad and as left and right buttons when emulating a mouse.
Another pair of buttons in the opposite corner of the device is available for auxiliary use.
In our experience, naturalistic pseudo-transparency simplifies discoverability and enables walk-up use where it would otherwise be impossible.
Once users understand the concept, however, many applications will use a less naturalistic notion of pseudo-transparency--in the simplest case nothing but a dot-shaped pointer.
Our nanoTouch prototype is tethered to a PC.
This facilitates prototyping and reduces the amount of hardware in the device, allowing for a smaller form factor.
The PC recognizes the device as an external screen connected via DVI.
Arbitrary Windows application can be run on the device by scaling the application window and placing it in the screen area mirrored on the device.
The touch pad is connected to the PC via USB.
It sends mouse move events and communicates touch and button presses as key press events.
One of the most useful details of lucidTouch was its naturalistic implementation of pseudo-transparency.
Picking up the device with one or two hands caused an outline of these hands to show up on the screen, suggesting the screen was indeed transparent.
This dramatically contributed to discoverability--first-time users instantly "grasped" the concept.
In order to verify our assumption that back-of-device interaction is the key to pointing input on very small devices, we conducted a controlled experiment.
There were two interface conditions: back-of-device touch interaction and frontside touch interaction enhanced with the shift technique.
The task was to select a 1.8mm square target displayed on the built-in screen.
The 2.4" condition was implemented using the full screen of a nanoTouch device.
The smaller screen conditions were implemented by masking the screen in software, as shown in Figure 12 .
This resulted in a 11mm callout at 8mm offset in the 0.6" condition and a 4mm callout at 4mm offset in the 0.3" condition.
We also optimized the layout algorithm for the tiny screens, always placing the callout in the quadrant opposite of the initial touch location.
Participants operated the shift condition using their finger tip.
For the 1.8mm target size used in the study shift escalated instantly.
There were four interface conditions, all of which were implemented on a nanoTouch device.
In the two back conditions, participants provided pointing input via the device backside.
The device was run in absolute mode.
To eliminate variation due to differences in targeting strategy, participants were encouraged to keep their index finger in contact with the device at all times.
The back condition was broken down into two subconditions for the method of committing.
In the back-press condition, participants committed selections by pressing the touchpad.
The target location was determined on release.
In order to minimize tracking errors, we applied a filter removing the last 80ms of move events before the "up" event, as suggested by .
In the back-button condition, participants committed selections bi-manually by pressing and releasing the corner thumb button using the non-dominant hand holding the device.
This version was inspired by a study of Li et al, which suggests that mode switching with the non-dominant hand offers performance benefits .
In the two shift conditions, participants acquired targets using the shift technique described earlier.
These conditions were implemented by overlaying a clear touch screen onto a nanoTouch device .
The device was rested against the edge of a table to prevent the weight of the touch overlay from causing fatigue.
We ran the original shift code published in , adjusted to render at original scale on the high-dpi lucidTouch screen.
The 2.4" and 1.2" screen size conditions used the original settings: a 16mm callout presented at an 11mm offset.
There were two shift sub-conditions: when using shifttakeoff, users committed by lifting their finger off the screen, as described in the original paper.
Improving on the published algorithm, we added the same filter used by the back condition: upon takeoff, all mouse movements occurring during the last 80ms were removed.
In the shift-button condition participants committed using the thumb button.
Participants performed a target acquisition task similar to that the one described in the original shift paper .
Participants started each trial by tapping a start button.
This removed the start button, displayed the target , and started the timer.
Now participants placed the pointer over the target and committed using press, take-off, or button press, depending on interface condition.
Targets turned light red on hover  and light blue on press .
If a target was selected correctly, a clicking sound was played; otherwise, an error sound was played and the trial was repeated .
Targets appeared in one of 9 different locations on screen, placed in the centers of the cells of a uniform 3x3 grid.
The use of the start button allowed us to control for distance between the four interface conditions on the same screen size.
However, we did not control for distance between screen size conditions for the following reason: for the 0.3" conditions, we had to use the entire screen as a start button  to keep fatigue manageable.
For the larger screen sizes, we could have kept distances comparable by using the same 30x40px start button and placing the target under it.
The resulting task felt contrived and taught us little about targeting on the respective screen sizes.
The study design was 2 x   with 3 repetitions for each cell.
The two types of committal methods for each interface condition was a between-subjects variable.
For each trial, we recorded task completion time and error.
Interface order and screen sizes were counterbalanced.
Half of the participants used back-press and shift takeoff; the other half used back-button and shiftbutton.
Participants received up-front training and at the beginning of each block.
The study took about 45 minutes per participant.
H1: Post-hoc t-tests for the shift condition, aggregated across button/takeoff revealed significant difference between the 0.3" condition and the other three screen sizes .
The differences between the 0.6" condition and the two larger conditions were borderline significant .
This supports our hypothesis, that the performance of the shift condition was indeed impacted by screen size.
H2: Paired-sample t-tests between interface conditions within commit methods for the two smaller screens were either significant or borderline significant.
For the 0.3" screen condition, back-button was less error prone than shift-button ; the difference between take-off and press was borderline significant .
This supported our hypothesis that the back conditions were less error prone than the corresponding shift conditions for small screens.
As expected, we found no significant difference in error rate for back for six paired comparisons across screen sizes .
Also as expected, the advantage of the back conditions for small screens does not carry over to larger screens.
The differences in error rates between back and shift for the two larger screens were not significant .
For the 2.4" screen, back-press actually had a higher error rate compared to shift-takeoff .
H3: We performed an independent samples t-test of button vs. takeoff/press for back and shift, aggregating across screen size.
Back-button was significantly less error prone than back-press .
There was no significant difference between shift-button and shift-takeoff.
We had three hypotheses:  we expected the shift condition to perform worse with decreasing screen size because of the occlusion problem.
We expected to see the same trend for task time.
Since the screen is so small with respect to the user's finger, visually communicating anything can become difficult as soon as the user's fingers make contact with the screen.
And that includes the visual presentation of targeting aids, such as shift's callout.
Another perspective on the problem involves the notion of duration.
The spatial aspects of the occlusion problem are straightforward: as illustrated by Figure 16, occlusion gets worse  the smaller the screen  the larger the finger  the more fingers, and  the further the fingers reach across the screen.
The other main factor, however, has gone unnoticed so far: duration: the extent of the occlusion problem is in fact the product of occluded screen surface times the duration of the occlusion.
And while front side targeting aids can reduce occlusion, they generally do so at the expense of targeting time , which in turn creates additional occlusion.
We performed an analysis of variance.
Since screen size conditions differed in distance, our data does not allow us to tell in how far screen size impacted task time.
Note though, that task times decrease with decreasing screen size for the back conditions , task times roughly increase with the shift conditions.
This is consistent with our observations with respect to error rates, shift performed poorly for very small displays.
We performed 8 post-hoc paired-sample t-tests with Bonferroni correction  to test H2, that task time for back would be faster than for shift for the same commit method and screen size.
For the 0.3" screen size, the difference in speed between the two interface conditions backpress and shift-takeoff was borderline significant  as was using button .
With the two larger screen sizes, the differences were not significant.
The other main finding is that back-of-device interaction allows for high accuracy across screen sizes.
When triggered using the non-dominant hand  error rates averaged 2%.
This condition did substantially better than the back-press condition with error rates up to 12%.
The latter value should be understood as an upper bound though; the pressure-based mechanism is clearly an early version and the error rate should be expected to shrink with improved engineering.
In summary, the study supports our hypothesis that back-ofdevice interaction continues to work on very small screens.
It suggests that back-of-device interaction is indeed a viable approach for bringing pointing input to very small screen devices.
As hypothesized, the back-of-device conditions outperformed the shift conditions in the very small screen conditions.
This resulted from the fact that the performance of the shift conditions decreased with decreasing screen size, while the performance of the back-of-device conditions remained largely unaffected.
The decreasing performance of shift is expected, because shift, like any front-side touch technique can evade the fat finger/occlusion problem only to a certain extent.
So now that we know that back-of-device interaction has promise, what should an application on, say, a nanoTouch clip-on look like?
Before application designers can start designing controls and write applications for such a device, they need to know the constraints inherent to back-ofdevice interaction.
For touch screens, for example, we know that users can reliably acquire targets of about 18mm and this knowledge drives the design of all higher-level components, from menus to applications.
But what are the respective constraints for back-of-device interaction?
To begin answering this question we conducted a second user study.
We investigated the relationship between target size and task time and error rate for the two main interaction styles of nanoTouch, which we introduce in the following.
There were three interface conditions, all of which were implemented using a nanoTouch device.
Unlike the previous study, we kept screen size constant at 2.4".
The precision-press and precision-button interfaces correspond to the back-press and back-button conditions in our first user study.
In the land-on condition, participants acquired targets by tapping the device backside without receiving any type of visual feedback from the device.
The other interaction style supported by nanoTouch is targeting without visual control , as introduced by earlier back-of-device designs, such as under-tableinteraction .
We will call these land-on interactions.
While land-on selection at the bottom side of an interactive table led to large error  we would expect more reasonable values on nanoTouch: the prototype is small enough to allow users to see a good amount of their finger.
One might hypothesize that this allows users to estimate the touch position by extrapolating their finger.
The precision task was identical to the first user study: Participants clicked an 11mm  start button to reveal the target and start the timer, and then acquired the square target.
We added an additional smallest target size of 0.6mm  to the button condition; piloting had indicated that this target size was too error prone in the press condition.
Use of the start button controlled for distance 18.4mm ; the start button was placed as in Study 1.
Half of the participants completed this task with the press interface, the other half with the button interface.
In other words, press vs. button was a between-subjects variable.
In order to offer both interaction styles at once, we can combine precision and land-on targeting into a single interaction model .
In such a combined model, all interactions initially proceed as a land-on interaction.
However, the user can request help, e.g., by holding contact with the touch surface beyond a certain time threshold .
NanoTouch then responds by revealing the pointer, allowing users to complete the task as a precision interaction.
Since this process resembles shift's, we call it escalation.
Compared to shift's callout, however, nanoTouch's pointer is fairly unobtrusive.
This makes it reasonable to escalate early and in most cases we will do so instantly.
In this case, there is no more distinction on the device level, but merely a distinction between two interaction styles.
Participants performed the land-on task using the land-on interface.
Unlike the precision task, we did not vary target size.
Instead, the screen always showed a point-sized target, indicated by a crosshair.
The participants' task was to tap the device backside as close to the target as possible.
We recorded the relative location of the tap with respect to the target.
The design of the precision task was 2 x   with 5 repetitions for each cell.
As mentioned above, the back-button condition was also tested against a 4px target, for a total of 5 targets.
Target positions were the 12 centroids of a regular 4 x 3 grid.
Method of committing was a betweensubjects variable: half of the participants used back-press and the other half used back-button.
For each trial, we recorded task completion time and error.
Target sizes and positions were randomized.
The design of the land-on task was within-subjects.
There were 12 target positions and 24 repetitions per position.
Target positions were the 12 centroids of a regular 4 x 3 grid.
The main purpose of the study was to determine error rates and task times for individual target sizes, positions, and interfaces in order to inform the design of back-of-device user interfaces.
As a result, we had no specific hypotheses, beyond the obvious expectation that precision button would be less error prone than precision press.
The blue line ends at 80px, where the point cloud of edge targets gets clipped at the edge of the device.
The magenta, middle line is based only on the two center targets, prevents it from getting cropped.
The red line at the bottom is based on the same data, but assumes perfect calibration of the device, such that the centroids of the tapping data  coincide with the target.
The data shown in Figure 20 to Figure 23 allows us to estimate task times and error rates for acquiring different types of user interface elements.
Example: When designing a soft keyboard on the backside of a 2'4" device we could choose a 12-key design.
Figure 23 predicts that, with user-specific calibration, error rates with be around 2% per keypress for the 80x80px buttons.
An alternative full keyboard design might break the device backside into 8 x 6 buttons measuring 40x40px each.
A land-on error rate of close to 40% suggests that we should use precision mode instead.
If used with a separate button, we reach an error rate of about 2% per key press.
We can use the same process to compute estimates for task times and to computer estimates for different screen sizes, such as the 1" diagonal pendant from Figure 1.
Figure 22 shows the distribution of touch locations for all participants for the land-on task.
Figure 23 aggregates the targeting data from Figure 22.
The blue line on top shows what percentage of taps was located outside of a square box of a given size centered around the target.
This value can be used as provides an estimate for the error rate that a square button of the respective size would offer .
In this paper, we have argued that the key to touch-enabling very small devices is to touch-enable the device backside.
While we have demonstrated the effect only for a specific technique--shift--it seems reasonable to claim that any pointing technique using the device front will run into the fat finger/occlusion problem once the screen gets smaller than the technique-specific threshold.
The presented backof-device design, in contrast, works practically independent of device size.
This opens up a very large space of new device designs, including the ones shown in Figure 1 and Figure 3.
They allow us to take a fresh new perspective on a space where touchscreen-based designs have not been able to succeed to date, such as watch-like form factors.
In this paper, we made four contributions:  nanoTouch, a back-of device small-screen device prototype,  a user study showing that back-of-device interaction works independent of device size, while front-touch combined with shift fails for screen sizes below one inch,  a second study providing data that application designers can use to make UI design decisions, and  four back-of-device concepts ranging from in size from ring to clip-on .
As future work, we are planning on directing our attention to interactive back-of-device applications, taking a closer look at steering and tracking.
We also plan on exploring the visual design aspects of back-of-device applications.
And finally, we plan on creating additional prototypes that explore new application scenarios.
