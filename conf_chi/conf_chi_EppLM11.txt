The ability to recognize emotions is an important part of building intelligent computers.
Emotionally-aware systems would have a rich context from which to make appropriate decisions about how to interact with the user or adapt their system response.
There are two main problems with current system approaches for identifying emotions that limit their applicability: they can be invasive and can require costly equipment.
Our solution is to determine user emotion by analyzing the rhythm of their typing patterns on a standard keyboard.
We conducted a field study where we collected participants' keystrokes and their emotional states via selfreports.
From this data, we extracted keystroke features, and created classifiers for 15 emotional states.
Our top results include 2-level classifiers for confidence, hesitance, nervousness, relaxation, sadness, and tiredness with accuracies ranging from 77 to 88%.
In addition, we show promise for anger and excitement, with accuracies of 84%.
User context includes information such as the user's location, situation, or expertise, but one often ignored type of context that could radically change our computer interactions is the user's emotional state.
If a user of a safety-critical application was frustrated or distracted, it could dangerously affect their performance.
If a user of an online tutoring system was frustrated or distracted, the system could adapt its presentation of learning materials to better suit that student's learning style.
For users of ordinary computer applications, frustration or distraction may not be dangerous, but can lead to increased errors.
Systems that detect and respond to a user's emotional state could improve user performance as well as satisfaction.
In addition, emotionally intelligent systems could also aid in computer-mediated communication by incorporating the user's emotional state into messages and by allowing users to naturally express emotional content to others.
Many approaches for detecting user emotions have been investigated, including voice intonation analysis, facial expression analysis, physiological sensors attached to the skin, and thermal imaging of the face.
Although these explorations have seen varying rates of success, they still exhibit one or both of two main problems preventing widescale use: they can be intrusive to the user, and can require specialized equipment that is expensive and not found in typical home or office environments.
Our solution is to detect users' emotional states through their typing rhythms on the common computer keyboard.
Called keystroke dynamics, this is an approach from user authentication research that shows promise for emotion detection in human-computer interaction .
Identifying emotional state through keystroke dynamics addresses the problems of previous methods by using standard equipment that it is also non-intrusive to the user.
To investigate the efficacy of keystroke dynamics for determining emotional state, we conducted a field study that gathered keystrokes as users performed their daily computer tasks.
Using an experience-sampling approach, users labeled the data with their level of agreement with 15 emotional states and provided additional keystrokes by typing fixed pieces of text.
Our approach allowed users' emotions to emerge naturally with minimal influence from our study, or through emotion elicitation techniques.
Despite progress in graphics capabilities and processing power, interactive applications still have considerable usability problems.
One main reason for these problems is that applications do not understand or adapt to users' context, such as their location, expertise, or emotional state.
As a result, applications often act inappropriately: they provide inappropriate feedback, interrupt the user at the wrong time, and increase frustration.
To solve these problems, we must make advances in two key areas: first, a set of mechanisms for gathering and modeling user context;
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
From the raw keystroke data, we extracted a number of features derived mainly from key duration  and key latency .
We then created decision-tree classifiers for 15 emotional states, using the derived feature set.
We successfully modeled six emotional states, including confidence, hesitance, nervousness, relaxation, sadness, and tiredness, with accuracies ranging from 77.4% to 87.8%.
We also identify two emotional states  that show potential for future work in this area.
In HCI, researchers have used physiological sensors to measure the affective state of a user interacting with technology.
Results have been produced by studying users playing video games , navigating web pages , using video conferencing software , and using mobile technology .
The above approaches have two main problems that prevent their widespread use: the sensing technology is obtrusive, and requires expensive specialized equipment.
For example, EKG is measured using electrodes attached directly to the user's skin.
In some cases, the area where the electrodes are placed needs to be shaved to prevent interference .
Although research is underway to integrate these sensors into interaction devices, they are currently intrusive and their mere presence may alter the user's emotional state.
In , a thermal camera is used is measure blood flow to a user's face.
Although unobtrusive, the equipment is specialized and not found in typical home or office settings.
To eliminate the need for intrusive and costly equipment, we propose to determine affective state via typing rhythms.
Affective computing refers to "computing that relates to, arises from, or deliberately influences emotions" .
We are interested in identifying a user's emotional state, so we must first consider how emotions are described, and what other approaches have been used to classify emotion.
The terms affect and emotion are often used interchangeably; we will use emotional state to refer to the internal dynamics  that are present during an emotional episode, and emotional experience as what an individual perceives of their emotional state .
Keystroke dynamics is the study of the unique timing patterns in an individual's typing, and typically includes extracting keystroke timing features such as the duration of a key press and the time elapsed between key presses.
Much of the previous research in keystroke dynamics has been in authentication systems, with the premise that, just as with handwritten signatures, the way that an individual types can be unique enough to identify them .
The use of keystroke dynamics for user authentication has been an active area of research, producing many studies , patents , and systems , whereby users are authenticated by providing the correct user name, password, and typing rhythm .
Anecdotal evidence suggests that strong emotional states can interfere with authentication ; however, little is mentioned of this and it is unclear whether the timing variance associated with these emotional states is similar between individuals.
Most of the authentication systems  use fixed-text models - that is, they use the same static piece of text  that the model was trained on.
There have been fewer approaches  that use models based on free text , as they do not perform as well as fixed-text models .
The length of the required training text varies between different studies; some require a few words  or full pages of text , which can create better performing models .
Although fixed-text models generally perform better than free-text models, the potential applications of free-text models are desirable.
Two main approaches have been used to describe emotions: categorical and dimensional.
The dimensional approach  uses two orthogonal axes called arousal and valence.
Valence describes the pleasure  or displeasure  of a feeling.
Labels for different emotional states can be represented in this two-dimensional space.
For example, anger would be a high-arousal, low-valence state.
Both the categorical and dimensional models of emotion have been used in prior approaches of identifying emotional state.
Some approaches use features easily discernable by other humans, such as facial expressions, gestures, vocal intonation, and language .
For example, face-tracking software is used to analyze facial expressions gathered from webcam images to infer users' affective states .
This approach has been extended to use thermal imaging to identify changes in blood flow patterns of the face that are synonymous with different facial expressions .
Other approaches use features that are less discernable to other humans, but can be measured by specialized equipment.
Free-text models have even been able to identify individuals typing in different languages  as long as the two languages have enough similar valid digraphs.
Most free-text studies require users to enter any `valid' text as sample text ; however, in  keystroke activity was monitored as a background process during normal computer use.
This method had three benefits: the user was less disturbed by the collection method, the data was obtained unobtrusively, and it reduced the cognitive load on the user by avoiding situations where they must think of something to type.
Classification algorithms for the analysis of keystroke dynamics for user authentication include neural networks , distance measures , decision trees , and other statistical methods .
Due to the differences in data collection approaches and classification methods, a comparison of performance across studies is difficult .
The purpose is to gather temporal data `in the moment' rather than retrospectively, which avoids problems of incorrect reconstruction or forgetfulness of the user.
The drawback is that researchers cannot control or balance the different states tested.
We chose an experience-sampling methodology for two reasons.
First, we were interested in emotional data gathered in the real-world, rather than induced in a laboratory setting through emotion-elicitation methods .
Our results are intended for use in real-world systems, and gathering the data for modeling from naturally occurring emotions increases our ecological validity.
Second, as this is a new affect sensing technique, we wanted to explore a wide range of emotional states, and emotional induction, in the laboratory, is limited to one or two emotional states.
In experience-sampling studies, users are typically outfitted with a signaling device that alerts them to complete a selfreport on their current state and/or situation .
We developed custom software for our signaling device and data collection that participants installed on their personal computers for use while they performed their daily tasks.
There has been very little previous work applying keystroke dynamics to affective computing.
Affective states were induced using films.
Physiological sensors were used in conjunction with the Self-Assessment Manikin  , a method of subjectively expressing affective state.
The authors found significant differences between the neutral state and other emotional states, but were unable to distinguish between the induced states.
Recent work by Vizer et al.
They achieved correct classifications of 62.5% for physical stress and 75% for cognitive stress , which they state is comparable to other affective computing solutions.
They also state that their solutions should be tested with varying typing abilities and keyboards, with varying physical and cognitive abilities, and in real-world stressful situations.
The data collection software was written in C# and used a low-level windows hook to scan each keystroke as it was entered by the user.
This program ran in the background, gathering keystrokes regardless of the application that was currently in focus.
The only visible sign that the application was running was an icon in the desktop system tray.
Based on the user's level of computer activity, the program prompted the user throughout their day.
At each prompt, the user was presented with their keystroke text from the previous 10 minutes to review, then with an emotional state questionnaire, and then with some fixed text to type.
The user could opt out of data collection at any time during the prompt if they were too busy or did not want to share their keystroke data .
Initial keystroke data was called free text as it was not constrained or influenced by our study.
The emotional state questionnaire contained 15 5-point Likert scale questions regarding a user's current emotional state: I am frustrated, I am focused, I am angry, I am happy, I feel overwhelmed, I feel confident, I feel hesitant, I feel stressed, I feel relaxed, I feel excited, I am distracted, I feel bored, I feel sad, I feel nervous, I feel tired.
For each statement, the user would: strongly disagree, disagree, neither agree nor disagree, agree, or strongly agree.
The user was then asked to enter a randomly selected piece of text from Alice's Adventures in Wonderland ; this was how we collected the fixed text from users.
To prevent copying and pasting, the user was unable to select the fixed text.
The user then had a chance to review what s/he just entered before submitting the data remotely to our data collection server.
The data collection server allowed us to preserve participant anonymity and supported remote users.
The two primary components of this work are the data collection process and the data processing required to create classifications of emotional state.
The data collection process consisted of gathering and labeling users' keystroke data.
The data processing consisted of extracting relevant keystroke features to build classifiers.
We wanted to gather keystroke data in situ - as participants performed their daily computer activities - but also needed to label each data point with the emotional state of the user.
To accomplish both of these goals, we used an experiencesampling methodology , whereby we periodically collect user keystroke data and user responses to emotional state questionnaires.
To narrow the scope, we used only aggregate features for this analysis and removed all key/graph specific features.
For each feature, we extracted the mean and standard deviation because during a sample period, the user could enter the same sequence of keys more than once .
Keystroke features used in our models are shown in Table 1 and the categories of features are described further.
The duration of the 1st key of the digraphs.
Duration between 1st key up and next key down of the digraphs.
The duration of the 2nd key of the digraphs.
The duration of the digraphs from 1st key down to last key up.
The number of key events that were part of the graph.
The duration between 1st and 2nd down keys of the trigraphs.
The duration of the 1st key of the trigraphs.
Duration between 1st key up and next key down of trigraphs.
The duration between 2nd and 3rd down keys of the trigraphs.
The duration of the 2nd key of the trigraphs.
Duration between 2nd key up and next key down of trigraphs.
The duration of the third key of the trigraphs.
The duration of the trigraphs from 1st key down to last key up.
The number of key events that were part of the graph.
Upon installation of the software, the participant was presented with a one-time demographic questionnaire.
We originally had 26 users take part in the study; however, not all participants completed enough study questionnaires to be included in the analysis - some completed as few as 2 over the duration of the experiment.
Unlike a laboratory experiment, the field study did not allow us to control the amount of participation.
To determine an informed threshold for inclusion in the study, we considered how many responses were needed to ensure that users were familiar with the study questionnaires and were calibrating their responses appropriately.
Participants who completed more than one questionnaire per day on average for at least half of the study duration were likely familiar enough with the questionnaires and the process to be considered to have finished the study.
Thus, we removed users with fewer than 50 responses, leaving 12 participants.
This process occurred prior to the analyses and all remaining results are based on this reduced dataset.
Overall, participants were proficient with word processing, email, and instant messaging applications, with usages of 37 hours a week , 1-2 hours a day , and more than 2 hours a day .
Ten participants indicated they spent at least half of their time on the computer that was collecting data.
Work computers accounted for 8 of the installations with the remaining 4 installations on home computers.
Most participants used desktop computers ; few used laptops .
One of these installations was on a virtual machine.
We only included English-speaking participants because we focused on English character sequences in the features.
Keystroke Duration Features : Keystroke duration features have been used extensively in previous keystroke dynamics work .
For our analysis, duration features were included for both single key features as well as graph features .
For single keys, the duration was the time elapsed between `key press' to `key release'.
For graph duration features , duration was measured as the time elapsed from the first `key press' event to the last key's `key release' event.
For example, for the digraph `the' the 3G_Dur would be the time from the `t' press event to the `e' release event.
Keystroke Latency Features : Keystroke latency features have been used in previous keystroke dynamics studies in authentication .
Keystroke latency is the time elapsed from one key release event to the next key press event or the `time between keys'.
Unlike the duration features, latency always involves two keys so there are no single key latency features.
Our keystroke latency features  were separated for each pair of keys found in the graph.
Other Keystroke Features: We included features that combined some aspects of the duration and latency features.
The raw keystroke data consisted of key press and release events, unique codes for each key, and a timestamp of when the key event occurred.
Extensive processing grouped these keys into graphs of 2 or 3 symbols.
Our keystroke features were mainly derived from the timing of single keystrokes as well as digraphs  and trigraphs .
Initially, we extracted all keyspecific features ; however,
We calculated the number of events that were found in each digraph and trigraph .
Although most digraphs contain 4 events  and trigraphs contain 6, there are scenarios where the user could type more.
This would result in 5 key events in a digraph.
This may be indicative of a user's emotional state, and has not been used previously in keystroke dynamics, to our knowledge.
Keystroke Feature Overlap: Some of the keystroke features that we described overlap slightly; however, in the classification section, we describe how we reduced this set using supervised attribute selection.
We included a few features based on the content extracted  from the free keystrokes, including separate features for the number of characters, numbers, punctuation marks, uppercase characters, and the number and percentage of `special characters' .
These features were used in only the free text models as we provided the fixed text to the user.
The number of mistakes  was calculated for both fixed and free text models.
There are many different ways to correct mistakes .
It was not possible to catch all of the possible correction scenarios as keystrokes were collected from different applications that we did not control.
However, this feature does give a general idea of the number of mistakes being made.
Outliers were removed by calculating the mean and standard deviation for all keystroke timing features for each participant and then removing the samples that were 12 standard deviations greater than the mean for that participant.
A large number of standard deviations were used to only remove trials where very long pauses were present.
We then recalculated all of the features with the filtered instance set.
This resulted in the removal of 0.07% of the samples collected.
This method of outlier removal was similar to the approach taken in .
All values were normalized for each user to facilitate the aggregate analysis.
Due to the large variations in the number of responses per user, we did not create user-specific models, but aggregated the data across participants for 1129 valid instances.
Models were created using the C4.5 supervised machine learning algorithm as it is implemented in Weka .
Although there are many classification algorithms, we used decision trees as a simple low-cost solution to investigate this new approach to identifying emotion.
The chosen decision tree algorithm also handles missing values, which we have in our data set.
All of our features needed to be labeled with the user's emotional state for classification.
We used discrete emotion categories to collect emotional state responses from users because these categories are close to the language commonly used to describe their emotional state.
The available options that were presented for each statement  became the target classes during classification.
Although we identified 31 fixed text features and 37 free text features, we needed to first identify which of these features were important to keep and which had little predictive value.
We used the correlation-based feature subset attribute selection method described by Hall in  and implemented in Weka  to select salient features in our set for each emotional state model separately.
We trained a number of model variations using different text types , numbers of target classes, and adjustments to compensate for class skew.
In addition to the 3 class-level  variations explained in the Feature Extraction section, we included variations that had 2 class-levels , removing instances in the neutral category to determine if keystrokes could differentiate between two opposing states.
Responses were not distributed evenly across all levels of each emotional state .
For example, the responses to the phrase, "I am angry" were skewed to the strongly disagree and disagree categories.
This is expected as users generally would not be very angry as often as not at all angry.
Class skew such as this can lead to unreliable classification rates.
To eliminate class skew, we used a method called under-sampling , which randomly removes instances from the majority classes to equal the class with the fewest instances, creating a uniform distribution .
We included models built using under-sampling and the original distribution.
For each emotional state model that used under-sampling, we repeated the classification process 10 times and report the mean classification accuracy and variance.
For performance of the free text models, see ; in the discussion of this paper, we discuss potential improvement to the free text models.
After removal of participants with fewer than 50 responses, the number of samples collected per participant ranged from 51 to 219 .
We used 10-fold cross-validation from the stratified training results to evaluate our models, which is standard practice when the data set's size is limited .
To more easily describe our top results, we defined a hierarchy of evaluation categories: Bronze, Silver, Gold, and Platinum .
Each category is based on the classification rates and the top three also incorporate false positive rate.
To be considered for one of our categories, the sample-to-feature ratio had to be greater than 10.
During the creation of the different model variations, instances were sometimes removed .
In classifier design, it is generally accepted that there must be 10 times more instances  per class than the number of features .
Another criterion for inclusion was that the Kappa statistic had to be greater than 0.4.
The Kappa statistic indicates how much the classification rate was a true reflection of the model or how much could be attributed to chance alone; Kappa values range from 0  to 1  .
We were very conservative in categorizing models as topperforming, to limit models to those with high classification rates, but also with little or no class skew, low false positive rates, high samples-to-features ratios, and high kappa values.
We considered models that achieved our top 3 evaluation categories as our top emotional state models.
Table 3 shows the results using 10-fold cross-validation.
The Bronze category is not included because it was susceptible to class skew as it only looked at overall classification rates and not classification rates for individual class levels.
Models in the Bronze category are presented in the next section as they show potential for future study.
Our top results were all based on 2 class levels  and most of them used under-sampling.
This means that these models were built using a reduced data set .
Despite this reduction, we are still achieving satisfactory samples to features ratios  as seen in Table 3.
N:M ranges from 11 to 49 with the most samples being used by the `tired' model that used the original distribution of data .
For 7 of the 8 top results in Table 3, under-sampling was used, and the correctly classified rate  and Kappa statistic show averages from the 10 random-samplings models that were used during under-sampling.
The variance columns in Table 3  indicate the variance from these 10 random samplings models.
Lower variance in the classification rates would suggest that the model results are truer indications of the predictive power of keystroke dynamics for emotional state detection.
When considering all of the factors  for the models in Table 3, the `tired' model performs the best.
The original distribution of responses for the `tired' query was more balanced than the other emotional states .
This resulted in two `tired' models reaching our top results, one using undersampling and the other using the original distribution.
These two models are quite similar, with similar numbers of instances, classification rates, and Kappa statistics.
As mentioned in the previous section, we considered emotional state models that made it to our Bronze category as emotional states with potential for future study, rather than as top-performing classifiers.
This is because these models exhibited uneven distributions , which can lead to artificially-inflated classification rates.
Each of the Bronze models in Table 4 are from unbalanced data.
Each emotional state from our top results in Table 3 also appeared in the Bronze category with an unbalanced version , except for the tired state, which had an unbalanced version in Table 3.
In addition, there were new representations from the `excitement' and `anger' emotional states.
As Table 4 shows, classification rates were high; however, these models are also highly skewed.
For example, the 2-state excitement model has an overall classification rate of 84.3%; however, when looking at the individual true/false positives for agree and disagree, we see that the `disagree' class has a 92.7% classification rate whereas the `agree' class has only 56.9%.
Also, the `disagree' class has 87.5% of the total number of samples.
The same pattern exists for the anger emotional state.
It makes sense why such class skew exists in both of the anger and excitement states.
These states have very high activation; it would be unusual for someone to be in a heightened state of anger or excitement for extended periods of time over the course of a normal workday.
Because of the class skew for these two states, there were not enough instances remaining after under-sampling for these models to achieve our top-performing categories .
Among the top emotional state models, the number of features were reduced from 31  to an average of 7.4 .
Using the correlation-based feature subset attribute selection method  allowed us to increase the sample-to-feature ratio with minimal loss to the classification rate.
Table 5 lists the features that were used for each of the top classifiers .
These features contain a fairly even number of both key latency and duration features with the 2G_1KeyLat_Mean and 2G_2Dur_Mean used in most of the models.
Our results show that keystroke dynamics can accurately classify at least two levels of seven emotional states .
In addition, we identified two other emotional states  that have potential for keystrokebased classification.
In this section, we discuss the benefits and drawbacks of using experience-sampling, the use of fixed text versus free text in keystroke-based modeling, aggregate versus individual analyses, the limitations of our data set, and opportunities for extensions to our work.
Although the mean number of collected keystrokes were similar between the free  and fixed text , the standard deviation for the free text was quite high .
A high standard deviation implies that some samples had very few free text keystrokes whereas the fixed text remained relatively consistent.
Future studies should ensure that enough free text data is collected.
One of the goals of our research was to fill the gap in the related literature on the real-world applicability of affective computing solutions.
Our choice to use keystroke dynamics was partly guided by this goal to create classifiers that are unobtrusive and inexpensive enough to be deployed in users' homes or workplaces.
Experience-sampling allowed us to focus on the eventual application of our keystrokebased approach.
Users' emotional states emerged naturally as compared to emotion-elicitation experiments .
Experience-sampling also provided us with the opportunity to perform exploratory research in this new field.
Before our study, there was no guidance on which emotional states might be identifiable using keystrokes; experiencesampling allowed us to take a broad approach to the problem and narrow down which emotional states we should target with future work.
This broad approach would be unfeasible using emotion elicitation because participants would have to be induced into each emotional state  in individual experiments.
In addition, mood induction does not necessarily work on all participants.
Collecting data using experience-sampling and remote data submission allowed us to collect labeled emotional state data with minimal administration overhead.
However, experience-sampling introduced some drawbacks with implications for our analyses.
Due to the uncontrolled nature of this methodology, we could not balance the distribution of classes for each emotional state, which led to class skew in the data.
These uneven distributions limited our interpretation of the results for unbalanced models, and limited the number of instances available for our balanced models that used under-sampling.
Despite this disadvantage, we feel that experience-sampling provides a data-collection methodology that can be beneficial when studying a wide range of naturallyemerging emotions in new areas of research.
We created emotional state models for the entire data set across all participants to maximize the number of samples, especially in the under-sampled versions of the models.
Creating models at the level of individual participants was not viable due to the number of collected samples with which to work.
The mean number of samples per participant was 94.1, and this number would have been reduced when adjusting for class skew using undersampling and in 2 class-level models.
Having so few instances would have caused our sample to feature ratio to be too small and our results to be less accurate and reliable.
However, we do not know whether there are large individual differences in how keystroke dynamics change with emotional state.
With enough samples per participant, personalized models could improve our classification rates.
The success of keystroke dynamics for user authentication is based on the fact that each person's typing rhythm is unique enough to identify them.
This suggests that individuals might have unique keystroke-level reactions to different emotional states.
For example, when stressed, some individuals may type faster and other may pause in their typing more frequently.
Accounting for these personal differences may allow us to build better performing models.
We took care to ensure that we did not artificially inflate our classification rates by ignoring the two main limitations of our data set: its limited size and its uneven distribution.
Half of the participants in our study submitted fewer than 50 samples over the course of 4 weeks.
For ethical reasons, we allowed participants to opt out of submitting data for each sample, but we tried to encourage participation through the use of data-based incentives .
After including only the active participants , we had 1129 samples over all participants.
This total was further reduced in the 2 class-level models though the elimination of the neutral category and in the under-sampled models by balancing the samples per class.
In future work, we would prefer larger data sets with enough samples to perform individual-level models.
Collecting user data for a longer time period and providing better incentives for active participation would increase our data set size.
The second problem with our data set was unequal distributions of responses across some of the emotional states.
Our free text models did not perform well enough to achieve our top evaluation categories and were not included in the results section.
We believe that this was due to setting the user activity threshold too low.
A low activity threshold caused the questionnaire to be presented to the user with fewer free text keystrokes than needed for analysis.
To address this problem of class skew, we used under-sampling, which reduced our data set size.
In our field study software, we prompted the user at random times to fill out the experience-sampling questionnaire.
We also allowed them to select the questionnaire explicitly if they wanted to submit their data.
Coaching participants to use this explicit submit feature when they were experiencing low-frequency emotions like anger could help improve the distribution of responses.
With the preliminary models provided by our work, we could deploy an adaptive version of the software that conducts modeling in the background and prompts the experience-sampling questionnaire when it detects that the user might be in a low-frequency emotional state.
Adding low-level features based on mouse kinematics might also improve classification.
In addition, using a multi-modal approach could provide better generalization of the models to different computer-based tasks that require varying amounts of typing or aiming.
Our software collected linguistic data and mouse kinematics, in anticipation of future analysis.
We could also combine our approach with established methods  for a more complete and robust model of user experience.
We used our own custom questionnaire to take a broad approach to collecting a variety of user states.
Using a validated emotional state scale  would provide additional validity to the keystroke dynamics approach.
Also, distinguishing between different emotions rather than levels of a single emotion would be beneficial.
Our work is the first to classify a broad range of emotional states using typing rhythms.
We have demonstrated the efficacy of this technique, which opens up opportunities for future research to refine, improve, and utilize this approach.
Because of our limited data set, we used all of the available samples in our models.
Filtering the samples based on the application context could improve classification rates.
For example, the keystroke dynamics gathered using word processing software are likely different than when writing code in an integrated development environment.
Samples could be filtered based on the application that was active during each keystroke entered, which would allow for application-specific models.
Although we did not filter samples based on the active application, our software did collect this information in anticipation of future analysis.
In addition, we would like to investigate how models personalized to individual users change classification rates, as described in the section on aggregated analysis.
When selecting our features, we used only generalized features, such as the flight time between two keys in a digraph.
We could also investigate specific features, such as the flight time between two keys in a specific digraph , or a number of specific digraphs .
We calculated these features from our raw data , but due to our data set size, we focused on the general features.
Rather than use individual features in a model directly, feature aggregation through an approach like Principle Component Analysis  could be used.
We investigated using PCA to select and aggregate features, but found that this process did not improve overall classification rates .
In addition, the resulting decision trees were difficult to interpret due to the feature aggregation.
We could also use aggregation of the target classes to improve our results.
Informal analysis of the 15 emotional states found many correlations.
Combining the co-varying states could improve classification rates and the potential application.
Adding other interaction-based features could improve classification.
The ability to recognize emotions is an important part of building intelligent computers.
Systems that could extract the emotional aspects of a situation would have a rich context from which to make appropriate decisions about how to interact with the user or adapt their system response.
There are two main problems with current approaches for identifying emotions that limit their applicability: they can be invasive and may require expensive equipment.
We presented a solution that determines user emotion by analyzing the rhythm of users' typing patterns on a standard keyboard.
To gather emotionally-labeled data, we conducted a field study where participants' keystrokes were collected and their emotional states were recorded via self report using an experience-sampling methodology.
From this data, we extracted keystroke features, and reduced our feature set using correlation-based feature subset attribute selection.
We created classifiers for 15 emotional states.
Our top results include 2-level classifiers for confidence, hesitance, nervousness, relaxation, sadness, and tiredness with accuracies ranging from 77.4 to 87.8%.
In addition, our results show promise for anger and excitement, with accuracies of 84%.
This work presents the first use of naturally-gathered typing rhythms to identify the emotional state of a computer user, and the first method of sensing a variety of emotional states unobtrusively and inexpensively.
This work is important as it moves us closer to creating emotionally-aware computers that can be widely deployed.
Bender, S. and Postley, H. Key sequence rhythm recognition system and method.
Bergadano, F., Gunetti, D., and Picardi, C. Identity verification through dynamic keystroke analysis.
Bergadano, F., Gunetti, D., and Picardi, C. User authentication through keystroke dynamics.
User identification via keystroke characteristics of typed names using neural networks.
Carroll, L. Alice's Adventures in Wonderland.
Chen, D. and Vertegaal, R. Using mental load for managing interruptions in physiologically attentive user interfaces.
Coan, J. and Allen, J. Handbook of Emotion Elicitation and Assessment.
De Silva, L. and Suen Chun, H. Real-time facial feature extraction and emotion recognition.
Dowland, P. and Furnell, S. A Long-term trial of keystroke profiling using digraph, trigraph, and keyword latencies.
Drummond, C. and Holte, R. C4.5, class imbalance, and cost sensitivity: why under-sampling beats oversampling.
Epp, C. Identifying emotional states through keystroke dynamics.
Fairclough, S. Fundamentals of physiological computing.
Gaines, R., Lisowski, W., Press, S., and Shapiro, N. Authentication by keystroke timing: some preliminary results.
Gunetti, D. and Picardi, C. Keystroke analysis of free text.
Gunetti, D., Picardi, C., and Ruffo, G. Keystroke analysis of different languages: a case study.
In Lecture Notes in Computer Science.
Hall, M. Correlation-based feature subset selection for machine learning.
