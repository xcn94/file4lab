We present and evaluate a novel user interface for indoor navigation, incorporating two modes.
The interface particularly addresses the vision-based localization method by including special UI elements that support the acquisition of "good" query images.
Mobile location recognition by capturing images of the environment  is a promising technique for indoor navigation in arbitrary surroundings.
However, it has barely been investigated so far how the user interface  can cope with the challenges of the vision-based localization technique, such as varying quality of the query images.
We implemented a novel UI for visual localization, consisting of Virtual Reality  and Augmented Reality  views that actively communicate and ensure localization accuracy.
If necessary, the system encourages the user to point the smartphone at distinctive regions to improve localization quality.
We evaluated the UI in an experimental navigation task with a prototype, informed by initial evaluation results using design mockups.
We found that VR can contribute to efficient and effective indoor navigation even at unreliable location and orientation accuracy.
We discuss identified challenges and share lessons learned as recommendations for future work.
Imagine you are at the airport, at a mall or in a museum and your smartphone gives you directions to your departure gate, that hot new fashion store, or the famous Dal i painting you want to visit.
While mobile navigation is omnipresent outdoors, it is not inside buildings.
Reliable indoor navigation is still a "hot topic".
While researchers are still looking for the optimal localization method, appropriate novel user interfaces for these scenarios have to be investigated.
An analysis of existing indoor localization techniques , shows visual localization to have multiple advantages to concurrent methods for indoor usage.
Using computer vision, this technique captures and matches images of the environment with previously recorded reference images of known locations.
However, we found that existing user interfaces  for pedestrian navigation are not appropriate for that  technique, since they do not particularly address the characteristics of visual localization.
As the device uses the camera to orientate and position itself, visual localization works similar to human orientation and wayfinding .
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
In that way, the UI can benefit from the strengths of visual localization, and the  localization quality can be improved through UI elements and the user interactions with them.
In this paper, we implemented a novel UI concept for an indoor navigation system which is specially fitted to visual localization, and we provide a first evaluation of this UI, based on experimental simulation, compared against the conventional augmented reality  technique.
Moreover, our work represents an example for interweaving the UI and the underlying localization technique of an indoor navigation system to the advance of both, arguing that localization and UI should be treated jointly for being most effective.
The structure of this paper is as follows: We begin with presenting related work, where we focus on existing user interfaces for navigation systems and on the particularities of visual localization.
Subsequently, we describe the implemented interface concept and UI elements.
We introduce the conducted study and discuss our experimental findings in a comprehensive way.
We finally share lessons learned in order to inform the design of future visual indoor navigation systems.
Based on the position of feature points, even the pose  can be detected, which is usually not the case with other approaches.
However, the image database must be built up once  and updated regularly when buildings and objects therein significantly change.
There are several concrete implementations of camera-based location recognition systems .
Hile and Borriello correlated a floor plan or a previously captured reference image to estimate the device's pose and to calculate an information overlay .
However, the system only works for static images.
After having motivated the visual localization technique, we provide an overview of pedestrian navigation user interfaces.
Besides rendered graphics, augmented reality  is considered an intuitive way to visualize a location and has been used in manifold ways .
In AR, virtual elements are superimposed over a live camera view, so that users do not need to translate between the virtual representation and the real world .
They found that adapting the interface to the users' preferences is particularly important.
AR can also convey information beyond navigation instructions.
Similar ideas could be adapted for pedestrian navigation.
Augmentations enhanced exhibits with additional information.
Visitors were guided along a predefined route through the museum when they searched with their phone for the next AR object.
An AR system which employed floor-projected arrows as way directions was evaluated better in terms of usability than a map-based system .
Researchers also recognized the value of landmarks for orientation .
A similar approach is presented by Beeharee and Steed .
They found that by topdown and bird's eye views of a panorama, users were quicker to locate objects in the environment than using a frontal view.
First, we discriminate visual localization against other techniques to localize a device inside buildings, and outline the advantages of visual localization.
By visual localization, we understand the usage of computer vision to correlate query and reference images by characteristic properties .
Feature matching has the advantage that image can be captured with the device's camera at any location, which then serve as query images.
No augmentation of the infrastructure with fiducial markers  is necessary any more.
WLAN fingerprinting , require dense coverage of access points.
This coverage is in many buildings not available, and it costs money and effort to establish.
Furthermore, a common camera-equipped smartphone is sufficient for visual localization.
By contrast, approaches based on signal metrics, such as angle  or time of arrival , require special hardware, such as directional antennas or ultra-accurate timers .
Signal-strength-based measurements are feasible with common hardware, but the location often can only be determined within a radius of 1 m or more , even in laboratory tests.
In the real world, where the typical density of access points is mostly lower, expected localization accuracies are likely to be inferior to those in controlled experiments.
Fiducial markers provide exact localization only at locations where such markers are placed.
Apart from these "key locations", the position needs to be estimated with relative positioning techniques, such as dead reckoning.
With a database of sufficiently densely recorded reference images, visual localization can be performed at almost any lo-
When going beyond key-point localization, as used in many prior systems , towards continuous guidance, as known from outdoor navigation, new challenges emerge.
In that case, the visual system must capture query images on a regular basis.
The challenge is here that quality and distinctiveness of the query images impact the location estimate.
Ideal query images are crisp and show characteristic areas with lots of visual information.
However, the camera-visible scene can happen to be blurred due to motion of the device, or can be not sufficiently unique .
The pose of the device plays a role as well - the typical orientation when holding a phone  entails that rather the floor is visible to the camera, but not corridors and rooms and the objects therein .
Dedicated UI elements for the visual localization method shall help to improve localization accuracy.
We assume that a visual localization system can determine its location better when the device is held upright, as if taking a photo.
In that pose, the camera points at regions in eye height, such as exhibits, posters or signs, which are potentially more discriminative motives for feature matching than if the camera were pointed downwards.
The user is thereby asked to bring the phone from a pose as in Fig.
Four indicator types fulfilling that purpose are proposed: * Text Hint: A notification to raise up the phone appears until the pose is such that sufficient features are visible.
This metaphor is inspired by an autofocus camera, motivating the user to find the "best" shot.
The user should steer the indicator into the green area.
Involving the user to help the system improve its position accuracy has already been used in other contexts for selflocalization.
For example, Kray et al.
Another way to draw the users' attention to feature-rich objects is to explicitly highlight them in the viewport.
Object highlighting is motivated by an additional benefit for the user: context-based services.
Like this, stores in a mall, individual shop windows, or even doors and doorplates can become points of interaction.
However, a convenient side effect is that typical "interaction areas" like posters or signs often have a very characteristic appearance and therefore also serve well as reference images for localization .
If they attract the user's attention and are focused with the smartphone's camera, they implicitly help improve the system's certainty of the location estimate.
Our implementation is based on the UI concept we have presented in earlier work .
It includes a panorama-based view as a complement to Augmented Reality and proposes different visualizations for motivating users to record "good" query images.
The concept is dedicated to visual localization and conceived as "live interface" during the entire navigation process, i.e., it is used not only for localization at a certain point on the route, but allows continuous guidance.
Additionally, it is prepared for the use of context-based services by interacting with objects in the environment.
The interface consists of two modes for continuous guidance: Augmented Reality  and Virtual Reality .
Augmented Reality enhances the video seen by the smartphone's camera by superimposing navigation information, such as a directional arrow.
Since users need to hold the phone upright for visual localization , this seems a reasonable interface for a visual localization system.
Users hold the phone as illustrated in Fig.
The alternative mode is Virtual Reality, which can be employed also when the phone is carried in a lower position.
It displays pre-recorded images of the environment  that are arranged to a 360 panorama on the mobile device.
Navigation arrows are directly rendered into the panorama, so that their orientation is fixed in relation to the virtual 360 view.
This is expected to have several advantages.
First, the device can be held in a more natural and comfortable way, as illustrated in Fig.
Second, we expect that the "hard-embedded" navigation arrows provide a more reliable navigation, as they also show the correct way in the panorama if the orientation estimate is not perfectly accurate.
Furthermore, in case no reliable localization estimate is possible, the frequency in which panoramas are updated can be lowered.
Hence, we expect VR to be more robust than the more conventional AR view.
A non-functional mock-up of the proposed UI concept has been evaluated in an online survey in prior work .
We summarize and analyze the results of this evaluation as a starting point for our investigation of the concept's effectiveness in practice.
Extending on this prior work, we developed a working system which was evaluated in a laboratory study.
The video demonstrations contained the simulated field of vision  in the upper part, and the simulated visualization on the smartphone in the lower part.
In four videos for each mode, different types of errors  were induced to the system's location estimate, so that the simulated output changed accordingly.
Subjects rated the perceived accuracy and quality of the guidance instructions they saw in the videos.
In the individual ratings of each video, AR was preferred in case of reliable localization, but VR was perceived as more accurate when errors were introduced.
The panoramas in VR helped subjects to orient themselves even if the location estimate of the system was incorrect.
However, when asked which method subjects would generally prefer, 58% chose AR.
This inconsistency motivated us to gain a deeper understanding of users' preferences.
The additional UI elements  were only evaluated in terms of understandability, but not in terms of effectivity.
Results do not tell if these elements really lead to more detected features and thus to improved localization.
It was only examined which of the Frame and Soft Border visualization is believed to be less distracting , but not what was their actual effect based on actual object tracking.
Subjects rated four visualizations  with respect to how likely it would make them raise the phone.
The most effective visualizations were the text instructions and spirit level metaphor, followed by color scale and blur.
Furthermore, subjects compared two object highlighting visualizations: Frame showed a rectangle around the object of interest, while Soft Border showed a semi-transparent overlay, smoothly fading out at the borders.
We hypothesized that Soft Border better hides the inherent inaccuracy and jitter effects of object tracking due to the lack of a sharp border, adding to a more stable, calm visualization.
As a consequence, distraction from the navigation task would be reduced with Soft Border compared to Frame.
In fact, subjects rated the Soft Border visualization equally attentionraising as Frame, but at the same time less distracting.
In order to evaluate the previously presented UI in an experiment, we built a prototype in Android 2.31 following the tool requirements in .
We implemented the described VR and AR modes as shown in Fig.
Users can either manually switch between VR and AR with a button on the top right of the screen, or the system can switch modes automatically based on the gravity sensor readings.
In an upright pose as in Fig.
Based on empirical trials, we set the threshold angles to an inclination of 35 for switching to AR, and to an inclination of 30 for switching back to VR.
We implemented the navigation mechanism with a Wizardof-Oz  approach .
This allow us to modify the accuracy of position and orientation estimates throughout the different study conditions.
Further, WOz enables comparable conditions for all participants.
A live localization system would not guarantee reproducible behavior in all trials.
With this app, the experimenter sends location information to the subject's device at the desired position of the route, and can deliberately trigger localization and orientation errors.
The navigation interface on the subject's device is implemented with OpenGL ES 2.0.
For VR, it displays 360 panorama images of key locations and draws the navigation arrow on top.
For AR, the directional arrow is anchored to virtual "key point" locations similar to VR, except that it is overlaid on live video from the rear camera.
The panorama photos of the route used in the experiment and the associated walking arrow directions for each key point have been prepared and stored in the WOz app.
For both AR and VR, the compass was used to auto-rotate the visualization, accounting for device orientation.
In VR, users could also drag and hold panoramas to rotate them manually; lifting up the finger re-enabled auto-rotation.
We draw the following conclusions and lessons learned from this initial evaluation, which motivate us to a further iteration of the presented concept, and to an experimental evaluation.
A questionnaire-based survey with mockup videos might not reveal the true strengths and weaknesses of AR and VR modes.
Users did not actually navigate in a building and thus could not evaluate certain aspects in situ .
Moreover, using the interfaces while walking  might have produced different results than evaluating them in a video .
Subjects perceived the VR mode to be more reliable in case of inaccurate localization.
However, they widely preferred AR in a direct ranking, which seems contradictory.
We hypothesize that in situ, preference for AR would be lower, since the phone must be carried in an uncomfortable pose for AR to work.
Such physical usage factors cannot be determined in an online study.
AR probably appeared in the mockup as the more elegant solution, compared to a "flip book" impression of VR.
No combined evaluation of AR and VR has been performed to see which mode subjects actually use more frequently in a navigation task.
For the automatic trigger, we used a FAST feature detector from the OpenCV framework for Android to detect the number of features in the camera's live image.
The anticipated position of the device  is determined by the phone's gravity sensor.
We also implemented an object highlighting function which we trimmed to detect posters on uniform backgrounds using the image processing pipeline depicted in Fig.
For each frame, a contour detection is applied after edges have been enhanced by a Canny edge detector.
The contour containing the most FAST features is regarded as the most interesting object in the scene, and is highlighted.
We created two visualizations: for the Frame highlight, a red rectangle is drawn; for Soft Border, a semi-transparent texture with gradient borders is drawn at the position of the chosen contour.
We hypothesize that users reach their navigation destination faster with VR than with AR, i.e., that VR is more efficient .
Further, similar to the online study, we suppose that VR will be perceived to be more accurate in case of errors .
Although subjects preferred AR over VR in the online evaluation  , we hypothesize that VR would be generally favored in a hands-on study .
We evaluated the described user interface concept regarding its ability to deal with the previously exposed challenges.
By these experiments, we aim at verifying the results of the initial mockup's evaluation.
We conducted three experiments, covering the following aspects of the navigation interface:  efficiency, perception and convenience of AR and VR under different accuracy conditions,  effectivity of UI elements specific to vision-based localization, and  convenience and distraction of object highlighting.
The accuracy of the system's location estimate was varied in four conditions , for both AR and VR.
Consequently, each user traversed the path eight times.
We decided to use the same path in all conditions for better comparability, but counterbalanced the order of conditions with a 4x4 Latin square to weigh out learning effects over all conditions.
Subjects were asked to rely only on the given instructions, so that they could not be sure whether the path would not vary.
Navigation instructions were fed into the subject's phone by the experimenter .
However, in the following we only use medians  and non-parametric tests to report the results.
Both in VR and AR mode, subjects clearly identified position and orientation accuracy differences between the No Error and the respective error condition.
This indicates that subjects were able to generally identify the induced position and orientation errors.
However, only with AR, p-values below 0.05 were observed for differences in perceived correctness between error and no error conditions .
The perceived correctness of instructions was rated significantly higher for VR than for AR.
Those results indicate that VR is generally considered to be more accurate than AR .
In error conditions, the experimenter replaced correct images and instructions twice by short sequences of misplaced  and misoriented panoramas .
Those errors were introduced at the same locations for all participants.
Start and end time of each run  were measured by the device.
Users were asked to "think aloud" while using the system and answered a questionnaire after each run.
Subjects were in average 25 seconds faster to reach their destination with VR  than with AR , which is a significant difference according to a paired sample t-test , and confirms H1.
With VR, no significant time differences between conditions were found.
With AR, differences between conditions were partly significant.
This signifies that AR works worse in case of  errors.
Asked for the preferred system, 50% decided for VR, 33% for AR, and 17% were undecided .
This strong tendency is presumably not only grounded in the quality of navigation instructions, which were perceived to be better in VR, but also in the convenience when using the system.
Subjects found carrying the phone more convenient in VR  than in AR , which is a significant difference  The required upright position for carrying the phone in AR was physically constraining.
One participant said that it could work "well for 200 meters, but not more".
Most subjects found it embarrassing to pass by other people in that pose, because others might fear being recorded.
This problem was not given in VR, because the camera in that case pointed towards the floor.
Subject rated the perceived accuracy in the conditions Without Error, Position Error, Orientation Error and Combined Error.
Subjects were presented the following statements: "The system seemed to know well where I am" , "The system seemed to know well in which direction I am looking" , "The navigation instructions were always correct" , and "Overall, I found the guidance accurate" .
Agreements to each statement were indicated on a symmetric 7-step Likert scale where -3 corresponds to "strongly disagree" and +3 to "strongly agree".
5 summarizes the responses in box plots.
Left: Perceived guidance accuracies in experimental conditions of AR and VR interfaces.
The box plots visualize the level of agreement to the statements on the bottom right.
Top right: Task completion time using VR and AR.
In AR, Subjects on average took 25 seconds longer, and differences between conditions were higher.
The indicator told subjects to collect enough features for relocalization.
As soon as subjects raised the phone until the bubble was centered on the scale, the indicator disappeared and a location update  was displayed.
To increase the degree of realism, the interface automatically switched between the AR and VR visualization based on the phone's inclination, as described in the Prototype section.
Subjects were not given any instructions how they should carry the phone.
We logged the inclination of the phone , whether the feature indicator was currently shown or not, as well as the number of detected FAST features .
After the experiment, users answered a questionnaire.
6 illustrates, based on an exemplary excerpt of the experiment's data, how the number of features per frame was correlated with the phone inclination and the state of the indicator.
The experiment also showed that subjects preferred the lower carrying position for VR mode, compared to the upright pose for AR mode.
They only raised the phone when told so by the visualization, but soon returned to the more comfortable carrying position.
None of the subject deliberately chose to carry the phone upright which would have activated AR mode.
Subjects responded that they found the pose-dependent switch between AR and VR convenient .
They also understood the meaning of the indica-
Reliable localization requires 100 to 150 features in the image .
While the indicator was visible, the average number of detected features per frame rose from 42 to 101.
Given that the amount of frames in which more than 150 features were detected was 20.7% with active indicator, and 8.1% with inactive indicator, the indicator significantly increased the probability for successful re-localization, which confirms H4.
0 I find the visualization convenient.
Soft highlighting Frame 0 The jiggling of the visualization distracted me.
The visualization drew my attention to the poster.
Soft Soft highlighting highlighting Frame Frame 0 0 I have been motivated by the the visualization visualization convenient.
I find Soft Soft highlighting highlighting Frame Frame 0 0 The was accurate.
The jiggling oftracking the visualization distracted me.
We now discuss the findings of Experiments 1-3, also in comparative view to the initial mock-up study , and formulate lessons learned.
We also report on issues that have not been addressed explicitly in our presentation of results, but which have become evident in the course of our study or were explicitly mentioned by participants when "thinking aloud".
VR mode turned out to be advantageous in several ways.
In Experiment 1, it brought subjects significantly faster to the destination, independently of the accuracy condition.
Further, the perceived correctness of instructions was higher in VR than in AR, which made the system more reliable even when panoramas were incorrect with relation to position and orientation.
Navigating using VR was also more convenient from a practical point of view, since this visualization did not require subjects to hold up the phone all the time .
Experiment 2 confirmed this, where subjects almost "automatically" chose VR when they had the choice how to carry the phone.
An additional argument in favor of VR manifested through the "think aloud" technique, where multiple subjects reported that they felt like unwantedly recording or "stalking" other passers-by when walking around with active camera in AR mode.
In the direct vote, subjects clearly preferred VR over AR, in contrast to the initial mock-up study, where subjects liked the AR visualization better.
This contradiction could be explained due to the fact that the AR UI might have appeared more appealing in the simulation, and that subjects could not really compare both in practice.
Moreover, the physical constraints of AR - the required pose of the phone - seem to be a "knock-out criterion".
Hence, we see the hands-on results as more plausible and more in line with the results for efficiency and convenience, which were likewise in favor of VR.
We thus recommend, as a guideline, the VR mode as primary interface for a visual navigation system.
Particularly when localization accuracy is not perfect, it allows still reliable and fast guidance, compared to AR.
We evaluated the two ways of highlighting objects, Frame and Soft Highlight, as described earlier and illustrated in Fig.
Our algorithm is currently optimized to detect square, feature-rich objects out of a uniform background.
This applies to, e.g., a poster on a wall, which we chose as scenario for evaluating the object highlighting mechanism.
It was tested beforehand that the posters could be robustly recognized.
Subjects pointed at the posters using both highlighting visualizations.
Feedback was afterwards collected by a questionnaire.
The results are summarized in Fig.
On a Likert scale from -3 to +3, subjects indicated that Frame drew more attention to the poster  than Soft Highlight .
Given that the visualization signals a possibility to interact with the object, they found Frame more convenient  than Soft Highlight .
The semi-transparency of Soft Highlight complicated readability of text on the poster.
Regarding distraction, the visible contours of the Frame visualization were perceived as more unstable.
During a navigation task, subjects would be more distracted by Frame  than by Soft Highlight .
Although this is a tendency towards H5,
The AR view, by contrast, can play out its strengths in two cases.
First, it can help to improve feature collection using the feature indicator.
In the study, the spirit level visualization contributed to a rise of visual features in query images, thus increasing the probability of reliable re-localization.
Hence, a visual navigation system could switch to AR mode when the location estimate is too inaccurate even for the robust VR mode, and ask and motivate users to relocalize themselves by pointing at a feature-rich scene.
Second, AR can integrate object highlighting which likewise may contribute to feature-rich query images.
We have investigated two highlighting visualizations and found that Frame-based highlighting of interesting objects contributed to high attention of users, but at the same time distracted them stronger during navigation.
The Soft Highlight visualization reduced visual jiggling, but aroused less visual attention and resulted in worse readability of text on posters and signs.
As another guideline, a way to combine the advantages of both visualizations could be to use Soft Highlight for peripheral objects during a navigation task in order not to distract subjects too much, and to employ the Frame visualization once a user focuses an object with the phone.
Since both VR and AR are useful components of a visionbased navigation interface, future work will have to investigate how both can be combined even better.
This is problematic in two ways: Such distinctive objects expose characteristic features and are thus important for visual localization.
As a consequence, image matching could fail after a change in the real world.
Second, also humans use landmarks for orientation.
When they see, e.g., a poster in the VR panorama image, they might search for this poster in the real environment to orient themselves, which could be irritating if it is not present any more.
A possible solution for that problem could be crowd-based updates.
Query images users take with their smartphone cameras can be included as new textures and continuously update the reference dataset.
However, more profound changes in buildings  that entail detours require not only texture updates, but also adaptions of the underlying 3D model and a different navigation path, which might eventually require re-mapping  the building.
However, the real environment often does not look exactly like the recorded panorama images.
While color-invariant feature descriptors can minimize the matching problem for the localization algorithm, differences in lighting conditions and exposure changes between subsequent panoramas have been negatively noticed by subjects.
However, it did not hinder them in finding their route.
To some extent, image post-processing  could solve this issue.
Mapping of  buildings, however, will often have to take place at night when they are closed for the public, and therefore exhibit significantly different lighting conditions than at day.
In order to ease mapping of landmarks between panoramas and the real world, characteristic objects could be highlighted in the interface with a similar approach to what we presented in this paper.
Another challenge are permanent changes to the real environment.
Posters or advertisements might be replaced from time to time .
Subjects reported that the frequent updates of the panorama images in VR mode  were partly irritating, especially when not permanently looking at the screen.
Since each panorama was slightly different in perspective and lighting, they had to "re-check" their position in reference to the panorama each time they looked back at the display.
Some stated to have used mostly the distance indicator , and to have looked at the panorama only for double-checking when approaching the turn location.
This leads to the idea of varying the frequency in which panoramas are updated during a path.
Instead of showing always the closest view to the current location estimate, a reduced set of panoramas could be used along the route, illustrating particularly the turns and difficult parts.
This could reduce the cognitive effort required for visually matching panoramas with the real world, at similar quality of guidance.
Although the evaluation presented in this paper provides valuable insights, it also has limitations.
First, this work evaluated interfaces with simulated localization data.
This was necessary to test the ability of AR and VR interfaces to cope with varying levels of accuracy.
Simulations can however not fully model a self-contained system.
It is subject to future work to evaluate our UI concept, which we have shown to be sound and useful, with an underlying live-working visual navigation system.
Further, it was not part of this work to evaluate the accuracy of visual localization.
However, we have shown that VR mode provides reliable guidance even with low  accuracy, making the UI adequate to work on top of a variety of visual localization systems, including such with lesser accuracy.
As we have tested responses to various error types and levels of accuracy, we believe that the results will be transferable to a broad range of real-world cases.
We have presented a user interface adapted to some unique challenges of visual indoor navigation, and evaluated a working prototype in a hands-on study.
Our concept combines virtual and augmented reality elements, and proved in quantitative and qualitative experiments to provide reliable navigation instructions even with inaccurate localization.
It actively contributes to feature acquisition which improves positioning certainty.
We identified challenges of visual localization and outlined ways for solving them.
We believe that vision-based approaches are a promising technique for indoor navigation.
Future work will have to evaluate approaches addressing the mentioned challenges in real-world studies, with a larger user base, and with a live localization system.
Azuma, R. A survey of augmented reality.
A natural wayfinding exploiting photos in pedestrian navigation systems.
Butz, A., Baus, J., Kr uger, A., and Lohse, M. A hybrid indoor navigation system.
Hile, H., and Borriello, G. Positioning and orientation in indoor environments using camera phones.
Hile, H., Vedantham, R., Cuellar, G., Liu, A., Gelfand, N., Grzeszczuk, R., and Borriello, G. Landmark-based pedestrian navigation from collections of geotagged photos.
Kelley, J. F. An empirical methodology for writing user-friendly natural language computer applications.
Kray, C., Elting, C., Laakso, K., and Coors, V. Presenting route instructions on mobile devices.
