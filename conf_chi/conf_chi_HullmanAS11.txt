Such features include active and passive collaboration tools that include threaded discussions, social embedding, tagging and other social annotation schemes.
The most evident impact of these designs is that the visualization artifact is no longer considered independently of social content as prior members' responses and observations become attached to the visualization.
This represents a significant shift from traditional visualization representation where any commentary or annotation was limited to a small group.
Online visualization communities may generate useful insights into data sets, under conditions that lead to the majority answer being more likely to be correct than any individual response .
It is also possible that visualization users may misinterpret data when one or more prior users within the community have made errors in their interpretations of data.
Of particular concern is the possibility of an erroneous information cascade in which initial members seed a discussion with inaccurate interpretations that get further distorted over time.
A viewer new to a complex set of data with numerous options for creating visualizations may rely on the visualizations and interpretations that prior users have generated to constrain the search space.
This is not unlike studies on patterns of bias arising from social proof in cultural markets like music downloading websites, where the popularity of artifacts becomes unpredictable and subject to a "rich get richer" dynamic .
The principle of social proof--which suggests that actions are viewed as correct to the extent that one sees others doing them--falls under the larger category of social influence effects: those where a subject's feelings, thoughts, or behaviors are influenced based on observations of others' behavior in a similar situation.
Studies of how social influence and conformity affect decision-making date back to the 1950's and earlier.
In the context of visual perception, Asch's line experiment famously showed that the subject's responses for a simple length judgment task can be influenced by the answers first supplied by confederates .
Many new experiments have since identified different types of social influence and the contexts in which they are effective .
Interpreting a data visualization involves a complex set of cognitive and perceptual processes that have been identified by psychological research on graph comprehension .
Social visualization systems have emerged to support collective intelligence-driven analysis of a growing influx of open data.
As with many other online systems, social signals  are commonly integrated to drive use.
Unfortunately, the same social features that can provide rapid, high-accuracy analysis are coupled with the pitfalls of any social system.
Through an experiment involving over 300 subjects, we address how social information signals  affect quantitative judgments in the context of graphical perception.
We identify how unbiased social signals lead to fewer errors over non-social settings and conversely, how biased signals lead to more errors.
We further reflect on how systematic bias nullifies certain collective intelligence benefits, and we provide evidence of the formation of information cascades.
We describe how these findings can be applied to collaborative visualization systems to produce more accurate individual interpretations in social contexts.
More individuals are generating and analyzing interactive data visualizations online than ever before, thanks to a growing number of social visualization sites like ManyEyes  and Swivel .
The need for such systems is in part driven by the ample availability of open data and the strong belief that collective analysis of this data might produce better understanding of this information.
Social interaction, through visualization annotation, is suggested to be a primary motivator of use .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Although there is some evidence that prior beliefs and expectations have an influence on graph interpretation , the focus of research has been on an individual's own prior beliefs rather than the effect of beliefs of others.
The work presented here seeks to extend graphical comprehension models to take into account social influences so that these models are appropriate for describing data interpretation in the context of social visualization systems.
At the basis of any interpretation from visualization is an underlying graphical perception task.
While we hope to eventually close the gap between basic graphical perception tasks and the higher-level interpretations that they lead to, our goal here is to provide initial evidence for the possibility for social influencers like social proof to occur in graphical settings.
We begin by modifying Cleveland and McGill's original experiments on the accuracy of visual judgment types .
By including socially-derived signals , and testing for other potential effects , we are able to assess the impact of social influence by adjusting the bias in these signals towards or away from the "true" answers.
Our work illustrates how social proof with an unbiased signal results in more accurate estimates but that a biased signal results in less accuracy.
Additionally, we extend the classical experiments to include a more difficult, and arguably more realistic task, that of judging linear associations between variables in a scatterplot.
We also provide evidence that biased collective estimates for a graphical perception task can emerge in social environments through information cascades.
We use the insights gained from our experiments to identify the implications for social visualization systems and discuss the impact of social proof and other forms of social influence on visualization research.
Social visualization environments have captured the attention of researchers in visualization, HCI, and CSCW who seek to understand the features of successful social data analysis systems.
Heer, Wattenberg, Viegas, and others have completed a series of works aimed at describing the space .
Motivated by work in various fields with socio-organizational bents, the authors of  present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication and social organization.
While highlighting many useful design considerations, the tone of these investigations is optimistic in that pitfalls that may stem from social processes, individual biases, and their combination are rarely considered.
Instead, researchers discussing socially-related challenges tend to focus the most on the tendency toward imbalances in contributions among members .
Graphical perception is a mainstay of visualization research  due to its potential to improve the efficiency of automatic presentations of data .
Graphical perception can be affected by both design and data characteristics, warranting its continued investigation.
A recent set of experiments executed by  demonstrated the use of Amazon's Mechanical Turk  as a means for replicating prior studies and producing new knowledge in this area.
Statistically comparable results are demonstrated despite the lack of control over screen resolution and other technical conditions.
Qualification tests consisting of sample questions for a target task help control for workers' prior experience with graph interpretation and statistical literacy.
For example, Heer and Bostock replicate one experiment from Cleveland and McGills's seminal study  to rank visual variables such as length, area, and color for encoding quantitative data.
A proportion judgment task executed on bar, stack bar, and pie charts , is used to rank the dominant visual variables on which the chart types are based.
Heer and Bostock's results match the original authors' ranking of visual judgment types and add rankings for additional chart types.
Though MTurk experiments do not address social influence, they do demonstrate a type of collective intelligence applied to perception tasks .
As Heer and Bostock and others have suggested , Mechanical Turk offers a greater diversity of subjects, scalable experimentation, and rapid responses.
An important point with regard to our study of influence is that many graphical perception tasks are based in intuitive judgment, such as scanning a plot to form an impression of the underlying distribution.
Intuitive judgments are typically faster, require less effort, and are less subject to over-thinking than analytic reasoning .
They can benefit the perceiver of a graph, by displaying properties of the data that remain hidden when only the statistical parameters are computed .
However, they can also mislead an observer's interpretation of a stimulus .
Hence, the fact that graphical perception results are replicable in some cases does not mean that subjects' answers are always correct.
In some cases, systematic biases at the individual level may affect responses .
Proportion-judging, for example, has been cited as one task where systematic biases can occur .
In such cases, rather than errors distributing evenly in both directions from the actual answer, such biases may lead to potentially "bad" social signals.
Linear association estimation is another task subject to numerous biases and considered relatively difficult for humans .
Intuitive estimates have in many cases been found to be lower than the statistical coefficient r .
Figure 1: Experimental Design Further, estimates can be affected by manipulating visual characteristics, complicating the accurate judging of an association.
While social data analysis from visualizations often involves further interpreting these underlying visual relationships, errors at the perceptual level undoubtedly play a role in determining the ultimate analyses.
Social psychologists use the term social influence to describe the tendency to respond in certain ways to the behaviors of others.
For example, conformity refers to changing one's behavior to match responses of others.
In analyzing conformity, social psychologists typically distinguish between informational motivations, arising from a desire to form accurate interpretations, and normative motivations deriving from a desire to obtain social approval .
This distinction is important in framing our work's focus on influence as applies to visualization perception to prior influence experiments that utilize perception tasks.
Asch's well-known line judgment experiment  showed that individuals asked a simple visual judgment question  responded differently depending on the answers first reported by other individuals in the room.
The powerful support provided that elementary visual judgments can be subject to social influence inspired various replications and extensions .
In the Asch paradigm, identifiability and social presence--normative social factors--characterize the setting .
However, subsequent experiments  suggest that a controversy with this experimental set-up is the difficulty of replication within different cultures or time periods.
Under conditions when others' opinions are expressed as quantitative estimates and "judge" and "advisor" are equally well-informed, averaging represents an optimal strategy for accurate estimation because it cancels out errors .
Yet in the case of social environments where multiple individuals view and interpret a visualization, can displaying information on prior responses lead to social signals that are biased, thus negating the effectiveness of averaging?
Our work is motivated by exploration of the multiple ways in which social information might be represented in interactive visualizations, including comments, bar and pie graphs, and graphical annotations .
We make use of a histogram as a concise representation of social information to avoid the additional complexities of comments that might confound our experimental objectives.
While not targeted at information visualization, recent studies on the social dynamics of online cultural markets motivate some of the design of the present work.
The concept of social or observation learning  describes cases where an abundance of options leads to conditions where popularity is taken as a signal of quality.
This theory has motivated experiments that simulate information cascades in social environments in which individuals motivated to make informed choice use social signals.
Similarly, work in recommender systems provides evidence that recommendations can change users opinions, where users tend to rate toward the system's predictions .
With regard to online reviews , it is shown that exposure of previous opinions induces trend following and ultimately the expression of increasingly extreme views.
The cost of expressing an opinion when other previous views are known tends to lead to a selection bias that softens the extreme views.
This form of influence, arising from information on the prior decisions of other community members, is similar to online collaborative visualization environments, where prior responses are commonly provided to the viewer .
As in the music downloading environments described by , the number of possible visualizations calls for a natural heuristic for dealing with the choice overload, and people may benefit through interaction, and notions of commonality when they coordinate their choices with others.
These results echo the concept of social proof, which states that a behavior is viewed as correct in a given situation to the extent that others are performing it, and that more people, more ambiguous situations, and an increased sense of similarity to others increases the power of the influence .
For  this  work  we  conducted a number of large-scale experiments using Amazon's Mechanical Turk .
We specifically tested the following main hypotheses: * H1: Adding a social information signal on prior workers' behavior  will directly influence subject's accuracy on the task at hand.
Below we describe the main experiments used to test these hypotheses.
The first extends Cleveland and McGill's seminal study of visual judgment types by examining how social proof affects visual judgments.
The second, which adds another layer of task difficulty, is informed by work in the estimation of linear associations .
Our first experiment was designed to ascertain the impact of social information on classical graph perception tasks.
As a control, we began by adapting Heer and Bostock's 1a experiments  using Mechanical Turk.
We limited our replication to three chart types distributed across their reported ranking of visual judgment types: a bar graph representing the high accuracy encoding type position along a common scale , a stack bar graph representing primarily length encoding , and a bubble chart representing relatively low accuracy circular area encodings .
We depict these within a representation of our experimental design .
Like Heer and Bostock, we used the first question to verify responses and required workers to first pass a qualification test of several examples with multiple-choice questions.
Each of the 30 unique chart versions was launched as an individual HIT to be completed by 50 workers for a reward of $0.05.
This was raised to $0.08 to increase completion speed .
Using the control data we implemented two social conditions through a social signal.
Specifically, a social histogram showed a distribution based on 50 previous answers for the same judgment task.
For each chart of the 30 charts, two social histograms were generated: Target M was set to the mean answer found in the control, and Target 1SD was set to one standard deviation from the mean in the direction of the greatest density of the control distribution .
We chose this particular target in order to test the case where the more incorrect social information might still be relatively believable.
In all situations, Target M means were closer to the true proportion values than Target 1SD.
In generating the histograms, we required that the value with the highest count in the histogram fall within three of the control mean used to generate the sample.
The task layout can also be seen in Figure 1.
Because participants saw a mix of biased  and unbiased  histograms, one possibility was that whatever version they saw in their first HIT would influence subsequent HITs.
Stated another way, if a biased histogram was shown first, was the subject less likely to rely on the social signal for subsequent HITs?
We saw no statistical difference  between perception errors in the subsequent charts, indicating that whether the first observed social histogram was accurate or inaccurate does not appear to impact how subsequent histograms were perceived.
We launched the 60 chart/histogram combinations in sets where each worker could do 30 unique chart tasks, with equal numbers of Target M and 1SD histograms and HIT order counterbalanced by the Mechanical Turk.
All workers taking part in the social condition completed a qualification task, as before, with the addition of several example and practice histogram reading tasks, presented separately from the proportion judgment portion of the qualification.
Figure 2 shows boxplots of the means from the control data and each of the social conditions.
After doing an ANOVA  we used a Tukey HSD test to compare the MLAEs across all three conditions.
We found a significant difference for Target M and 1SD .
A significant difference also exists between Target 1SD and the control , although not between the Target M condition and the control .
These results are consistent with our hypothesis 1 .
To control for the mix of between- and within-subjects data we conducted two sensitivity analyses: collapsing errors by individual by condition , and controlling for effects of particular individuals on results by re-running the ANOVA and Tukey test using a systematic leave-one-out design.
In both cases, final t-tests yielded significant results  for all but the Target M and control comparison.
A small majority  of the workers accepted all 30 available HITs.
We included in analysis all workers who had completed HITs regardless of the number .
We used the verification question to exclude workers with incorrect responses.
Because we were concerned with the bias that outliers might lead to in the worst case scenario of charts with a large standard deviation in responses, we removed outliers by defining a range around the actual proportion for each chart  based on approximately 3x the largest standard deviation for a chart and omitting values outside of this range .
To validate our control experiment, we use the midmeans of log absolute errors  using log2 for each chart and bootstrapping .
We find that the ranking of judgment types is preserved and that the rough shape and ranking of visual judgment types by accuracy are preserved .
An average of 22% of workers completed all 30 tasks in a sequence.
We again considered all HITs for analysis regardless of the number done by individuals, removing outliers and those who didn't understand the task according to the procedure for the control.
We also excluded HITs where the histogram verification question was answered incorrectly.
A total of 7.2% of HITs were removed.
As above, we calculated the MLAEs for each of the 60 unique chart/histogram pairs.
In a second two-part experiment similar to 1, we pseudo-randomly generated 30 correlation values to use in generating scatterplots .
We chose these bins after the suggestion  that many statisticians see |r| values below 0.5 as "small", and values of |r| as "large" only when they are 0.8 or greater.
For each correlation we generated 100 pseudo-random x values from a Gaussian distribution as well as pseudo-random y values for each.
In the individual tasks, the worker is shown a labeled scatterplot of the two variables X and Y and asked to estimate the value of the linear association between them on a scale of 0 and 100, the scale chosen after .
As in experiment 1, the 30 unique plots were launched as 30 individual HITs with N=50 assignments.
The workers were then asked to enter a value between 0 and 100 describing the linear association in scatterplot A, where 100 represented perfect linear association and 0 represented no association.
Each worker first took a qualification test showing examples of 100 and 0 association plots plus two additional examples and three practice tasks.
Due to the relative difficulty of accurately estimating correlations, we allowed subjects to pass the qualification given 11 out of 12 correct answers.
The B scatterplots used for comparison to the A plot in the first question of each HIT had an average difference of 61 from the true value of the A scatterplot.
We modified the control as in experiment 1, adding two social influence conditions for each of the 30 scatterplots, where the target guess used to generate the distribution for the histograms of previous answers is the actual mean guess in one condition  and one standard deviation in the other  which ranged in practice from 3.07 to 22.2.
In order to keep the presentation of the scatterplots A and B the same as in the control, we presented the histogram and histogram question beneath the two plots and question 1.
Question 3, which asked the worker to estimate the association, was below the histogram and question 2.
The 60 chart/histogram pairs were launched as individual hits in sets of up to 30 unique charts, with equal numbers of M and 1SD histograms and order counterbalanced between workers.
We first noted that accuracy, as defined by the log absolute error of estimates, dipped at linear association levels of 50 and 80, then jumped back to the more expected trend.
We attribute these jumps to the anchoring effect of the question text on the page.
Recall that 50 and 80 were used to describe the transitions from low to medium to high correlations.
Because this may have led to a slight bias in utilizing these answers, when the correlation was in fact 50 or 80, error was reduced.
In future experiments we hope to eliminate these types of signals.
However, this is not critical for the current experiment as we are not seeking to compare errors by task difficulty and data is aggregated for all correlation types.
Additionally, outside of this difference, our pattern of log absolute errors accuracy measures matched prior results, in that the accuracy of estimates of association declined as the actual linear association moved closer to 50.
An average of 58% of the workers accepted all 30 HITs in a sequence though we again considered all HITs in analysis, removing those that qualified as outliers using the procedure described for the Control.
We also excluded HITs where the histogram verification question was answered incorrectly .
We calculated the midmean log absolute error accuracy measure as in Experiment 1  for each of the 60 unique chart/histogram pairs, and ran an ANOVA .
The lack of significance of the ANOVA and the relatively high errors from the Target M condition led us to consider the assumptions behind the hypothesis that as a social signal becomes more biased, errors will increase.
Under this condition, 75% of the workers accepted all 30 HITs in a sequence, yet we considered all completed HITs for analysis .
We used the verification questions to validate that subjects understood the task, excluding HITs with one or more wrong answers.
We also excluded from analysis outliers that were more than 50 off from the actual correlation in the scatterplot , defining this boundary using approximately 3x the largest chart standard deviation.
For each of the scatterplots, we calculated the mean estimated linear association and standard deviation.
We compared the pattern of results from our experiments to those of prior work in linear association estimation .
In looking closer at the data, however, we realized that the results remained unclear with regard to H1, because in 16 of the 30 cases, the Target 1SD histogram value was in fact closer to the truth.
Considering that prior research has shown humans to be relatively bad at linear association estimation, this outcome is plausible.
Yet because the ordering is random, this confuses our relationships.
To overcome this complication, we regrouped the data based on which of the two histograms displayed a mean answer that was closer to the actual association  versus the histogram that was farther from the actual association .
With this measure, our previous observation--that the farther the social signal is from the actual, the less accurate the estimate--holds.
Tukey's HSD test showed no significant difference between the Control and Closer condition , nor between the Control and Farther condition .
However, we saw a significant difference between the Closer and Farther condition .
We discuss these results further in the discussion section.
To test for anchoring we ran a validation experiment that displayed the social histograms as in Experiment 1 but labeled them as something unrelated to the chart .
For example, the histogram might be labeled "Temperature Recorded at Location 11" and a bar chart as "Employee Salary in Company R" .
This condition also made a clear delineation using different colored backgrounds behind each sub-task in the single HIT.
We ran the conditions as 30 HIT sequences for $.10 a HIT with N=25 unique workers who had not yet done any of our prior tasks.
Under the social condition with delineation, 72% of workers accepted all 30 HITs in the sequence .
After performing an ANOVA , we used Tukey's HSD test to find that the delineated Target 1SD MLAEs were not significantly different than the control .
As the same non-delineated Target 1SD histograms were different from control in the social task , we can infer that anchoring is likely not contributing to increased errors as the histogram shifts from the "true" value.
Before going further to investigate the possibility that "bad" social signals might naturally emerge in a social visualization environment, we validate that the results of experiments 1 and 2 did in fact represent social influence rather than non-social influences.
As examples of the latter, anchoring and adjustment  are psychological heuristics that subconsciously influence the way people intuitively assess probabilities.
A subject starts with an implicitly suggested reference point , and makes adjustments to that number based on additional information.
Anchoring, which is related to priming, is the general activation of a particular idea or ideas based on associations between a stimulus and that idea.
Although the previous experiments clearly demonstrate that if an individual is presented with social histograms their judgment will change, it is not entirely obvious that different histograms would emerge from the same social process .
Stated another way, we seek to establish whether judgments of the n+1th person are influenced by the number of previous judgments  and the distribution of those judgments.
If such a cascade pattern holds, an initial bad estimate may grow or become more entrenched as more and more people contribute their estimates.
To simulate an iterated process we presented 1500 HITs that displayed histograms with varying n's and displaying a histogram with different means.
An indication informing the participant that they were person n+1 in a series of individuals was made in 3 places on the interface .
The n variable was varied from 1 to 37 in steps of 4.
Note that the control experiment serves as a test at n=0 .
In total, we produced 30 histograms per chart.
This was done for all 10 circular area plots  yielding 300 total variants, which we launched as 10 HIT sequences at $.08 per HIT with 5 workers per variation .
We constructed a number of linear models to test for the relationship of actual answer  against histogram mean, n, and the true proportion.
Specifically, the model: Answern+1 = b0 + b1* true_proportion + b2*histogram_mean + b3 * histogram_mean*n + e attempted to capture the increasing effects of n on the answer as well as the participant's personal evaluation of the true proportion.
Sensitivity analyses with robust standard errors were performed using generalized estimating equations in R to account for the repeated measures per person.
These findings were robust to control for the correlation induced by collecting multiple measurements per study subject.
To mitigate information cascades, a designer might have the intuition to mask the social signal  until a sufficient number of samples is obtained.
However, as demonstrated by the lack of significant difference between Target M and the control, there does not appear to be any benefit  in displaying this information.
This result is only surprising if one assumes that graphical perception lacks systematic bias.
However, because individual judgments are wrong, and generally wrong in the same "direction," the overall collective opinion does not appear to be any better than the individual one.
Worse, in situations such as the linear estimation task, when systematic bias and estimation errors were so high that Target 1SD histograms were equally likely to be closer to the true correlation as Target M , there is a clear indication that the social signal, even the individually-derived one, has a negative impact on perception.
This observation indicates that caution is necessary--or at least awareness--when designing systems in domains with systematic bias.
Modeling the main effects of the actual value and the suggested histogram mean on the answer produced an adjusted r2 of .8122.
Both the actual value and histogram mean were positively associated with the person's answer with a slightly large effect for the true proportion  than the histogram mean .
Interestingly, we could find no significant effect of n in this model or any other we used for sensitivity analysis.
The results of this experiment suggest that hypothesis H2b does not hold.
In other words, the opinion of 5 people counts the same as that of 30, and the first judgment, erroneous or not, sets the stage for all subsequent answers.
For the sake of completeness we discuss below several potential limitations of our experimental conditions that might partially affect the results.
The main finding in this work is the evidence we provide that responses to online graphical perception tasks can be subject to influence from socially-derived information signals such as social proof via prior responses, and that such biased signals are possible given a situation where any n+1 person can see the responses of the n individuals who saw a graph before him or her.
Social influence, construed more broadly, is known to be a result of multiple features.
In this study we have targeted a specific type of social influence, one centered around social proof, which we believe can serve as a jump-off point for future work.
One opportunity for future work is to address the reasons why, as in our Experiment 3, the number of previous responses  did not impact the model.
Given that the results of our social conditions did appear to be based on the socially-derived signal, we would expect that more responses would result in a stronger social information signal, exerting a greater effect.
Effects of this sort have been validated in other contexts by .
Yet this was not suggested by our data.
We hypothesize that it may be that the MTurk environment did not support the type of systematic processing that may have been required for a worker to sufficiently understand the relationship between the total count in the histogram and the potential value of the social signal.
Utilizing a true iterated experiment  may yield a different result.
In addition, while we chose a one standard deviation difference in order to investigate cases where the social signal remains believable, future work might offer more insight into how biased a social signal can be while still exerting similar effects.
We also note that our study is not designed to induce normative social influence stemming from a desire for social approval.
The histograms are informational in nature.
Because the decisions of workers in our experiments were not witnessed in the presence of others, workers may have felt more confident in deviating from the distribution.
While some recent research suggests that anonymity need not always degrade social influences effects in computermediated environments, it is suggested that a sense of social identity must be in place for influence to occur .
Furthermore, social influence is also known to be stronger when signals come from others whom the subject deems similar to her/himself .
The extent to which a person identifies with message source  is a significant factor in determining information processing strategies plus outcome of influence attempt .
Such theories indicate that stronger influence might be achieved had the presence of the other workers and their similarity to the subject been emphasized.
Because users of social visualizations sites are not commonly designed around anonymity, additional normative effects may lead to more significant effects on judgments within these frameworks.
Another future inquiry might further investigate the effects of task difficulty on influence, as the relative difficulty has been cited to have an effect on the degree to which people accept advice .
Yet such knowledge would offer designers of social visualization systems further insight into the particular types of situations where the risks of social influence are most heightened.
The contributions we make to social visualization system design are based on experiments that focus on a narrow type of task and environment.
However, our results indicate that graphical perception, a key first step in the interpretation of visual information, can be influenced by social signals that may be present in collaborative visualization systems.
Clearly, actual systems like ManyEyes are complex environments.
Factors such as expertise and interest in the content, prior experience with graph interpretation and statistical literacy , undoubtedly play a role in such systems.
Such environments present individuals with graph comprehension tasks of varying difficulty, and may also present situations that fall along a continuum of objectivity with regard to the pattern being visualized.
For example, while the tasks we investigate here have objective answers in the true proportions and linear associations that are visualized, there are many tasks where an objectively true answer might not be possible, such as graph aimed at visualizing an evaluation formed by subjective sentiments on a topic.
These may still serve as important points of discussion and collective analysis, and thus potential distinctions in social influence patterns as determined by objectivity may offer further insight for designers.
However, we also identify that systematic bias makes many socially derived signals  erroneous on the whole, and these signals do not commonly provide a definite benefit over individual assessments.
This calls into question some of the benefits of "collective intelligence" and highlights a number of design risks.
We also identify that initial seeds in social signals  allow information cascades to rapidly take hold and impact all future answers.
Collective visualization systems hold a great deal of promise for the great influx of data experienced today.
However, previous work on visualization systems frequently ignores social effects, treating visualization interpretation as an individual process.
As our study highlights, there is a need to form new theories and models that explain the impact of social processes on community-driven visualization environments and lead to new systems.
In this paper, we have presented evidence suggesting that responses to graphical perception tasks online may be subject to social influence.
We demonstrate through a largescale study that social proof has an impact on visual judgment, and with it, perceptual accuracy.
