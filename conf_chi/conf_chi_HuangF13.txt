This paper studied how social transparency and different peer-dependent reward schemes  affect the outcomes of crowdsourcing.
The results showed that when social transparency was increased by asking otherwise anonymous workers to share their demographic information  to the paired worker, they performed significantly better.
A more detailed analysis showed that in a teamwork reward scheme, in which the reward of the paired workers depended only on the collective outcomes, increasing social transparency could offset effects of social loafing by making them more accountable to their teammates.
In a competition reward scheme, in which workers competed against each other and the reward depended on how much they outperformed their opponent, increasing social transparency could augment effects of social facilitation by providing more incentives for them to outperform their opponent.
The results suggested that a careful combination of methods that increase social transparency and different reward schemes can significantly improve crowdsourcing outcomes.
These examples have merely begun to demonstrate the potential of crowdsourcing as a social computing technique that can be applied in a wide range of situations.
However, quality control is still one of the biggest issues for crowdsourcing2 .
For example, although the ESP game can successfully recruit players to create a large amount of image labels, a study  showed that many of these are lowquality labels that can be generated by robots with little real world knowledge.
Moreover, the reliability of workers recruited from AMT is also questionable.
People have found that there are many spammers or even robots in AMT, which greatly reduce the quality of the outputs collected from this platform .
As a result, designing mechanisms that can ensure the quality of crowdsourcing outcomes is crucial to its success.
Though the foci of these two fields are slightly different, researchers sometimes use these two terms interchangeably.
We use the term crowdsourcing throughout because the findings of our study can be applied to diverse applications; however, research in human computation is also highly related to this study.
For more discussion about the similarities and differences between crowdsourcing and human computation, please see Quinn et al.
Crowdsourcing has been proven as an effective way to solve various kinds of problems .
One of the most notable examples is the ESP game , which recruits people to generate image labels while playing an online game.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The term social transparency has been well studied.
For example, the survey in Stuart et al.
The kind of social transparency discussed in this paper is similar to the identity transparency according to their definition.
For instance, a study  on an online suburban community network showed that using real name not only created accountable online information exchange but also helped promote local commerce in the community.
The idea of increasing social transparency to enhance accountability in collaborations was first introduced by Erickson and Kellog .
They argued that in the physical world, people make everyday decisions based on various "social cues."
Some of these social cues allow people to perceive the extent to which their behavior is acceptable for others , and people often learn to adjust their behavior based on the perception of these social cues.
In addition, the more that two persons know each other, the more likely people will be influenced by the social cues.
In other words, the availability of social cues often increases the perception that a person is accountable for his or her actions.
However, these social cues are often missing when people collaborate with each other remotely through a digital system.
To address this problem, they implemented a social translucent system that made social cues salient to the collaborators using the system.
They showed that a social translucent system can make the users more aware of the presence of others, and as a result, the users can know that they are accountable for their actions.
They designed WikiDashboard , a visualization tool that helps users identify the page that was edited by which editors and when the edits were made.
Their study  showed that manipulating the editor-accountable visualization significantly affected the trust of the readers.
Therefore, they argued that their visualization can improve the trustworthiness of Wikipedia articles.
They also mentioned it is possible that this social transparency created by their visualization tool can encourage writers to be more responsible .
As a result, the quality of the articles also improves.
In addition, research in social identity theory  suggests that identity formation process  makes group members more commit to their group.
These studies have demonstrated that social transparency can effectively improve online collaborative work.
Because crowdsourcing also utilizes the power of multiple workers to complete a collective task, it is reasonable for us to hypothesize that increasing social transparency  in crowdsourc-
To test this hypothesis , we designed a study to test how increasing social transparency would affect the quality of outcomes generated by crowd workers.
We are also interested in how social transparency may interact with different reward schemes to influence outcomes.
Before we discuss the details of the study, we will first review how different reward schemes can affect the performance of crowd workers below.
Micro-tasks in crowdsourcing typically are trivial and tedious to workers.
Therefore, one notable reason that crowd workers fail to generate high-quality work is the lack of motivation .
Therefore, it is important for requesters to design a crowdsourcing system that can offer enough incentives to workers .
The most common incentive used to recruit workers in crowdsourcing is monetary reward.
Nevertheless, previous research on financial incentives  in crowdsourcing suggested that increasing monetary reward seemed to augment the amount of work produced but not the quality.
This finding implies that monetary reward alone might not be enough to motivate workers to generate high-quality work.
Interestingly,  found that making the reward of the workers codependent on each other can significantly increase the quality of their work.
Specifically, the crowdsourcing system created by them used the answers of its workers to evaluate future workers of the system.
Every worker in the system knew that his/her answer would be used to evaluate the answer of future workers.
Therefore, since the chance that two workers generated the same wrong answer was small, if a worker produced a wrong answer, the workers that were evaluated by this erroneous answer could not earn rewards regardless of whether or not they generated the correct answer.
Their study showed that workers put more effort in solving the task to prevent other workers from being penalized unjustly.
This study suggests that it is possible to design other peer-dependent reward schemes to enhance the performance of crowd workers.
However, not all peer-dependent reward schemes can motivate workers to perform better.
Previous research has shown that poor designs of peer-dependent reward schemes in a collaborative work environment could even lead to worse outcomes .
We summarized three possible effects of peerdependent reward schemes between crowd workers on their motivations below: * Social loafing: Previous studies have shown that when individuals work in a group and only the outcome of the group is evaluated, individual performance becomes significantly worse than when they work alone .
For example, an experiment showed that when subjects conducted a rope pulling task, individual effort decreased as the group size increased.
Harkins  argued that because it is difficult to evaluate each individual in the group when working on a collective task, people feel that they can hide in the crowd such that there is a diffusion of responsibility from the individuals to the group.
As a result, their effort levels tend to drop.
Since crowdsourcing is essentially a large collective task, this phenomenon of social loafing can decrease the motivation of workers to do well.
Therefore, it is important for researchers to study how social loafing can impact the performance of crowd workers and design mechanisms to mitigate its potential negative effects.
Altruistic motives refers to the ideas that workers have the tendency to behave so as to benefit their colleagues, even if the behavior entails a negative utility for themselves.
Studies  have shown that workers will adjust their effort level when the rewards of other workers depend on them.
Previous research  also showed that when workers in a crowdsourcing system know that their answers will be used as evaluation standards by the system, they work harder to prevent other crowd workers from being unjustly penalized due to their errors.
A study  showed that when conducting a brainstorming task, subjects who were paired with another subject and knew their individual work would be evaluated generated significantly more ideas than subjects who worked alone.
Harkins  suggested that social facilitation can be conceived as the complement of social loafing.
That is, when the outcomes of each individual in the group are evaluable, the workers are motivated to outperform other members in the group.
As a result, the collective outputs of the entire group are also better.
Study  also highlighted that worker effort is positively related to the performance of workers who see the work being executed.
Similarly, social comparison theory  suggests that people have the tendency to compare their own work to others.
These results suggest that social facilitation can effectively enhance individual performance in collaborative work.
To summarize, previous research has shown that a set of complex social effects may interact to influence the motivation of the workers.
It is not clear, however, how directly manipulating social transparency and reward schemes may make each of these social effects more or less salient, such that a designer of a crowdsoucing system can more reliably predict how these effects can be utilized to improve outcomes.
In a teamwork scheme, in which workers in a group are rewarded based on the collective outcomes of the group, there are two possible effects that may influence the motivation of the workers.
First, social loafing can negatively impact the performance of workers because they feel that they can hide in the crowd and rely on their partner's effort.
If this is the case, we should find that social loafing is more salient when a worker perceives that their partner is already at a high level of performance.
In such situation, the worker may not be motivated to put extra effort in the task as the collective outcome is likely to be good.
In contrast, a second factor, altruistic motives, can positively impact the performance of workers in the same reward scheme because a worker may not want their partner to feel that his or her poor performance may impact their collective reward.
If this is the case, we should find that altruistic motives may be more salient when social transparency is high, as the perception of the presence of their partner may enhance the effect of altruistic motives.
In a competition scheme, in which rewards of workers depend on the differences between the outcomes of the workers and their colleagues, it is possible that social facilitation can improve their performance because they are motivated to outperform each other.
If this is the case, we should find that pairing with a worker with a high level of performance should motivate a worker to work even harder.
To investigate how different peer-dependent reward schemes improve of degrade the performance or crowd workers, we created three conditions that differ in the reward schemes.
We used a 3x2 design to see how different peer-dependent reward schemes and social transparency levels affect the performance of crowd workers.
300 subjects were recruited from AMT.
We asked the workers to perform tasks individually , paired with another worker as a team , and competitively against another paired worker .
Half of the workers worked anonymously , and the other half of them were asked to report their demographic information  .
When the workers who revealed their demographic information were paired with another worker, their demographic information was shared with their paired workers.
We also examined if there was any interaction effect between social transparency levels and reward schemes.
The workers in our experiment were asked to count the words with a particular part-of-speech  in a list of 30 words.
Each word in the word list had equal probabilities to be a noun or not a noun.
There were two advantages of this task that made it a very good test-bed for us to examine how the factors affected the quality of the outcomes.
Moreover, the workers needed to put in a certain level of effort to solve the task, and to a large extent, outcome quality increases with the level of effort.
This task is also representative of the tasks used in previous crowdsourcing systems .
To evaluate the performance of the workers, we first computed the label accuracy of each collected label.
The label accuracy shows how close the label is to the gold standard answer.
In our experiment, we also controlled the levels of social transparency of the workers.
The two levels of social transparency used in our experiment were the following: * Anonymous : This was the lowest level of social transparency; the only information shared by workers and the system was the worker ID in AMT.
If the system paired the workers, the paired workers cannot get any information from each other.
When the workers worked with another worker, this information was also shared with their teammate or opponent.
Therefore, the numerator of the second term in the formula is the distance between the label generated by the worker and the gold standard answer, and the denominator is the distance between the worst possible answer and the gold standard answer.
Therefore, the label accuracy is a number between 1 and 0, where 1 represents that the label is exactly the same as the gold standard, and 0 means that the answer is the worst possible answer.
Then, the average label accuracy of each worker was computed by averaging the label accuracy of the ten labels generated by the worker.
We used the average label accuracy of the workers to measure their work quality.
In our experiment, we compared the performance of workers under three different peer-dependent reward schemes: * Individual : This system was used to imitate traditional crowdsourcing systems, where the bonus of workers only depends on their own performance.
Therefore, in this system, worker performed the task individually, and their bonus did not depend on other peer workers.
Their bonus was decided by the average of the performance of both workers.
In other words, the workers on a team were codependent on each other because only the collective outcomes were evaluated.
This allowed us to examine whether social loafing  or altruistic motives  really created an effect on worker performance in a team environment.
The workers under competition reward scheme were also paired with another worker.
However, unlike the previous condition, the paired workers were opponents.
The bonus of a worker was decided by the positive difference between the worker and his/her opponent.4 Therefore, the workers would have the incentives to outperform their opponents in order to earn more monetary reward.
The title and the description of these HITs were both "Count the nouns in a word list".
We did not require our workers to pass any qualification tests.
The price tags for the HITs were $0.05, which were the rewards the workers could earn regardless of their performance.
On the HIT page, we provided a link to route the workers to our experiment website and a text area for the workers to enter an eight-digit completion code.
When the workers completed their jobs, our website showed them the total bonus they could earn for their performance and the unique completion code that they would have to enter on the original HIT page on AMT.
We then paid the workers both of their rewards  and bonuses  based on the completion codes they entered.
The system also automatically rejected repeated workers by checking if their AMT worker ID already existed in the database.
When the workers visited the experiment website, they were randomly assigned to one of the 6 conditions .
It also showed how the bonuses for the workers were decided; the exact words we used for each condition in the introduction were as follows: * IA, ID: When you finish the task, you can earn a bonus which is your accuracy times 0.1.
EX: If your accuracy is 0.73, you can earn $0.07 bonus  * TA, TD: You will be paired with another worker in the system as a team.
Your bonus will be decided by the average of your accuracy times 0.1.
EX: If your teammate's accuracy is 0.93 and your accuracy is 0.53.
5 We used the same 10 word lists for all workers.
One possible concern is that some of the workers released the information to others.
However, we found that the correlation between the performance and the arrival time of the worker was very low , which shows that the workers should not get extra information because the average accuracy didn't go up.
Therefore, you and your teammate can get $0.07 bonus each  * CA, CD: Another worker will be chosen to be your opponent.
Your bonus will be the positive difference between your accuracy and your opponent's times 0.5.
EX: If your opponent's accuracy is 0.79 and yours is 0.93, your bonus will be 0.14*0.5=0.07  However, if your performance is worse than your opponent, you won't earn the bonus.6 After workers read the introduction, those that needed to reveal their demographic information  were asked to enter their information .
The anonymous workers  skipped this step and went to the next step directly.
Then, we used an asynchronous interaction design  to make workers in conditions TA, CA, TD, and CD feel that they were matched with another worker.
We adopted this asynchronous interaction design to prevent the workers from gaming the system by starting multiple tasks concurrently in the experiment website, which allowed them to be paired with themselves.
This design has been adopted by many GWAPs , and previous research 
In this design, the workers were redirected to a worker matching page .
This page paused for a few seconds and indicated that the system was trying to match the worker with another worker .
These workers also were redirected to a waiting page after each question to wait for the paired worker to respond.
The workers who revealed their demographic information and were paired with another worker  also received a message that showed the demographic information of their paired worker before the task began .
After that, the workers in all conditions would see the interface for performing the noun counting task .
The system specifically told the workers to count a word as a noun if one of its definitions is a noun because it is possible for a word to have multiple definitions and only some of them are nouns.
This made the gold standard answer of the question clearer to the workers.
At the bottom part of the interface, it showed the workers their current label accuracy for the questions they had answered.
The workers within the teamwork reward scheme could see their teammate's current label accuracy and the current average label accuracy for their team .
The workers under the competition reward scheme could see their opponent's current label accuracy and the current difference between them .
The workers that were paired with another worker and shared demographic information could also see their colleague's information at the bottom of the interface .
Finally, after the workers completed 10 questions, they would be redirected to the completion page that showed the completion code .
The design of the experimental interface of the six conditions is summarized in Figure 3.
The results showed that social transparency level did significantly affect the performance of workers  = 7.58, p < 0.01.
The mean of the average label accuracies for the workers with different social transparency levels and peer-dependent reward schemes are summarized in Figure 4.
This result supports our hypothesis that increasing the social transparency of workers makes them more accountable.
Therefore, the quality of outcomes of crowdsourcing system also became better.
Moreover, although the ANOVA shows that the interaction effect between social transparency levels and peer-dependent reward schemes was not significant, we did find some interesting patterns in the result.
On the other hand, when the workers revealed their demographic information, the differences between the workers whose rewards were codependent on their peers and those who worked individually became significant .
The average label accuracies of the workers who worked as a team and competed against each other were both significantly higher than individual workers.
Nevertheless, we can see that social transparency had a better ability to enhance outcome quality when the workers felt that their information was shared with their colleagues.
In contrast, if they felt that their information was only shared with the system , social transparency did not have a significant effect on enhancing the performance of crowd workers.
A 2X2 two-Way ANOVA  for the workers who cooperated with the paired worker  showed that social transparency significantly affected the performance of the workers  = 34.23, p < 0.01.
This result has two important implications.
First, it indicates that social loafing negatively affected worker performance in teamwork conditions.
That is, team members were less diligent when they found that they could earn a higher bonus because their teammates already performed very well.
Second, social transparency between team members can enhance the performance of the workers regardless of their teammates' performance.
This shows that sharing demographic information between team members increased social transparency, which possibly increased the motivation of the workers through reciprocal altruism to put more effort into the task.
As a result, simply asking workers to share demographic information before the tasks was enough to motivate workers in teamwork conditions to perform better than workers in individual conditions.
The overall performance of the workers suggests that the two different peer-dependent reward schemes  used in our experiment had no significant difference on enhancing worker performance.
They both worked well when the workers revealed their demographic information and did not have significant effects when workers worked anonymously.
To gain more insight about how these two reward schemes differed, we measured how workers reacted to feedback with different accuracy levels.
If social loafing created a negative effect on workers in teamwork conditions, then one would expect that when workers received high-accuracy feedback from their teammates, their effort levels should decrease because they already could earn a high bonus by relying on the performance of their teammates.
In contrast, workers who received high-accuracy feedback from their opponents in the competition conditions should perform better because social facilitation suggests that workers were motivated to outperform their colleagues.
To test if these hypotheses are true, we divided the feedback into two levels  using the median label accuracy of all feedback.8 We then calculated the average label accuracy of the next question based on the feedback accuracy level of the previous question.
We then performed a threeway ANOVA on accuracies with label accuracy level , reward schemes , and social transparency  as independent variables.
A 2X2 Two-Way ANOVA  for the workers who competed against their paired worker  showed that the effect of social transparency was significant  = 20.92, p < 0.01.
However, the accuracy level of the feedback from opponents did not significantly affect the average label accuracy of workers  = 0.12, p = 0.91.
This interesting result shows that when workers competed against each other anonymously, performance was better when they received a low accuracy feedback.
However, when the social transparency was high, their performance was significantly higher when they were received a high accuracy feedback.
This suggested that social transparency had a significant interaction with the level of accuracy of the feedback provided by their opponents in the competition reward scheme.
It appears that social transparency was key to the social facilitation in the competition conditions.
As a result, they were more likely to work harder when they received a low accuracy feedback than when they received a high accuracy feedback.
On the other hand, our analysis showed that when the workers received high-accuracy feedback from their opponent, they were motivated to perform even better than their opponents.
This led to better overall performance for workers in the high social transparency competition conditions.
Our analysis shows that revealing demographic information can increase social transparency, thereby motivating workers to put more effort into the task when interacting with their teammates.
Moreover, in a competition reward scheme, workers who share their information also have more incentives to outperform each other.
This suggests that social transparency can make social interactions between crowd workers more engaging.
Therefore, it is possible that the strategy of increasing social transparency can be applied to other reward schemes that hinge on social interactions between crowd workers.
For example, this strategy can be applied to the external assessment scheme proposed by Dow et al.
Asking crowd workers who served as assessors to provide their own personal information is possible to make them provide more responsible feedback and assessment and therefore became better shepherds for other workers.
Furthermore, workers in peer consistency evaluation 
We believe that the findings of our experiment could be applied to various mechanism designs that involve social interaction between workers.
This shows that the effect of social loafing was stronger than the effect of altruistic motives when the bonuses depend on the collective outcomes.
However, this does not mean that altruistic motives cannot improve worker performance.
In future, mechanisms that better utilize altruistic motives and prevent the system from suffering social loafing could be designed.
For instance, the system could make workers feel that their performance can affect the bonus of other workers but does not let them benefit from the good performance of others .
This would allow us to separate the effects of social loafing and altruistic motives so that the system could better utilize the power of altruistic motives.
The results of our experiments show that social transparency between paired workers made them perform significantly better.
However, a limitation to our study is that we didn't vary the level of financial incentive or the task design.
Therefore, it is not clear if the result of our experiment can be applied to diverse settings.
Especially, the task used in our experiment offered only a small amount of payment and allowed workers to guess the answer without putting much effort.
This could make our experiment attract some spammers which skewed the results.
As a result, it is possible that the mechanism of reducing anonymity used in our experiment only works for workers with low motivation.
Since we have found some significant differences between conditions, the effects should not affect the comparisons.
This made the workers within the competition reward scheme perform significantly better than those who work individually.
These results suggest that designing various peer-dependent reward schemes can successfully motivate workers to generate outcomes of higher quality.
Since previous research  has proven that increasing financial incentive does not always motivate workers to perform better, the reward schemes that utilize the existence of peer workers can be both more useful and more cost-effective.
In future studies, we would like to tease out the effects of self-identifiability and peer-identifiability.
The information sharing conditions share both the information of the worker and his/her peer.
This makes it difficult to know if the better performance is because of the worker shared the information or learned the information from the peer.
Therefore, separating the effects will help us gain more insight on how social transparency affect the motivation of crowds.
In addition, we would like to examine whether strategically selecting the paired workers can further enhance the performance of crowd workers.
Previous research  has shown that it is not necessary to really pair two online workers to create social interaction.
Using interface design  alone can make workers feel that other workers are working with them concurrently.
This design allows the system to actively select the opponents or teammates for the workers.
For example, since our results have shown that high-accuracy feedback from opponents can motivate the workers in competition conditions perform better, we can strategically select one of the high-accuracy workers as an opponent to the new worker.
In addition, we can also deliberately select one of the workers with low-accuracy to be the teammate of the new worker in teamwork mode to reduce the effect of social loafing.
Moreover, it is also possible to vary the level of financial incentive to see how it affects the performance of workers with different social transparency levels.
The higher financial incentive can make the workers more accountable for their actions, but it also can reduce the intrinsic value of the workers and makes the effect of social loafing even stronger.
To understand the effect, a systematic study that involves different levels of financial incentive can be conducted to tease apart the complex interactions of these effects.
The results of our experiment showed that social transparency between paired workers  significantly improved outcome quality.
However, increasing social transparency between crowd workers also raised the concern for privacy issues .
Some online labor markets do not allow requesters to gather personal information from workers in order to protect the workers.
For example, AMT prohibits requesters to collect personal identifiable information10  from workers.
Reducing the anonymity of crowd workers might even make some of them unwilling to work on HITs.
This could decrease the efficiency, which is one of the biggest advantages of crowdsourcing.
One possible way to address this issue is to utilize the idea of social translucence as mentioned in Erickson et al.
This could help requesters ensure outcome quality by strengthening the social connections between workers and preventing the workers from feeling that their privacy was invaded.
We believe this "virtual workspace" idea can be useful to crowdsourcing researchers and practitioners and is worth further investigation in the future.
This paper found that increasing social transparency between workers effectively enhanced the quality of crowdsourcing outcomes.
That is, when workers were asked to provide their demographic information  to their teammates or opponents, they performed significantly better than those who worked anonymously.
Since there are many possible social interactions between the workers in a crowdsourcing system, we believe our findings present a novel way to enhance outcome quality of crowdsourcing systems.
Moreover, we found that social loafing created a negative effect on the performance of team members.
When the workers received high-accuracy feedback from their teammates, they put less effort in their work because they realized that they could already reap a bonus based on the performance of their teammate.
However, social transparency was able to make the workers more accountable to their teammates, so the workers performed significantly better when they revealed demographic information to their teammates.
In addition, our results suggest that social transparency also made the effect of social facilitation significant.
When two anonymous workers competed against each other, the workers were not motivated to perform better.
In contrast, when the paired workers shared their information, the workers had more incentive to outperform their opponents.
Law, E., and Von Ahn, L. Human Computation.
Synthesis Lectures on Artificial Intelligence and Machine Learning.
Mas, A., and Moretti, E. Peers at work.
Working Paper 12508, National Bureau of Economic Research, September 2006.
Financial incentives and the "performance of crowds".
Millen, D. R., and Patterson, J. F. Identity disclosure and the creation of social capital.
Postmes, T., Spears, R., Lee, A., and Novak, R. Individuality and social influence in groups: inductive and deductive routes to group identity.
Human computation: a survey and taxonomy of a growing field.
In Proceedings of the 27th international conference extended abstracts on Human factors in computing systems, CHI EA '09, ACM , 3937-3942.
Cheap and fast--but is it good?
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP '08, Association for Computational Linguistics , 254-263.
Stuart, H. C., Dabbish, L., Kiesler, S., Kinnaird, P., and Kang, R. Social transparency in networked information exchange: a theoretical framework.
Lifting the veil: improving accountability and social transparency in wikipedia with wikidashboard.
In Proceedings of the twenty-sixth annual SIGCHI conference on Human factors in computing systems, CHI '08 .
26. von Ahn, L., and Dabbish, L. Labeling images with a computer game.
27. von Ahn, L., and Dabbish, L. Designing games with a purpose.
