Every day users of social networking services ask their followers and friends millions of questions.
These friendsourced questions not only provide informational benefits, but also may reinforce social bonds.
However, there is a limit to how much a person may want to friendsource.
They may be uncomfortable asking questions that are too private, might not want to expend others' time or effort, or may feel as though they have already accrued too many social debts.
These perceived social costs limit the potential benefits of friendsourcing.
In this paper we explore the perceived social costs of friendsourcing on Twitter via a monetary choice.
We develop a model of how users value the attention and effort of their social network while friendsourcing, compare and contrast it with paid question answering in a crowdsourced labor market, and provide future design considerations for better supporting friendsourcing.
Facebook  that show users actively monitoring the way they present themselves, suggesting that many categories of questions are discouraged by existing friendsourcing practices.
Further, most questions that pass the selfcensorship bar still remain unanswered .
Questioners reported feeling like they "owed" anyone who answered the question, and answerers reported a similar transaction, either gaining future favor or repaying an obligation; such norms of reciprocity  are particularly important among certain demographics, such as users in Asia  or users with disabilities .
As people continue to ask questions of their social network, these costs may rise above a person's level of comfort, forcing them to either stop asking questions or delay .
In such cases, perceived social costs on the part of the questioner influence the benefits they are able to realize from friendsourcing.
These costs include the questioner's valuation of friends' expending time, attention, and effort, the cost of future efforts needed for "repaying" answerers, as well as the possible impact on a questioner's curated online persona.
As the costs rise, questioners may limit informational requests, diminishing the potential efficiencies of friendsourcing.
A richer understanding of the factors influencing the likelihood of friendsourcing can benefit not only for users of social networks, but also designers of tools and systems that support friendsourcing.
In this paper we introduce an experimental methodology to investigate the perceived social costs of SNS question asking by assigning a monetary value to friendsourcing.
We chose Twitter as the basis of our experiment; previous literature has demonstrated vibrant Twitter question asking behavior across a wide variety of users .
We contribute a quantitative and qualitative findings about the factors that influence users' valuation of the social costs of friendsourcing, including demographic characteristics, the number of questions previously asked, user's interest level in a given question, and audience-appropriateness.
Additionally, we investigate an alternative means of answering social questions that may be better suited to private or anonymous questions.
Previous literature has investigated using paid crowdsourcing to answer social questions , finding not only that crowd labor can provide good answers, but also that the anonymity can make participants feel more comfortable.
One does not have to look far on a social networking service  like Twitter or Facebook to see people friendsourcing information seeking by asking questions of their networks.
Prior research by Morris et al.
These questions not only provide informational benefit when answered, but also provide social benefits to both questioner and answerer .
Further, questions are one of the more valued types of content on social networks .
However, people have also reported that they were not inclined to post or answer personal, religious, political, monetary, or health questions because of how they might be perceived .
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
Combining these two findings, there is an interesting opportunity for future friendsourcing technology based on modeling and adapting to users' perceived costs.
For some questions where friends may have extra domain knowledge or necessary context, systems might reduce the barriers to entry and costs for friendsourcing them.
Others that anyone may be able to answer could go to a crowd of ready and willing knowledge workers or a web search engine.
A system might help a person anonymize or redirect questions that are deemed too costly or personal and are at risk for self-censorship.
By combining the power of friendsourcing and crowdsourcing with an awareness of social and personal costs, technology might become even more helpful for answering important questions.
In particular, social network information-seeking activity seems to relate to existing social capital, perhaps due to the potential mix of online and offline interaction .
However, interacting online, especially in the case of friendsourcing, may come with additional costs.
In general, psychologists have shown that people are not always willing to ask for help if it may incur costs of effort or make the responder think less of them .
Norms of reciprocity and fair trade may also make people less likely to incur future debts or make excessive use of friends' time or resources .
These costs are also evident when people have a choice between searching the web and friendsourcing.
People are wary of spamming a person's social feed or demanding too many resources , and prefer search to friendsourcing for very concrete and very vague information needs .
However, friendsourced answers often contain personal or contextual information that improves their quality, though they take longer to receive and may not provide more information than a search .
Recent research has hybridized search and friendsourcing to mitigate costs and maximize benefits.
SearchBuddies  were automated agents that responded to friendsourcing requests on Facebook with algorithmically generated content, including links to relevant Web resources and suggestions of Facebook friends with relevant expertise.
MSR Answers detected questions on Twitter and used a crowd of microtask workers to answer them; answers were largely well written and arrived quickly .
Another potential benefit of asking an anonymous crowd over friendsourcing is in their lack of connection to the asker.
They may be more frank, or less likely to judge an asker publicly based on their question.
In a study that had people friendsource and crowdsource fashion decisions, participants appreciated the blatant honesty of the crowd, though friends knew more about their personality and activities .
In general, such questions seem to be well liked by followers and friends .
Cross-cultural studies have found similar social network Q&A behavior in Asia, where expectations of reciprocity were more explicit than in the U.S. .
They noted a preponderance of rhetorical questions, and discovered that the response rate for questions was surprisingly low .
A more recent, larger-scale study of Twitter logs  found that only 35% of "?"
Friendsourcing on social networks is closely related to the relationships of friendsourcers and their respondents as well as their potential audience as a whole.
Users of Facebook who sought social support were appreciative of the network's affordances for broadcast, but needed to balance the social costs of privacy with their anticipated audience .
Indeed, some friendsourced questions may not have any informational utility at all, but purely exist for the purpose of satisfying emotional support .
While weak tie responses were perceived as more useful, askers' level of satisfaction did not align with answerers' tie strength .
However, on Facebook self-reported information-seeking behavior aligns with perceived bridging social capital .
On social networking services like Facebook and Twitter, activities like sending directed messages or consuming oth-
To estimate the social costs of friendsourcing, we considered approaching the issue using canonical measures such as social capital.
However, previous literature suggests that both bridging and bonding social capital may not relate strongly to favor asking on social networking services .
Further, such measures do not capture the broader set of behaviors that may indicate rising social costs.
To accommodate this broader set of possible outcomes, we chose to approach social costs in friendsourcing more generally.
While we choose to observe social costs in a general sense, there are several likely components of this construct with respect to friendsourcing.
For example, as users post questions, friends expend time and attention noticing these questions.
Should friends reply they might also expend significant effort to give an answer.
A questioner may value these friend expenses as a social cost.
Replies may come with costly expectations of future reciprocation .
Participants in social networks also engage in personality maintenance .
Posting overly personal or too many questions may impact a users' managed persona, incurring more social costs.
One way to understand the intrinsic motivations associated with rising or falling social costs is to apply an opposing extrinsic motivation.
Imagine a situation where a person earns five cents for every question they friendsource during the next hour.
This may provide enough motivation to post, overcoming any social costs.
However, if they only receive one cent per question, then the extrinsic reward may not sufficiently cover the social costs of friendsourcing.
By varying payment across participants, we are able to assign a monetary value to the social costs of friendsourcing information seeking under specific circumstances.
In our experiment, we have participants make a series of 20 decisions with mounting social costs.
Participants are asked to choose either to friendsource a question on Twitter or to sacrifice some of their potential study compensation in order to pose the question to a crowd of microtask workers.
If a participant chooses to friendsource, then the question is scheduled to appear on their feed with a set delay.
As the study continues, a queue of posts and sacrificed money builds.
We vary the absolute amount of money that may be sacrificed as a between-subjects experimental condition, though it is always 1/20th of the total gratuity.
For example, a participant may pay $0.05 from their $1.00 bonus money to avoid friendsourcing a question, while another may be asked to pay $0.25 from their $5.00 bonus.
This limits the influence to an absolute value difference rather than a difference relative to the total bonus.
After choosing to friendsource or crowdsource each of their selected questions, participants are debriefed .
Following the end of the experiment, our system automatically posted the questions marked for friendsourcing to the participant's Twitter feed spaced at eight-hour intervals, so that we could measure response rate and the quality of answers received.
We wanted to select a social network that had as much of a broadcast feel as possible, the default behavior to be sharing, and, if possible, the outcomes of our participants' perceived choices to be public so as to maximize the potential social costs.
Facebook and Google+, both because of privacy features and the system's and participants' active selection of who can and cannot see content, are ill suited for this kind of participation.
Facebook and Google+ also have a relatively slow pace of post and response that may change the impact of potential costs.
In the case of Facebook, additional algorithms operate on Newsfeeds, only allowing subsets of the audience to see a given post.
This makes it hard for users to estimate the size/composition of their audience .
In comparison, Twitter is generally open and fast-paced.
Because of the 140-character limit, posts are generally succinct.
The barrier for replying is very low, since the length limit constrains the complexity of the answer.
All tweets are potentially visible to all followers, as a "Newsfeed" style algorithm does not mediate them.
The Twitter API allows for an app to pose as a user  and post.
For these reasons, Twitter seemed like an ideal candidate for our preliminary work in this area.
The social  costs of friendsourced information-seeking on other networks likely differs; comparing the impact of social network design is an area for future work; however, this paper focuses primarily on the costs associated with friendsourcing on Twitter.
To avoid potential experimenter biases if we were to create the questions, we sought existing exemplar questions on Twitter using a grounded approach.
We sampled 61,286,532 public tweets that we suspected were questions from the Twitter Firehose during a oneweek period between May 9th, 2013 and May 16th, 2013.
We only collected tweets that ended with a question mark , did not contain URLs, were from the English language region, and weren't replies or retweets.
From that initial set we further restricted our criteria, selecting only tweets that started with question words; contained time zone information ; and included hashtags.
This seems to parallel existing log studies of question asking on Twitter .
We selected the 2,365 hashtags  that had at least 50 tweets made to them over our week.
We manually searched for tags that implied questions, ultimately producing a list of 40 hashtags to use for finding questions.
From these 40 candidate hashtags, we randomly sampled 30 questions each and had 3 different Mechanical Turk workers rate their quality, classify their content as factual/opinion/recommendation, friendship/favors, or rhetorical, and mark whether a stranger who doesn't know the author could conceivably answer the question.
Table 1 shows a coding of the hashtags based on the worker ratings.
Based on the Turkers' labels, we identified four common question categories common on Twitter: Factual, Personal, Product, and Recommendation.
For each category we chose 15 tweets that required minimum context and could be answered with at most a web search.
We corrected grammar and unified the hashtag topical markers to correspond to the four identified categories.
Table 2 contains several example questions from each category.
We also recruited participants through internal organization email lists and Mechanical Turk tasks.
Upon later analysis, the source of participants did not have a significant effect on our results.
We implemented a prescreen system that used the Twitter API.
We limited eligibility to U.S.-based participants aged 18 or older with had public Twitter accounts, at least 50 tweets in total, at least 10 tweets over the last 30 days, at least 20 friends, and at least 20 followers.
This prescreening was meant to ensure that participants had enough Twitter activity to make posting several tweets during the study seem natural, and had a sufficient network size to incur potential social costs.
Participants who passed were assigned conditions by round-robin .
Our prescreen received roughly 4200 hits, of which approximately 750 actually approved our Twitter app.
Of those, 397 passed the prescreen.
184 of those approved applicants successfully completed the entire study .
These limited response numbers are not surprising since we stated in the prescreen our Twitter app may be making posts to one's account during the study; several potential participants suggested that requirement was too onerous.
Once participants passed the prescreen, they were directed to a SurveyGizmo page that contained the study.
We implemented a system using Django and the Twitter API to schedule posts if a user chose to friendsource.
We included a description of crowdsourcing markets so as to make sure participants knew what asking a crowd of workers entailed.
How many active Facebook users are there in Africa?
To explore very high costs, we also added a $0.50 condition.
Each condition had between 30 and 31 participants .
Participants received a base gratuity of $5 for completing the study, with a potential bonus of 20 times their condition's payment level .
Choosing to friendsource created a queue of posts to be sent after concluding the study.
We told participants that our app would space tweets out with 8 hour gaps, as that time seems short enough to evoke feelings of "bothering" someone while at the same time is not so often it could be conceived as pure spamming.
Future studies may be able to explore a range of delay conditions.
For each question category we identified earlier , we asked participants to choose 5 questions out of the 15 canonical questions per category  to build up their set of 20 friendsource/crowdsource decisions.
We asked participants to select questions that matched their voice and were interesting.
They had the option to write their own if they did not find adequate questions.
This sort of approach is also common in information retrieval studies of search queries, as a way to strike a balance between realistic, personally relevant prompts and prompts that can be controlled for and compared across a set of participants .
If a person chose to write-in for a particular question choice, we mark that the decision was the result of a write-in.
We screened the writein questions for quality, excluding from analysis any writein questions that were obviously gibberish or unanswerable.
Some example participant write-in questions include: "What song should I buy next on iTunes?"
Participants chose  their questions prior to learning that they would need to decide whether to friend- or crowdsource.
We also consider the previous behavior of a participant as a within-subjects variable.
For example, if a person has chosen to friendsource twice and pay for the crowd once, then we record that their prior friendsourcing ratio was 0.66.
This variable allows us to capture if a person is feeling as though they have "bothered" others enough.
We chose to normalize the prior behavior because, depending on the condition, some higher friendsourcing rates are very sparse and we wanted disparate behaviors to remain comparable.
H2: Higher friendsourcing ratios will reduce the likelihood of selecting friendsourcing for subsequent questions.
Alternatively, if bothering someone is not a serious concern for participants, they typically friendsource lots of questions, they view friendsourcing as relatively low cost, and/or they highly value the answers of their followers, then the relationship may even be positive, as the incremental social cost of tweeting more questions might decrease after a sufficient number have already been posted.
The decision to friendsource is likely to be mediated by a participant's interest in the question .
We suggest that as participants are more interested in getting an answer, they will be more likely to pose it to a known, trusted entity  rather than a paid crowd of anonymous laborers.
Participants also may be likely to be more interested in questions in their own voice or ones that fit their online persona.
H3: Higher interest in a specific question will increase the likelihood of friendsourcing it.
H4: Write-in questions will be more likely to be friendsourced compared to canonical questions User traits may also impact the valuation of social costs.
Participants who tweet more often may be more likely to friendsource as the additional tweets might blend more naturally into their typical level of activity; their past use of the service might also represent a build-up of social capital , which could lower the social cost to posting their own questions .
We also suspect that younger participants may be more likely to choose friendsourcing over paid crowdsourcing, both because their lower incomes might impact their economic valuation of social costs and because their network composition  might entail less-complex audience-collapse-related  persona maintenance, thereby reducing the social costs.
We predict that payment condition will be positively related to choosing to friendsource.
As the costs of turning to the crowd rise, they begin to eclipse the rising social costs.
H1: Higher crowdsourcing costs will result in increased reliance on friendsourcing.
We also predict that a higher friendsourcing ratio will correspond to a reduced likelihood of tweeting another question out of fear of accruing too many social debts.
184 participants completed the study.
Overall, they generated 3680 different friendsource/crowdsource decisions.
The majority of questions that participants chose were from our canonical lists; 416  were written in by participants.
We had two independent coders screen the write-in questions for quality, giving each question a yes/no based on whether it was intelligible and could conceivably be answered by anyone.
The coders evaluated the questions with a Cohen's kappa of 0.92.
75.0% of these decisions were to friendsource rather than crowdsource.
The age  and gender identity  distributions of our 184 participants are roughly in line with the natural population of Twitter .
Participants had made an average of 8441 total tweets  at the time of our prescreen, following a roughly exponential distribution.
They averaged 511 friends and 414 followers.
Most participants  self-reported as having asked questions using Twitter previously , though there may be some self-selection towards question askers in our study population.
We asked participants to estimate the percentage of their Twitter followers that were friends and family, colleagues, or strangers.
Participants reported a roughly even split between friends/family  and strangers , with a smaller percentage of followers being colleagues .
For each of the 3577 decisions, we had participants rate how interested or disinterested they were in receiving an answer on a seven-point Likert scale.
Overall participants were mildly interested .
Table 4 depicts the coefficients, errors, and significance estimates for the model.
As H1 predicted, the amount of money a person had to sacrifice to choose the crowd influences their choice of friendsourcing or crowdsourcing.
Even as social costs rise over several successive friendsourced questions, more money at risk makes a person resistant to sacrificing it.
Yet, a portion of people still choose to sacrifice as, for them, the social costs are still too high no matter the bonus.
A person's interest in getting an answer to the question also makes them more likely to friendsource, supporting H3 .
Interestingly, the prior percentage of questions a person friendsourced seems to increase the odds of future friendsourcing.
This disproves H2, and may suggest that once people have friendsourced a few questions, they may consider it easier to keep going.
Participants might not be as worried as previously thought about bothering others, or there may just be a general preference towards friendsourcing .
This is an area for future study.
Participants who had made more tweets prior to the study were marginally more likely to keep friendsourcing , providing weak support for H6.
Older participants were less likely to friendsource questions, supporting H5.
We hypothesize that this may be due to different compositions of their expected audience.
Participant age is significantly negatively correlated with the percentage of friends and family followers , positively correlated with the percentage of colleagues , and positively correlated with the percentage of strangers .
To understand how and why participants choose to friendsource versus crowdsource based on their internal estimated cost/benefit calculation, we modeled their choice.
We developed a logistic regression model predicting for either friendsourcing a question to Twitter  or paying to send to the crowd .
However, Factual questions were the least likely to receive replies , Personal and Product had roughly the same proportion of questions that got replies , and Recommendation questions got slightly more replies .
These differences are significant =38.5, p<0.0001, suggesting that participants' followers were more interested in providing recommendations than answering factual questions that a participant could answer using a search engine.
Repeating the experiment with professionally oriented questions and/or higher payment conditions might illuminate which audience and economic factors impact older users' friendsourcing choices.
Participants' qualitative responses reinforce some key aspects of the model.
When people chose to friendsource, 57.6% professed a desire to save bonus money.
29.3% also cited wanting to know what followers had to say as their reason for friendsourcing.
Of those that chose to sacrifice money, 50% cited concerns about bothering friends.
In our retrospective questions during the mandatory debrief, 60.3% of participants cited wanting to save bonus money as a goal of choosing friendsourcing, while 37% were more interested in what friends had to say than the crowd, and 26.1% didn't want to bother friends and followers.
A majority of participants  thought that it was acceptable to friendsource questions to Twitter at least once a day.
Some cited Twitter as a "tool to utilize as often as necessary," while others explained "it gets annoying to some."
They cited concerns about flooding and spamming, stating, "too many questions will annoy people" and "too much looks like an annoying spambot."
One participant mentioned, "because twitter is a rapid conversation sort of thing... it's more acceptable to just shoot questions into blank space and see if you get a response."
On the other hand, another participant stated "only rarely do I ask advice on Twitter, because it really isn't part of my Twitter `persona.'"
Another cited using search engines instead, saying, "I usually tweet about specific things.
I don't ask global questions.
I go to search engines to get answers..." These opposing perspectives illustrate that improvements in modeling social costs and the "economics" of friendsourcing might be made by personalizing models to account for individuals.
Such costs  may also change over time as norms of social network use evolve.
After all of the friendsourced questions had been tweeted, we polled the Twitter API for all replies to the questions.
We located 1010 replies for 602 friendsourced questions, giving a response rate of 16.8%.
135 participants, 73.3%, received at least one reply .
The median latency for receiving the first reply to a question was 695 seconds, or roughly 10 minutes .
Median reply length was 48 characters .
Generally, participants received answers for more than one question .
There was a marginal relationship between total tweets posted prior to the study and receiving a reply, though log and log are significantly correlated .
Each participant chose  five questions from each of the four categories  to create their 20 total questions.
There are slight differences in the number of decisions in each of the categories because of excluded write-in questions .
While we have investigated potential costs of friendsourcing, we have not yet examined the alternative: the costs of crowdsourcing the same questions.
Would crowd workers demand an amount of money comparable to what the participants were willing to sacrifice to avoid social costs?
We sent the same batch of 60 canonical questions to crowd workers on Amazon's Mechanical Turk task market, seeking 5 different responses for each question.
We replicated this process at the same time for each day of the workweek, increasing the payment per question each day in $0.01 in-
Workers could answer up to all 60 questions .
Examining the duration between posting the questionanswering task and getting responses, the workers took a mean 8,765 seconds  and median 6,046 seconds , and a standard deviation of 9044 seconds.
Some workers submitted responses in as little as 17 seconds from the time the job was posted.
The median length of responses was 40 characters .
We then had two new crowd workers rate each response on a scale of quality that ran from -2 to 2.
0 were ones that were incorrect or incomplete.
1 were satisfactory answers, and 2 were excellent answers.
We also had them decide whether or not they would be satisfied getting that response.
We averaged the two raters' responses.
Across all questions the raters generally considered the crowd answers to be satisfactory .
1010 answers were deemed satisfactory, 354 split the raters' opinions, and 136 were deemed unsatisfactory.
This aligns with the quality ratings, of which only 121 responses were rated below 0 quality.
There were generally few differences between payment conditions .
The 5c condition received slightly higher ratings, and 2c responses had on average responses with 12 fewer characters.
If we consider the payment condition as a continuous variable, there are no significant differences.
In general, whether we paid the Turkers 1 cent per answer or 5 cents, we received decent, useful responses in a timely fashion.
It is plausible that the differences we saw between payment conditions were just artifacts of worker selfselection and the population that was online that particular day of the week.
604 of the replies were deemed satisfactory, 250 split raters, and 156 were unsatisfactory .
The Twitter replies overall arrived faster than the crowdsourced replies, with a mean latency of 7,136 seconds later , a median of 677 seconds , and a standard deviation of 29,282 seconds.
The time difference between our crowd workers and real Twitter responses is significant after log-transforming response latency to account for exponential distributions =21.2, p<0.0001.
Some Twitter responses came in as soon as 3 seconds after posting.
There was no significant length difference between Twitter and worker responses.
One possible reason for the difference in rated quality is that friendsourced answers may require information about social context to interpret properly.
A remark that seemingly does not answer anything may in fact be more valuable to the friendsourcer than an informational response.
Interestingly, no crowd worker submitted responses that contained jokes, while the raters noted that several of the tweeted responses were off-color or humorous.
For example, one participant chose to friendsource the question "Is stretching more important before or after a workout?"
The received the response "After.
Stretch the truth to say you've done more than you actually have."
This points to one specific advantage that friendsourced answers possess: they can make use of shared social context and previous rapport.
While crowd workers are most certainly capable of social banter , their answers are usually intended for a general audience, though this can also be a benefit if the question asker purely needs information and is not also using friendsourcing as a means of social interaction .
Regardless, crowd workers provided slightly superior informational value compared to friendsourcing.
Because they were paid, they delivered a 100% response rate.
Through our controlled study and regression analyses, we have explored several potential factors that affect users' decision to friendsource information seeking on Twitter.
By varying payment, we were able to gain an understanding of how participants estimated the value of their friends' attention and effort, their own future reciprocal efforts, and/or their persona maintenance .
Participants were more likely to friendsource when provided higher monetary incentives.
In the high payment conditions this amounted to a 10-20% increase in likelihood of friendsourcing.
This is strongly mediated by several factors, including question content and interest in receiving an answer to a particular question.
However, several participants suggested that friendsourcing anything at all was too onerous.
They opted to sacrifice all of their bonus money rather than post.
While we controlled for past question asking behavior by incorporating a self-report into our model, we may not have explored the full spectrum of how people valuate the costs of friendsourcing.
There may be other border cases such as those who never friendsource, or those whose social cost valuation is higher than the incentives provided in our study, and there may be some self-selection based on who chose to participant in the study.
Some people who estimate friendsourcing to be very costly may not participate in a friendsourcing study.
This may help explain why friendsourcing was the default behavior.
The fact that we opted to conduct a controlled study with monetary choices also presents limitations.
Because we had participants make choices within an experimental environment, their expectations and social cost calculations may have been skewed.
While we gained some specificity in terms of identifying the way people estimate social costs, it would be reinforced by future in-situ examinations of behavior.
Observation bias might compel participants to ask more questions if they feel that is the more desirable behavior.
The use of sacrifice rather than pure bonus in the experimental design is also a potential limitation; using a positive paradigm  may have shifted users' price points.
Similarly, because we used a synthetic situation, the timing and content of questions may not perfectly reflect actual friendsourcing behavior online.
However, because we gave participants the ability to write their own questions and we selected participants who posted at least twice a day already, these factors were hopefully mitigated.
While we controlled the rate at which questions would be posted as it is likely to influence the social cost calculation, in practice people regularly change their posting rate as a means to reduce social costs.
Further, because the study ran over the course of two weeks, different social conditions across the Twitter network may have affected participants' likelihood of posting.
In the future one might vary the rate of posting much like we varied price, looking for the way participants estimate the costs of "bothering" over different time spans.
One could also introduce more strict control of question content, for example evaluating the costs of very general questions versus private health questions versus questions that may affect online personality maintenance.
Twitter also provides an inherent limitation to this work.
Posting behavior on a particular social network is not fully generalizable to other social networking services.
One significant area for future research would be to expand the scope of this methodology into different social networking services such as Facebook or Google+, which may exhibit different effects as a result of more reciprocal behavior, closer ties, different conventions regarding posting and replying, and potentially different audience compositions.
Studying social network/forum hybrids such as Quora and StackOverflow may also provide interesting findings in this space.
Despite these limitations, our research points to several areas for future design and research.
We already see in our analysis and in past work that people do choose not to friendsource and instead self-censor.
This may because they don't want friends to expend too much effort, or may also be related to other factors such as the contents of questions.
For example, a person may not want to friendsource a question about an embarrassing health problem, even if they have several friends who have medical training and could give advice.
This points to a rich area for design of friendsourcing technology that reduces the perceived costs of asking.
In the case of private information, we might be able to reduce costs by granting more anonymity or providing a use case in-between directed messaging and broadcasting to get at domain or trustworthy knowledge sources.
We might be able to design networks that better expose the actual effort people go to in answering questions so as to make sure people are not off in their social cost estimations.
Interestingly, social proof might actually exacerbate the problem.
We already see participants feeling a push for reciprocity, and if we surface that someone is answering everyone's questions, friendsourcers may feel more in debt.
Our work with Mechanical Turk answerers and past work such as MSR Answers  suggest another direction for designing friendsourcing marketplaces - for some questions, it may be better to have an anonymous crowd ask them.
We have shown that people estimate their friends' attention and effort at drastically higher values than it costs to get a comparable 
This suggests benefits in developing approaches that hybridize friendsourcing and crowdsourcing.
When a question might be too bothersome, we could provide a person the ability to ask a crowd with as little effort as posting.
Participants even may be more comfortable asking private questions to strangers .
However, the crowd may not be able to answer all questions.
The crowd is ill suited to answer those that require personal context or ones that might better be fulfilled with social rather than informational content.
Intelligent agents could dynamically identify questions that are suited for different types of responders and assign them accordingly.
This may even function within a social network, perhaps directing friendsourced questions to domain experts or those especially skilled at providing social support.
As more and more people friendsource against finite attention and effort resources, this sort of intelligent question assignment will become increasingly important.
In this paper we developed a novel methodology for studying how social network users estimate the social costs of friendsourcing through a system of monetary choices.
We employed this methodology to examine social question asking on Twitter, demonstrating that participants assign specific social costs to friendsourcing, and that assigning in-
We demonstrated a link between question content, participants' desire for an answer, and age with the way participants estimated the social costs of friendsourcing.
We demonstrated that even a five-cent difference in monetary cost changes participants' question asking behavior, and parallel it with the costs of getting comparable answers from a crowd of workers.
In general, even one cent can provide comparable or better informational quality as compared with Twitter replies, though with slightly increased latency.
With these two findings in mind, there are rich areas for future research, including further examining social costs both within Twitter and in other social networking services, and also in designing new systems that might minimize friendsourcing social costs or hybridize social friendsourcing with microtask markets.
Lampe, C., Vitak, J., Gray, R., and Ellison, N. Perceptions of Facebook's value as an information source.
Morris, M.R., Teevan, J., and Panovich, K. What do people ask their social networks, and why?
Morris, M.R., Teevan, J., and Panovich, K. A Comparison of Information Seeking Using Search Engines and Social Networks.
Morris, M.R., Inkpen, K., and Venolia, G. Remote Shopping Advice: Enhancing In-Store Shopping with Social Technologies.
Asking questions of targeted strangers on social networks.
Oeldorf-Hirsch, A., Hecht, B., Morris, M.R., Teevan, J., and Gergle, D. To Search or to Ask: The Routing of Information Needs between Traditional Search Engines and Social Networks.
Paul, S., Hong, L., and Chi, E. Is Twitter a Good Place for Asking Questions?
Sleeper, M., Balebako, R., and Das, S. The post that wasn't: exploring self-censorship on facebook.
Stutzman, F., Vitak, J., Ellison, N., Gray, R., and Lampe, C. Privacy in Interaction: Exploring Disclosure and Social Capital in Facebook.
Teevan, J., Dumais, S.T., and Horvitz, E. Potential for personalization.
Teevan, J., Morris, M.R., and Panovich, K. Factors affecting response quantity, quality, and speed for questions asked via social network status messages.
Thom, J., Helsley, S., and Matthews, T. What are you working on?
Uehara, E. Reciprocity reconsidered: Gouldner'smoral norm of reciprocity'and social support.
Journal of Social and Personal Relationships, .
Wills, T. Perceptual Consequences of Helping Another Person.
Culture Matters: A Survey Study of Social Q&A Behavior.
Zhao, X., Salehi, N., and Naranjit, S. The many faces of Facebook: Experiencing social media as performance, exhibition, and personal archive.
Bernstein, M., Bakshy, E., Burke, M., and Karrer, B. Quantifying the invisible audience in social networks.
Investigating the appropriateness of social network question asking as a resource for blind users.
Brenner, J. and Smith, A. Pew Internet Report: 72 % of Online Adults are Social Networking Site Users.
Burke, M., Kraut, R., and Marlow, C. Social capital on Facebook: Differentiating uses and users.
Burke, M., Marlow, C., and Lento, T. Social network activity and social well-being.
DePaulo, B. and Fisher, J.
The costs of asking for help.
Ellison, N.B., Steinfield, C., and Lampe, C. Connection strategies: Social capital implications of Facebook-enabled communication practices.
Evans, B.M., Kairam, S., and Pirolli, P. Do your friends make you smarter?
Hecht, B., Teevan, J., Morris, M.R., and Liebling, D. SearchBuddies: Bringing Search Engines into the Conversation.
Jeong, J., Morris, M.R., Teevan, J., and Liebling, D. A Crowd-Powered Socially Embedded Search Engine.
Jung, Y., Gray, R., Lampe, C., and Ellison, N. Favors from facebook friends: unpacking dimensions of social capital.
