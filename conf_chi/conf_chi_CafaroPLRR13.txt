Museums are increasingly embracing technologies that provide highly-individualized and highly-interactive experiences to visitors.
With embodied interaction experiences, increased localization accuracy supports greater nuance in interaction design, but there is usually a tradeoff between fast, accurate tracking and the ability to preserve the identity of users.
Customization of experience relies on the ability to detect the identity of visitors, however.
We present a method that combines fine-grained indoor tracking with robust preservation of the unique identities of multiple users.
Our model merges input from an RFID reader with input from a commercial camerabased tracking system.
We developed a probabilistic Bayesian model to infer at run-time the correct identification of the subjects in the camera's field of view.
This method, tested in a lab and at a local museum, requires minimal modification to the exhibition space, while addressing several identity-preservation problems for which many indoor tracking systems do not have robust solutions.
Shared output is most commonly achieved by using very large displays.
Supporting simultaneous, multi-user input is more of an open challenge, however - many approaches have been tried, from using mobile devices as opportunistic user interfaces  to using multi-touch tables  to using tangible user interfaces .
Embodied interaction  is yet another input approach that has been gaining popularity in museums.
Various sensing technologies can be used to track the movements of human bodies as a means of providing input to the exhibit.
Embodied interaction is appealing because it does not require visitors to use devices 
The richness of the learning experience offered by embodied interaction exhibits, however, is directly affected by how much control visitors have over the interaction, and the transparency of that control.
In other words, engaging meaningfully with the exhibit is dependent on whether visitors can link the effects seen in the shared exhibit to their own individual actions .
Supporting such interactive control via embodied interaction requires a system that accurately tracks visitor movements in space .
Because these exhibits are multiuser, pro-learning embodied interaction also requires preserving the identity of each user, in order to accurately attribute actions to individual visitors .
Indoor tracking of individual users has remained something of a challenge, however, despite the proliferation of sensing technologies applied to the problem.
There is usually a tradeoff between fast, accurate tracking and the ability to preserve the unique identity of users.
Over the past two decades, there has been increased interest in how exhibits might promote interactive learning and sociability - in other words, supporting not just how visitors interact with the exhibits, but also how they interact with one another while in the presence of exhibits .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To support personalized interactions, we needed to build a system that combines both fine-grained tracking and reliable identity assignment and correction - the focus is not on indoor positioning per se.
We present a method that combines two input technologies, commercial camerabased motion tracking and Radio Frequency Identification  technology.
Our system prototype  is a combination of a shared display, plus two components:  a Microsoft KinectTM; and  a single RFID reader.
The KinectTM is used to track visitors within the exhibition space, which supports fine-grained location resolution  and is highly reactive .
The use of inexpensive RFID tags  allows the system to identify who is in the exhibition room.
We developed a probabilistic Bayesian model that combines data from each of the two components of our system to:  preserve the identity of visitors who enters a new exhibit and  to resolve identity ambiguities that occur when visitors occlude one another or temporarily step outside of the camera's view frustum.
Single-camera systems are limited by the view frustum of the camera, so unless a camera can be mounted on a high ceiling to cover the entire exhibit space , or unless smart strategies are used to manage camera movements , the camera can easily "lose" individuals.
A room can be outfitted with multiple cameras to completely cover space, but apart from the expense, this also involves significant alterations to the space , and a careful placement of cameras to prevent occlusion-related misidentification .
Unfortunately, calibrating these systems is non-trivial and this approach still doesn't support the unique identification of visitors - which is needed if the interaction design for the exhibit calls for associating a visitor with a specific profile, as would be needed for personalized exhibit experiences.
Fiducial symbol-based strategies could be used to establish identity , but these have limitations when the symbol is moved too far away from the camera , or when it is tilted too far.
Alternatively, users could wear special-purpose colored or reflective clothing visible from multiple angles , but this may be impractical .
Some systems, including the KinectTM, can perform limited face recognition to disambiguate amongst visitors, but these approaches suffer the problem of false positives or false negatives when identifying multiple users .
Furthermore, the recognition rate decreases in poor lighting conditions  and its performance may be poorer with different ethnic groups .
When the interaction design for an exhibit is continuous , such mis-assignments could entirely derail the interactive experience.
Two people interacting with our system prototype.
A 65" display is used in combination with two RFID antennas  and one KinectTM camera  We tested our system during in-laboratory experiments and with museum visitors at a local history museum.
The lab tests were designed to test the performance of the system using repeated pre-scripted user motions known to be problematic for the system, while the in situ tests were done to assess the performance under naturalistic use conditions.
While we are hardly the first to explore embodied interaction with museum exhibits, our problem space  demands a system that supports both location awareness  and context awareness .
Here we review prior work on location awareness and unique identification in indoor spaces.
Infrared  distance sensors provide fine-grained location detection like camera systems .
They can be constructed in different ways, either by assembling a grid of IR beams and sensors that gets interrupted by visitor bodies   or by using IR cameras to detect the presence of visitor bodies or of Infrared beacons .
Beam-based IR systems require a fair amount of work to build, are only as accurate as the grid spacing , and are not able to uniquely identify users .
IR camera systems require a line of sight and, unless visitors wear specialized beacons , such systems cannot uniquely identify approaching visitors.
Radio Frequency ID  systems work by transmitting radio signals from an antenna  and "listening" for the signal's "reflection" as it bounces off specialized printed circuits .
The main advantage of RFID systems is that the radio waves are not line-of-sight only - they can penetrate most non-metallic substances.
RFID systems are extremely reliable when identifying tags in the read range of the reader, which is why they are so commonly used by manufacturers and shippers to track the location of items in warehouses.
Parts and parcels are not prone to moving about of their own volition, however - so most RFID tracking systems are configured to detect only when a card enters or exits a space, like the Intellibadge  system, not to triangulate the precise location of a card.
Likewise, the RFID technology employed in   is suitable to identify the users of the display; but its limited localization is insufficient for supporting embodied interaction.
There have been several attempts at expanding the capacity of RFID for localization for tracking items in motion.
However, the granularity of the localization that can be achieved by these systems is generally at the level of one room, which is insufficient for highly-interactive applications.
Zigbee-based systems  have an accuracy of 1.5-2.0m, which may not be enough for controlling a highly-interactive system within a small interaction space.
Among the most accurate and responsive RFID tracking systems are Ultra Wide Band systems, such as the commercial Ubisense, which still have properties unsuitable for highly-interactive applications .
Another system achieved finer control by combining passive RFID tags  with an ultrasonic sensor, where distance from the display would zoom pictures associated with the user in or out according to the direction of movement .
This approach still requires a custom installation and extensive calibration owing to the narrow line-of-sight of many ultrasonic sensors.
Our system differs from existing approaches for its ability to overcome the trade-off between fast, accurate tracking and the ability to preserve the unique identity of users, thus allowing exhibit designers to create personalized embodied interaction experiences.
Compared to many commercial tracking systems such as Nikon Metrology iGPS our system does not require a direct line of sight and requires the user to carry only an inexpensive credit-card sized tag.
Compared to Ultra Wide Band RFID system such as Ubisense, our system is more fine-grained  and faster  when the tagged object  is walking within the exhibition space .
Furthermore, our research is different in that:  we exploit low-cost, easily-obtainable technology ;  installation requires little to no modification of the exhibit space;  we develop a Bayesian probabilistic model that supports robust identification for applications that require reliable recognition of individuals.
Given the limitations of many single-technology methods for both localizing and identifying individuals, quite a few researchers have explored combining different methods to support both of these needs.
Concurrent radio and ultrasonic signals , can get 2-dimensional positioning sufficient for room-based embodied interaction, but the ~10x10cm precision is not sufficient for fine-grained interactions.
Hello.Wall  uses one long-range and one short-range RFID reader to define zones of interaction, with fine-grained interactivity supported with a specialized handheld device.
Our pilot exhibit, CoCensus, is situated in a history museum, and encourages visitors to explore the US Census data.
Data sets, even when visualized attractively, are typically not all that engaging, but there is evidence that interactive data visualizations like Hans Rosling's Gapminder might change this.
To help visitors explore a visualization of Census data, we allow them to "role play" as the data subsets associated with their own self-identified ethnicities .
We use embodied interaction to help visitors "feel" the connection to their data as they explore them: when a visitor approaches the display, his subset of data becomes more prominent on a map .
Our design metaphor is a mirror, reflecting the users' own data in response to their body movements within the interaction space, in order to engage visitors in joint explorations of rich data sets.
When two visitors move within the exhibition space, they collectively conceal and reveal patterns of where their respective data subsets do or do not intersect, sparking conversations such as comparing the settlement patterns of different immigrant groups.
The metaphor for our collaborative data exploration design is a data mirror.
Visitors approaching the exhibit should see "their" data reflect their body movement.
Indeed, part of the learning that occurs at the exhibit is in the emergent discussions among visitors about their respective data sets, which are highly personalized, so it is critical that visitors are able to identify which portions of the data display are under each person's control.
The example scenario highlights the functional requirements of an indoor tracking system that guarantees a fully personalized and embodied interaction experience.
More specifically, the system should be:  Reactive; the system must be fast enough to allow continuous and transparent interaction with the user.
When a visitor is moving within the exhibition space, the system should be able to continuously track its current position.
When a visitor leaves one exhibit and moves closer to another, the new one should proactively respond to the visitor's approach.
For instance, if we want to emphasize the data of the visitor who is closer to the screen, the system should be able to determine if one person is just one step ahead of the other.
On the one hand, in a public space, it may be impractical to ask people to wear special purpose clothing or to carry heavy or expensive devices.
Nor do we wish to unduly alter the exhibition space - many museums  are housed in historical buildings where changes to the walls and ceiling  are impossible.
These four functional requirements  suggest the use of a compact camera tracking system, such as the KinectTM.
We performed a formative study in a museum, using only the KinectTM and Primesense OpenNI.
With solo visitors, 5 out of the 9 visitors lost their identity at some point during their 15-minutes interaction with the system.
For instance, when a guided-tour group walked through the exhibition space, a new identity was assigned to the visitor.
Without any other supporting technology, such as face recognition or fiducial symbols , the KinectTM can track a shape in the exhibition space, but is not able to associate a previously-defined profile to that shape, e.g.
The use of the RFID technology is a good fit for supporting the camera tracking system in preserving identities.
For instance, when one visitor enters an exhibition, the new shape seen by the KinectTM can be associated with the new RFID tag and the system can reactively display the data related with that user's profile  and mirror that user's movements, as the KinectTM is able to detect human shapes within its field of view and track them with fine-grained resolution in the three-dimensional space.
However, this process becomes challenging when two or more people enter the room at the same time, a common occurrence.
The camera tracking system detects two or more new shapes, the RFID reader two or more new IDs, but who is who?
One trivial solution would be to make visitor A always enter the exhibit room before visitor B, who needs to enter before visitor C and so on.
Of course, this would not be feasible in a museum, when multiple people are supposed to move freely from one room  to the other.
This motivates the need for a more intelligent merging of visitor detection sensors, such as the probabilistic Bayesian model here described.
Formally, let us denote as  the number of subjects being tracked, and define the set  = {1, ... , } of RFID identifiers, and the set  = {1, ... , } of KinectTM shapes.
At time , a tuple  = , , ,   is received, where ,  {0,1} identifies which of the two antennae attached to the RFID reader received the signal, and ,   is the strength of that signal.
A tuple  = ,  of  coordinates is also received from the KinectTM, representing the position of the  -th shape.
Our methodology is based on exploiting the statistical dependence between the data coming from the camera tracking system and from the RFID antennas.
In order to do so, we developed a probabilistic Bayesian filter where the identification function  is treated as a random variable representing the hidden state of the system.
One assumption sometimes made when tracking indoor radio signals is that, for a given location, the RSSI is normally distributed .
For this reason, we use histograms to provide a richer, nonparametric characterization of the probability distribution of signal strengths.
Since RSSIs assume integer values only, it is natural to assign each integer level to one bin of the histogram.
During execution, a window of  RSSI observations is kept for each RFID tag in range, and the derived histogram is compared to the fingerprints that have been stored for specific, known locations using the Kullback-Leibler  divergence.
At that point, the Maximum A Posteriori  value of  is chosen as the actual one.
Details of the methodology follow.
The estimation at time  of the position of the RFID tags is combined with the information  coming from the KinectTM to perform the probabilistic Bayesian update described in the following.
In our case, we have: where  is a normalization factor.
In our case, we have:
Intuitively, this distance is small when the positions estimated through the RSSIs accurately describe the real location of the subjects, assuming that tag  is carried by subject .
This quantity is therefore inversely correlated with the likelihood of the equation above.
Experimentally, we observed that a negative exponential relation is a good fit for this correspondence.
Therefore, the overall update is: The parameter  controls the magnitude of the update: a small value makes the system more robust to local perturbations, while a large one makes it more responsive.
At each time step, the system sets the current identity assignment to the configuration assuming the maximum value .
It is reasonable to assume that the signals received on the two antennas are conditionally independent, given the position of the RFID tag.
Under these circumstances, it can be shown that the symmetrized KL divergence for the joint distribution RSSI for the two antennas can be computed as the sum of the two  components, i.e.
Upon this measure, a kernel function1 is computed as , ,   =  ,, , where  is a parameter of the model that needs to be tuned appropriately.
We tested the system in our research lab to assess its reactivity and stability under different scenarios.
We then evaluated it during two sessions at a small history museum in Chicago, to determine if the level of performance we achieved was sufficient for actual visitors to find it usable.
60 museum visitors participated to the in situ evaluation.
An RFID tag was given to each person before entering the exhibition space.
Experimental set-up The system was deployed by mounting a 65" screen, the two RFID antennas, and the KinectTM on a movable TV stand on wheels, hence providing high flexibility with respect to the venue configuration.
The RFID infrastructure is based on a Thingmagic Astra reader, which is connected to an additional 6dB antenna using a 1m long coaxial cable.
We used Primesense OpenNI to obtain tracking data from the KinectTM for all our tests but the second user-study at the museum, in which we used the Microsoft SDK.
During the tests reported in the following section, we used a 2.4x2.4m interaction space divided for calibration into a 4x4 grid.
Our system configuration is shown in Figure 3.
We analyzed different instances of three main test cases: Initialization, Tag Exchange, Stress Tests.
Initialization: Two people enter the exhibition space together.
Under Far x, Far y, and Close, two users enter the exhibition space and remain still for 60 seconds, within the average 2 minute linger time observed at most museum exhibits.
In the first case, they stand at variable distance from the screen, but farther than 1 m from each other on the x-axis of the grid; in the second case, they stand farther than 1 m from each other on the y-axis of the grid; in the third case, they both stand within a radius of 1 m. Under Moving, two people enter the interaction space, stand for 10 seconds facing the screen , and then start "interacting with the system", i.e.
We expected this case to be the most challenging Initialization test, as RSSI is notoriously sensitive to tag movements; however, we also expected it to be the more realistic scenario in a museum setting, Tag Exchange: Two people exchange their tags.
This may happen if two visitors decide to swap the set of data that each of them is controlling.
In our scenario, this means exchanging the nationality; if each visitor was controlling different features  with their motions, it would mean swapping "embodied functionality".
It is worth noting that these tests serve double-duty: they also replicate how the system reacts when an erroneous identity assignment occurs.
In both test cases, two people enter the space, stand for 5 seconds, interact with the system for 40 seconds; at  = 45 seconds, they exchange tags.
Under Keep pose, nothing else happens; under Swap pose, the two users also swap their position on the grid: for instance, if user A was in cell A3 and user B in cell C1 when the swap occurs, A gives his tag to B, wears B's tag and moves to cell C1, while B moves to cell A3, then resume their interaction with the system.
Stress Tests: we tested our system under some un-realistic scenarios, which are however known to be critical for the RFID infrastructure.
Under Free movement, two users enter the interactive area, stand facing the screen for 5 seconds, and then start walking, running, and jumping in the space, frequently occluding each other and facing different directions.
Under Circles, two people face the screen for 5 seconds and starts to walking in circles in two different halves of the interactive space.
We designed this last case because the RSSI is notoriously influenced by the angle between the tag and the antenna.
Parameter tuning In order to gather the fingerprint information, we performed a reading of 1000 RSSI values in each of the 25 corner points of a 4x4 grid, which took less than one hour.
To train the values of  and , we collected 1000 RSSI values from 20 randomly selected points within the 2.4x2.4m interaction space, and chose the parameters providing the best tracking accuracy.
Noticeable discrepancies in the optimal value of  were observed across the interaction space.
Acknowledging them, we set  = 5 for those points with  80cm or   180cm, and  = 20 for all the other points.
The optimal value of  was estimated at 0.1 in all cases.
Note that the overall setup phase took only a few hours to complete, and it is mostly automatized.
Setting  equal to 0.01 guarantees a good trade-off between robustness and responsiveness.
A window length  of 20 was used during the experiments.
This is in line with our original intention to create a system that requires only a short calibration that can be performed without any knowledge of how the system works.
Preliminary In-lab Evaluation The in-lab evaluation was performed with members of our research group.
Each test case was repeated 10 times .
We marked the 4x4 calibration grid on the floor, to make sure that each test was repeatable.
It is defined as the time elapsed until the system performs a swap to an identity assignment that:  afterwards is kept for at least 20 seconds;  is coherent with the ground truth.
Multiple-Object Tracking Accuracy  : measures the accuracy of the system and is defined as the fraction of  during which the system assigns the correct identities.
High values of accuracy allow the system to mirror the action of each user on her/his own data.
It is defined as the difference between ,  and  .
This value measures the reactivity of the system in re-assigning identities.
The results of the in-lab evaluation are reported in Table 1.
In the first row, we report one instance of the users' paths showing how they interacted with the system, generated from the tracking data collected with the KinectTM.
The first user is shown in green, the second in red.
The thick black line represents the screen location, and the darkest areas are those where the user spent the most time.
As we expected, the system is faster and more accurate initially identifying two users when they are far from each other along the x-axis  rather than when they are far on the y-axis, as the two antennae are placed to the left and right of the screen.
This result will change depending on different antennae configurations.
Also, as we expected, the average time required by the system to stabilize  was higher when the users were moving, than when they were standing far from each other on the xaxis .
Surprisingly, though, the accuracy  of these two test cases is comparable, even though the RSSI is affected by tag movement.
Figure 4 illustrates the meaning of these parameters in one instance of the Tag Exchange test case.
In all the other test cases ,  is the value in the interval , ,  ,  has no meaning and  is the same.
Furthermore, Moving proved to be a better scenario than Far y, probably because receiving data from different locations  reduced the risk of standing in spots that were more difficult for the RSSI-based system to disambiguate.
Once under Far y and once under Close the system failed to stabilize to the correct identity assignment during the 60 seconds interaction.
The cause of this problem and whether or not a wrong identity assignment may persist during a longer interaction should be investigated in future work.
For our analysis, we incorporated this information in the average  for each of the two test cases; we decided to consider these two instances as outliers for , and  , as the system never reached a stable identity assignment .
We did not observe a significant difference between the two tests cases.
As we expected,  , is considerably higher than , , as the system has to adjust itself and switch from one stable configuration to the opposite one.
It is worth noting that , includes the time the two users require to give the tag to each other, wear it and, under Swap pose, to move in the grid ; this additional time generally varied between 3 and 6 seconds.
The results of the two stress tests were surprising.
As we expected, the accuracy  of Circles is the lowest of the in-lab test cases; however, this value is still high , considering that the RSSI is known to be dependent on the tag angle and on tag movements.
Under Free movement, we even obtained values of both , and  that outperform many of the other in-lab test cases.
This tendency is very promising for highly interactive scenarios, given the range of interactions this would support.
In Situ Evaluation The in situ evaluation took place at a small history museum and cultural center, in two separate 2-weeks sessions .
30 museum visitors participated in each session, in groups of 2 people, for a total count of 60 people.
Our in situ trials, one in a main hallway and one in a regular gallery space, were both located where many non-interacting visitors walked through the interaction space.
We also removed the grid that we used for the calibration, to prevent people from being influenced by the cell limits.
We marked a 1cm-thick blue line on the floor 90 cm from the screen  but did not point it out to visitors.
The results  of the first in situ evaluation are presented in Table 1 , while the interaction patterns exhibited by visitors are shown in Figure 5.
In all cases the system stabilized to the correct identity assignment .
As we expected, these results were less good than in-lab: the average time needed by the system to stabilize was 10s, and the accuracy after the initialization phase was 90.3% on average.
We performed a second museum session in order to validate the previously obtained results with a different KinectTM API .
The results of this second evaluation are presented in Table 1 .
The accuracy of the system  is coherent with the values observed with OpenNI.
However, the system was now faster in disambiguating the identity of people .
A possible explanation for this improvement is that the Microsoft SDK seems to be faster and more robust in recognizing human shapes at the beginning of the interaction.
It is worth noting that switching from OpenNI to Microsoft SDK was transparent to the Bayesian model and to the user.
The moderator explained how the system worked and asked each visitor to pick up one RFID tag and to specify the ethnicity whose data she/he wanted to explore.
After that, the moderator invited the two participants to enter the room with the shared screen.
No constraints were given on the interaction.
During our in situ evaluation, the system worked as desired to promote data exploration.
We observed users engaged in conversations related to data interpretation, such as "I'd never realized that there were so many Germans and Italians in Chicago!"
We also observed some design parameters that should be considered when constructing personalized embodied interaction experiences in a museum setting.
Non-participating visitors passing by the exhibit are a challenge that cannot be avoided, certainly during exhibit use and possibly also during calibration.
Despite our best efforts to perform calibration at "dead times" at the museum, people crossed the interactive area several times, which we suspect slightly affected the in situ system performance.
Blocking off the space during calibration, or if that is impossible, calibrating when the museum is closed is recommended.
During use, we had a number of passersby walk behind the users or sometimes even between the users and the screen.
The identity resolution process is accelerated if we take the simple measure of discarding any Kinect-detected shapes outside the marked interaction boundaries, and then relying on the algorithm to assign valid RFIDs to shapes within the interaction boundaries.
Stepping outside the interaction space.
During the first in situ evaluation, visitors stepped outside the interaction space that we had previously calibrated in 11 cases .
The second in situ evaluation also confirmed the users' tendency to stand really close to the screen  when talking about the data.
The KinectTM camera is not able to track people when they are close to the sensor.
Even though our system was successful in recovering the identity of those users, the overall accuracy  was affected.
Also, visitors cannot actively interact with the system when they are not tracked.
In order to minimize this recurrent behavior, a more tangible delimiter  could be installed to mark this interaction boundary.
Even though the system takes few seconds to adjust at the beginning of the interaction, visitors never noticed this initialization time.
In all the test cases, they stood for longer than 10s after entering, just looking at the screen.
Also, people never made any remarks about identity mismatches in 28 cases .
This is probably due to the fact that:  people were engaging in conversation about the data;  most problems occurred when people were standing close to each other, so they were not able to really see any effect on the screen.
However, as ambiguous identity assignments may derail the interaction experience, we plan on damping the system responsiveness when the probability level of the current identity assignment drops below a certain threshold.
How people carry the tag.
At the beginning of 4 tests people entered the exhibition space holding the tag in their hands, until the moderator asked them to pin it to their shirt.
This behavior might have influenced the accuracy of the system.
During the second in situ evaluation, we inserted the tag into a badge holder that people naturally wore around their necks, which resolved the problem.
In this paper we presented and validated a model that combines input from a mid-range RFID reader with input from a commercial camera-based tracking system: we used the KinectTM as a source of tracking data and we augmented it with an RFID-based identity assignment monitor to attain tracking precision and identity preservation not possible with any existing system.
Visitors can actively control and explore their own data with their bodies, in a shared space.
Some guidelines should be considered when this system is incorporated in the design of a museum exhibit, in order to optimize its performance.
Even though we installed and tested one single prototype, our vision is that a museum might add multiple such exhibits throughout their galleries - while our system would not track visitors in-between these locations, it would allow for continuity of experience from exhibit to exhibit.
If many interactive exhibits are installed across the museum, they can support an experience that is sensitive to not just on who you are but also to what you have/haven't seen already in the museum.
Future work should investigate the performance of this system in a bigger interaction space, which may allow the presence of a greater number of users.
The personalized perspective leads visitors to noticing and questioning patterns in the data, and making comparisons across each other's data sets - this phenomenon will be further investigated in future work.
Personalized interactions have the potential to fundamentally change museum experiences.
In museums, visitors often tend to linger in a peripheral zone before engaging with an exhibit, especially if strangers are present.
If this system can issue personalized "invitations" to visitors to "step right up", it can help break down these barriers and possibly even get visitors to speak to each other .
Modern technologies, including handhelds, may tend to isolate us from one another.
On the contrary, the system that we presented might be able to bring us together.
