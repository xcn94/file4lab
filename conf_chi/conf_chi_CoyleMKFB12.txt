David Coyle1, 5, 6 James Moore2, 3 Per Ola Kristensson4 Paul C. Fletcher2 Alan F. Blackwell1 1 Computer Laboratory and 2 BCNI 3 Dept.
Cognitive neuroscience defines the sense of agency as the experience of controlling one's own actions and, through this control, affecting the external world.
We believe that the sense of personal agency is a key factor in how people experience interactions with technology.
This paper draws on theoretical perspectives in cognitive neuroscience and describes two implicit methods through which personal agency can be empirically investigated.
We report two experiments applying these methods to HCI problems.
One shows that a new input modality - skin-based interaction can substantially increase users' sense of agency.
The second demonstrates that variations in the parameters of assistance techniques such as predictive mouse acceleration can have a significant impact on users' sense of agency.
The methods presented provide designers with new ways of evaluating and refining empowering interaction techniques and interfaces, in which users experience an instinctive sense of control and ownership over their actions.
In more recent years the sense of control - or "Experience of Agency" - has become the focus for a significant body of research in cognitive neuroscience .
Within this literature the experience of agency is defined as a person's innate sense of being in control of their actions and through this control of being responsible for, or having ownership of, the consequences of those actions.
Researchers in this field have developed techniques through which the experience of agency can be empirically investigated.
We believe that the experience of agency, as defined in cognitive neuroscience literature, is key to how people experience interactions with technology.
In this paper we consider how implicit measures for the experience of agency can be applied in HCI research and practice.
We provide a brief review of literature on agency and introduce two implicit methods through which it can be empirically investigated.
We then describe two experiments that investigate the sense of agency in simple human computer interactions.
The first experiment investigates the effect of a change in the input modality of an interaction on user's sense of agency.
It compares traditional keyboard input with skin or body-based input and shows that changes in input modality can have a substantial effect on the sense of agency.
The second experiment investigates what happens when a computer assists users in achieving their goals.
The results of this experiment suggest that up to a certain point, the computer can assist users whilst they still retained a strong sense of agency.
Beyond this point users experienced a significant loss in their sense of agency.
This paper makes several contributions to the field of HCI: 1.
It introduces two methods through which the sense of agency can be implicitly measured.
It demonstrates that changes in input modalities can have a substantial effect on the user's sense of agency.
It demonstrates that assistance from a computer can have a significant effect on the user's sense of agency.
It demonstrates an experimental means of mapping the personal agency characteristics of an assisted interaction technique.
The seventh of Shneiderman's Eight Golden Rules of Interface Design states that designers should strive to create interfaces that "support an internal locus of control" .
This is based on the observation that users "strongly desire the sense that they are in charge of the system and that the system responds to their actions".
The presence of this rule in Shneiderman's list reflects the critical importance of the sense of control in human computer interactions.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
It discusses the implications of these findings and provides an agenda for future research in this area.
From a more general perspective this paper demonstrates the benefits of applying methods derived from cognitive neuroscience to provide a more in depth understanding of interactions with technology .
We therefore start with a brief comparative review of diverse perspectives on agency in cognitive neuroscience and HCI literature.
In subsequent sections we will introduce an implicit means of measuring the experience of personal agency.
Before this, however, it is useful to briefly consider some of the diverse perspectives on agency in previous HCI research: Media agency: together with collaborators, Reeves and Nass have undertaken an extensive body of research on the `Media Equation' .
This work suggests that people treat computers as social actors  and respond to computers and other media in ways derived from their response to other people.
In HCI, the creation of user interfaces which include embodied agents  - photographs or animated characters - is one example of an explicit attempt to co-opt this response as a design resource that engages users and improves the interaction experience.
Intelligent agents/interfaces: in HCI contexts, components of intelligent user interfaces often demonstrate autonomous behaviours .
Such systems have the potential to blur the boundaries of user and machine agency and are the subject of the second experiment reported in this paper.
Design agency: semiotic analyses of user interfaces understand the designed product as a text, `read' by users as a message from the designer.
This understanding of the interface as the 'designer's deputy'  relies on the recognition of a design stance, in which the designer as an agent is recognised to have had a particular intent in the presentation of the product .
Agency in the laboratory: in science and technology studies, the status of machines in laboratories as either neutral instruments, or objects of human observation, draws attention to the way in which they have characteristics attributed to them through their participation in social processes of knowledge production and interpretation .
Suchman in particular has drawn rich connections between agency of robots and software agents in scientific contexts, and the political and philosophical implications of artificial intelligence, robotics and HCI .
It is important at this point to highlight a significant difference between the concepts examined in this paper and those examined in prior HCI research.
The Media Equation, for example, examines how users respond, or attribute characteristics, to computer and media agents.
In this paper we do not focus on attributions of agency to other people or things.
Instead we offer a new perspective, complementary to those described above, which focuses on how people experience their own sense of personal agency when interacting with technology.
That is, we have the capacity to intentionally bring about change in the world through our actions.
Humans also have a corresponding conscious experience of this capacity.
This is the sense of agency.
There is a distinction, then, between the fact of controlling an action and the immediate sense or experience of having done so.
It is the experience of ourselves as agents that allows us to say, "I did that".
Neurocognitive research on the sense of agency aims to shed light on the processes supporting this experience.
Broadly speaking there are two theoretical positions.
One view focuses on the motor system and suggests that the sense of agency is generated by neural process dedicated to the initiation and control of voluntary movement .
In this view, the sense of agency is based on the result of a comparison between the predicted and actual sensory consequences of movement .
If this comparison reveals no discrepancy then `I' am the agent responsible for the particular event.
The alternative theoretical position downplays the specific contribution of the motor system .
Here the sense of agency is taken to be a reconstructive inference that one's intention has caused an external event.
Within cognitive neuroscience much of the interest in the sense of agency stems from the fact that degradation of this experience is characteristic of certain psychiatric and neurological disorders.
One of the most striking examples is the passivity phenomena in schizophrenia.
Here, people feel that their actions  are not under their own control, and are instead under the control of some external force or agent.
For example, a patient described by Mellor  reports, "It is my hand and arm that move, and my fingers pick up the pen, but I don't control them."
Such reports highlight a profound disruption in the mechanisms supporting the sense of agency, the consequences of which can be immensely disabling.
However, the sense of agency is a remarkably fragile and malleable experience in all of us.
There are instances in which we feel an exaggerated sense of agency.
For example, people have been shown to place higher bets when they, rather than someone else, initiate a gamble, despite the odds being the same.
Building on the work of Libet et al.
Repeated experiments have shown that voluntary human actions are associated with systematic changes in our perception of time.
The interval between a voluntary or intentional action and the action outcome is typically perceived as shorter than the actual interval.
This is illustrated in figure 1.
If a person takes a simple action - e.g.
They are also likely to perceive the outcome of their action as having happened earlier than it actually did .
Haggard coined the term `Intentional Binding' to describe this phenomenon as it is contingent on several factors.
In the absence of outcomes people are found to more accurately report the timing of actions.
For the temporal binding effect to occur, an action must be intentional and it must lead to an outcome.
Under these conditions our perception of the timings of our own actions and of their outcomes become bound more closely together.
Haggard also demonstrated that unintentional actions - e.g.
In this case however, the perceptual change tends to occur in the opposite direction .
Now the interval between actions and outcomes is perceived as being longer than the actual interval.
In experimental settings involuntary actions can be induced in a number of ways, ranging from transcranial magnetic stimulation to moving a person's finger with a mechanical arm or even with a piece of string tied to their finger.
In the years since his first experiments , a large number of studies have been undertaken which validate and build on Haggard's initial observations .
The scientific consensus is now supports the conclusion that time perception in voluntary actions - and the binding effects associated with such actions - provides a robust implicit metric for the sense of agency.
Higher intentional binding values correlate to a greater sense of personal agency.
Baseline error The participant is asked to attend to the Libet clock and then press the button whenever they want to.
In this case the button press does not cause a beep.
The person reports the time on the clock  when they pressed the button.
The actual time of the button press is also recorded.
Baseline error = actual time - perceived time Active error The participant attends to the clock face and presses the button whenever they want to.
In this case the button press causes a beep.
The person is asked to report the time on the clock  when they pressed the button.
The actual time of the button press is also recorded.
Baseline error The participant attends to the clock face.
On this occasion the person takes no action and the computer randomly generates a beep.
The participant reports the time on the clock  when they heard the beep.
The actual time of the beep is also recorded.
Baseline error = actual time - perceived time Active error The participant attends to the clock face and presses the button whenever they want to.
In this case the button press causes a beep.
The participant reports the time on the clock  when they heard the beep.
The actual time of the beep is also recorded.
In order to investigate intentional binding, Haggard developed an empirical method though which the binding effect can be implicitly assessed.
He adapted Libet's clock method  for measuring peoples' perceived timing of events.
The Libet clock  Figure 2: a consists of an analogue clock face with Libet Clock an arm that rotates clockwise through a full cycle once every 2560ms.
The clock is typically displayed at the centre of a screen and is quite small relative to the overall screen size.
Each measure outlined above is repeated multiple times and the mean value for each is then used to calculate Intentional Binding.
A person sits in front of the screen and is asked to report where the hand on the clock is pointing when certain events occur.
The clock face is deliberately small so that the person can attend to the clock without making significant head or eye movements.
Using the example of a button press, table 1 illustrates the four measurements and the calculations used to measure the intentional binding effect.
In the active conditions there is a fixed action/outcome interval for all trials.
Each measurement is repeated multiple times  and mean time perceptions errors are then calculated.
These mean values are then used to calculate the action, outcome and total Intentional Binding values.
The Libet clock method has a number of strengths and limitations.
The most attractive feature is that it provides a robust measure of the total intentional binding effect.
It can also distinguish and quantify both action and outcome binding.
However, as it requires people to visually attend to a clock, this method is not suitable if the task being studied includes significant visual elements.
It also requires four blocks of measures for each action/outcome condition and is time consuming to administer if the experimental design includes a large number of action/outcome conditions.
Both methods make use of intentional binding, a phenomenon extensively examined in cognitive neuroscience literature, but not yet investigated or exploited in HCI research.
In the following sections we present two experiments in which these methods have been used to empirically investigate peoples' experiences of interactions with technology.
Once the device is attached to a person's arm they can control a computer by tapping different places on their hand or arm.
For the appreciative conference audience this paper gave rise to many questions.
In particular, one audience member asked: "So, what's it like to be a button?"
Whilst this question may appear frivolous at first sight, we believe it taps into an important issue for the HCI community.
In recent years researchers have developed a wide range of new interaction techniques and devices, investigating concepts such as embodied , affective  and brain-computer  interaction.
These new approaches offer the potential to dramatically reshape the experience of interacting with technology.
The question "what's it like to be a button?"
Inspired by the work of Harrison et al., our first experiment provides an empirical investigation of the question "what's it like to be a button".
More generally this experiment asks if changes in the input modality of an interaction can have an effect on the user's sense of agency.
As such, the techniques described in this experiment provide an empirical means of assessing and comparing the experiences provided by new interaction techniques.
We compare a traditional input device - a keypad - with a skinbased input device.
Subsequent to Haggard's initial work, Engbert et al introduced interval estimation , a second method through which the sense of agency can be implicitly assessed.
As with the Libet clock method, this method involves measurements of the perception of time for actions that cause an outcome.
In this case however people are simply asked to give an estimate of the time interval between the action and outcome.
Trials are again repeated multiple times, but here the interval between the action and outcome is randomly varied.
After each trial the participant gives their estimate of the time between the action and outcome.
This estimate is then compared with the actual interval and the interval estimation error is calculated.
This method is considered less robust than the Libet clock method.
Another limitation is that it cannot distinguish where a binding effect may be occurring; i.e.
However, it has the benefit of requiring just one measurement for each action/outcome condition under observation.
It is also suitable for use with a wider range of experimental tasks, including designs that involve significant visual elements.
Overall this method is best suited to studies comparing binding effects across experimental conditions, rather than studies aimed at robustly measuring binding for a given condition.
It is also well suited to studies comparing a larger number of experimental conditions - as is the case in experiment 2 below.
By comparing people's mean estimation errors in different interaction conditions it enables empirical comparisons of the sense of agency in different conditions.
For the purposes of this experiment we did not require a skin-based device as complex as that described by Harrison et al.
We did however require that our device be able to capture the user's actions reliably, with a high degree of temporal accuracy and without requiring a training period.
Our device, figure 3, bears a stronger resemblance to an earlier device described by Amento et al.
A piezoelectric contact microphone is placed on the user's arm and connected to a computer through a high-fidelity external soundcard .
Vibrations in the user's arm are monitored using this microphone and analysed to detect when the user taps on their arm.
The overall capture device and software is sufficiently robust that users can move their arms and hands without triggering false arm taps.
It has a temporal accuracy of less than 3ms.
This is equivalent to the temporal accuracy with which the MAC used in our experiment reports button presses from our USB keypad.
Figure 4 illustrates the overall procedure for trials in this experiment.
As the task undertaken by participants does not involve a significant visual component, we make use of the Libet clock method.
One independent variable is manipulated: the input modality.
In condition 1 - the button press condition - participants press a button on a keypad to cause a beep.
In condition 2 - the skin-based condition participants cause a beep by tapping on their arm.
The skinbased capture device is attached to the participant's left arm and they tap the inside of this arm with their right hand, , as in figure 3.
There is a fixed interval of 250ms between the user's action and the beep in both input conditions.
Participants sit at a desk, with both arms on the desk in each condition.
They face a computer screen showing a Libet clock.
To initiate a trial they press a footswitch.
The onscreen clock begins to rotate.
Participants are instructed to allow the clock to rotate for a short while and then make the action  whenever they want to.
They then report the time on the clock face for one of the four measures outlined in Table 1.
The trial was repeated 40 times for each measure, with specific measures blocked together.
A within-subjects design was used.
Across the two input conditions this gave 320 trials per participant .
The measurement blocks for each condition were grouped together and the order of the input conditions was alternated for odd and even numbered participants.
The order of the measurement blocks was randomised within the input conditions, but was balanced for odd and even participants.
This study was approved by the University of Cambridge, Computer Laboratory Ethics Committee.
Participation was on the basis of written informed consent.
The experiment lasted approximately 70 minutes for each participant.
Participants for this experiment were recruited through an email to a University bulletin board.
21 participants - all right handed and aged 20 to 40 - completed the experiment.
Three participants were excluded from our analysis.
In two cases our capture device did not work effectively.
A third participant incorrectly interpreted the experimental instructions.
Each participant received a 15 gift certificate.
Table 2 shows the action, outcome and total binding effects for the button press and skin-based conditions.
These values represent the overall means across 18 participants.
To test the statistical significance of our results we first conducted a 2x2 repeated measures analysis of variance comparing participants' action and outcome binding values across the two input conditions.
We then conducted a Bonferroni corrected paired sample t-test, with a null hypothesis of no difference, to compare the total binding values.
The result shows a significant difference in binding between the two conditions, t=4.05, p<0.01.
Overall 15 of the 18 participants experienced greater binding in the skin-based condition than in the button press condition.
Two further paired sample t-tests were conducted to compare the baseline errors for the two input conditions.
As the comparisons of baseline errors for the two input conditions indicate no significant differences, we can conclude that any binding differences between button and skin-based input are caused by differences in the active condition, i.e.
With a fixed delay of 250ms between actions and outcomes participants experienced higher binding on both actions and outcomes for skin-based interactions.
A total binding of 42.92ms was observed in the button input condition.
It is worth noting that this value is consistent with the results of prior binding experiments that have used button inputs .
In the skin-based condition participants experienced a total binding of 109.47ms.
Given the correlation between intentional binding and the sense of agency, this experiment allows us to conclude that people experience a greater sense of agency when they interact with technology via skin-based input, as compared with traditional keypad input.
We believe that the finding in this experiment has a number of important implications for HCI research.
It provides empirical evidence that new interaction techniques can provide different experiences of control and ownership to those offered by traditional mouse or keyboard interactions.
The methods described also provide an implicit means of quantifying this difference.
We chose to assess two dramatically different input conditions.
In future the methods described here can be used to assess and quantify the differences for a larger range of interaction techniques, including changes more subtle than those assessed here.
We could for example assess the difference in interactions with a touch screen via a stylus versus direct finger interaction, or differences in interactions that incorporate techniques such as haptic, embodied or physiological input.
In addition to comparing input techniques the method described here can be used to compare the different interaction experiences for a specific input technique when other conditions of the interactions change: e.g.
Greater consideration is given to these possibilities in our final discussion and conclusions section, but at this point we will move on to our second experiment.
Many user interactions with technology are more intermediate.
In particular `intelligent' user interfaces often seek to interpret and act on the intentions of the user.
The user's actions are voluntary, but the outcomes may be assisted.
Our second experiment is designed to investigate what happens to the user's sense of agency when a computer interprets their intention and helps them to achieve their goal.
Our experiment investigates agency in a machine-assisted point-and-click task.
We chose this task because pointing is a basic and ubiquitous action in user interfaces.
As a consequence, several pointing enhancement techniques have been designed and evaluated .
One of our primary concerns in designing this experiment was model simplicity.
As this is the first study of machine-assisted binding we chose a machine-assisting pointing model with as few free parameters as possible.
This allows us to manipulate the assistance given to participants, while at the same time having to change a minimum number of system parameters.
In our experiment participants use a mouse and are asked to hit targets on the screen as quickly and as accurately as possible.
Participants are assisted by an algorithm that has the effect of adding "gravity" to targets, causing the mouse pointer to be attracted to the spatially closest target.
Our algorithm models horizontal and vertical movement but updates them independently of each other.
Since the algorithm is onedimensional it is applied twice: once for the horizontal and once for the vertical update of the mouse position.
Figure 5 describes the overall procedure for trials in this experiment.
As this experiment involves significant visual elements we make use of the interval estimation method to implicitly assess people's sense of agency.
Participants sit at a desk facing a computer screen that is initially blank.
They press a footswitch to start a trial.
This causes a red start area to appear at the centre of the screen.
Participants use a mouse to move the cursor into the start area.
After waiting in the start area for a short period two green targets appear, one to the left and one to the right of the start area.
The targets are equidistant from the centre of the start area.
The participant is asked to hit one of the targets  as quickly and as accurately as they can.
Hitting a target causes a beep , with a randomised time interval between hitting the target and the beep.
The participant is then prompted to estimate the time between hitting the target and hearing the beep.
Two targets are provided  as this allows the participant to make a voluntary choice in each trial.
The computer has to interpret this choice before giving assistance.
Participants are given a number of important instructions about the targeting task prior to the experiment: * Participants are told that there is no difference between the two targets.
They can choose the target they want to hit in each trial.
They are asked not to hit the same target each time or simply alternate between the targets.
Participants are told not to move toward the green targets as soon as they appear.
Rather they are told they can wait in the start area for as long as they want, moving toward their chosen target whenever they are ready.
It is important to note that we are not primarily interested in the speed with which participants can hit targets.
Rather we are interested in differences in participants' interval estimates when our independent variable - the assistance level - is changed.
We tested four distinct levels of computer assistance: no assistance, mild assistance, medium assistance and high assistance.
In the no assistance condition the gravity algorithm is not used and the computer gives the participant no assistance.
In pilot studies we found that many people were unaware of the different assistance levels given in these conditions.
In both the mild and medium conditions the computer only assists the user if they are actually moving the mouse.
In the high assistance condition the cursor moves quickly to a target once the user leaves the start area, even if the user stops moving the mouse.
Our aim at this level was to make the assistance obvious to participants.
Given the potential for ordering effects in this experiment, we chose to have 24 participants, thus allowing full counterbalancing of the order of the four assistance level blocks.
In designing this experiment we following the procedure for interval estimation experiments described by Ebert and Wegner .
The experiment has a within-subjects design, with each participant completing one block of trials for each of the four assistance levels.
Rather than using an entirely random interval between actions and outcomes they recommend the use of three fixed intervals: 150ms, 400ms and 700ms.
Participants are told that the beep interval is fully random and that it ranges from 50 to 950ms in steps of 50ms.
Each of three intervals - 150m, 400ms and 700ms was randomly repeated 12 times per assistance level, giving totals of 36 trials in each block and 144 trials per participant.
Participants use an input box with radio buttons to give their estimates of time intervals.
Prior to beginning the experimental trials each participant undertook a short practice period involving 12 trials.
Participants pressed a button on a computer screen.
This triggered a beep and they estimated the interval between pressing the button and beep.
They were then told the actual interval.
In the practice trials the interval did vary randomly between 50ms and 950ms.
These trials served multiple purposes.
Firstly, they were intended to improve participants' accuracy by giving them a better sense of the brief intervals used in the experiment.
Second, by exposing participants to a full range of random delays, we aimed to reinforce their expectation that the experimental trials would also involve randomised delays.
This experiment was run on a desktop PC running Windows 7.
Participants used a Microsoft optical USB mouse.
The pointer sensitivity was set to the default level for Windows 7, with enhanced pointer precision switched off throughout all trials.
The red start area was 100 pixels high and wide.
The green targets each had a diameter of 35 pixels.
We used a 24-inch widescreen monitor with a resolution of 1920x1080 pixels.
The centre of each target was 810 pixels from the centre of the start area.
Whilst this experiment was primarily focused on an empirical assessment of the sense of agency, participants were given an opportunity to comment on their experiences.
Upon completion of each of the four blocks of trials an experimenter asked participants: "Did you notice any problems with that block of trials?"
This question was deliberately open-ended and did not make any reference to agency or assistance from the computer.
This study was approved by the University of Cambridge, Computer Laboratory Ethics Committee.
Participation was on the basis of written informed consent.
The experiment lasted approximately 50 minutes for each participant.
Participants were recruited through an email to a University bulletin board.
In all 27 participants - all right handed and aged 20 to 40 - completed the experiment.
Three were excluded from our analysis.
Two participants hovered over the targets for long periods  and one missed the target 18 times in one experimental condition.
This left us with 24 participants, with the order of the experimental blocks fully counter-balancing for the four assistance levels.
Each participant received a 15 gift certificate.
Table 3 shows the mean interval estimation errors and standard deviations for each assistance level.
These values represent the overall means across 24 counter balanced participants.
Lower estimation errors indicated a higher sense of agency.
To test the statistical significance of our results we first conducted a repeated measures analysis of variance that compared the interval estimation errors of participants across the four assistance levels.
Bonferroni corrected paired sample ttests were then conducted between the successive assistance levels, with a null hypothesis in each case of no difference in the estimation errors.
We do not find a significant difference between the no assistance and mild assistance conditions, t=0.036, p=0.97.
Similarly we do not find a significant difference between the medium and high assistance levels, t=0.419, p=0.679.
There is however a significant difference in estimation errors between the mild and medium assistance levels, t=3.08, p < .01.
They also indicate a similarly high sense of agency when the computer gave them a mild level of assistance, with no significant difference in participants' interval estimation errors at the no assistance and mild assistance levels.
When the assistance level was taken one step further, to the level we have termed medium assistance, results indicate a significant loss in the sense of agency, as indicated by positive interval estimation errors.
There was no significant difference between participants' estimation errors at the medium and high assistance levels.
Participants' comments in response to our queries about problems after each block of trials strongly indicate that they were aware of differences between the no assistance level and each of the three assisted levels .
This suggests that while participants were aware of the assistance from the computer in the mild assistance condition, they still experienced a high sense of agency.
This overall result has a number of important implications.
Firstly it shows empirically that, up to a certain point, the computer could assist users whilst also allowing them to maintain a sense of control and ownership of their actions and the outcomes of those actions.
Secondly it suggests that beyond a certain level of assistance users experienced a detectable loss in their sense of agency.
This experiment suggests that for this particular assisted input algorithm and possibly for assisted input systems more generally there may exist a tipping point or sweet spot.
This is the point at which a computer can help people and potentially maximise task performance - e.g.
A third important implication of this work is that the methods applied here can be used to map the personal agency characteristic of computer assisted interactions techniques.
Prior to undertaking this experiment our initial expectation was that - if assistance from a computer did indeed have a measureable effect on peoples' sense of agency - we would find a gradual effect, with successively higher levels of assistance leading to a successively lower sense of agency.
The results paint a more complex and compelling picture.
This experiment used interval estimation as an implicit metric for participants' sense of agency.
1, 2, 4, and 5 - would provide a more detailed mapping of users' sense of agency when interacting with the system.
Our primary aim in this experiment was to investigate if changes in assistance from a computer led to experimentally measurable changes in peoples' sense of agency.
However, as was the case with the first experiment in this paper, we believe the methods described here have a broad range of applications.
They could be used to compare different machine learning/user assistance algorithms.
They could also be used to refine the parameters of a particular assistance technique or algorithm.
As stated previously, we focused on simplicity in designing this experiment.
One of our choices was to have an experimental design in which it was extremely unlikely that the computer would incorrectly interpret the user's intentions and therefore hinder, rather than assist, them.
In future we believe it will be interesting to use the techniques described here to investigate the effects on people's sense of agency when `intelligent' systems make mistakes and incorrectly interprets users' intentions.
For example, what quantifiable impact does a violation of the user's intention have on their experience of control and, once lost, how long does it take before users recover their sense of control?
It may also be possible to implicitly investigate techniques that mitigate losses in the sense of control.
The methods described in this experiment can provide a valuable means of empirically addressing such questions and thereby help developers to make informed design trade-offs.
They will allow people interacting with technology to instinctively sense that "I did that".
Cognitive neuroscience research has demonstrated that the experience of agency is highly malleable.
In this paper we have demonstrated two specific ways in which changes in interactions can significantly affect the user's experience of agency.
We believe this work has tapped into a research area rich in possibilities.
We chose to focus on input modality and assistance from a computer, and have offered specific examples of the future research questions in each area.
However, a wide range of factors relevant to HCI research have been shown to have an effect on the experience of personal agency.
For example, distortions in feedback or inconsistencies across different feedback modalities can be detrimental  and it will be interesting to investigate and compare the impact of different feedback techniques on user agency.
Uncertainty and contingency  also have an impact and are worthy of investigation in the context of HCI research on uncertain control .
Implicit measures of the experience of agency will also be relevant to cross-population investigations of interaction experiences.
For example, studies suggest that this experience changes across the lifespan.
In particular older groups may experience substantial differences on their sense of agency .
It will be interesting to examine the design factors that maximise the sense of personal agency in such groups.
Giving the growth in HCI research on health and wellbeing, and in research that addresses mental health  and seeks to support people experiencing psychotic difficulties , we believe the methods described in this paper can also play a valuable role in understanding the unique requirements of these user groups.
Finally, this paper has focused on implicit measures for the sense of agency.
Research in cognitive neuroscience has provided robust evidence for the validity of these methods and they have the benefit of enabling researchers to empirically assess the experience of agency as it happens.
In future we also believe it will be interesting to combine these methods with subjective methods, in which users are explicitly asked to report on their experiences of control and ownership.
This will allow us to provide a deeper understanding of users' preferences regarding the experience of agency in human computer interactions.
Personal agency is a crosscutting experience essential to many aspects of our day-to-day lives.
It links to concepts such as control, ownership, responsibility and causality, and has a significant impact on how we perceive and interact with our world and with the people and things in it.
In this paper we have sought to provide a new insight into the experience of personal agency as it relates to interactions with technology.
In doing so we have drawn upon prior research in cognitive neuroscience and have introduced two implicit measures through which the experience of agency can be empirically investigated.
These methods have been extensively investigated in cognitive neuroscience literature, but, prior to this paper, they have not been exploited in HCI research.
Measurement and evaluation are key components of HCI research and practice.
Measurement enables comparison, which enables us to make systematic improvements and consider trade-offs in the design of systems.
Prior HCI research has provided a rich set of techniques through which many task performance aspects of system usability e.g.
In this paper we have demonstrated a means through which an underlying experience - the sense of agency - can be empirically investigated.
David Coyle's work was supported by an IRCSET-Marie Curie International Mobility Fellowship in Science, Engineering and Technology.
Per Ola Kristensson was supported by the Engineering and Physical Sciences Research Council .
Paul Fletcher is funded by the Wellcome Trust and the Bernard Wolfe Health Neuroscience fund and is part of the Behavioural and Clinical Neuroscience Institute.
