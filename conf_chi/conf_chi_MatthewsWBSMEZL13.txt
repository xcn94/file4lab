Online communities are increasingly being deployed in enterprises to increase productivity and share expertise.
Community leaders are critical for fostering successful communities, but existing technologies rarely support leaders directly, both because of a lack of clear data about leader needs, and because existing tools are member- rather than leader-centric.
We present the evidence-based design and evaluation of a novel tool for community leaders, Community Insights .
CI provides actionable analytics that help community leaders foster healthy communities, providing value to both members and the organization.
We describe empirical and system contributions derived from a long-term deployment of CI to leaders of 470 communities over 10 months.
Empirical contributions include new data showing:  which metrics are most useful for leaders to assess community health,  the need for and how to design actionable metrics,  the need for and how to design contextualized analytics to support sensemaking about community data.
These findings motivate a novel community system that provides leaders with useful, actionable and contextualized analytics.
However, attaining these benefits is no simple matter and an effective community leader is a critical success factor .
Because of their essential role in fostering community success, online community leaders are a growing population in enterprises.
However, there is a critical gap between these guidelines and the actual practice of facilitating valuable communities.
Leaders' unique role involves assessing community progress on goals and intervening to guide the group towards achieving those goals.
Static guidelines cannot help with such assessments; rather dynamic community analysis tools are needed.
However, there is virtually no research on tools for community leaders.
When community analysis tools have been built and evaluated, they are membercentric   or created for research purposes .
Only a few commercial tools explicitly focus on community analysis , but they report no motivating research or evaluations.
Furthermore, the large online communities literature proposes many possible metrics for assessing community health but we know little about which would be useful to leaders in practice.
Our aim is to develop and evaluate novel tools that facilitate effective leadership in online communities.
We present the evidence-based design and evaluation of Community Insights , a new tool to help leaders foster healthy communities that provide value to members and the organization.
In contrast to many prior tools, CI was codeveloped with leaders using an evidenced-based approach, not intuition about leaders' needs.
Online communities are becoming increasingly prevalent in enterprises, with easy-setup social software and increased awareness of benefits to businesses and employees.
These benefits include breaking down organizational and distance barriers to knowledge sharing and collaboration ; improved skills and ability to execute and retain staff ;
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
It outlines key information for community leaders or best practices that might inform possible tool needs.
We summarize key findings from these studies and their implications for designing leader tools.
Importantly, prior works don't provide explicit guidelines for designing tools, or predict key system features leaders will actually use and want.
Many design implications are derivable from prior work, but it is difficult to know which would actually be useful.
The present research helps identify which of these prior ideas are useful in practice.
Several authors discuss the importance of leaders assessing the health and success of their community , both to facilitate improvements and to justify continued funding of the community.
But success metrics proposed in prior work are rarely validated.
Iriberri and Leroy  survey community success factors and metrics.
Again, these works don't directly examine leaders' main challenges, motivate designs for leader tools, or observe how leaders use such tools.
Many authors suggest that identifying social roles helps members know whose posts to read.
A role taxonomy for Usenet groups is presented in : Newbie, Celebrity, Elder, Lurker, Flamer, Troll, and Ranter.
Social network analysis has identified key roles via "online signatures" .
Other work develops algorithms for identifying experts , who are recognized as important in communities.
We test whether and how identifying certain social roles is useful to community leaders.
Community analysis tools can be divided into three classes, depending on whom they are designed for: member-centric, researcher-centric, and leader-centric.
The design and evaluation of CI is our main contribution, addressing the limitations of prior work.
The most common class of community analysis tools is researcher-centric.
These include interfaces or algorithms to provide researchers with a deeper understanding of community behaviors, but which are not usable by endusers.
For example, History Flow visualizes Wikipedia article changes, enabling researchers to study author collaboration .
Other researcher tools identify emergent leaders  or experts , model the evolution of communities , and automatically rate contributions .
Member-centric tools include UIs for community members as their intended users.
For example, Netscan depicts "social accounting metrics" for Usenet to help members "form cooperative relationships by offering a better sense of the other players involved" .
They found active users were interested in author metrics, whereas light users focused on content metrics.
CI includes many similar metrics--top contributors, top posts, statistics about posts and people and detailed descriptive statistics about individuals.
However we examine the utility of these metrics for enterprise community leaders.
Other projects aim to increase members' understanding of social dynamics behind Wikipedia articles, to improve member's ability to interpret and trust the information presented .
Even though community leaders are not the intended users of these systems, they demonstrate the value of social accounting information for end-users.
We explore the social accounting information that enterprise community leaders want and their needs surrounding assessments of community health.
Research on leader-centric analysis tools is incredibly rare.
The work shows it is important to understand how moderators actually use tools.
The monitoring tool affected how moderators operated, allowing them to promote discussion: posting comments about content members were currently viewing.
Our work also seeks to explore how tools enhance the activities of enterprise online community leaders.
We are not aware of any leader-centric research systems, but a few commercial tools aim to support leaders.
Many commercial tools support web analytics, helping website owners understand visitors' actions on their site, emphasizing metrics like number of visitors, page views, and conversions .
Closest to our work is Lithium Customer Intelligence Center, which allows leaders to monitor customer communities' health .
It includes 6 health metrics--content value, number of members, traffic, responsiveness, interaction, and liveliness--and one combined metric--the Community Health Index .
However, the derivations and evaluation of these metrics are not reported.
Other commercial analytics tools have the same limitation.
We include similar metrics in CI  but evaluate their usefulness to leaders.
Participants were mainly experienced leaders , recruited because they had thought deeply about their role.
We also recruited a smaller set of inexperienced leaders , to gain a broader perspective into this growing leader population.
Only 4 out of 26 participants were managers.
This research was conducted in a global enterprise offering technology products and services to businesses.
The company widely encouraged employee leadership of, and participation in, internal online communities and made commercial technology, IBM Connections Communities, available to all employees.
All communities we studied used this tool, which enabled leaders to easily create a community space with various social tools like forums, blogs, wikis, files, and bookmarks.
As a result, there was a proliferation of communities and widespread membership, with 111,557 communities and 487,941 distinct members over four years.
Communities ranged in size from a couple to tens of thousands.
Many employees were members of multiple communities.
Leaders of these internal communities were the subjects of our research.
Communities assigned one of two designated roles to each person in their online space: owner or member.
Members can view and post content, but edit only their own content.
Owners have members' rights plus they can edit any content, add/remove members, and configure tools.
Formal community leaders, the subject of our research, are always designated as owners, although not all owners are formal leaders.
Throughout the rest of the paper, for simplicity we assume that owners are community leaders, using owner to refer to a system-designated community leader.
To inform CI's design, we conducted initial requirements interviews with 12 leaders.
Leaders were recruited in an online forum for people who led online communities of various types.
Our goal was to determine how to create leader tools.
We asked leaders how they enacted their role, the challenges they faced, and their needs.
Interviews were semi-structured, conducted by phone, lasting 60 mins.
They were audio recorded and transcribed.
Transcriptions were iteratively analyzed by 4 analysts for themes pertaining to our research questions.
Community leader needs concerned two major questions:  How healthy and successful is my community?
Leaders explained that they currently had very little support for answering these questions, as there were no community analytic tools made available.
They had many suggestions about what information would help them.
We identified 6 categories of information needs: understanding  people;  content;  participation;  sociability, how well the community fostered member relationships;  leadership, the impact of leader's actions; and  comparable communities, i.e.
The first three categories were discussed as the most critical needs by participants, so we describe these in detail.
What do members want of the community?
Who are the important people, e.g.
Leaders believed that answering these questions would help them understand dynamics, distribute information, plan events, and engage with particular people and sub-groups in the community.
What types of value are derived from content?
Does content match member needs?
Leaders wanted this information to assess the value of content and to provide better content.
What other activities and interactions are taking place?
Leaders saw participation as a strong indicator of health--and the community's ability to achieve its goals.
Our research involved 26 participants: 9 for requirements interviews , 14 for evaluation interviews , and 3 who participated in both.
CoPs tended to have more leaders who posted less often; team communities had fewer leaders who tended to post more and be explicitly assigned to their role .
For 13 participants, leadership was a formal job responsibility.
The CI design was inspired by these leader interviews.
The final three-- sociability, leadership, and comparable communities-- were seen as less critical by leaders so were left for future work.
Even within these three categories, prior work has suggested many types of information that might be proFigure 1.
Community Insights screenshot of the Participation page for the "Mac" technical support vided.
Navigation to the People and Content pages is in the left bar.
CoP leaders were interested in jobs effectively?
We could not rely on prior work to tell us interaction around end-user generated content, similar to what information was useful because most tools have not been evaluated with leaders, and because our leaders prior work .
This was a secondary goal for teamworked in an enterprise rather than an internet context.
For based communities, which primarily focused on publishing example, if we relied on prior literature, we might have team relevant information.
Leaders of both communities focused on disruptive member behavior .
However, types were interested the same information--leading to enterprise community leaders do not have this problem.
More critically, unlike prior work, leaders weren't just The interviews enabled us to initially prioritize the many interested in passive understanding.
Instead they wanted information to inform actions to improve their community.
Identifying the most information: particular people or sub-groups for proactive useful subset of metrics in the enterprise represents a contribution of this work.
1 leaders but discussed at length in prior work.
To ensure that CI protected members' privacy and complied with privacy policies enforced by different countries, we consulted with an employee data privacy expert in Europe, where employee privacy laws are the strictest.
We modified our interface in the ways this expert suggested to ensure we upheld community members' privacy.
In the following sections, we describe the metrics in more detail to illustrate support for leaders' two major needs:  assessing community health and value and  determining actions they could take to improve community health.
Interview participants wanted information to assess community health.
CI supported this by including metrics that most leaders emphasized as key health indicators.
These four metrics represent the top requests related to community health from participants.
However, participants wanted to go beyond simple health metrics to assess the value of the community for members' everyday work.
We therefore computed metrics for value of posts and most valuable posts.
Value of a post was modeled by a linear regression function combining number of views, comments, and recommendations each post received.
Precise weights were determined from community feedback, as members rated in a survey various posts for the value each offered to their job.
This is similar to  except we asked our raters to focus on extrinsic value of posts in supporting their job.
Posts receiving many views, comments, and/or recommendations were ranked higher than those receiving only a few.
Our requirements interviews indicated the importance of providing leaders with guidance on actions to take to improve community health and value.
However the optimal action might depend on the state of the community.
For example, if a community is losing members, leaders want to diagnose why, to reverse the trend.
Based on participant feedback, we developed several ways to support remedial action.
While other research and commercial systems propose metrics in the people and content categories , those systems don't explain how this information might be used to change community behavior.
While prior research argues for the utility of similar information to certify content or expertise , leaders wanted role information for additional purposes.
They wanted to identify and engage with members who might volunteer to generate new content, participate as event presenters, or lead initiatives.
They wanted to identify topics or high interest content to promote it in a newsletter or solicit similar content.
Thus, we designed CI to profile particular people, content, and topics of interest, but with the goal of making this information actionable.
The top contributors metric leveraged the value of posts metric already described.
We analyzed each contributor's posts, ranking their aggregate value using the value of posts metric.
To provide actionable information, we designed profiles for the top ten contributors, shown in Fig.
Profiles help leaders identify core members and potential volunteers, along with tasks the leader might ask the member to do.
For example, the profile suggests topics of expertise upon which the person might present.
Leaders wanted to identify and communicate with particular community sub-groups.
This included people from a particular geographic region or business unit, those recently joining or leaving the community, and high status people like managers.
The People page helps leaders identify such sub-groups via the geographic locations, business unit, managers/not and membership change widgets.
More importantly we facilitate leaders engaging with sub-groups by clicking on elements in these widgets.
The list enables the leader to easily contact the new members, as suggested in prior literature .
Leaders requested guidance on actions to help them foster more healthy and valuable communities.
CI includes a "help & tips" page for each metric offering concrete suggestions for interpreting and acting upon it.
Such action-oriented tips set CI apart from more passive analytic systems, which present data without suggesting how users can act upon it.
Tips were seeded by the research team using ideas taken from leader interviews and prior work.
Users could also submit their own tips to share with other community leaders and recommend tips .
An example of a membership change tip is shown in Fig.
Leaders argued that static data was not as useful as trends, which better indicate when and how to act, e.g., reversing a negative trend, continuing activities leading to a positive trend, etc.
In CI, we show trends where appropriate.
However, participant feedback and later data analysis showed that community maturity affects trends.
For example, membership change is tiny in established communities.
Thus, we designed a mix of data to illustrate significant trends for different stages of community development.
Leaders of established communities wanted to know about changes in members' topical interests; thus we show most valuable posts in the last month, as well as of all time.
Similarly, we show top contributors not only of all time, but also in the last month and last week, an iterative addition based on user feedback.
In both cases, this allows leaders to assess current behavior in the context of the entire history of community interaction.
CI is an interactive web application: its user interface is implemented with the Dojo toolkit  and d3.js .
The backend is a Java application over a relational database .
Data from Communities is only accessed during a batch, import process occurring once a week.
The database process models and provides the metrics and analytics needed for the system.
Number of page views and unique users of CI pages, and use of widgets .
Our interviews showed that leaders were successful in using CI for both major design goals:  assessing health and value and  determining actions to improve community health.
For each, we describe which metrics were most useful.
However, we also discovered that understanding community data is incredibly complex, leading to further design iterations.
In the rest of this section, we present usage and interview data about whether and how leaders were able to interpret and act on CI information.
We deployed CI on our company's intranet in late-October 2011, including data from 45 communities.
We advertised the deployment: announcing it on internal social software, notifying community leaders in the company, and demoing it to community leaders.
Leaders added their communities to the system throughout the deployment, which is ongoing.
In late-August 2012, CI included 470 communities with 2532 unique owners and an average of 9 owners per community.
We began logging in February 2012-- recording each action , when the action happened, by what user, and for what community-- collecting 6.5 months of CI usage data.
In assessing health, leaders most often discussed participation data: views, posts, contributor type, and overall activity.
Table 1  indicates how often users actively examined these.
Instead our data shows that member views was by far the most used health indicator.
Leaders wanted consumption data focused on participant demographics; i.e.
However, leaders aired reservations about simple, one-sizefits-all health metrics, pointing out the need to consider specific community goals when measuring success.
We conducted 1-hour evaluation interviews via phone with 17 active leaders.
Participants were chosen to be those who frequently accessed CI  and represented communities of different sizes, ages, and types.
As of the writing of this paper, they had accessed CI pages an average of 176.7 times on 14.8 distinct days for 4.9 distinct communities.
Participants viewed CI while we asked semi-structured questions about health, action, and understanding.
Health: In what ways did leaders assess community health using CI?
Which metrics were most useful?
Action: In what ways did leaders analyze and act with CI?
Which metrics were most useful?
Understanding: What issues did leaders experience interpreting the community data CI presented, if any?
Detailed notes were taken during sessions and analyzed for themes pertaining to these research questions.
Leaders were not just interested in passively understanding raw data.
Instead CI successfully helped community leaders determine remedial actions.
The most commonly discussed actionable information were people data .
We now discuss four types of information participants report led to action:  Identifying particular sub-groups or members for active engagement.
Leaders wanted to intervene most often to engage with specific subgroups or members, e.g.
Leaders were particularly enthusiastic about the top contributors metric, using this list to engage with the most popular contributing members to take on informal leadership roles, elicit their feedback, acknowledge stellar contributions, and evaluate their contributions to a team.
To better support this, leaders asked for new analytics to identify other useful types of people such as "experts," "influencers," top contributors from particular geographies, frequent consumers in particular job roles, and so on.
Leaders actively used CI to diagnose and address current or potential community health problems.
Interpreting analytics was a major challenge for leaders.
We heard many comments like "How does 13% contributors compare with other communities?"
We knew from our initial interviews that interpretation of analytics was critical, so we designed the "help & tips" to better support interpretation and action.
However, help was not enough.
Leaders needed contextualized analytic support for sensemaking, i.e.
We therefore worked with leaders to identify their main types of sensemaking activities:  Compare with meaningful general baselines: compare my community's data with `good' and `bad' thresholds derived from norms for the enterprise.
To better support this, leaders asked for more sophisticated analyses of activity patterns to identify potential problems.
They wanted guidance on metrics relating to their particular goals, since CI targets communities with a variety of goals.
Participants were constantly looking for successful precedents to emulate to improve their own community and leadership skills.
So then I can go back into my community... and look at it and when I compare it with those forums that I created this month, it reveals whether it was the language of the forum, or if it was the aesthetic beauty of the forum that really made the difference, or was it the positioning of the forum that made the difference.
So that comparative study that I could do is really important."
I might want to contact other community managers who have higher rates of engagement / posting than I do to get some ideas on how to improve participation rates."
Leaders examined CI to evaluate the effects of particular actions or events to help with future planning.
Most leaders organized regular events involving phone conferences or presentations that members could attend.
Leaders logged how many people attended, downloaded event materials, discussed the meeting, and so on.
Interpretation was a challenge for initial interview participants in early email feedback.
However as we have already seen, CI did enable leaders to relate trend data to particular events and interventions, where leaders explored changes surrounding those events against baseline data.
Trend data also enabled leaders to spot exceptions, which appeared as spikes and dips in time-based charts, for example, the spike in membership in Fig.
We iteratively updated CI to support some of the other types of sensemaking activities.
This change met with positive feedback from users: "One more thing I `love' on the tool:
While prior work on community building guidelines describes multiple, often conflicting metrics and actions that leaders might take to create "successful" communities, it does not usually evaluate these or link them to system support.
Our results suggest the need to develop new metrics focused around achieving specific actions within the community, rather than focusing on overall growth measures.
These empirical observations also have direct design implications for tools to help leaders.
New types of action-oriented analyses: We were able to support action-oriented analyses by implementing `tips' about how leaders could exploit community analytics.
In these tips, we guided leaders to the best metrics for identifying: particular members and sub-groups for leaders to engage; problems that require leader remediation; successful examples for leaders to emulate; and ways to evaluate particular events in the community to help future planning.
We have also developed algorithms for assessing the value of content and ranking contributors.
Contextualized analytics: Prior work has not provided or evaluated tools for leaders.
It therefore neglects to discuss critical challenges in helping leaders make sense of community data.
To address this, we developed contextualized analytics, data presentations that provided adequate context to support interpretability, and proposed six instances.
Furthermore, we followed this work with the development of new metrics and visualizations to enable leaders to compare their community with others and explore how their actions have impacted member activity.
These new features have been evaluated in a separate study .
In the future, CI could support further empirical and theoretical work on communities.
For example, prior literature has many proposals on tactics to enhance interaction and participation.
CI might provide an evaluation environment in which researchers explore the effects of specific interventions on participation.
This would help systematize the currently complex and often competing recommendations in the literature around actual data.
CI might also help advance systems work.
Currently we have developed our own algorithms for identifying valued content and top contributors.
However, other researchers have developed similar algorithms using machine learning methods and evaluated these using corpus based methods.
In future work we might explore actual user reactions to competing algorithms in the context of a working tool, CI.
New research is also needed to explore extrinsic metrics, e.g.
Systematically evaluated leader-centric community tool: We have iteratively deployed CI with community leaders from 470 communities over 10 months, gathering detailed feedback.
Despite the importance of proactive leader focused efforts , as far as we are aware CI is the first leader-centric community tool, and certainly the first one to be actively co-developed integrating leader feedback.
New empirical findings: Our empirical contributions are to  re-evaluate metrics using leader feedback to determine their utility,  show that leaders want actionable data presenting the interventions they want the system to inform, and  show that raw data is inadequate to support sensemaking so contextualized analytics are needed.
These empirical findings are embodied in our system.
Finally our interviews suggested a major difference between enterprise and internet communities as we saw few examples of misbehaviors, e.g.
Rethinking metrics - less generic and more actionable: Prior literature emphasizes growth in both volume of posts and community size as key health metrics .
In contrast our interview and usage data show that leaders were most concerned to see views as a health indicator.
Overall however, leaders doubted that simple metrics alone were particularly useful, as communities have very different goals overall and are at different phases of their existence .
We developed a novel user tool created via extensive userled design that enabled leaders to better assess community health.
We discovered critical design characteristics for such tools: they need to support action-oriented information and provide contextualized analytics.
Further, we advanced theories about community evaluation metrics.
In future work we will extend our tool to contribute to theory and practice in this critical area.
