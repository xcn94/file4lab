In text entry experiments, memorability is a desired property of the phrases used as stimuli.
Unfortunately, to date there is no automated method to achieve this effect.
As a result, researchers have to use either manually curated Englishonly phrase sets or sampling procedures that do not guarantee phrases being memorable.
In response to this need, we present a novel sampling method based on two core ideas: a multiple regression model over language-independent features, and the statistical analysis of the corpus from which phrases will be drawn.
Our results show that researchers can finally use a method to successfully curate their own stimuli targeting potentially any language or domain.
The source code as well as our phrase sets are publicly available.
In contrast, today text is entered into mobile devices in many different languages, where text entry methods might perform very differently .
This fact evidences the necessity of an adequate sampling method, aimed at exploiting the huge amount of text corpora available in many languages.
We present a method for sampling memorable and representative phrase sets, based on a multiple regression model over language-independent features, so that it can generalize to other languages, and the statistical analysis of the  corpus from which phrases will be drawn.
An interesting property of our method is that, being data-driven, phrases may contain unusual vocabulary as long as it is representative of the task or domain.
Our method is validated in two user studies, showing that researchers can now gather their own stimuli for a given language or domain.
In text entry experiments, participants are prompted with phrases  that must be entered as quickly and accurately as possible.
Although it may seem more natural to have users enter free text and increase thus the external validity1 of the experiment, it is critical to make the text entry method the only independent variable in the experiment, and increase thus its internal validity.2 Indeed, if users were asked to type "as fast as possible" they would introduce rather biased text.
Hence, researchers typically use pre-selected phrases, measuring the dependent variables  in a text-copy task.
This eliminates noise and facilitates the comparison of text input techniques.
In general, copy-tasks should prefer memorable stimuli .
Unfortunately, to date there is no automated method to achieve this effect.
For the past decade, text entry researchers have predominately used the MacKenzie and Soukoreff phrase set , which contains 500 phrases that were manually selected according to three criteria: moderate in length, easy to remember, and representative of general English.
More recently, Vertanen and Kristensson  released a phrase set based on genuine mobile emails.
In contrast, repositories like the Linguistic Data Consortium, Data Wrangling LLC, or the ELRA catalog provide a plethora of large multilingual corpora that could be curated to automatically build phrase sets tailored to specific tasks or languages.
Paek and Hsu  devised a procedure for creating representative phrase sets by randomly sampling sets of n-grams and choosing the set with less entropy with regard to the original dataset.
Although mathematically sound, this procedure does not guarantee that sampled phrases are memorable.
Moreover, the phrase set generated in this way  contains incomplete sentences and near two thirds of the words are out of vocabulary,3 sometimes with extremely unusual punctuation symbols.
This might pose a threat to the internal validity of text entry experiments.
In a text-copy task, phrases can be briefly shown at the start or left visible throughout.
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Copyright is held by the owner/author.
Publication rights licensed to ACM.
Longer phrases are generally harder to process.
The higher this ratio, the harder the phrase is to process.
Shorter words are easier to process.
Higher variability leads to higher processing effort.
Lower perplexities may indicate that the phrase is easier to process.
Phrases with high probability are likely to be more usual expressions and so they should be easier to memorize.
Ideally, to compute OOV, Ppl2, Ppl3 and LProb as accurately as possible we would need full knowledge of the "universe" of the target language or domain.
Being this impossible, we resort to using a sufficiently large corpus U and consider that such corpus is descriptive of the desired language, task, or domain.
In our experiments, U is the N EWS corpus4 .
Note that C U , otherwise it would lead to over-trained estimations for the phrases in C .
Genzel and Charniak  postulated in their "entropy rate" principle that speakers tend to produce sentences with similar entropy, so that they can be easily understood.
Therefore, too informative sentences  should be harder to process and, in consequence, less memorable.
However, entropy has been shown to be a weak predictor of processing effort, being the latter better correlated with word length and word frequency .
In general, shorter and frequent words take less time to read and therefore are easily understandable .
These observations are key to our work.
We are interested in a sampling method to select, from a fairly large text corpus C , those sentences that are good candidates for conducting text entry experiments.
Such a method should select phrases that are: 1. representative of the task, domain, or language, for ensuring the external validity of the experiment; 2. memorable, for ensuring internal validity; 3. complete, since fragmented phrases are often confusing.
We assume that the corpus C contains text that users are likely to write in a real-world context.
So, a statistical analysis should uncover commonalities of C , such as phrase length or word frequencies, and the sampled subset should present similar characteristics.
Then, representativeness and memorability can be pictured as antagonist forces, since e.g.
For instance, a dataset containing only phrases like "this is it" or "and so on" would not be very appropriate for conducting a text entry experiment.
We therefore devise a single-pass procedure where each phrase is assigned an expected memorability value that is compensated with a representativeness score.
The CER model was fitted according to generalized linear regression , since it allows for any distribution of the model features.
We suspected that not all of the features described above, besides being intuitively useful, would be strong CER predictors.
For instance, linear models assume that factors are independent of each other.
So, in order to decide the best features for the model, we used the Bayesian Information Criterion  approach .
BIC was chosen over other criteria in the literature because it tends to build a simple model and converges as the number of observations increases.
We trained the model with the data released by Vertanen and Kristensson , which was drawn from the E NRON M OBILE dataset and provides 22,390 CER-labeled sentence memorization tasks completed by 386 MTurk workers.
All workers were from the United States and India, either native speakers or having a competent English level.
Workers had to memorize a sentence and then type it after pressing a continue button.
We first looked at readability tests such as the Coleman-Liau index  or ARI .
However, they were designed to gauge the understandability of whole documents and are overly simplistic; i.e., they only contemplate sentence and word lengths, and systematically retrieve extremely short phrases.
Fortunately, the accuracy  of a transcribed sentence, measured as the character error rate , is widely accepted to be a good proxy of memorability .
Therefore, we focus on modeling CER to predict memorability.
Together with the previously discussed observations, we will attempt to express CER as a function of language-
Eventually 2,390 "data points" were considered.
BIC revealed that the significant features to predict CER were only Nw, OOV, SDchr, and LProb.
As observed, Nw, SDchr, and OOV have positive weights, implying that CER increases when sentences get longer, words are more infrequent, and words have a highly variable amount of characters.
Conversely, LProb having a negative weight implies that the more likely the sentence, the less prone users are to make mistakes.
In addition, OOV receives a much higher weight than the rest of the features, indicating that infrequent words are specially correlated with high CER values.
Moreover, PC  was set to that of M AC K ENZIE, resulting in phrases of 5-6 words, since estimating i for Nw according to E UROPARL would be heavily skewed and therefore would tend to retrieve the longest sentences.
Even though such sentences would be representative of this specific topic , they may not be reflective of everyday language and thus would not be very appropriate for conducting text entry experiments, where memorability is important.
We recruited 20 native Spanish speakers aged 28-38 using the available University's mailing lists.
All participants had a qualified intermediate or advanced English degree according to CEFR.5 Each participant was shown a phrase for 5 seconds or until the first keystroke, whatever happened first.
Next, the phrase disappeared and users had to write it  with a physical QWERTY keyboard.
Participants entered 20 phrases in a randomized order from each dataset, resulting in 1,600 annotated phrases in total.
The results are show in Table 2.
Together with CER , we report the words per minute  to give an overview of the participants performance in terms of input speed.
We also report the time since the phrase was loaded until the first keypress , which provides an estimate of the time spent memorizing each phrase.
Presumably, selecting sentences with the lowest CER estimates would yield the most memorable ones, although such sentences would end up being those with few and short words.
To compensate this effect, we also need to ensure that sentences are representative either of general language, the sentence corpus, or the desired task for the text entry experiment.
To achieve this, we estimate the empirical probability of the features present in Eq.
Then, the representativeness of a sentence is given by:
Moreover, adjusting the metaparameters i and i allows text entry researchers to finetune the kind of phrases that will be eventually used in the text entry experiment.
Finally, since we want to retrieve phrases with high memorability  and high representativeness , we define the final score assigned to a phrase by the following expression:
Post-hoc pairwise t-tests  revealed that the phrase set derived by our method compares favorably to state-of-the-art phrase sets , and that participants performed significantly worse in the random sampling condition, both in terms of CER and memorization time.
Overall, we observed more variability in our data in comparison to previous literature , which motivates the need to provide participants with sentences in their native language.
In light of the previous study, we generated 3 phrase sets of 500 sentences each by tapping into the Spanish version of E UROPARL.
We used random sampling, the n-gram sampling procedure , and our method.
In all cases, we used lowercased phrases of 3-10 words with punctuation symbols removed.
We then repeated the same experiment with the same participants.
The results are show in Table 3.
We tapped into the M AC K ENZIE dataset  and the N GRAM dataset .
Both datasets have been reported to be memorable by native English speakers , so we replicated the analysis with nonnatives.
We also sampled 500 sentences at random from the public E UROPARL dataset , and 500 more following our method.
Post-hoc pairwise t-tests  revealed that the random and n-gram conditions performed equally similar, and that our method performed significantly better than the other sampling procedures.
This study shows that our method generalizes well to Spanish, which is a language quite different from English.
Many text entry methods require constant visual attention, such as eye typing or dialing a contact while driving.
For experiments trying to emulate these or similar situations, memorability is critical since it can be difficult for participants to consult often the reference text.
Memorability is also desirable to unburden the participants and let them exclusively focus on the text entry method.
Until now, sampling methods aimed to select "representative" phrases, but memorability was largely ignored.
This work therefore significantly contributes to HCI by making it possible to curate large text corpora in potentially any language and any task or domain.
Our findings are of special relevance to text entry researchers interested in conducting experiments tailored to the linguistic capabilities of their participants.
Memorability was found to be correlated with sentence length, word variability, word frequency, and ratio of infrequent words.
In sum, shorter phrases with frequent vocabulary are easier to remember.
These findings were consistent in all of our experiments, and ultimately are aligned to previous key findings in the literature.
Table 4 provides an overview of the type of phrases that can be deemed as being either of good or bad quality, as scored by our method.
This illustrates a means to filter desired phrases from the phrase set used as stimuli, in order to ensure the validity of the text entry experiment.
We have shown that our method can generalize to different domains and languages different from English.
This provides text entry researchers with a scalable and unprecedented capability.
However, it must be noted that our features may not be applicable to every possible language.
For instance, Chinese words are not even formed by individual letters.
Therefore, there is still an opportunity for future work.
Thus one possibility would be experimenting with other memorability predictors, which could lead to better models that would in turn improve our sampling method.
We hope that this work will be suitable for use in a variety of text entry evaluations.
Coleman, M., and Liau, T. L. A computer readability formula designed for machine scoring.
Danescu-Niculescu-Mizil, C., Cheng, J., Kleinberg, J., and Lee, L. You had me at hello: How phrasing affects memorability.
Genzel, D., and Charniak, E. Entropy rate constancy in text.
A theory of reading: From eye fixations to comprehension.
Keller, F. The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data.
Kristensson, P. O., and Vertanen, K. Performance comparisons of phrase sets and presentation styles for text entry evaluations.
MacKenzie, I. S., and Soukoreff, R. W. Text entry for mobile computing: Models and methods, theory and practice.
MacKenzie, I. S., and Soukoreff, R. W. Phrase sets for evaluating text entry techniques.
A., and Weddrburn, R. W. M. Generalized linear models.
P. Sampling representative phrase sets for text entry experiments: a procedure and public resource.
Schwarz, G. E. Estimating the dimension of a model.
Vertanen, K., and Kristensson, P. O.
A versatile dataset for text entry evaluations based on genuine mobile emails.
