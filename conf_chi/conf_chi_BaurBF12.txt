Virtual Projection is inspired by its optical counterpart for transferring information between handhelds and stationary displays such as tabletops, PC displays or large public displays.
By fixing the virtual projection to the display, the frustum can also be used to  select regions,  interactively apply filters, and  post multiple views.
ABSTRACT overcome screen-space limitations on handhelds , navi-
Handheld optical projectors provide a simple way to overcome the limited screen real-estate on mobile devices.
We present virtual projection , an interaction metaphor inspired by how we intuitively control the position, size, and orientation of a handheld optical projector's image.
VP is based on tracking a handheld device without an optical projector and allows selecting a target display on which to position, scale, and orient an item in a single gesture.
By relaxing the optical projection metaphor, we can deviate from modeling perspective projection, for example, to constrain scale or orientation, create multiple copies, or offset the image.
VP also supports dynamic filtering based on the projection frustum, creating overview and detail applications, and selecting portions of a larger display for zooming and panning.
We show exemplary use cases implemented using our optical feature-tracking framework and present the results of a user study demonstrating the effectiveness of VP in complex interactions with large displays.
One of their appeals is the simplicity of interaction: Aiming at an appropriate surface projects the image, and changing posture and direction adjusts the image's position and orientation.
This behavior is based purely on optics, allowing us to intuitively grasp it, based on our own experience with the physical world.
Naturally, the laws of physics also bring drawbacks; for example, that the projected image is tightly coupled to the projector's movement.
In1this paper, we apply the metaphor of optical projection to digital surfaces in the environment.
We use a handheld device, tracked in 6 DOF, to support Virtual Projection  on one or more displays .
The simulated nature of VP allows us to flexibly adjust the parameters of optical projection, avoiding unwanted distortions, jitter, and intensity variations, and eliminating the need to continually point the projector at the surface on which it is projecting.
This also frees the frustum, so that it can be used for selecting areas, either for navigation or for applying filters.
Our work makes several contributions:  We explore the implications of VP as an interaction technique and show how decoupling the projection from the projector and adjusting transformations can improve interaction.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
VP applies selected aspects of the device's pose to the image transferred to a display:  opticalprojection transformation with keystone distortion,  optical-projection transformation minus keystone distortion,  translation and scaling only,  translation only,  full-screen .
As shown in Figure 1, VP can mimic the behavior of optical projection on digital surfaces.
Aiming a tracked handheld device at one of the system's secondary displays creates a simulated projection; moving the handheld controls the projection's position, size, and orientation--all with a single gesture.
To support this, we track the handheld in 6 DOF, based only on the device's camera.
Not using an external tracking system avoids the need for additional environmental infrastructure, allowing VP to run on any suitable mobile device with a live video camera and any available displays running the system.
Handheld : The handheld device from which a VP originates.
Frustum: The projection frustum originating from the handheld and aimed at a secondary display.
View: The data and user interface shown on a handheld device and/or within a VP.
Views on a handheld are always displayed full-screen.
Active View/Active VP: A view that is currently shown on both handheld and secondary display.
Only one view can be active on a handheld at one time, while a secondary display might show additional inactive views.
Fixed VP: A VP that is fixed to a secondary display and, thus, decoupled from the handheld.
Since projection is simulated in VP, we can modify the transformation at will, with full control over the results.
Our implementation supports five different variations, depicted in Figure 2:  Fully simulated optical projection with distortion is desirable only for a small set of use cases.
Which of these variations is best depends on the target display type, as well as the application: For example, fixing orientation on a vertical desktop makes sense, as does scaling maps or background images to be full screen size.
The rigid coupling between a handheld optical projector and its projection requires that the user aim where the projection is to appear.
In VP, the handheld is not the source of the projection's light, so we can completely decouple the virtual projector from its projection at any time.
We allow the user to break the coupling, fixing the projection to the secondary display.
Users can also regain control when desired.
In our implementation, a long-press gesture on the handheld's touchscreen creates or reacquires a projection.
Releasing the touch fixes the projection or deletes it if `dropped' outside the display area.
When a user leaves the environment, projections that are still fixed to the display remain behind.
Decoupling projector and projection has two benefits:  Users can create multiple projections and place them sideby-side, one at a time.
The frustum can manipulate content, either in a VP  or on the handheld .
With multiple VPs on the secondary display, the frustum  can be used to select a VP and activate it on the handheld.
By simulating projection, VP enables two extensions to the optical projection metaphor: adapting perspective transformation and decoupling projector and projection.
The tight coupling between projector and projection introduces distortion when the central axis of the projector frustum is not perpendicular to a planar projection surface.
This effect is often referred to as keystoning and results in a wedge-shaped distortion of the ideally rectangular image in one or both axes.
Tracking mechanisms based on cameras and fiducials  or other sensors  have been used to correct this for optical projectors by determining the transformation between the projector and the room.
VP walkthrough:  Shaking the device to create a view.
Figure 3 shows a walkthrough of our user interface:  To create or change to an existing view, users shake the handheld to open the view manager.
This area is tightly coupled to the handheld and moves accordingly.
Shoot & Copy eliminates the need for fiducial markers to determine the spatial relationship between the handheld and display through content arrangement .
Deep Shot further allows arbitrary screen content by matching the captured image to the remote display's content .
Each of these systems, however, only allows for discrete selection, instead of continuous interaction.
In contrast, Pears et al.
Like DeepShot, Herbert et al.
While these techniques allow area selection instead of using the center only, they all require a static display subregion.
VP builds on this earlier work and represents a major advance.
VP  tracks the handheld's spatial relationship continuously, instead of using a single image ,  needs no additional markers, unlike Pears et al.
Previous work supports pointing at secondary displays with handhelds.
Relative and indirect pointing is used for mouselike control of a cursor on the display.
Nevertheless, relative techniques require users to track a cursor visually.
Absolute and direct pointing techniques avoid using pointers .
Early systems combined stationary projectors with tracked mobile devices to simulate mobile projection .
PenLight  simulated a pen projector by tracking a pen and projecting on the table where the pen pointed.
These systems have a constrained projection canvas, but can conceptually be extended to any surface, in the spirit of Everywhere Displays .
Pico-projectors are now small enough to be embedded in handhelds.
Preprint version - Full version will be published by ACM in the proceedings of CHI 2012, Austin, TX, USA handheld's local content  or augmenting external information .
However, built-in projectors  are tightly coupled to device motion, introducing jitter and image distortion.
Tracking the handheld and its projector can reduce this .
Cameras are also used to track a projector in space: Raskar et al.
Mobile projection systems make possible interesting interaction techniques.
MotionBeam uses the projector's movements for manipulating virtual characters .
Bonfire takes mobile projection even further: users directly interact with the projected image using touch .
In each of these systems, however, the mobile device's alignment is always crucial, while VP allows decoupling projector and projection.
When replicated, the VP contains an exact or scaled copy of the handheld's content.
This case is typical of real handheld projectors .
A VP may also show an extended version of the handheld's content to better utilize available screen space .
Both the VP and the handheld view can be linked.
That is, both devices show related but different views of the same content .
Finally, the VP and the handheld's view can be independent, with no relationship between them .
Projections create a new display that can be used to interact with other displays.
Research on cross-display interactions addresses both additional virtual layers and active mobile devices--often relying on the concept of magic lenses : Benko et al.
In SecondLight, passive sheets of paper allow for similar interactions .
VP can be used for the same interactions but differs in that the device tracks itself instead of being tracked by the secondary display.
Other projects investigate content transfer between two displays to shorten the interaction distance  or make private information public .
Placing handhelds directly on interactive surfaces allows for temporarily displaying personal, mobile content .
These systems either require redirecting local input  or displays that are tracked at all times.
Similar to DeepShot , VP overcomes this by direct pointing.
This characteristic determines how closely the behavior of handheld and VP are coupled geometrically: full, restricted, and decoupled.
In full coupling, the VP replicates a real projection's behavior including all distortion ; e.g., to simulate a real portable projector.
Coupling can be partially restricted to avoid unwanted transformations ; e.g., to display an upright photo with flexible position and scale.
When the VP and the projector are fully decoupled, the VP is used solely for selecting a display without influencing the content's transformation ; e.g., to show a full-screen presentation or video.
This characteristic determines if and how the VP can influence the handheld view.
We define four possible variations: none, remote, snapshot, and continuous.
The VP might provide no input at all  for non-interactive content  or when a VP shows an inactive view.
Alternatively, remote interaction on the projection is mirrored on the handheld ; e.g., a projected web browser used with the secondary display's keyboard and mouse to change the URL.
The frustum created by the handheld can also be used to select regions of the  VP, either discretely or continuously.
In the discrete snapshot case, aiming and tapping on the handheld makes the selected region the focus ; e.g., navigating within a large map to select a region of interest for display on the handheld.
VP can take different forms for different use cases.
What they all share is applying the metaphor of projection to use available screen space on a secondary display to improve interaction with the handheld.
Based on experience with our implementation, we have categorized some of the different kinds of VP by identifying the following set of characteristics.
Items in each category are mutually exclusive.
The central locus for manipulating a VP's content is the handheld.
We identify six different ways: none, indirect, direct, physical, pointing, and frustum.
When a VP's content is static and non-interactive, no manipulation  takes place; e.g., this may be used for replicated photos.
Preprint version - Full version will be published by ACM in the proceedings of CHI 2012, Austin, TX, USA handheld.
If the VP's and handheld's views are synchronized, direct manipulation of the VP can occur on the secondary display or the handheld.
The abovementioned webbrowsing example uses this type of content manipulation.
Similar to using the projection as input to the handheld, the frustum can be used for manipulation.
The handheld's position and orientation relative to the secondary display can manipulate its content by physical motion, similar to position-aware optical projection .
The handheld can further act as a pure pointing device, either at all times  or when triggered by touching the device .
An exemplary use is activating a projection on the handheld by tapping.
Finally, the entire frustum  can manipulate a VP .
We developed a prototype framework to explore the characteristics we identified, reusing the same client-server architecture as in our work on Touch Projector .
Overall, our setup contains:  the handhelds ,  software on all displays in the environment, and  a server that manages all connections between the handhelds and secondary displays.
All connections use wireless LAN .
In contrast to Touch Projector, however, one display is the server.
As in Touch Projector, each handheld streams its live camera feed wirelessly to the centralized server.
Using feature tracking and calculation , the server  detects the targeted display, and  calculates the spatial relationship between the handheld and this display.
All client interactions  are routed through the server, as they may affect the behavior of a synchronized view.
Templates are screenshots of all secondary displays, and the images in which to find the templates are the live video frames of all handhelds.
If a template is found, SURF responds with the homography  between both image planes.
The server attempts to find the screenshot of each display in every incoming video frame of a handheld.
As VP can span multiple displays, the result may contain multiple displays and their corresponding transformation matrices.
Allowing for dynamically changing screen content.
Our system modifies the display's visual content throughout the interaction.
For example, even moving a projection changes the screenshot and the image features for later matching.
Thus, having only a static screenshot of each display is insufficient and will quickly increase matching and calculation errors.
Instead, our framework extends the original idea of matching a screenshot: Each display periodically takes screenshots, calculates the image features, and sends this information to the server.
Thus, the server has the most recent screen content available for template matching.
Since wireless image transmission introduces additional delay , synchronization is necessary.
To accomplish this, we timestamp screenshots , as well as video frames.
Since the server has no knowledge if a received video frame is as recent as the screenshots from all displays, it stores screenshots and features of at least the past two seconds.
When a new video frame arrives, the server picks the screenshot closest to that frame to minimize offset and calculation errors.
Our system thus represents a novel real-time markerless tracking approach that supports continuous feedback and dynamic screen content.
We intended VP for regular, unmodified displays in uninstrumented environments.
Thus, we chose to rely solely on the handheld's built-in camera  and the secondary display's visual content, instead of external tracking hardware .
As stated earlier, most existing approaches for tracking spatial relationships between a mobile device and large display using live video use markers, either physically attached to the display or digitally superimposed on the screen's content .
Unarguably, these approaches track well, but markers must be visible to the mobile camera at all times and thus clutter the display or its surrounding frame.
To avoid this, we decided to use a markerless approach, similar to Herbert et al.
We use speeded-up robust features  , which tries to find one or more templates within a given image using feature matching .
Note that the template does not need to be fully present in the image .
Preprint version - Full version will be published by ACM in the proceedings of CHI 2012, Austin, TX, USA image transmision and ~8 fps for tracking, with a delay of 100-130ms.
Accuracy depends mainly on the handheld's distance to the display .
Recent tests with an iPhone 4S and a more powerful server  achieve the same speed with unscaled 320x240 pixel frames.
While overall speed  remains constant, accuracy increases due to the larger, better quality image.
Faster client and server processors, along with better transmission technologies will produce further improvements.
Our implementation supports multiple simultaneous clients, whose number is limited by the server's calculation speed .
Likewise, speed decreases as the number of secondary displays grows.
Although these displays calculate features on their own before sending them to the server, the server must match each feature set to incoming frames.
In addition, more displays increase the chance of two displays having very similar content , which will cause calculation errors.
A very simple, yet powerful, use case is to clone the handheld's visible content.
This resembles the core idea of mobile optical projection.
However, decoupling the projection from the handheld allows for subsequent interactions.
Figure 5b shows how users can create Post-it-like notes, writing messages and posting them on the large screen.
Since projections are synchronized, users can change a note's content after it is projected, which is immediately visible on both the handheld and secondary display.
Instead of cloning only visible content, views that do not fit on the small display can be extended.
In this scenario, the entire content of the handheld  is cloned to the large display.
The area that is currently visible on the handheld is shown on the projection with a thin white border .
Users can interact with the projection in two ways: Manipulating  the handheld's view updates the selection frame on the large screen, and changing the projection frustum determines the region shown on the handheld.
Similar to the Photo Viewer example, users can project web pages on the secondary display.
The visual representation remains the same; a white border denotes the handheld's visible sub-region .
While conceptually similar, the web browser can be controlled on both displays: using touch input on the handheld, and mouse and keyboard on the secondary screen.
When users click on links , the website is updated synchronously on both devices.
Users are able to project video onto the large display.
However, the video is then no longer shown on the handheld.
Instead, the handheld presents controls to manipulate the video .
Several drawbacks result from the restrictions of a consumer handheld and the computer-vision approach:  Featurebased tracking requires display content containing rich features .
Our experience shows that this is rarely an issue in practice, provided we avoid roughly uniformly colored backgrounds.
If content changes more rapidly , calculation errors are introduced.
We expect that future improvements in server and handheld speed will transcend the last two limitations.
However, other solutions must be found for the first two limitations.
Our framework can be used to create synchronized projected views by aiming the handheld device at a display and long-pressing 
Switching between views and navigating within them happens by aiming at a view and tapping, or by shaking the device to open the view manager showing all available views.
The view manager also enables users to create a new projection .
Here , we show example use cases that demonstrate the potential of VP.
Table 1 classifies each case by the characteristics we discussed earlier.
Preprint version - Full version will be published by ACM in the proceedings of CHI 2012, Austin, TX, USA the secondary display, as long as the VP remains on it .
Audio can be played on the secondary display's speakers, the handheld , or both.
A small handheld display typically requires many panning and zooming operations within maps.
Here, a larger section of a map is projected onto the secondary display, with the handheld showing a subregion in greater detail .
In contrast to previous extended view examples , the handheld is now a magic lens --the mobile view is continuously updated in real-time by moving the handheld in front of the projected map.
Zooming is achieved by moving the handheld closer to or farther from the display.
This is an example of focus  plus context  views .
Users can select an image filter  on the handheld and apply it to a photo on the secondary display.
As in the magic lens Maps example, the handheld's frustum defines the filter's area, which is updated in real-time .
Filters are placed by long pressing and affect all VPs they overlap.
Filters are composed by placing them above each other.
We used a targeting task inspired by existing Fitts's Law tests .
However, we had a target quadrilateral instead of a distinct target point: We asked participants to try to align the projection as well as possible to the target, while being as fast as possible.
We computed as error measures docking offset, distance between centroids of the projection and target, and scale and orientation differences.
At the start of each task, participants had to press a start button on the handheld.
They then placed the object inside the target area, using the given technique and projection type.
Once the target was placed , the handheld switched back to the start button.
We measured time from pressing the start button to placement.
All projection types were tested using one technique before switching to the next technique.
After each technique, participants answered a short questionnaire about the technique's applicability to the task and their fatigue.
Exploring the complete space of VP characteristics in a study would have been unmanageable, so we decided to evaluate the central underlying aspect: Controlling the handheld's frustum on another display with different types of projection.
To estimate how well VP worked for each, we measured speed and accuracy for a targeting task.
We compared four different Techniques: two variants of VP and two controls .
VP  places a source quadrilateral, using the position and orientation of the handheld device relative to the large display.
While aiming, a yellow border is shown on the target display.
A participant must do a longpress and release  to place the projection.
We chose a second VP technique, VP , to evaluate the impact of more fine-grained touch-based interaction on VP: Once a participant puts a finger  on the handheld screen, they can additionally change the translation of the projection by moving the finger.
Lifting the finger places the projection.
Minimap shows a miniature representation of the large display on the handheld and uses absolute mapping.
The source initially appears in the center of the large display and can be dragged with one finger or rotated/scaled using a two-finger pinch.
A double-tap confirms placement of the projection.
Touchpad is a relative input technique in which the entire handheld screen can be used to drag  or rotate/scale  the source.
As with Minimap, a double-tap confirms placement.
We decided to test three different Projection types: Translate-Scale-Rotate , Translate-Scale  and Translate .
For each Projection, only the specified transformations are applied to the source.
We used a within-subjects design, counterbalancing Technique across participants using a Latin Square.
For each combination of technique and projection type, we had four blocks.
A block contained four trials, each of which had a different target position and shape or orientation respectively.
Transformations were randomized within a block and the first block was used for training and discarded from the analysis.
Dependent variables were speed and accuracy.
We formulated three hypotheses:  For complex projections , the VP techniques would be faster than the control techniques, due to fewer required movements.
For the VP conditions, we used our feature-based tracking approach, with a display-filling, feature-rich background image to assist tracking.
We logged all interaction with the handheld  and all tracking results for each frame.
Each participant took about 45 minutes to complete the study including post hoc questionnaires.
We compared overall task time and docking offset, as well as coarse positioning with separate repeated ANOVA measures.
For pair-wise post hoc tests, we compared against an  of 0.05 that was Bonferroni-corrected for repeated measures based on the number of repetitions.
Before the main analysis, we performed a 4 x 3 x 3  within-subject s ANOVA and found a significant effect for Block  on task time, indicating the presence of a learning effect.
Subsequent pairwise comparison showed that participants were significantly slower in Block 1 compared to the other two for all techniques and variations except Translate .
As well, for Translate, our techniques showed lower task times.
Thus, we also discarded Block 1 for all following analyses.
Both VPs are slightly less accurate than Minimap and Touchpad for almost all Projections.
However, all interfaces still achieved very high accuracies of over 92% , which is sufficient for many real-world uses.
Explanations might be that the tracking sometimes was inaccurate and that the VP techniques did not allow correcting a placement after lift, unlike the other two.
We aggregated task completion times across blocks and performed a 4 x 3  within-subjects ANOVA.
The reason for this interaction can be seen in Figure 6 as the increasing "complexity" of the projection type  influences task times.
While all techniques performed nearly equally for Translate , Minimap and Touchpad task times gradually increase with Projection complexity.
All p-values were compared against a Bonferroni-corrected  = 0.0083.
However, only Minimap differed significantly from the two VP conditions.
As the VP techniques were faster than both Minimap and Touchpad at a cost of slightly higher offsets, we additionally evaluated "homing speed" .
We suspected that both VP techniques would reach a coarse position even faster than the control techniques, but take longer for fine adjustment .
To test this, we defined Offset  as predefined docking offsets and measured the time from beginning of a trial until each offset was reached.
With this, we performed a 4 x 3 x 3  within-subjects ANOVA.
We found a significant main effect for all independent variables .
We also found significant interactions for all combinations .
From Figure 7, we see that time until an Offset is reached increases with Projection complexity for Minimap and Touchpad.
Simple Projections did not influence homing speed of different techniques for all Offsets .
In contrast, post hoc multiple means comparisons  of medium-complex Projections  showed VP  outperformed both Minimap and Touchpad , whilst VP  only showed a significant difference compared to Minimap for 25% and 20% .
For further increase in complexity, post hoc tests revealed both VPs are faster than Minimap  and Touchpad  for all Offsets.
Participants were asked for their opinion after each technique.
VP Pure was most commonly described as `intuitive' or `easy to learn/use' .
While similar comments were made about VP Thumb, its distinctive feature of using the thumb was only lauded by three.
For both VP techniques, participants mainly criticized the inaccurate tracking and difficult precise interaction .
Opinions on Touchpad were less consistent, but participants liked it in general, for example because it relied on a `known gesture' , even though some found it `too slow' .
Finally, Minimap elicited mixed comments.
While using it for a Translate projection was considered easy  and the visual feedback was liked , half the participants criticized it for difficulties in rotating/scaling  or found the visual feedback too small .
Most complaints were grounded in the `fat-finger' problem and subsequent difficulties in accurate placement and confirmation.
One participant joked that Minimap was `perfect for gnomes.'
Our quantitative results show that for complex Projections, both VP techniques significantly outperform Minimap and Touchpad, supporting H1.
The noticeably higher performance of Touchpad compared to Minimap can be explained by participants' familiarity with touchpad devices, and the display's resolution and resulting `fat-finger' problem on the handheld's display .
In terms of coarse positioning, both VPs significantly outperformed the other two techniques for complex transformations, as we had expected .
Our assumption is further supported by both VPs outperforming Minimap for Translate-Scale at all Offsets.
We also assumed that there would be not much difference among the techniques when the projection type allows only simple transformations .
However, we note that with less complexity, Minimap and Touchpad perform equal to or even slightly better than both VPs.
Nevertheless, using only Translate will rarely be the case .
Most surprising, however, is that VP  performed worse than VP , although fine-grained positioning would have been possible.
One explanation may be the interference between device and finger movement, both of which controlled the result .
A sequential approach  may overcome this at the expense of longer task times.
While this may improve accuracy, we believe that, at least in the presented use case, accuracies > 92% are still acceptable.
The central theme of decoupling the projection from the handheld device allows using the frustum for interactive filtering or selecting display subregions for navigation.
We implemented an opticalfeature-based tracking framework for smartphones and used it to apply VP to a wide range of static and dynamic screen content.
A user study showed the effectiveness of VP for performing complex placements with a single gesture.
Our software and tracking framework allows quick creation of additional virtual views, which we plan to explore further: Our prototype already supports multiple clients, but only some views  allow meaningful collaboration.
Mobile projection can benefit multiuser scenarios, which we would like to explore.
As handhelds stay synchronized to VPs, interaction could even happen remotely.
Additionally, interacting with a handheld device brings with it implicit user identification.
Another direction to explore is 6DOF device tracking.
While we have only projected onto 2D surfaces, we could create interaction techniques that use the full 3D frustum to place 3D objects, cut through 3D scenes, or control avatars.
We would like to thank our reviewers and Raphael Wimmer for their insightful comments, our participants and Doris Hausen for their time, and Andreas Butz for his support.
This research is partially funded by the State of Bavaria, the iCORE/NSERC/SMART Chair in Interactive Technologies, Alberta Innovates Technology Futures, NSERC, and SMART Technologies Inc. 1.
Sweep and point & shoot: Phonecam-based interactions for large public displays.
Baudisch, P., Good, N., and Stewart, P. Focus plus context screens: Combining display technology with visualization techniques.
Beardsley, P., van Baar, J., Raskar, R., and Forlines, C. Interaction using a handheld projector.
Benko, H., Ishak, E.W., and Feiner, S. Collaborative mixed reality visualization of an archaeological excavation.
Toolglass and magic lenses: The seethrough interface.
Blasko, G., Coriand, F., and Feiner, S. Exploring interaction with a simulated wrist-worn projection display.
