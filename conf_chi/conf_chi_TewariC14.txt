Early literacy is critical to child development, and determines a child's later educational and life opportunities.
Moreover, preschool children are incessantly inquisitive, and will readily engage in question answering and asking activities if given the opportunity.
We argue here that question asking/answering technologies can play a major role in early literacy.
We describe the design and evaluation of a conversational agent called Spot, with the goal of engaging children in a 20-questions game.
Towards this goal, we conducted a feasibility study to determine if children's questions are "on-topic" and suitable for ASR/dialogue systems.
We evaluated Spot's performance at conducting a game of 20-questions against that of a human partner.
Moreover, a growing body of research is also suggesting that pre-school children are voracious inquisitors.
One recent study found that preschoolers ask approximately 80 questions/hour .
These questions are an essential part of language development: they provide primary experience with question construction, statement construction, explanation construction, complex tenses etc.
At the preschool level, most questions are fact-based, e.g.
Fact-based questions are readily answered by short statement responses.
Children may ask follow-up questions, but in general the chain of conversation is short.
Explanation-oriented questions seek richer answers with causal links or chains .
They will often be met with additional requests for more information, or a universal "why."
Children have strong preferences for the form of the answer , although less so for content.
Question-asking, not surprisingly, goes beyond literacy and is an integral part of children's cognitive development .
So attention to this activity may have benefits in cognitive development as well.
Children evidently need some form of linguistic engagement for many hours a week, with a language-able partner who can engage with them in age-appropriate language-learning activities.
Since research in early child development suggests that for pre-school children question-answering serves as a frequent and heavily-utilized medium, this linguistic engagement can come in the form of interactive questionanswering systems.
Since children spend a significant amount of time playing alone, or out of home, there might be instances when they don't find an adult around to answer their questions.
There might also be times, when the adult doesn't have sufficient information at hand to answer a child's question.
This explains the need for expert interactive systems that can work as engaging question-answering agents.
Many national organizations recognize the essential role of early literacy in a child's later educational and life opportunities , , .
Hart and Risley  note that early literacy is an enormous challenge and will require lengthy and regular language experiences for the child.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We envision a projection system with a virtual character that acts as the question-answering agent.
The aim is to create a virtual play space that can keep the conversation grounded in a context.
Grounding is important because context specific speech recognition is more feasible than that in scenarios that lack context .
For example, the character will show the child several objects, then hide one and ask the child to guess what it is by asking questions about it.
The game engages children in language use, and also in concrete questions about things in the world and their properties.
The envisaged solution is shown in Figure 1.
Effectiveness in this case is primarily restricted to questionanswering efficiency, flow of communication and affect/engagement.
Moreover, literature suggests that vocabulary accrues from language use .
It is not feasible to measure vocabulary richness, especially in constrained activities like question answering.
Therefore, in this round of research priority was given to the volume of language use.
We see the following contributions for this work:    Testing the feasibility of question-asking/answering behavior in preschoolers, and if technology can be designed to support such behavior.
Investigation of engaging preschoolers in context of language usage, using technology.
Using common and effective parenting styles to design and develop a system for preschoolers.
In this paper we describe a two-phase study, one phase using a human language partner, and the second using a system which approximates figure 1.
Rather than relying on speech recognition and dialog interpretation, we used a Wizard-OfOz system.
The goal of the studies was to explore the feasibility of the envisaged solution: whether students would ask "on-topic" questions, whether the questions matched some templates, and whether they would be engaged by the game.
Phase 1 involved 12 children studying at the same preschool, playing a 20-questions game with a familiar researcher.
It contributed to answering the following research questions: 1.
Are children's questions predictable and deterministic, when grounded in an activity like 20 questions?
Is the repair required in such a dialogue limited and feasible?
Is it possible to effectively "nudge" preschoolers to solve problems without disengaging?
With the growth of conversational technologies, the possibilities for integrating conversation and discourse in elearning are receiving greater attention in both research and commercial settings.
Conversational agents build on traditional education systems, providing a natural and practical interface for the learner.
They are capable of offering support for each individual, and recognizing and building upon the strengths, interests and abilities of individuals in order to create engaged and independent learners.
However, the current interactive conversational tutors are geared more towards older children, who have a larger set of knowledge or skills than pre-school children and are easier to understand, and also focus on specific skills or domains.
The key difficulty in developing an agent for such a younger audience is maintaining children in their ZPD  .
The project CACHET examines the responses young children have to interactive conversational agents using electronic stuffed toys .
These toys are designed to speak, respond to touch via sensors, gesture with motors, and be linked to a PC wirelessly to provide support and feedback while a child plays games encouraging number and language learning .
Children were able to skillfully navigate through the games, however, and were adept at asking for help when they were aware of and were not irritated with the toy .
This technological adroitness suggests high potential for interactive agents for younger children.
Phase 2 involved the same participants as phase 1.
Half of them played the game with the same researcher.
The other half played the same with  Spot, an agent that we designed and implemented.
Effectively, 6 children played just with the familiar human in both the phases and the remaining 6 first played with the human and then the agent.
In phase 3, all the participants played with  Spot.
Phases 2 and 3 built on phase 1, and answered the following research question:
There is also some recent reflective work in media psychology  and education  that critically analyses the results of experiments with pedagogical agents in general.
Most such work calls for testing with younger audiences.
Moreover, prior research has shown that projection systems can be effective mediums of interaction for younger children.
This could either be for remote collaboration with other children  or communication via media with parents .
Overall, we did not find examples in the literature of systems that use projections of virtual agents to do questionanswering based games for preschool children.
12 children  participated in our feasibility study.
The participants in the study were 4 and 5 year old children at a preschool in California.
Previous research suggested that 3-year-old children would be too young for such an experiment .
The preschool that was chosen as the location for the study was a research preschool.
The session started with a demo trial, where the two objects were cat and ball.
In the demo trial, one of the researchers asked questions and another researcher answered.
If the child did not understand the demonstration, it was repeated till the child was comfortable in contributing to the questions being asked.
After the demo trial, 6 more trials followed.
Each child was asked to identify the two objects at the start of each trial.
For each question that a child asked, a truthful answer was given.
After each question-answer pair, the child was asked if they wanted to ask more questions or were ready to guess.
Sometimes the child would just guess without perceivably having enough information to make a guess.
Each child went through 7 pairs of objects, including the demo trial.
For each trial, the stimulus pair of objects presented got more difficult.
Increased difficulty meant increased similarity in the stimulus pair.
For example, a cat and a ball are easily distinguishable, but a bicycle and a car are harder to differentiate.
In formal terms, the more difficult stimulus pairs were closer to each other in terms of parts, functions and properties.
A list of all the pairs is given in Table 1.
It should be noted that no object was repeated across two trials.
This was done to level the amount of practice children receive with each object.
The study was conducted in a research room on the preschool premises, reserved for that purpose.
The room was equipped with a one-way mirror and audio equipment that allowed a visual supervisor to monitor the study at all times.
The presence of the visual supervisor was required by the preschool's protocols.
During the study two researchers were present for all sessions, in addition to the child.
The children could see the researchers, but not the visual supervisor.
A video camera recorded the child's and researchers' activity at all times.
The preschool consisted of two classrooms, namely east and west.
As per preschool protocol, no child was allowed to be outside the classroom for more than 20 minutes in one session.
No child could attend more than three study sessions in a week.
Overall all participants attended one such session, and the entire phase 1 took three weeks.
Each classroom had a circle time from 10am every morning, which is when the researchers got to interact with the participants before starting the study.
The research team attended several  circle times to become familiar with the study participants.
Familiarity was important because a large part of the study involved playing games with researchers on the team.
During the study each child attended a session individually.
Before each session a researcher from the team went to one of the classrooms and invited participants to attend a study session.
Out of the consenting children one was escorted to the study room.
As stated above, the study session could not last more than 20 minutes.
Each session comprised of multiple question-answering exchange trials.
Each child was shown photographs of two objects.
After this both the photographs were shuffled, one was put away and one was retained.
After the study, all the videos were transcribed.
Care was taken that critical incidents like questions, explanations, hints and off-topic conversations were recorded.
Previous literature suggested that most of these questions fall in one of the three categories: parts of objects, functions of objects and properties of objects .
Six individual coders/raters were asked to classify the questions into one of the three categories.
Analysis of these ratings was done to answer the research questions allocated to this phase.
It was important to keep the hints finite and deterministic as they would eventually be built into the agent.
It turned out that hints were effective in what they were supposed to do.
In about 90% of the cases of a child finding it hard to frame questions, a hint helped them.
In cases where both the hints did not work, the trial was considered incomplete and the next trial was started.
Phase 1 helped us determine if building such a system is feasible, if children's questions are deterministic, if the repair required in dialogue is limited, if it is possible to effectively prod children of this age to solve problems without disengaging.
The next section describes how the agent/system was built, given the feasibility study.
Analysis of the coded data revealed that 80% of the questions asked by our participants were about the part, property or function of objects.
The remaining 20% were guesses about what the object could be, example "Is it a cat?".
Out of the non-guess questions, 46.5% were property related questions, 30.5% were function related questions and 23% were part related questions.
Therefore, it was clear that most questions in such scenarios would deal with one of these three objects.
This helped us create the database of possible questions expected by the agent.
Participants in our study needed limited explanation.
On average researchers had to intervene only 1.67 times per child.
This was generally in cases where a child responded in a manner that represented an inaccurate understanding of the game.
In all such cases, one of the researchers would repeat the explanation of the game.
All such cases helped us build conversational edge cases into the agent later.
This is more formally known as repair/error-recovery in dialogue systems.
Children were adept at inferring the objects.
In phase 1, they were able to successfully solve 80% of the trials conducted.
Sometimes however, they would still try guessing the object without having enough information to solve the problem.
This is in addition to the final guess that they would use to solve the problem.
Overall 40 such inaccurate guesses were recorded across the 12 participants.
The system that was built, replicated the same task as in phase 1.
An interactive agent in the form of a puppy dog character conducted the game sessions instead of a human.
The character introduced the game, showed two objects, hid one and then played a 20-questions game till the child guessed the hidden object correctly.
The character followed a script, the design of which is explained in the dialogue subsection.
During the study, sometimes a child would talk about objects or contexts that were not grounded in the environment.
Any such deviations from the task at hand were counted as an offtopic dialogue .
It was noticed that approximately 5% of the utterances by the participants were not grounded in the environment, and were counted as offtopic.
Sometimes  a child would find it hard to frame a question about two objects.
Predictably, more than 70% of such cases happened for objects with high levels of similarity, namely: apple and orange, bicycle and car.
In such cases, hint mechanisms were used to help them think of possible questions.
Phase 1 was used to come up with effective hint mechanisms.
The system worked through modules shown in Figure 2.
The speech recognition component of the system was wizard-ofOz.
Collecting audio data and corresponding transcriptions while we evaluate our system was the motivation for using wizard-of-Oz method .
So for everything that the child said, the wizard  would transcribe the speech.
Using the questions asked in phase 1, templates of possible questions were created.
If the incoming utterance's transcription matched any of these templates, it was sent to a keyword search routine, else it was sent to an edge case handler that used redirections .
This answer was either "yes, it does" or "no, it doesn't".
If the keyword search routine was unsure of the structure, but sure of the semantics of the question, a yes/no response was given.
If the question was an invalid  question, responses expressing soft disapproval were generated .
Moreover, if the child found it hard to frame a question and there was silence for some time, the system would detect this and generate a hint.
In cases where the wizard felt that the participant does not understand the activity, he would send a command directly to the agent logic and Spot would replay the explanation of the game.
The audio responses were pre-recorded audio.
The voice used to record these responses was that of a female in her early 20s.
The frequency range of her voice was deemed appropriate for Spot.
Throughout the duration of the study no child commented on the voice texture, and all of them seemed comfortable.
In terms of the form that the agent could take, the need was for a character that is gender-neutral, engaging and likeable.
A puppy character was employed to be the agent .
He was named "Spot", and will be referred to by that name henceforth.
Previous research also suggested that such agents should have lifelike characteristics .
To create engaging videos with minimal effort, we used Machinima from the SIMS Pets game.
SIMS is a widely used "god" game that supports highlevel control of characters, including non-human characters.
Machinima is the process of using in-game recording facilities to record segments of game action under high-level control of the player .
A puppy character was created in the SIMS create-a-pet tool and machinima videos were recorded  with various different personality traits set.
In the create-a-pet tool, depending on what personality is chosen the pet character responds through gestures.
This was ideal for the study, as the character was supposed to respond to the child's questions, not just with speech but also through gestures.
Valence is the emotional tone given to interactions.
It can be both, positive or negative.
Hart and Risely argue that this comprises of prohibitions, approvals and repetitions.
However, there was a possibility for Spot to be viewed as inappropriate if it explicitly disapproved any child behavior.
Therefore, softer disapprovals were included into Spot's script.
The script was designed to include things like, "That's not really the right question to ask.
Do you want to ask something else?"
Spot's script also contained affirmative feedback like, "That's right" and "That's great" to encourage further questioning.
Spot used repetitions in the context of the question.
Spot: Yes it does/No it does not.
In terms of the hint mechanism, Spot used the techniques tested in phase 1, if a child found it hard to frame a question.
Although children did not deviate from the game frequently during phase 1, any dialogue system is incomplete without certain edge cases or redirections.
Most chatbots use such redirections to direct users to topics within the chatbot's knowledge base.
Redirections in Spot's script were encouraging and/or affirmative:
These represent the kind of utterances used by parents.
This refers to categorization of utterances in terms of the responses that they can prompt.
Hart and Risely  argue that there are three levels of prompts that parents use as discourse functions.
These represent the relationship between utterances of the speaker and listener.
This refers to categorization of sequence of utterances in an interaction.
Hart and Risely  argue that this consists of initiations, response and floorholding.
Spot's script contained initiations that can draw the child's attention if they deviate from the game.
This involved saying things like, "Are you still there?
It should be noted that these questions are different from the ones mentioned under discourse, in that they are only posed if the child goes off-
Spot used a variety of gestures to convey meaning during a game session.
These were mappings of the content of the response to specific gestures.
Research suggests that gestures can be of four types : iconic, deictic, metaphoric and beat.
Iconic gestures represent object attributes, spatial relationships and actions.
Such gestures were used to signal: affirmative or negative response to a question, right or wrong guesses and an invalid question.
Spot nodded its head for an affirmative response and shook it for a negative response.
For an invalid question Spot turned shy and stepped back.
For a right guess Spot jumped and for a wrong guess shook his head and looked away.
Deictic gestures involve connecting speech to location of objects, more specifically pointing.
Spot used deixis to introduce objects during a trial, as shown in Figure 3 .
Spot did not use any metaphoric and beat gestures as they might have been too complex to interpret,
It should be noted that even though the researchers conducting the study were not teachers or parents, they were significantly familiar to the participants because of exposure during circle time  and also during phase 1.
The procedures in the agent condition were very similar to the human condition, except that Spot was supposed to conduct the entire game session.
As mentioned already Spot used a script to go through its dialogues.
The part of setting up the game remained the same for all users.
In each game session Spot introduced itself, explained the rules of the game step-by-step and then went through the trials with different object pairs.
In any given trial, Spot would first show the two items, identify both of them, then convert them into question marks.
After this, through some animation, one question mark would leave the screen and the other one will go into a box.
All of this is depicted in Figure 3.
Once the object was hidden, the child was expected to ask questions to figure out the hidden object.
After this, the game took different conversational routes for different participants.
Phase 2 of the study followed a between subjects design.
Participants from phase 1 were randomly assigned to the human and the agent condition.
To make sure that two groups were not different to each other a priori, we conducted individual two-tailed t-tests on the data from phase 1.
We tested the two groups for any significant difference in terms of the following parameters: total questions on property, total questions on part, total questions on function, total questions overall, total explanations required, total offtopic count, total hint, total questions successfully solved.
None of the t-tests suggested significance, so there was no reason to believe that the two selected groups were different from each other in a priori question answering behavior .
Moreover, just like phase 1 in a particular session a child went through 7 trials including the demo trial.
The object pairs used in the trials for phase 1 and phase 2 were completely different, and are summarized in Table 1.
The procedures in the human condition were the same as phase 1.
The only change was that the human conducting the session tried to stick to the dialogue script designed for Spot, and deviated only if the child went off-topic or got confused.
In simple words, the script was supposed to be overruled only if the conversation needed "repair", despite the strategies used in the script.
The adherence to script was done so that the verbal exposure in the two cases  is comparable.
The deviations were allowed because parents in practice use a lot of strategies to engage children and technology cannot replicate all of those.
Phase 2 was therefore designed to compare and contrast Spot's limited,
The environment and setup for the human condition was the same, as phase 1 because the premises and the room used remained the same.
However, for the agent condition we decided to project Spot on a wall using a projector.
This was done because of multiple reasons.
Firstly, having the character on a wall made sure that mona lisa effect exists and gets preserved in the room .
Secondly, research suggests children this age have higher depth of search for interactions that are less manual  .
Thirdly, projector offered the advantage of larger size and form-factor.
The layout of the room can be seen in Figure 4.
In the same fashion as phase 1, all the video data from the study was transcribed.
Again, six individual coders/raters were asked to classify the questions into one of the three categories: parts of objects, properties of objects and functions of objects.
An analysis of the ratings is contained in the next subsection.
I It was found th hat Spot was ab ble to engage children in sho ort c conversational sequences, sometimes even better than th he h human conditio on.
Across the 7 trials in the session, children in n the human condition c asked d 55 questions and this numb ber w was 86 for the e agent conditi ion.
A two-tail led t-test for th he to otal number of questions ask ked, pointed to owards statistic cal s significance .
0 Analyzin ng th he individual question q catego ories, it was fou und that children a asked more property  an nd p part questions  1 in the age ent c condition.
The numbers of qu uestions on fun nctions of objec cts w were not signi ificantly differ rent.
It should d be noted th hat c children asked d more prope erty questions for objects of in ncreased similarity, and as sked more par rt questions for f o objects that wer re easy to distin nguish.
This tre end is predictab ble g given prior rese earch , and did d not differ across a condition ns.
H However, child dren in the agen nt condition suc ccessfully solved m more problems than the ones in i the human co ondition .
We hypothesize that this is becau use o of the increased d number of qu uestions in the agent conditio on, b because childre en can effective ely use questions as a means to s solve such prob blems .
Chi ildren had lim mited tendency to g guess without proper inferen nce, which is consistent wi ith p phase 1.
There were 28 such guesses g in the human h conditio on a and 15 in the agent conditio on.
The individ dual numbers of g guesses in both h conditions were w not signif ficantly differe ent f from each other r .
It should be b noted that th he r results from pha ase 1 were com mpared to the re esults from pha ase 2 for the hum man group.
We e did not find d any significa ant d difference in pe erformance  of th he group assig gned to the hum man condition, in phase 1 an nd p phase 2.
T The participant ts did not ex xhibit any sign nificant learnin ng e effect, which is s reasonable be ecause the obje ect pairs used in p phase 2 were completely different from phase p 1 and the t a activity at han nd was not so omething that is unfamiliar to p preschoolers .
Therefore, it i is reasonable e to assume th hat a any change in the t performanc ce of the agent group across th he tw wo phases wa as caused by the introductio on of the agen nt.
S Since the aim of o this research was to design an agent that can p produce questio on-answering experience e that is comparable to in nteracting with h a familiar hu uman, these re esults are high hly e encouraging.
All totals have been plotted in Figure F 5.
In term ms of explanat tions, children in the human n condition needed d explanation 18 times and tho ose in the agen nt condition needed d it 15 times.
Moreover, th there was no significant differen nce in the num mber of times explanation wa as required by a ch hild .
T The total numbe er of hints requ uired by the group a assigned to hum man condition was 17, and th hat required n group was 12 by the agent condition 2.
It is clear f from these nu umbers that childre en follow a sim milar pattern as p phase 1 in talki ing to Spot, and th here was a lim mited need fo or explanation, hints and offtopi ic dialogue han ndling.
Moreov ver, in all such edge cases Spot do oes as well as a familiar huma an, if not better r.
H Handling redir rections and offtopic conv versations is an im mportant chara acteristic of an ny question-an nswering system m. I Interactions wit th Spot, just li ike interactions s in phase 1 did d n not deviate from m the topic much.
Childre en only went offtopic o 6 time es in the human c condition, and this t number was 5 for the ag gent condition.
During g the session, ch hildren appeare ed to enjoy the interaction with th he agent.
Some e of them said things like "I w want to ask more q questions", "I really like wh hen he jumps" ", "Spot is clever, but he lets me win."
Thr ree participant ts in agent conditi ion wanted to g go through mo ore trials when the session me session, child dren were inter rviewed for ended.
After the gam any im mmediate experi ience that they y might want to o share.
All children en who were a part of the a agent condition n, said they enjoyed d the session.
T This observation n was the same e for human conditi ion.
We ackno owledge that th here might be participant respons se bias, but the e ratings do poi int towards a po ositive user experie ence in the agen nt condition.
It s should also be n noted that 8 out of t the 10 participa ants in the agen nt condition said d they want to play y again.
The res st said they wan nt to play a diff ferent game with S Spot.
Some o of these post-game session n interview accoun nts also explain n the quantitativ ve results we ha ave seen so far.
Tw wo out of the ten n children in th he agent condition reported that the ey wanted to kn now how much h Spot knows an nd therefore asked a lot of question ns, even when t they were sure of what the hidden n object was.
N Novelty factor could also hav ve played a ldren to ask mo ore questions.
Tw wo children de emanded playin ng m more games wi ith Spot, asking g things like "c can he play mo ore g games?"
Three children said they t liked Spot t and the fact th hat h he could talk.
None of the pa articipants paid d attention to th he f fact that there was w a wizard tra anscribing thin ngs that they sai id.
F Four children did d wonder how w the projection n on the wall was w b being executed.
During video transcription, we recorded an nd a appropriately ta agged any incid dents of distrac ction.
In both th he h human and th he agent con ndition, children had limited te endency to loo ok away or get t distracted from the activity at h hand.
We hypothesize that the e activity in itse elf was engagin ng to o them, and th hey found it ev ven more enga aging when Sp pot h helped them thr rough it.
P Participants, pro ocedures and en nvironment setu up were the sam me a as phase 1 , except wi ith a fully auto omated agent an nd n new set of object pairs.
It sh hould be noted d that all the 12 p participants wer re recruited for r this phase.
Th he reason we did d n not do a human n condition for this phase was to collect mo ore d data, and because we ha ave already established the t e effectiveness of f Spot's intera action against that with a re eal p person.
The con ntribution of ph hase 3 was to demonstrate th hat e even a fully au utomated  system m would still gi ive s similar performa ance.
Moreover r, we did not ex xpect any learnin ng e effects, as the activity a was a familiar f one an nd something th hat p preschoolers do o on a daily ba asis .
Given that no learnin ng e effect was obse erved in phase 2 , there was no n r reason to believe in its existenc ce for phase 3.
It was observed that t the automate ed Spot had very similar perform mance as the w wizard-of-Oz Sp pot.
If we cross s-compared the hum man condition , the agent condition n  e automated a and the agent condition n , ag gainst each other, the fully aut tomated Spot's performance e was not signific cantly differen nt from the w wizard-of-Oz a agent.
This seems logical, becau use Spot was already built to handle mismat atches and erro ors, and Leven nshtein distance es in query matchi ing, mostly han ndled any add ditional errors t that speech recogn nition introduce ed.
For exampl le if the object t in context was a cat, and the qu uestion asked w was "does it m meow?
However, be ecause of the use of edit di istance, the keywor rd search rou utine would a assume that th he speaker intende ed to say, "m meow".
After t transcription, it was also observe ed that 87% o of the incomin ng queries wer re matched correct tly with an inte ended response.
In simple wor rds, in 87% of the cases, the au utomated Spot was able to produce a response that the wizard would have e generated.
M Moreover, in terms o of the measure ed parameters, there was no significant statistic cal difference between the p performance o of the fully automa ated Spot and the wizard-of f-Oz Spot from m phase 2.
Given that the object t pairs were dif fferent, this ma ay not be a fair co omparison, but t the contribu ution of phase 3 was to replica ate performance e similar to the e agent conditio on in phase 2, for n new object pa airs, which clea arly happened.
To give a visual understanding g of the resul lts we have p plotted the average e values of the e parameters for the two cond ditions from automated Spo phase 2 and for the a ot from phase 3.
We plot average es because ther re were 6 parti icipants in each h condition in pha ase 2 and all of them  were involv ved in the evaluat tion of the auto omated Spot in phase 3.
T The system, as discussed so far f was develop ped in a way th hat e even if Spot doesn't d understand the exact t utterance of f a p participant, usin ng edge case ha andling and hin nt mechanisms s it w would still be able to drive the convers sation along th he c context, which in the case of 20-questions 2 is constrained.
For F th his phase, we e substituted the t wizard co omponent in th he s system design with w GoogleTM speech recogn nition.
Given th he s simplicity of children's c spee ech vocabulary y, and the large s spectrum of voi ice features ou ur system was expected e to wo ork w with, this was the most logic cal choice.
Stan ndard techniqu ues li ike error looku up tables and Levenshtein L dis stances for que ery m matching were e used to mat tch incoming transcription to q question templa ates, but discu ussion on those e are outside th he s scope of this paper.
Given the importance of question-answering in the early years of a child's life, our initial problem was to design a question-answering agent that could help preschool children with short question-answering sequences.
Phase 1 of our study was dedicated to studying the feasibility of such a system, given how open-ended dialogues with preschoolers can be.
Using 20-questions as an activity that can constrain question-asking behavior, we reached the following conclusions.
Firstly, children's questions are predictable and deterministic, when grounded in an activity like 20 questions.
Secondly, the repair required in such a dialogue is limited and feasible.
Thirdly, it is possible to effectively prod preschoolers to solve problems without disengaging.
In phase 2 and 3 of the study we designed and implemented a question-answering agent using commonly used parenting styles  and machinima videos.
We found that children asked more questions and solved more problems in the 20-questions game with the agent, than if they play with a familiar human.
Prior education research  suggests that the two observations might not be independent, as children use questions as tools to solve problems.
Moreover, we found that the tendency to deviate from the task at hand was no more in the agent condition, than the human condition.
And even in case of deviations, some standard edge case handlers built into the agent's script were able to take care of the redirection to original dialogue.
We also found that children did not need significantly more hints or explanations in the agent condition, than they did in the human condition.
Even though our results were positive and demonstrated strong effects, there were a few limitations to our study.
All the findings mentioned in the paper are restricted to a game of 20-questions and not to open ended question-answering.
Open-ended conversational sequences are much harder to handle given current technological capability.
Moreover, our participants were part of a research preschool and therefore had experience being a part of research studies.
However, some of these limitations are essentially avenues for future investigation in this domain.
Despite the hype around how conversational systems  can make information more accessible, more rigorous research is clearly called for.
In this paper, we have identified need and opportunities for questionanswering based conversational games in the everyday lives of preschool children.
Based on these insights, we have investigated how preschoolers ask questions in a constrained environment, and how we can build technology that can handle such questions and keep children engaged.
We have demonstrated that such a system, if carefully designed, can at times perform better than a familiar human in the short-term.
We hope that experiences and the challenges we have encountered will be beneficial to our colleagues who seek to conduct more rigorous, longer-term evaluations of such conversational applications under naturalistic conditions in similar environments.
We would like to thank the Early Child Education Program, the Child Study Centre  in particular for providing the necessary setting to conduct such a research experiment.
This work would not have been possible without help from volunteers.
We would like to thank Timothy Brown, Samantha Dove and Peiyi Ko, for their help in conducting pieces of the study.
Abu Shawar, B. and E. Atwell, Fostering language learner autonomy via adaptive conversation tutors, in Corpus Linguistics.
Aleven, V., K. Koedinger, and K. Cross, Tutoring Answer Explanation Fosters Learning with Understanding, in Artificial Intelligence in Education.
Simulating Instructional Roles through Pedagogical Agents.
International Journal of Artificial Intelligence in Edu- cation, 2005.
Boyarskaya E, Hecht H, "The Mona Lisa effect: Is it confined to the horizontal plane?"
Byrd D., van der Veen T., McNamara J.
Callanan M. and Oakes L. Preschoolers' questions and parents' explanations: Causal thinking in everyday activity.
Chouinard, M. M. Childrens questions: A mechanism for cognitive development.
Clarebout, G., Elen, J., Johnson, W.L., and Shaw, E. Animated pedagogical agents.
An opportunity to be grasped?
We hope that contributions made in this paper can help shape the next steps in building question-answering systems  for younger children.
We think that another solution of the initial envisaged problem could be a "character" toy  with speech recognition and synthesis capabilities.
This could take the form of a plush toy or a favorite TV character.
This could be a common internal platform  that can be used with various external skins.
Future work could compare the individual and relative effectiveness of these solutions against each other, in similar activities.
We chose the particular game for this study to be "ASR-friendly".
De Pietro, O. and G. Frontera, TutorBot: an application AIML based for Web-Learning.
Advanced Technology for Learning, 2005.
Dillenbourg, P. and J. Self, People Power: A Human-Computer Collaborative Learning System in Intelligent Tutoring Systems.
Feng, D., E. Shaw, J. Kim, and E. Hovy, An Intelligent Discussion-Bot for Answering Student Queries in Threaded Discussions, in 2006 International Conference on Intelligent User Interfaces.
Frazier B., Gelman S., and Wellman H.. Preschoolers' search for explanatory information within adult-child conversation.
Fryer, L. and Carpenter R., Bots as Language Learning Tools.
Language Learning and Technology., 2006.
Goffin V., Allauzen C., Bocchieri E., Hakkani-Tr D., Ljolje A., Parthasarathy S., Rahim M., Riccardi G., and Saraclar M., "The AT&T WATSON speech recognizer," in Proceedings of IEEE ICASSP-2005, Philadelphia, PA, USA, 2005.
Person, and D. Harter, Teaching Tactics and Dialog in AutoTutor.
International Journal of Artificial Intelligence in Education, 2001.
Grigoriadou, M., Tsaganou G., and Cavoura T., Dialogue-Based Reflective System for Historical Text Comprehension, in Workshop on Learner Modelling for Reflection at Artificial Intelligence in Education.
Hart, B. and Risley, T. Meaningful Differences in the Everyday Experience of Young American Children, Paul H. Brookes, 1995.
Heffernan, N.T., Web-Based Evaluations Showing both Cognitive and Motivational Benefits of the Ms. Lindquist Tutor, in Artificial Intelli- gence in Education.
Kerly, A., Ellis, R. and Bull, S. CALMsystem: A Conversational Agent for Learner Modelling.
P. Agents with faces: The effect of personification.
In the 5th IEEE international workshop on Robot and Human Communication, 1996, pp.
Kramer, N.C: Psychological Research on Embodied Conversational Agents: The Case of Pedagogical Agents.
Journal of Media Psychology 2010.
Stone, and G.D. Stelling, Lifelike Pedagogical Agents for Mixed- Initiative Problem Solving in Constructivist Learning Environments.
User Modeling and User-Adapted Interaction, 1999.
Lowood, H. High-performance play: The making of machinima.
Luckin R., Connolly D., Plowman L., and Airey S. With a little help from my friends: children's interactions with interactive toy technology.
Journal of Computer Assisted Learning, Special issue on Children and Technology:165-176, 2003.
Makela K., Salonen E. and Raisamo R. Conducting a wizard of oz experiment on a ubiquitous computing system doorman.
In Proceedings of the International Workshop on Information Presentation and Natural Multimodal Dialogue, pages 115-119, 2001.
McNeil D.l, Hand and Mind: The University of Chicago Press, Chicago IL, 1992.
NELP: National Early Literacy Panel  Developing Early Literacy, National Institute for Literacy , available from site www.nifl.gov 29.
NFCL: National Family Literacy Organization, main site www.famlit.org 30.
Perez-Marin, D., Pascual-Nieto, I.: Conversational Agents and Natural Language Interaction.
Techniques and Effective Practices, IGI Global, 2011 32.
Johnson, and R. Ganeshan, Pedagogical Agents on the Web, in International Conference on Autonomous Agents.
Taylor, K. and Moore, S., Adding Question Answering to an ETutor for Programming Languages, in AI-2006, 26th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence.
A Question-answering Agent Using Speech Driven Non-linear Machinima.
Wood, D. Scaffolding, contingent tutoring and computersupported learning.
Developing a media space for remote synchronous parent-child interaction.
In Proceedings of the 8th International Conference on Interaction Design and Children .
