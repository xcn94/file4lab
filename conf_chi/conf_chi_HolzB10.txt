It is generally assumed that touch input cannot be accurate because of the fat finger problem, i.e., the softness of the fingertip combined with the occlusion of the target by the finger.
In this paper, we show that this is not the case.
We base our argument on a new model of touch inaccuracy.
Our model is not based on the fat finger problem, but on the perceived input point model.
In its published form, this model states that touch screens report touch location at an offset from the intended target.
We generalize this model so that it represents offsets for individual finger postures and users.
We thereby switch from the traditional 2D model of touch to a model that considers touch a phenomenon in 3space.
We report a user study, in which the generalized model explained 67% of the touch inaccuracy that was previously attributed to the fat finger problem.
In the second half of this paper, we present two devices that exploit the new model in order to improve touch accuracy.
Both model touch on per-posture and per-user basis in order to increase accuracy by applying respective offsets.
Our RidgePad prototype extracts posture and user ID from the user's fingerprint during each touch interaction.
In a user study, it achieved 1.8 times higher accuracy than a simulated capacitive baseline condition.
A prototype based on optical tracking achieved even 3.3 times higher accuracy.
The increase in accuracy can be used to make touch interfaces more reliable, to pack up to 3.32 > 10 times more controls into the same surface, or to bring touch input to very small mobile devices.
Figure 1:  The Generalized Perceived Input Point Model: a user has repeatedly acquired the shown crosshairs using finger postures ranging from 90  to 15 pitch .
The five white ovals each contain 65% of the resulting contact points.
The key observation is that the ovals are offset with respect to each other, yet small.
We find a similar effect across different levels of finger roll and finger yaw, and across users.
We conclude that the inaccuracy of touch  is primarily the result of failure to distinguish between different users and finger postures, rather than the fat finger problem .
Our RidgePad prototype uses this observation to deduce finger posture and user ID during each touch.
This allows it to exploit the new model and obtain 1.8 times higher accuracy than capacitive sensing.
Acquiring a small target on a touch screen is error prone.
We can examine how inaccurate touch is on a given device by letting users acquire a small target repeatedly: the more inaccurate the device, the wider spread the distribution of the sensed contact points .
When acquiring a target of finite size, such as a button, wider spread results in an increased risk of missing the target.
The common explanation for the inaccuracy of touch is the fat finger problem .
In this model, the softness of the user's skin causes the touch position to be sensed anywhere within the contact area between the user's fingertip and the device.
At the same time, the finger occludes the target.
This prevents the target from providing visual feedback so that users cannot compensate for the randomness.
Researchers have therefore argued that users cannot reliably acquire targets smaller than a certain size.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
While the fat finger problem has received a lot of attention, we argue that it is not the true reason for the inaccuracy of touch.
We argue that the perceived input point model is the primary source of the problem; Vogel and Baudisch mention it in the same paper that discusses the fat finger problem .
When a user tries to acquire a target, the center of the contact area tends to be located a couple of millimeters off the target location--typically "below" the target .
The fact that touch devices report the touch location at an offset increases the risk of missing the target.
Unlike the fat finger problem, however, the "perceived input point" is a systematic effect.
This allows compensating for the effect by applying an inverse offset when recognizing touch .
In this paper, we generalize the perceived input point model in order to reduce touch inaccuracy even further.
We hypothesize that the offset between the center of the contact area and the target depends not only on the x/y location of the target, but also on the wider context of the touch interaction.
The wider context in this generalized perceived input point model could potentially include a larger number of variables.
In this paper, we examine the following four: 1-3.
Angle between the finger and the touch surface .
The related work suggests that pointing might be affected by changes in finger orientation   and finger "steepness"  .
By considering all three finger angles we implicitly switch from the traditional 2D model of touch to a model that considers touch a phenomenon in 3-space.
Each user might have a different mental model of how to acquire the target.
While touch is well understood in the macroscopic world , note that there is probably no universally agreed upon interpretation for determining what exact location a finger is touching.
To verify these assumptions we conducted a user study.
Figure 1a gives a preview of our findings.
The oval chart is taken from one of the participants of the study who repeatedly acquired a crosshair target with five different levels of finger pitch ranging from 15 to 90; each of the five white ovals shown in Figure 1 contains 65% of one of the resulting contact point distributions.
The key observation is that all five ovals are offset with respect to each other, yet small.
We find a similar effect across different levels of finger roll and finger yaw, and across users.
We conclude that the inaccuracy of touch  is primarily the result of failure to distinguish between different users and finger postures, rather than the fat finger problem .
We consider this as support for the proposed model.
In the second half of this paper, we switch to an engineering perspective.
We present two devices that exploit the new model in order to achieve higher touch accuracy.
Both devices model touch on per-posture and per-user basis and increase accuracy by applying respective offsets.
Figure 1b illustrates the workings of one of them, the RidgePad prototype.
It extracts user ID and posture from the user's fingerprint during each touch interaction.
The ridges of the shown fingerprint belong to the front region of a fingertip.
RidgePad uses this observation to deduce finger posture.
This allows RidgePad to exploit the new model and obtain 1.8x higher accuracy than capacitive sensing.
A prototype based on optical tracking achieved even 3.3 times higher accuracy.
The increase in accuracy can be used to make touch interfaces more reliable, pack up to 3.32 > 10 times more controls into the same surface, or, assuming future miniaturization, to bring touch input to very small mobile devices.
A popular approach to precise touch is to address the occlusion problem using targeting aids.
Zooming-based techniques reduce occlusion  although they cannot fully resolve it.
Other targeting aids remove occlusion entirely by separating the user's hand from the pointer .
On the flipside, targeting aids make touch less direct and may therefore reduce the intuitiveness of input.
They also increase targeting time; offset cursor, for example, incurs a task time penalty of 250ms to 1000ms .
Different types of touch technologies are able to extract different subsets of finger posture.
Microsoft Surface detects finger orientation by analyzing the diffuse reflection of the hovering hand.
Capacitive technologies, such as the FingerWorks iGesture pad  estimate finger orientation based on the eccentricity of the contact area.
Some researchers have proposed exploiting finger postures in order to enable additional functionality.
Wang and Ren, for example, proposed occlusion-free pie menus  and hand orientation-aware gesture processing .
Their algorithm detects finger yaw by observing the changes in contact area over time.
Finger roll has been proposed as the basis for a new gesture language .
The specific implementation of MicroRolls, however, cannot distinguish between finger rolling and finger dragging.
Therefore rolling serves here primarily as an alternative way of performing a drag gesture.
It can also extract rolling and dragging from a single interaction.
Only little research has been done on the impact of finger posture on touch.
Wang and Ren examined the impact of specific finger postures and gestures on input accuracy and contact area .
It is commonly understood that a target needs to have a certain minimum size in order to allow for reliable acquisition.
In the light of the generalized perceived input point model, it seems plausible that the disagreement about minimum button sizes was caused by differences in study conditions, Pitch, for example, has the two levels `fingertip' vs. `nail' for Vogel and Baudisch, while it is part of a gesture that combines pitch with yaw for Wang and Ren.
The use of a tabletop system required Forlines' users to reach across the surface, resulting in particularly low pitch values .
Wang and Ren distinguished fingers, while other authors did not.
They also recalibrated x/y offsets for every participant, thereby effectively using a per user calibration .
Finally, there are differences in how users commit a selection, such as take-off  or a button press with the other hand .
The purpose of this study was to verify this assumption.
Our main hypothesis was that a variation of touch context, i.e., a variation of finger posture and/or user ID, would result in distinct clusters of touch positions.
A participant has repeatedly acquired a target using five different finger postures.
Each one results in a distribution, which we illustrate using an oval.
If touch inaccuracy is governed primarily by the fat finger problem, we expect to see large ovals, all of which are centered on roughly the same point .
If the inaccuracy of touch, however, is primarily explained by the generalized perceived input point model, we expect to see ovals that are visibly offset with respect to each other .
Fingerprint scanners have traditionally been used to identify users.
More recently, they have also been used as parts of interactive systems.
Sugiura and Koseki's, for example, used a fingerprint scanner to allow users to invoke different functions by touching with different fingers .
Several patents explain how to control a pointer using a fingerprint scanner.
Ferrari and Tartagni's device allows controlling a mouse pointer as a relative touchpad .
Gust analyzes optical flow in order to extract motion .
Akizuki's device implements an absolute touchpad .
A device by Bjorn and Belongie can distinguish whether it is being touched by a fingertip or a flat finger .
Unlike all of these patents, the RidgePad prototype presented in this paper is able to extract rolling and dragging from a single interaction , a crucial feature for the method proposed in this paper.
Figure 3 shows a participant during the study.
A touchpad showed a single target, which participants acquired repeatedly.
During each trial, participants first touched the start button on the pad .
Then participants assumed the finger angle for the current condition with their right index finger and acquired the target.
Participants committed the touch interaction by pressing a footswitch.
This recorded the touch location reported by the touch pad, played a sound, and completed the trial.
Participants did not receive any feedback about the location registered by the touchpad.
We took the following four measures to minimize the impact of other potential factors.
First, participants kept their head in a fixed position above the touchpad, as shown in Figure 3.
Second, the crosshairs marking the target extended beyond participants' fingers, allowing participants to maintain a certain amount of visual control during targeting.
Third, the use of a footswitch allowed us to avoid artifacts common with other commit methods, such as inadvertent motion during takeoff.
And finally, participants were told to use as much time as necessary and that task time would not be recorded.
Every participant completed their 600 trials in under 40 minutes.
Session order was counterbalanced across participants.
Within a session, participants performed a sequence of 150 trials with the touchpad in one orientation and then a second sequence of 150 trials with the touch pad in the opposite orientation.
Pad rotation was counterbalanced across participants.
For each sequence, participants completed 5 blocks of 5 angles  6 repetitions each.
The order of finger angles was counterbalanced across trial blocks.
Each participant completed all conditions.
Each participant completed 5 angles  2 pad orientations  2 sessions  5 blocks  6 trials per block = 600 trials.
Participants acquired the target using their right index finger with five different levels of pitch and five different levels of roll .
We varied pitch between "close to horizontal" = 15 and "straight down" = 90.
Pitch values beyond that caused the fingernail to touch first, which clashes with many types of capacitive devices.
A roll of 0 meant that the nail was horizontal.
We varied roll between "rolled slightly left" = -15 and "rolled fully to the right" = 90.
Varying roll and pitch separately allowed us to keep the number of trials manageable.
During the pitch session, participants kept finger roll horizontal , while they used a fixed pitch angle of 15 during the roll session.
Combinations of pitch and roll are to be interpreted pitch-first.
A pitch/roll of 15/45 thus means "assume a pitch of 15 and then roll the finger 45 to the right".
We also studied the third angle, i.e., yaw.
However, there was no need to vary it, because yaw takes places in the plane of the touchpad.
As a result, we can reconstruct all levels of yaw by rotating the touch locations  post-hoc in software around the target.
This, however, requires knowledge of the target location.
Since the capacitive pad cannot see the target, we approximated its location by testing two levels of touchpad orientation .
We then determined the rotation center as the center of gravity among all touch locations, before we flipped the 180 condition and merged its samples with the 0 condition.
To make sure that the 180 condition be identical from the participants' perspective, participants operated a second `okay' button on the opposite of the touchpad .
We had one main and four dependent hypotheses.
Our main hypothesis was that a variation of touch context, i.e., a variation of finger posture and/or user ID, would result in significantly different clusters of touch positions.
The dependent hypotheses spell this out for the individual variables.
Pitch: Different levels of pitch result in distinct touch location clusters.
In other words, we expected to find higher spread across pitch levels than within a given pitch level.
Figure 5: Clusters of touch locations for each of the 12 participants .
Crosshairs represent target locations; ovals represent confident ellipsoids.
All diagrams are to scale.
Note how different patterns suggest that each participant had a different interpretation of touch.
Figure 5 summarizes the complete touch location data obtained from this study.
Each column summarizes the recorded locations for one participant; the top chart shows aggregated clusters of touch locations for the different levels of roll, the bottom chart shows the aggregation of the pitch session.
All ovals in Figure 5 represent confidence ellipsoids that contain 65% of the recognized touch locations per condition.
The crosshairs in each chart is the target location.
Figure 6 shows two examples in additional detail .
We analyzed the effect of pitch using a repeated measures one-way ANOVA.
To better understand the nature of the differences, we decomposed the differences in recognized touch position into differences along the finger axis  and across the finger axis .
Changing finger pitch had a significant effect on recognized touch positions  along the finger axis.
Pair-wise comparisons using Bonferroni-corrected confidence intervals showed that the touch locations of all levels of pitch were significantly different .
However, pair-wise posthoc tests showed no significant differences.
A repeated measures one-way ANOVA found a significant main effect of roll on sensed touch position along the finger axis .
Bonferroni corrected pair-wise comparisons showed a significant difference between 90 roll and all other roll levels, as well as 45 vs. -15 and 0 .
We ran a two-way ANOVA on finger pitch and participant both along and across the finger axis, using participant as a random factor.
We found a significant interaction between pitch and participant and significant main effects for both .
For each participant, we ran separate oneway ANOVAs on finger pitch to determine where the effect was particularly evident.
Similarly, we ran a two-way ANOVA on finger roll and participant.
We found significant main effects for participant along and across the finger axis, as well as for finger roll along the finger axis.
We further found a significant interaction between finger roll and participant along and across the finger axis .
This indicates that each participant exhibits a different behavior and touch pattern in response to finger roll.
We ran one-way ANOVAs on finger roll separately for each participant.
We found a significant main effect of finger roll both along and across the finger axis for all participants , except one whose error rates did not differ significantly across the finger axis.
As hypothesized, all three angles had an impact on touch location and lead to distinct clusters, supporting hypotheses 1-3.
As expected based on Forlines et al.
A "flatter finger" caused the touchpad to locate targets farther away from the target towards the user's palm.
Somewhat surprisingly, variations in roll impacted touch location primarily along the finger axis as well, more than across .
Finally, as expected, there also was a significant effect of yaw on touch location.
This is also obvious in Figure 5 where none of the groups of ovals are centered on the target.
This emphasizes the fact that global offsets, as applied by Vogel and Baudisch  need to consider hand yaw.
Instead, accuracy now means size of clusters, as all other factors can be compensated for.
Since the cluster sizes for Participants 3 and 4 are comparable, this means both participants will perform equally well under the new model.
Also as hypothesized, there was an effect of user on the touch location.
As shown in Figure 5, the clusters of recognized touch positions varied across participants, and they did so quite substantially.
Figure 6 shows a particularly different pair: For Participant 4, touch locations vary drastically with pitch, while pitch has very little impact on the touch locations produced by Participant 3.
Based on this chart, one might think that Participant 3 is simply more accurate than Participant 4, e.g., that Participant 3 performed the task with additional care.
Whether this is true or not is a matter of perspective.
When we look at the size of the individual clusters of the two users, we see that they are roughly comparable.
This means that both participants reproduced the target location equally well.
What differs between the two participants is their mental model.
Participant 3's understanding of touch coincides strongly with the capacitive touchpad model.
So while Participant 3 is more fit than Participant 4 when operating today's touch devices, when using an input device based on the generalized perceived input point model this is not the case anymore.
As we explain in the following sections, such a device compensates for roll, pitch, yaw, and user ID.
Overall, and most importantly, our study supports our main hypothesis: roll, pitch, yaw, and user ID all lead to distinct clusters .
As a matter of fact, these clusters are clearly separated, as discussed earlier when explaining Figure 2.
Our findings therefore support that the generalized perceived input point model indeed explains a significant part of the inaccuracy of touch, rather than the fat finger problem.
These observations suggest that a device should be able to obtain improved accuracy by applying compensating offsets for each condition.
The data from our study allows us to make predictions about the performance such a device might achieve.
Figure 7 shows a summary.
Each bar denotes the diameter of a round button that contains 95% of all touches, assuming that we apply compensating offsets for different subsets of factors.
Each bar was computed by mapping the centroids of different sizes of clusters to the target center location.
For the traditional touchpad condition , no mapping was applied.
For the "per yaw" condition, the centroid of all touches was moved to the target.
For the `per yaw and roll/pitch' condition, the centroids of each roll cluster and each pitch cluster were moved to the target.
For the `per all' condition, the centroids of each roll cluster and each pitch cluster for each participant were moved to the target.
As illustrated by the chart, each additional piece of information should allow the device to further improve its accuracy up to a factor of 2.75 if all angles and user ID are included.
Instead of buttons measuring 15mm, such a device should allow users to reliably acquire buttons measuring 5.4mm.
A factor of 2.75 suggests considerable potential.
In order to exploit it, however, we need a touch device capable of extracting these four additional variables from a touch interaction.
We have created two prototypes of such devices.
One of them is RidgePad.
Every calibration trial produces a pair of a fingerprint image and an associated target position relative to the center of the fingerprint, i.e., an offset.
All pairs are stored in the user's profile.
The profile is user-specific, but not device-specific.
This allows users to calibrate future devices instantly using an existing profile that is associated with their fingerprint.
RidgePad is a touch input device that implements the requirements of the generalized perceived input point model based on a fingerprint scanner.
Traditional touch devices, such as the FingerWorks pad obtain only the contact area of the finger with the surface.
RidgePad obtains the same contact area--from a fingerprint scanner, however .
The fingerprint offers two additional types of information.
First, it allows RidgePad to identify the user.
Second, it allows RidgePad to analyze the portion of the user's fingerprint that is located inside the contact area.
Based on its analysis of which part of the fingerprint touches the screen , RidgePad infers all three finger angles, i.e., yaw, pitch, and roll.
This mechanism allows RidgePad to extract rolling and dragging from a single interaction, as shown in Figure 9.
During actual use, users touch RidgePad's surface just like any other touch device.
RidgePad computes the center of the contact area as a reference point.
It then compares the observed fingerprint with all fingerprints in its database .
RidgePad compares fingerprints using the generic image-matching algorithm SURF by Bay et al.
SURF extracts features from images, such as intersections of lines.
It then finds the image transformation that maximizes the number of features that line up.
The number of fingerprints in the profile that have some match with an observed fingerprint is typically large.
To determine which fingerprints are most likely to represent the pitch and roll position of the observed fingerprint, RidgePad simply uses the number of features SURF was able to match as a metric.
This works because two images are likely to exhibit similar features if and only if similar parts of the finger touched the surface.
All features typically match only if pitch, roll, and yaw were identical.
Based on this similarity function, RidgePad looks up the k closest matches in the user's profile .
RidgePad then averages the offset values associated with the chosen neighbors  and finally adds that offset to the center of the current touch location.
By default, RidgePad is only as good as a regular touch pad.
It achieves improved accuracy through calibration.
During calibration, users repeatedly acquire a single target on the fingerprint scanner.
It is not necessary for users to touch under specific roll, pitch, and device rotations; the more postures users cover, however, the more postures will benefit from improved precision.
The Guardian fingerprint scanner in our prototype offers a 3.2"  3.0" touch area.
As common for fingerprint scanners, it works based on frustrated total internal reflection .
Unlike FTIR implementations in current tabletop or wall systems such as , the glass surface is illuminated from below and its illumination is frustrated by the finger.
The generic nature of its algorithm makes RidgePad particularly robust and flexible.
RidgePad finds matches for a given yaw/pitch/roll/user fingerprint, because it finds other fingerprints that "look" similar; nowhere in the system are they ever labeled with angles.
While we designed the algorithm to work with roll, pitch, and yaw, it is independent of any such specifics.
It should therefore be straightforward to extend the algorithm to other features, such as finger pressure.
One of the limitations of the current implementation is time complexity.
The Guardian fingerprint scanner in our prototype requires a noticeable pause before transmitting a picture.
In addition, our non-optimized prototype code sequentially compares fingerprints with all fingerprints in the user's database, which takes 200-300ms for each comparison.
Future versions should be able to achieve real-time performance by extracting features up-front and comparing feature vectors using more suitable data structures.
USER STUDY 2: TOUCH PRECISION WITH RIDGEPAD Figure 10: The three interfaces: the fingerprint scanner simultaneously implemented the fingerprint interface and the control interface.
The red cameras below to the optical tracker interface, which was based on OptiTrack VT100 cameras.
Between trials, participants tapped the touch pad.
They committed using the footswitch.
To verify the performance of touch devices based on the generalized perceived input point model, we conducted a second user study.
We compared RidgePad and a device design based on an optical tracker with a traditional baseline condition.
Similar to the first study, we analyzed the effect of roll, pitch, and yaw.
Our main hypothesis was that RidgePad and the optical tracker would outperform the baseline condition.
The optical tracker interface, in contrast, obtained these angles from direct optical measurement of the markers position in 3-space.
Since fingerprint interface and optical tracker interface required per-user calibration, we used 80% of all trials  as training data for the respective calibration procedures.
We used the remaining 20% of all trials  for the actual analysis.
We tested three interface conditions, all of which were implemented by the hardware setup shown in Figure 10.
This particular setup allowed us process every targeting trial with each of the three interfaces simultaneously.
The fingerprint interface was implemented using RidgePad and employed the algorithm described in the previous section.
The control interface simulated a traditional touchpad interface.
It received the same input from the fingerprint scanner as the fingerprint interface.
However, this condition did not use the fingerprint features and instead reduced the fingerprint to a contact point at the center of the contact area.
The optical tracker interface was implemented based on a six-degree of freedom optical tracking system .
To allow the system to track the participant's fingertip, we attached five 3-mm retroreflective markers to the participant's fingernail .
The extreme accuracy of the optical tracker made this interface a "gold standard" condition that allowed us to obtain an upper bound for the performance enabled by the generalized perceived input point model.
As in our first study, participants acquired a single target repeatedly.
The target was drawn onto the surface of the fingerprint scanner.
Half of all participants acquired a target marked with crosshairs similar to our first study.
The other half of participants acquired a target marked with only a dot.
The additional independent variable crosshairs vs. dot allowed us to study the impact of the occlusion problem.
As in the first study, participants pressed "okay", acquired the target, and committed using a footswitch.
All participants completed all trials of one session in about 30 minutes.
Participants completed the same roll/pitch combination as in the first user study plus four additional variations of roll across 45 of pitch .
Participants completed the study in two sessions; the second session was identical to the first, except that we rotated the scanner for the same reasons as in our first study.
We counterbalanced the order of all conditions within sessions as well as sessions and rotations across participants.
Overall, participants completed 2 sessions  5 blocks  5 repetitions  13 angles = 650 trials.
We had two hypotheses: 1.
Optical trackers track angles with extremely high precision.
We therefore expected the optical tracker interface to redeem the entire accuracy benefit suggested by the first study, i.e., an improvement of a factor of 2.75 compared to the simulated capacitive control interface.
RidgePad cannot reconstruct angles quite as accurately as an optical tracker.
Still we expected the fingerprint interface to improve input precision substantially compared to the simulated capacitive control condition.
Average spread for each interface was 1.9mm/1.4mm/ 0.9mm for the crosshairs conditions and 2.2mm/1.9mm/ 1.2mm for the dot conditions.
The fact that dot targets performed successfully as well, however, suggest that the methods proposed in this paper also apply to targets that are subject to the occlusion problem .
This study supports our claim that touch devices can increase accuracy by exploiting the generalized perceived input point model.
Figure 12 shows another perspective on the results.
It shows the minimum target sizes that users can acquire with 95% accuracy for each of the three interfaces.
Sizes were computed so as to include 95% of all touches across participants and conditions .
The circles on top of Figure 12 illustrate the resulting buttons to-scale.
The fingerprint interface achieves a minimum target size 1.8 times smaller than the control interface.
The optical tracker interface reduces target size by a factor of 3.3.
The resulting button occupies less than 10% of the size of the control interface button.
Similar to the analysis of our first study, we compared the spread of recorded input locations .
We compared the mean input spread for each participant when using each interface.
We ran a one-way ANOVA on averaged per-participant spread with participant as a random variable and found a significant main effect of interface on spread .
Pair-wise comparisons using Bonferronicorrected confidence intervals showed statistically significant differences of spread between all interfaces .
The control interface caused the largest amount of average spread , followed by fingerprint-corrected locations .
Locations corrected with the optical tracker interface had the lowest average spread .
This means that the spread of touch input after fingerprint-based correction was on average 2.2 times smaller than when uncorrected.
On average, optical-tracker-based corrections brought down spread by a factor of 3.3 compared to the control interface.
In this paper, we made two types of contribution.
On the one hand, we made a technical contribution.
The RidgePad device achieved 1.8 times higher accuracy than simulated capacitive and we demonstrated that the use of high precision 3D tracking can more than triple touch accuracy.
This substantial increase in accuracy can be used to make touch interfaces more reliable or to pack up to 10 times more controls into the same touch surface.
A more distant future version of RidgePad might leverage in-cell technology to bring touch input to very small mobile devices, such as interactive watches and rings.
On the other hand, we made a contribution in the theoretical domain, which we tend to think of as at least equally important.
We introduced a new model for touch inaccuracy, the generalized perceived input point model.
We presented a user study, the findings of which are congruent with our new model, while they refute the fat finger problem, which was traditionally considered the primary source of touch inaccuracy.
This paper also contributes a new perspective on touch.
Touch has traditionally been considered a 2D phenomenon, most likely because touch screen interaction required only two coordinates, i.e., an x/y coordinate pair.
The proposed model, in contrast, establishes touch as a phenomenon of not only the touch surface, but of a wider context of threedimensional factors.
While we primarily investigated the user's finger posture in 3-space, this wider context may include additional factors, such as head position, device orientation, parallax, and so on.
Tracking these additional factors might allow future devices to realize even larger improvements in touch accuracy.
Additional research is required here.
Finally, we learned about users.
We found that users are not inaccurate--they are just different.
The most likely explanation for this difference is that touch on a millimeter scale was never defined in the first place.
For targets on this almost microscopic scale, pointing means to "dock" a comparably large, asymmetric object with a tilted surface.
Comparing the arrangement of ovals across Figure 5 clearly shows that no two participants of our study had the same mental model of how to accomplish this.
Contact area--the determining factor in today's touch technology--might be a factor, but clearly only one of many.
Additional research is required to reach a new and more detailed understanding of users' mental models of touch.
