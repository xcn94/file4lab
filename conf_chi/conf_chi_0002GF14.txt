We present CADament, a gamified multiplayer tutorial system for learning AutoCAD.
Compared with existing gamified software tutorial systems, CADament generates engaging learning experience through competitions.
We investigate two variations of our game, where over-theshoulder learning was simulated by providing viewports into other player's screens.
We introduce an empirical lab study methodology where participants compete with one another, and we study knowledge transfer effects by tracking the migration of strategies between players during the study session.
Our study shows that CADament has an advantage over pre-authored tutorials for improving learners' performance, increasing motivation, and stimulating knowledge transfer.
However, existing gamified approaches for software learning have not embraced the benefits of social and crowd-sourced learning solutions.
In particular, there have been a number of studies that have shown the benefit of "over-the-shoulder" learning environments, where users can observe and acquire skills directly from their peers or user community .
Since many existing gamified approaches to software learning have been "single-player", they miss out on this potential benefit of social learning.
In this paper, we present CADament a new multiplayer online game system for learning to use AutoCAD, a feature rich software application for creating mechanical and architecture design.
CADament is designed to be an online game that any number of players can join.
All active players compete head-to-head, attempting to complete a series of short time-coordinated levels, faster than an opponent they have been matched up against.
Two variations of this game are explored - one in which players see a split screen of their opponent's video while completing the levels, and one in which players review their opponents' video after each level has been completed.
By adding a multiplayer element, players have the opportunity for not only independent learning in an gamified environment, but also have the opportunity of acquiring new skills by observing the workflows and strategies of their opponents.
Furthermore, the multi-player environment adds an additional competitive aspect to the gaming environment, potentially increasing a player's motivation to learn and improve.
After describing our system, we present an empirical evaluation of our system.
In this study, we compare the two variations of our game, with a baseline help condition, where participants complete levels independently.
We propose a user group study methodology where participants compete with one another, and we study knowledge transfer effects by tracking the migration of strategies between players during the study session.
Our study shows that CADament has an advantage over pre-authored tutorials for improving learners' performance and stimulating knowledge transfer.
Learning to use feature-rich software applications is a notoriously difficult task for new users.
As such, there has been a long line of research on understanding and improving the learnability of software.
As a result, numerous mechanisms for providing assistance have been proposed, such as online help, interactive tutorials, and video-based assistance.
Unfortunately, the process of learning to use such traditional mechanisms can be tedious, and research has shown users are resistant to relying on help systems .
To address this limitation, gamified approaches have been recently explored, in an effort to increase the engagement level of learning .
Such approaches rely on design elements that are typical in games, such as feedback, guidance, time pressure, and rewards.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Since the early days of HCI research, software learnability has been an important research topic .
Early results include Carrol et al.
A full review of research on software learnability is beyond the scope of this article.
We direct the reader to a recent survey on this topic .
Also particularly relevant to our work is the research of Twidale and colleagues on "over-the-shoulder" learning .
Such work states that although some people like to learn a system on their own, many prefer learning in a more social context.
As such learning how to use a computer application is often a collaborative activity.
Our multiplayer game system is designed upon this premise.
Early work in software help indicated the importance of minimalist and task-centered help systems .
This is because of an active user's production bias , where a user does not want to take time away from the task to focus on learning about the system.
Although static online help supports quick access, it can be difficult to communicate complex graphical operations through text.
There has been some debate over the benefits of animated or video based assistance.
Earlier work argued that such materials enforce a passive learning process, force users to work at the pace of the video, and may be detrimental to long term learning.
However, recent work, such as ToolClips  and MixT , have shown clear benefits of using video based assistance for certain graphical operations.
Pause-and-Play also provides a unique way to deliver video content, automatically pacing the video based on user progress .
Ambient Help  mimics an over-the-shoulder learning environment by looping videos of other user's workflows on a secondary ambient display.
By automatically loading videos related to the user's current task, it was found that users discovered new concepts and workflows from the videos.
Alternatively, interactive tutorial systems can be integrated within the software application itself .
Stencils-based tutorials  overlay a stencil over a software application with a hole to click-through and guide a user to perform the correct steps.
The concept of flow  provides a general guideline for creating an engaging user's state of "optimal experience".
Qualifying factors for achieving flow include: perceptions of clear goals, immediate feedback, and matched skills and challenges .
Gamification is the use of design elements, which are characteristic for games, in non-game contexts .
Gamified systems have been used in many domains including educations .
The level of engagement that tutorials provide could potentially be increased by integrating gaming elements.
To create engaging learning experiences, several games were designed for learning software, including the Microsoft Ribbon Hero , Adobe LevelUp , Jigsaw , and GamiCAD .
The above systems were primarily designed for single player user experiences, so their competition level is limited.
Also, because the content of those games are predesigned, the learning experience is not flexible and adaptive.
When the player successfully covers the content, the educational experience has come to an end.
Here, we proposed a general approach to apply multiplayer gamification in learning software applications.
By providing a view into other player's strategies, users are continuously exposed to new content that they can adopt.
Because learning often happens in social environments, such as classrooms and schools, the multiplayer environments of modern games show promising applications in the educational domain .
However few games have been developed to support an over-theshoulder learning experience.
For example, multiplayer games for teaching math may not show exactly how other students do the addition tasks.
Recent research work has also investigated the educational benefits of 1-on-1 competitive games such as chess and StarCraft II .
However, in such games, one experienced player can only be directly challenged by another player in the same round.
CADament leverages video streaming and broadcasting technology, and it allows one player to have many challengers.
CAD drafting contests 1 are popular events among CAD software learners.
Usually, competitors need to draw specified views of a given drawing problem, or model small objects within a designated time period, and submit their drawings or models using emails or a website.
Informal CAD drafting contests often happen in CAD forums, where users challenge each other by posting and completing drawing and modeling tasks.
Similarly, image editing contests, such as Layer Tennis 2, exist in the professional designer communities.
The Photoshop contest is played through sequential alternating editing of an image using email or a web server.
A common problem of those software contests is that the usage skills and techniques are difficult to be transferred to other learners, as players and spectators cannot observer their opponent's workflows.
In CADament, videos are recorded and live streamed from software users working on the exact same tasks, then videos are integrated in a competitive multiplayer gaming environment.
We hope that the synchronized and contextbased videos can improve the user's over-the-shoulder learning experience, while multi-player gaming creates a feeling of excitement and engagement.
The emergence of the world-wide-web has made multiplayer games extremely popular.
In general, the game elements in single player games are also available in multiplayer games, such as an epic background story, rewards, time pressure, feedback and clear goals .
But multiplayer games have several unique elements.
Previous research into the experience of video games has shown the importance of the role of challenge in the engaging game experience .
A game that allows for diverse people to play diverse ways is often more interesting and rewarding.
Single player games challenge players through pre-designed levels and time pressure.
Players are motivated by rewards and stimulus and play against pre-programmed challenges and/or AI-controlled opponents, which often lack the flexibility and ingenuity of regular human thinking.
Multiplayer games introduce different challenges in direct general competitions and head-to-head competitions.
General competitions are based on the individual's score or relative standing .
Head-to-head competitions exist in many types of multiplayer games, such as racing, sports tournaments and MMORPG .
In those games, people have different personal styles when playing against each other.
Sharing is a core element of multiplayer games, where players often share the same task space, common goals and challenges, unified feedback and performance metrics .
Many multiplayer games setup real-time interactive communications among players to help them observe their opponents' actions and provide an immersive experience.
This actually encourages learning by watching other players who exhibit different strategies.
For example, in Need for Speed World, a popular massively multiplayer racing game, drivers all share the same racing tracks and real time graphical effects during a race.
Drivers not only see other cars' current positions and rankings in a map, they can also observe real time actions and techniques used by other players.
In some other games, player's actions are shared after or between tasks.
In Wordament, a successful word spelling game, players can only see other players' performance and answers during the breaks between tournaments.
Some games have both forms of sharing.
The pace of video games is the rate at which players experience new challenges.
It can be maintained at appropriate levels throughout the game , so that a game's design applies pressure but does not frustrate the player.
Player fatigue can be also minimized by varying pacing during gameplay .
Multiplayer games often do not have a "pause key" as in a single player-game, since the progress of the game is synchronized among all players.
For example, MMORPG games, such as World of Warcraft, never stop within a virtual persistent game world and continue to exist and evolve while a player is offline and away from the game.
Other multiplayer games, such as Wordament, generate rapidly-short-iterating rounds.
Slow periods follow intense ones and forced "time-outs".
This type of lightweight design can offer opportunities to socialize, catch your breath and anticipate things to come.
It often achieves a balance between competitive and enjoyable experiences while softening the impact of defeat.
Rapidly-short-iterating rounds also assist the knowledge transfer by learning skills from many different players and help new users practice the same tasks multiple times.
In this section, we define four design goals, to guide the design of our multi-player software learning game.
In terms of engagement, we aim to provide an environment that users will enjoy and be motivated to learn the software and improve their skill level.
Competitive game play, including both head-to-head competition and general competition, helps learners strive to be better and creates a feeling of excitement and increases engagement level.
In general, we hope that the system can assist transferring knowledge among users.
Over-the-shoulder learning  is informal, spontaneous help - a given interaction that is often used by people to learn from their colleagues in their workplace.
We create a similar environment to show other players' solutions within the context of the game.
Authoring tutorials for software applications is a time consuming process.
We wish to create a system that can help experienced users easily share their knowledge and skills to novice users and lower the cost of authoring learning content.
In multiplayer online games, every competitor is implicitly incentivized to be a content producer and create learning experience for other players.
The learning experience should match a learner's skill level to create an "optimal experience" or flow .
We design CADament, a competitive multiplayer online game for learning AutoCAD, which is a widely used software application for both 2D and 3D drafting and design.
A car racing theme was used.
CADament runs continuously in 30-seconds rounds, with a 30 second break between rounds.
Players can join the game anytime and participant in the immediate next round with all online players.
In each round, players solve a drafting task using AutoCAD.
Each task, Tn, repeats in 3 consecutive rounds, with a break B in between each round, before being replaced by the a new task Tn+1 .
This rapidly-iterating round format should help the players learn skills from different challengers and allow new users to practice the same tasks multiple times.
In Record & Replay, the players' screens are recorded during the competition.
Players can watch their opponent's recorded video during the breaks between rounds.
This allows users to focus on the skills used by their opponents.
In Split Screen, players can see their opponent's screen in real-time while completing the tasks.
The real-time sharing simulates a head-to-head competition experience, similar to many online multi-player games.
Both designs can simulate an over-the-shoulder learning experience.
In the Replay mode, learning happens during the slower pace of the break period, and the player can focus entirely on completing the level during the round.
In the Split mode, users have the opportunity to learn from their opponents strategies right away in the same round, instead of waiting for the next break.
During the rounds, each player competes against another online player to complete a drafting task using AutoCAD.
At the beginning of each round, the initial drawing data will be automatically opened in AutoCAD.
In Split mode, every player produces a live broadcast of their AutoCAD screen area.
A player sees their opponent's screen next to their own .
Through the live streaming video, the player is able to observe her/his opponent's detailed actions.
An overlay indicates the opponent's mouse behaviors and keystrokes .
In Replay mode, the opponent's live video stream is not displayed but is replaced by a blank image.
Additionally, the players' application screen videos are saved and played during the 30 seconds break period.
The tasks in the game were designed to help players to continuously increase their skills through progressive disclosure.
For our prototype, we developed a set of tasks for the TRIM tool.
An actual deployment could iterate through tasks for many different tools, or contain tasks requiring multiple tools.
Tasks gradually increased in difficulty, and eventually looped back to the initial task.
CADament does not enforce a specific workflow, or a single approach that all players must follow.
Instead, CADament compares each player's drawing with the pregenerated final result every time a command is executed, and reports the percentage of the task that has been completed.
For example, if the task requires the user to trim 4 lines, then after trimming 3 lines a player's progress would be 75%.
In our design we leverage a sharing mechanism to assist software learning and knowledge transfer.
By sharing the screen of online players, we can create a mixed experience of head-to-head competition and over-the-shoulder learning.
We explored two different game modes to allow players to see each other's screens, Record & Replay  and Split Screen .
In both game modes, a task bar is also displayed below the main screen .
A before-and-after image describes the goal of the current task.
A timer starts counting down as soon as the round starts.
The players' real time progress is visualized using two progress bars, and a waving checkered flag is displayed once a task is finished.
Our matching algorithm is based on the rankings for the immediately preceding round: 1.
The leader is matched to the second place player.
A player that successfully completed the last task will be matched with the player who was one rank above.
A player that did not complete the task will randomly play with one of the players who completed the task.
If no player completed the task, everyone will be matched with the player who has the highest task completion percentage.
When there are multiple candidate opponents, based on the above rules, the opponent is randomly selected from the candidates.
Figure 4 shows the view during the breaks between rounds.
The performance data of a player and the player's opponent for the previous round are displayed on the left side of the screen.
A ribbon is displayed for the winner of the head-tohead match-up.
A leaderboard shows the rankings of all players currently competing, sorted first by the task completion percentage and then by speed .
Additionally, in the Replay mode, a video player automatically plays the opponent's video after the round on the right .
Users can also replay the opponent's video during the 30 second break time by clicking the `replay video' button.
At the beginning of each round, CADament matches each player with an opponent.
The mapping from player to opponent is not one-to-one.
That is, if player A is matched to player B, player B is not necessarily matched to Player A.
This allows for more flexibility in the matchups.
For example, all players  could be matched against a player slightly better than themselves.
Many-toone matchups are also possible.
For example, all players could be matched against the leader in a single round.
In addition to the above requirements, there are two types of situations we try to avoid.
First, if everyone plays against the top ranking player, a novice player may feel frustrated if she/he keeps being defeated by the best player.
CADament updates a large amount of player's game data frequently in real time and synchronizes the game progress on every client.
We used WebSockets  in our client-server communication component, which enables high frequency updates from the server to support a realtime gaming experience.
For sharing the players' screens and actions, screen captured videos need to be broadcasted over the internet.
In the Split game mode, the actionviewing delay between one online player's action and the generated video being played at another player's game needs to be within an acceptable range.
In order to achieve this goal, we deployed a live video streaming server .
Videos are encoded at each player's computer in real time, and then "fed" to the streaming  servers.
Finally, they are distributed to multiple clients for viewing.
We optimized the whole process so that the action-viewing delay is less than 2 seconds in our local lab intranet environment.
In the Replay game mode, CADament records every player's screen stream to their local computer first, and then starts streaming based on demand when the round ends and an opponent needs to view the video.
To answer these questions, we performed a between-participants study with 3 conditions - one condition for each game mode, and a third baseline condition consisting of a single player tutorial environment.
Twenty-four participants  between the ages of 19 and 30 were recruited via online postings in two universities.
None of them had used AutoCAD.
All participants played video games occasionally.
Five of them play multiplayer online games regularly.
One participant reported not playing any multiplayer video games at all.To conduct the study, we divide the 24 participants into 3 groups of 8, with each group being assigned to one of the conditions .
For each group, we randomly choose 4 of them to serve as "trained" users, and spent 15 minutes to teach them how to use the TRIM tool.
This allowed us to simulate an environment where both novices and experienced users were participating, introducing knowledge into the gaming environment.
The TRIM tool works by selecting a set of "cutting edges" and then selecting a set of lines to trim, at the point which they intersect a cutting edge.
The order of steps which must be performed can be difficult for a new user to master, and there are many shortcuts which could be used to improve efficiency with the tool.
As such, it serves as a good tool to use within a learning study .
To further understand the effects of knowledge transfer, we gave the trained participants two additional "tips" which we could reliably track the usage of throughout the study.
The trained participants were split into two pairs.
Pair A was taught Tip #1 and pair B was taught Tip #2.
The two tips are typically exhibited by expert users, and can help users finish trimming tasks efficiently, but may not be immediately obvious to novices: Tip #1 , users can select multiple objects to trim by dragging the mouse over the drawing area.
Tip #2 , users can press the ENTER key and select all objects as cutting edges.
The third condition , which served as a baseline, was a non-gamified single-user environment.
The 8 participants  in this condition completed the study individually at separate times.
The Tutorial condition was also performed using the CADament framework, but we removed all of the game elements  from the system.
Instead of playing with other people, participants in the 3rd condition worked alone on the drafting tasks, and were provided with a tutorial website .
The website consisted of a main page that explained how to use the TRIM tool with text and a video.
In addition, at the bottom of the page there were two links to pages that explained the two tips.
These pages also included both text and a video.
As such, the amount of information within the tutorial was equivalent to the information that the trained users were seeded with in the other two conditions.
The webpage was available during the 30 second breaks.
Similar to the Replay condition, participants could review material after the task, and between rounds.
Apparatus The study was conducted in a room with 8 PCs.
A standard mouse and keyboard were connected to each PC.
All PCs were connected to a local Gigabit Ethernet and the internet through a regular office connection.
Separating panels were set up to prevent participants watching their neighbors' screens .
While this environment is not a true representation of a distributed online environment, it provides a close approximation for the purpose of our research questions.
The study consisted of a series of 30-second rounds separated by 30-second breaks, as described in the design of the CADament system.
A total of 12 training tasks were designed for the TRIM tool, and were presented in increasing order of difficulty.
Each task was repeated for 3 rounds before moving to the next task.
To measure the users' learning progress, we also included a testing task.
The same testing task was injected before the first task, and after the fourth, eighth and twelfth task.
Each of these four instances was repeated for 3 rounds.
In addition, a warm-up task was included at the beginning of the study.
The warm-up task used the LINE tool, and was used to familiarize participants with the study environment.
The warm-up task was repeated 3 times.
In total, participants completed 48 training and testing rounds.
Since the system ran on a timer, the study took 48 minutes.
Participants answered a questionnaire and provided comments after all training rounds and test rounds were finished.
The entire session took approximately 75 minutes, including the pre-training time.
To better explain the testing task results, we wanted to study how knowledge was being transferred from experienced users to novice users.
In Figure 11 and Figure 12, we visualize each player's usage and exposure to a tip.
Usage of a tip is shown as a solid red dot and exposure to the tip as an outlined dot .
Each row represents one of the six users who were not initially trained with the tip, and we show the trials in which either event occurs  until the player has successfully completed three tasks using the tip, which indicates that a player has adopted that tip.
Figure 9 shows the average completion times across all untrained users in each of the testing tasks.
A time of 30s is assigned to rounds which were not completed.
By the last task, untrained Replay users completed the testing tasks in 6.4 seconds on average, which is less than half of the time for the two other conditions.
The top chart in Figure 11 shows that the 6 players in the Replay condition, who did not have any training on Tip #1 adopted this skill after an average of 5.7 exposures.
Among those 6 participants, 2 participants learned the basic TRIM command during the pre-training .
The bottom chart shows that only 3 out of 6 players in the Split condition adopted Tip #1, after an average of 11.8 exposures.
The results for Tip #2 are even more drastic.
Figure 12 shows that 5 out of 6 players in the Replay condition adopted Tip #2, after an average of 9 exposures while in the Split condition, only 1 player adopted it, after an average of 16.6 exposures.
Together, this indicates that the Replay condition provides for better knowledge transfer.
Note that Tip #2  seemed to be harder to distinguish from watching a video, which may explain the lower adoption rate.
More importantly, Figure 13 again indicates that the Replay condition is more effective for learning, because every participant in this condition progressed from failing the tasks, to succeeding using both tips.
In contrast, no untrained participant in the other two conditions ever completed a trial using both tips.
We can also compare the content that participants were exposed to throughout the study.
In both of the multiplayer game conditions, the skill level of the opponents was adapted to each player's skill progress.
For example, there are only a few markers under the blue lines in Replay and Split conditions.
This implies that our match making algorithm tends to select more knowledgeable opponents for each player.
However, in the self-learning tutorial condition , we can see that the tutorial content was very static.
It shows that participants rarely accessed their tutorial content even as their skills progressed.
We measure perceptions of enjoyment, frustration, and usefulness through a survey given after all training rounds and test rounds were finished.
In the post-study questionnaire, participants were asked to rate 5 questions on a 1 to 5  Likert scale.
We ran Wilcoxon Signed Ranks tests to compare their answers .
The responses to question 1 and 3 were significantly different  between Replay and Split, and the responses to question 1, 3, and 5 were significantly different  between Replay and Tutorial.
The main result is that participants found the Replay system significantly more enjoyable and engaging than both the Split system and the Tutorial system.
Although people did not indicate significant difference in terms of frustration level, it was clear that, in the Split condition, learners have difficulty to learn from their opponent's video while they were working on a cognitively demanding task at the same time.
We next analyzed the untrained participants' skill progression over the training and testing tasks for the three conditions .
Specifically, we wanted to track and visualize if an untrained participant failed or succeeded to complete the task and if they used one or both tips .
The blue lines in Figure 13 indicate the outcome of a given trial for each untrained participant.
Participants either failed, or succeeded using no tips, one tip or two tips, which is represented in the vertical axis.
Each marker represents the exhibited behavior of the player's opponent for each round.
For the Tutorial condition, a marker shows which webpage was displayed on the screen  for a trial.
If the marker is filled-in, the user played the video associated with the tutorial content.
It can be seen from Figure 13 that by the end of the study, all participants were able to complete the tasks successfully.
Many online games offer a voice chatting room function.
It would be reasonable to expect players to use similar verbal interactions during real online game play.
Chatting could also be used as an extra channel for learning.
Authoring software tutorials is a time consuming process.
CADament presents a method for developing tutorials using multiplayer gamification.
As an important advantage over pre-authored tutorials, each player generates a learning experience for the other player.
On the other hand, when there is a shortage of experienced players, based on our video sharing design, pre-recorded video could also be used to simulate real players in the head-to-head competitions.
Software users also have difficulty to find tutorials based on their current skill level.
It is encouraging that both multiplayer game designs of CADament generate adaptive learning content through match making and competing with other players .
There are several reasons that performance did not improve as much in the baseline traditional tutorial condition.
First, users may have been less motivated to work on improving their performance, since they were not in a competitive environment.
Second, users may have seen a video for a tip, but not realized the implications of adopting that tip on their performance.
In contrast, users in the two CADament conditions would see the tip by an actual user, and realize immediately that adopting the tip would be necessary to improve performance and stay competitive in the social learning environment.
In this paper we focused on three elements of multiplayer games: competition, sharing, and pacing.
In the future, we would like to investigate other important elements in multiplayer online games, such as collaborative task solving and social communities.
It would be interesting to further compare CADament to single player gamified tutorials and investigate the effect of skill retention.
We also want to generalize the multiplayer gamification attributes to other software applications and higher level learning tasks.
We designed CADament, a multiplayer gamified tutorial system.
Compared with existing gamified software tutorial systems, CADament generates engaging learning experiences through competitions.
Active players compete head-to-head, attempting to complete a series of short timecoordinated levels.
CADament is also a crowd sourcing platform for players to collaboratively produce learning content while they are competitively playing the game.
We investigated two game designs, where over-the-shoulder learning was simulated at different pacing stages in the game.
Our study shows that the multiplayer game has advantages over pre-authored tutorials for improving learners' performance.
Study participants were not allowed to talk about how to use the TRIM tool.
However, they still were allowed to talk.
During the study, we observed rich interactions among players.
After the first few rounds, some players started to verbally challenge other players, especially when the leaderboard showed up.
