User comments posted to popular online tutorials constitute a rich additional source of information for readers, yet current designs for displaying user comments on tutorial webpages do little to support their use.
Instead, comments are separated from the tutorial content they reference and tend to be ordered according to post date.
We propose and evaluate the TaggedComments system, a new approach to displaying comments that users post to online tutorials.
A laboratory evaluation with 16 participants shows that, in comparison to the standard comment layout, TaggedComments significantly improves users' subjective impressions of comment utility when interacting with Photoshop tutorials.
An element of online tutorial design that has received little attention is the comments section.
A recent analysis found that user comments on popular text- and image-based tutorials constitute a rich and diverse source of information .
For example, readers thank the author, write contentspecific questions, provide answers to other users' questions, offer tips, and make requests for help on tasks that are extensions of the given tutorial, or are only tangentially related.
Despite the utility of these comments, current comment display interfaces place the comments at the bottom of the tutorial and order them chronologically, doing little to leverage their value.
This design is problematic when the reader seeks information likely found in the comments, as the next example illustrates.
Consider a user attempting to solve a problem that a previous user has faced with the tutorial.
To find relevant comments, she would need to scroll to the bottom of the tutorial, and read through the chronologically-ordered list of comments, sifting through dozens of unrelated comments .
If she does manage to find a relevant comment, she would then need to re-locate her place in the tutorial content before continuing the tutorial.
This added cognitive load would be on top of any created by trying to follow and learn from the original tutorial content.
We present an alternative design for tutorial comment sections of online text- and image-based tutorials that allows users to tag their comments, and optionally pin them to particular locations in the tutorial content.
We also describe the results of a laboratory evaluation examining TaggedComments' impact on users' experience completing a Photoshop tutorial.
Our results show that users' perceptions of comment utility were significantly higher with TaggedComments than with the standard comment organization.
Our results also suggest that TaggedComments helped create a sense of community surrounding the tutorial.
Information interfaces and presentation : Miscellaneous.
Feature-rich software can be difficult for users to learn and use .
While most applications include in-application help pages, users' help-seeking practices typically involve turning to their favourite search engines , where they are directed to a large array of web-based tutorials.
Given the now important role of online tutorials, there has been strong interest within the research community in improving the experience of interacting with tutorials .
Copyrights for components of this work owned by others than the author must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Publication rights licensed to ACM.
Our coverage of related work focuses on approaches to improving application tutorials, alternative forms of crowdsourced help for feature-rich software, and comment tagging in support of other types of complex tasks.
Prior work on enhancing application tutorials falls into three primary areas: automatically generating tutorials, supporting tutorial selection, and supporting use of tutorials.
We discuss each area in turn.
One line of work has looked to improve application tutorials by making it easier for authors to create them.
Much of this work has focused on automatically generating tutorials based on workflow demonstrations.
For example, Grabler et al.
Taking a different approach, Chronicle provides interactive, annotated document histories .
In this sense, our work shares similarities with systems like IP-QAT  and LemonAid , which provide in-application assistance for use of software through integrated, crowdsourced Q&A.
In contrast to this prior work, tutorials are typically multi-step documents describing complete and often complex workflows.
As a result, most tutorial comments are targeted toward the tutorial, the workflow it describes, and the use of the software to accomplish that workflow, as opposed to use of the software in general.
Tutorials also include multiple stakeholders and interested parties that must be addressed in any design: the readers and the tutorial author.
While we are not aware of other systems that support tutorial comment tagging, our approach is motivated by prior work demonstrating the value of tagged contributions in support of other complex tasks.
CommentSpace, a collaborative visual analytics tool, encourages users to tag their comments as hypotheses, questions or to-do items .
An evaluation showed that when using CommentSpace, users generated higher-quality responses for a data synthesis task than when they performed the task using untagged user comments.
Similarly, PathFinder supports citizen science by allowing individuals to annotate findings and comments .
An evaluation of Pathfinder showed that the increased organization afforded by tagged comments had a number of positive subjective impacts on users' perceived utility of the system.
Work on helping users select between tutorials has primarily focused on highlighting the application commands used in the tutorial.
Delta supports tutorial selection by displaying the commands used for each workflow  .
In contrast to these approaches, TaggedComments uses a summary of the community's response to a tutorial to help guide selection.
Lastly, there have been a number of approaches to designing more effective and engaging tutorials.
Stencil-based tutorials scaffold the user's progress through the tutorial by highlighting the commands required at each step .
Sketch-Sketch Revolution guides the user through a drawing tutorial, providing feedback and stroke guidance .
TApps use automation to reduce the effort involved in completing a tutorial, while still allowing the user to experiment with parameter settings .
Recent work has also proposed gamifying tutorials as a mean of increasing tutorial engagement .
There has been little work on enhancing tutorials using feedback from its user community.
One exception is FollowUs, which captures video demonstrations from users and makes them available on a step-by-step basis .
An evaluation showed that users experienced less frustration with FollowUs than they did in a control condition where only the author's demonstration was available.
In this section we describe our design for the TaggedComments system , which aims to enhance the role of user comments within a tutorial.
In designing the system, our goals were as follows: * Comment visibility: User comments contain a wealth of information, but are typically deemphasized at the end of the tutorial.
Our goal was to promote comments, increasing their visibility to other tutorial followers.
Our goal was to allow both tutorial authors and consumers to personalize their use of comments according to their information needs.
We wanted to reduce the separation between comments and their target tutorial content.
To accomplish these goals, the system asks users to both tag their comments and, when appropriate, to "pin" their comments next to the tutorial content to which their comments refer.
The next section describes how users tag and pin their comments, followed by an illustration of how TaggedComments leverages this information.
Figure 1: Left: The TaggedComments interface, consisting of: the comments section ; a graph representation of community response ; an area for users to "pin" comments to the tutorial content ; an image carousel of user-submitted results .
TaggedComments augments the traditional tutorial comment posting interface with a request for information to categorize the comment.
As shown in Figure 1, when the user enters a comment, s/he is asked to categorize the comment using one of six pre-defined tags.
These categories, based on Lafreniere et al.
We chose to focus on pre-defined tags for two reasons.
First, prior work suggests that pre-defined tags offer a number of advantages over free-form tags .
Second, using pre-defined tags allowed us to build dialogs into the system that prompt users for additional category-specific information to make the comments more valuable.
To accommodate emerging comment categories, users also have the option to enter their own free-form tags.
In addition to tagging their comments, users can choose to associate a comment with a position in the tutorial content by placing a "pin" on the vertical strip separating the tutorial content and the commenting interface  and Figure 2 for a closer view.
Each pin displays the number of user comments associated with that location and a callout to help reinforce the association between the pin and the comments.
We initially experimented with allowing users to pin the comments within the text itself , but pilot testing revealed that readers had difficulty locating the pins with such tight embedding.
The addition of tags and pins to comments enables the system to highlight community feedback, support personalized comment use, and provide in-context community help.
We describe each feature below.
To help users decide whether a tutorial is worth their time and effort, the system provides a succinct summary of community feedback based on the tags assigned by users.
Below the graph, the system displays an image carousel of example images  that users have produced by following the tutorial .
This information could also be embedded in search results, or a list of tutorials in a repository , to enable users to quickly evaluate a number of potential tutorials based on the percentage of appreciation comments, suggestions, and requests for help associated with each.
This kind of summary mechanism captures community response to the tutorial and could complement command-oriented approaches  for helping users to select between tutorials.
In addition to guiding tutorial selection, highlighting community feedback reinforces the notion of the tutorial as a "community meeting place"  and may encourage users to post.
There is also evidence that the act of tagging can encourage participation and create a greater sense of community .
To enable users to personalize the comment display to their particular information needs, TaggedComments includes features to filter and organize comments.
For example, a tutorial author might want to look only at requests for help when attempting to improve a tutorial, or requests for help on related tasks when planning to author a new tutorial.
Tutorial readers, on the other hand, might wish to focus only on suggestions and tips.
With TaggedComments, users can filter comments by clicking on the corresponding label in the bar graph or by selecting a category from the drop down menu.
Figure 3 shows an example of both an original set of comments and those filtered according to the "Suggestions" tag.
For this particular tutorial, tagged by us for illustration, there are 6 suggestion comments buried within 68 social comments.
This is a typical ratio for tutorials with comments on the web .
Clicking on a pin in the vertical strip next to the tutorial brings the user to the associated comment as shown in Figure 2.
Users can also navigate to pins from the comments section using "Go to Pin" buttons .
We conducted a laboratory study comparing TaggedComments to a control condition designed to emulate existing tutorial comment systems.
The goal of our study was to see how users would react to TaggedComments' interface, and to understand how its features would change how users referred to comments while completing tutorials.
We chose Adobe Photoshop, a feature-rich application with one of the widest showings of web-based tutorials, as our target application.
Participants were compensated with a $15 honorarium.
Our study followed a within-subjects design, with each participant attempting two different text- and image-based tutorials, one per condition.
We chose a within-subjects design to elicit comparative statements and to account for individual variability .
To increase external validity, we used tutorials from the web and the user comments already posted on them as a starting point for the tutorials used in the study.
In selecting tutorials from the web, we had three primary criteria.
First, we wanted each tutorial to include a reasonable number of comments, some of which could be helpful in completing the tutorial.
Second, we wanted two tutorials of similar length and complexity.
Finally, we wanted tutorials for tasks likely to be achievable by participants with moderate levels of Photoshop expertise, to ensure access to a sufficiently wide participant pool.
The two tutorials selected are shown in Figure 4 and summaries of their characteristics are listed in Table 1.
The Background task involved creating an abstract background and the Stitches task involved creating a stitches effect around the borders of text.
As can be seen in Table 1, we modified the tutorials to balance their length, and to keep them feasible to complete within a 90-minute study session.
These modifications consisted of removing steps from the beginning  and end of the tutorials.
Both tutorials included two steps that were difficult to complete by following the instructions as written, but for which there was helpful advice contained in the user comments.
For example, in Step 3 of the Background tutorial, the author suggests using Motion Blur to turn a noisy image into a series of crisp vertical bars.
This effect, however, does not work properly with all canvas sizes .
Below are two comments pertaining to this issue, a request for help and a reply with a suggestion:
In the Default condition, participants completed a Photoshop tutorial with comments displayed below the tutorial content in reverse chronological order by posting date .
This is the standard design for comment systems on web-based tutorials.
We recruited 16 volunteers , ages 18-40 via signs posted around a university campus.
Participants were pre-screened to ensure that they had some experience using Photoshop.
Following the tip suggested in the comment above would allow the user to complete Step 3 properly.
Each tutorial included an additional step where comments could potentially be helpful  but were not as critical.
While we initially intended to use only original user comments, we found that there were more helpful comments on the Background tutorial than there were on the Stitches tutorial.
To compensate, we created and added 15 contentspecific comments  to the Stitches tutorial, using similar terminology and commenting styles as the original comments.
We then selectively removed non-content related comments from both tutorials to balance the number of comments on each tutorial and the percentage of comments pertaining to tutorial content.
Finally, we manually added a tag to each comment, using the tagging scheme described earlier in the paper.
After completing a demographics questionnaire, participants completed the two tutorials described Table 1, one with TaggedComments, and one with the Default interface.
Interface and task order were fully counterbalanced across participants.
Prior to beginning each tutorial, participants were given a brief demonstration of the interface to be used in that condition.
While completing the tutorial, we asked participants to write down the three-digit code that we assigned to each tutorial comment for any comment that they tried to use to help complete the tutorial.
Participants were given 30 minutes to complete each tutorial.
After completing each tutorial, participants completed the NASA Task-Load Index , which measures perceived mental workload .
At the end of the session, participants completed a postsession questionnaire and participated in a short semistructured interview.
Each session lasted approximately 90 minutes.
Data were captured via log files, videotape and audiotape.
The videotape and audiotape recordings were later coded and transcribed for analysis purposes.
The experiment was run on a desktop computer with 12 GB of RAM and a 21-inch monitor with 1920x1080 resolution.
The TaggedComments interface was implemented using JavaScript.
We did not find significant effects or interactions for the order of the conditions or tasks, so we do not discuss these in our analysis below.
In all figures, error bars show standard error.
We begin by describing the impact of the two interfaces on participants' subjective impressions of comment utility and their ability to find comments, followed by their impact on task performance and comment use.
Responses to our questions regarding comment helpfulness, and participants' ability to find comments both clearly favoured TaggedComments .
1 The majority of participants also expressed a preference for the TaggedComments system in a post-study questionnaire, with 11 participants preferring TaggedComments, 2 preferring the Default interface and 3 expressing no preference.
Despite this clear difference in subjective preference, we did not find a difference in perceived mental workload as measured by the NASA TLX, either overall  or on any of the individual subscales.
Using the video recordings and the log files, we analyzed the extent to which participants were relying on the comments in the two conditions and the strategies that they used to search through the comments.
Table 2 displays the following: the number of visits to the comment section , the total time spent in the comment section , the number of comments participants appeared to read , and the number of comments that participants attempted to apply to their task .
While there are no significant differences between the two conditions, the data suggest two possible trends.
First, TaggedComments might have encouraged people to read through more comments than the Default interface.
Second, participants might have tried to apply a larger number of comments to their task with TaggedComments than with Default.
As with the impact of TaggedComments on task performance, further data would be needed to verify these trends.
Figure 8 summarizes the different strategies that participants used to find comments in the two interfaces.
With TaggedComments, 14/16 participants visited the comments section.
Participants' predominant strategy was to use the pins ; however, filtering was also used .
With the Default interface, only 7/16 participants made meaningful use of the comment section, with the remaining nine participants either not visiting the comments at all or scrolling through the comments quickly.
The 7 participants who did look at the comments did so by either systematically scrolling through them or attempting to locate relevant comments using keyword search via Ctrl-F.
The above results suggest that a larger number of participants made a concerted effort to use comments as an information source when using TaggedComments than with the Default interface.
That scrolling and quickly skimming comments were not used extensively also indicates that the features offered by TaggedComments provide participants with a viable alternative strategy for making use of comment data.
We analyzed three measures of task performance: overall time spent completing the tutorial, time spent on the tutorial not including time spent in the comments section, and task completeness.
We determined time spent in the comment section from the video by using mouse position as a proxy for gaze, a technique validated in prior work .
Figure 7  displays the impact of condition on task time.
The lack of significant performance differences between the two conditions indicates that, at a minimum, promoting user comments did not interfere with participants' abilities to complete the tutorials.
Given the large variability in completion times, however, it is also possible that we might start to see impacts of TaggedComments on performance with a larger number of participants.
With the pinned comments, it was not like I had to search through all of the comments, because I could just pick the comment  It was very useful for me.
To explore TaggedComments' potential role in promoting a sense of community surrounding the tutorial, we asked participants in the interviews to reflect on if and how the design might influence their willingness to post comments.
Participants spoke to two reasons for an increased willingness to contribute.
The first was a desire to help others:
I would probably feel that the odds of my question getting answered relative to what I'm asking would be higher.
With the other format, it lends itself more to people saying 'oh well you should read the question better'.
Whereas this one, I feel like -and it might just be my perception -- because it is linking much more closely and much more organized, if people were actually trying to help, they would be more directed to do so.
Personalized Comment Use: Participants valued the ability to filter the information according to their needs either explicitly or using the colours  as visual guides.
As expected given the experimental tasks, participants wanted to focus on the content-specific comments and to ignore social comments intended for the author of the tutorial:
I think that it's good that you differentiate `appreciations'  `how to do it'.
In the  there were too many comments about just appreciations, just thanking the author  You go through many comments thanking the author of tutorial and you cannot actually find the answer to what you are looking for  The colour coding makes it easy to tell which ones are actually going to be useful.
Like the little 'yay thumbs up' stuff -- much easier when you can just skip over that.
In both cases, the manner in which TaggedComments' organizes contributions appears to be helping participants view user comments as a legitimate help resource, one that they are interested in contributing to.
Not all participants, however, felt that TaggedComments would encourage them to post comments.
Two participants indicated that they often work with old tutorials and feel that there is little chance that their posts will be seen or responded to:
One of the big factors -- it is so silly -- but the comments are just so old.
So of course there is nobody who is contributing anymore to the tutorial that's been there for 5 years on-line and nobody ever looks there or needs help anymore.
These comments show that users are picking up a sense of how active the user community around a tutorial is, and that this is influencing their choice of whether or not to contribute.
In our study, TaggedComments had a positive impact on users' subjective experience of interacting with tutorial comments.
Participants found comments significantly more helpful and easier to find with the TaggedComments interface, and they expressed a strong preference for its design over the status quo.
These positive subjective impacts are encouraging on a number of fronts.
Our interview data suggests that improved perception of comment utility might increase community participation, which could improve the quality of community help over time.
Designs that display community feedback in a more attractive, accessible manner could also make completing tutorials more compelling, motivating more users to expand their skill sets.
While our study results suggest that TaggedComments improved the subjective experience of completing tutorials, we did not find a significant improvement in task performance.
We believe that there are several reasons why this may have been the case.
First, conducting our study on the granularity of entire tutorials allowed us to investigate participants' experience of using TaggedComments in a realistic setting, but it also introduced noise into our performance measures.
For example, some participants had trouble completing tutorial steps leading up to those that were intended to be challenging, and for which useful comments existed.
When participants did reach these steps, some proceeded with incorrect results or skipped over the step entirely.
A more tightly controlled study could address these issues by asking participants to complete a series of individual tutorial steps presented in isolation.
This would eliminate the impact of difficulties leading up to a step.
Second, not all participants had sufficient Photoshop skills to complete the tutorials.
As a result, some participants were able to find a tip in the comments, but did not have the necessary skills to apply it successfully.
While it is difficult to accurately gauge knowledge of complex software such as Photoshop, we could try using a pre-screening sample task in place of self-reports of expertise.
Finally, our study did not hide the fact that tutorial comments were potentially useful, and that comment use was a feature being studied.
This allowed us to elicit comparisons of the two systems from participants, but it may have also diminished the effect of a feature of TaggedComments -that it reveals to users that comments contain valuable information.
We suspect that more participants would have disregarded comments entirely in the control condition were they not aware that we were interested in measuring their use.
This could be tested using a between-subjects study design, where the role of comments is not revealed to participants.
While we evaluated TaggedComments as applied to Photoshop tutorials, there are a number of aspects of the design of the system and findings from the evaluation that are likely to generalize beyond this particular application.
First, Photoshop tutorials follow a format that is typical of many text/image-based tutorials for software use .
The one caveat is that TaggedComments' image-carousel feature would need to be modified to provide previews for other media, such as audio.
If one considers video-based tutorials, the notions of promoting, personalizing and integrating comments are also applicable, and there are existing analogues to pinning comments for time-based media.
For example, SoundCloud provides facilities for users to post comments at particular points of time for streaming audio files, with these comments visible within the timeline.
Beyond tutorials, we also envision applications of TaggedComments to other forms of long-form authored content, such as blog posts and news articles.
An open question with respect to the TaggedComments' design is whether users would be willing to tag their comments.
While only a deployment could provide a definitive answer, we believe there are reasons to be optimistic.
Wikipedia and StackOverflow  provide two further examples of communities where users align with cultural norms surrounding contribution.
One could also imagine designs that explicitly reward tagging, such as promoting only tagged comments , or delaying the appearance of non-tagged comments for a certain period of time.
Finally, one could explore supplementing user-supplied tags with automatically generated ones, following work on automatically identifying recipe refinements within user comments .
In a sense, TaggedComments allows people to become coauthors of a tutorial after the fact, while leaving the original tutorial content unchanged.
This represents an interesting alternative to community refinement via wiki-style interfaces, where content continually evolves.
A system like TaggedComments preserves author control and recognition, while still enabling the community to tweak and improve the content.
This is particularly important in tutorials for content-generation applications such as Photoshop, where authors often use tutorials as a platform to demonstrate their skills and abilities.
TaggedComments allows individuals to contribute to improving a tutorial's content in an organized manner, while allowing the tutorial author to maintain control over his/her work.
TaggedComments introduces a novel way of incorporating community feedback within a tutorial.
In TaggedComments, user comments are highly visible, tightly integrated with the tutorial content, and personalizable to the individual needs of both tutorial followers and authors.
Our study revealed that this design led to greater perceived comment utility and was preferred to the status quo.
Many users also responded positively to the increased sense of community surrounding the tutorial.
As future work, we plan to evaluate TaggedComments' role in aiding tutorial selection, as well its impact on user commenting quality and quantity.
We are also interested in understanding the utility of the design from the tutorial author's perspective, including whether or not it helps focus their efforts, and their attitudes towards this model of community refinement.
Finally, we intend to investigate the generalizability of the design and findings to other types of tutorials and long-form content.
