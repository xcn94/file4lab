We present an experimental study of automatic camera control in the performance of collaborative remote repair tasks using video-mediated communication.
Twelve pairs of participants, one "helper" and one "worker," completed a series of Lego puzzle tasks using both a static camera and an automatic camera system that was guided in part by tracking the worker's hand position.
Results show substantial performance benefits for the automatic system, particularly for complex tasks.
The implications of these results are discussed, along with some lessons for the use of motion tracking as a driver for camera control.
This shared visual context can be used to facilitate the negotiation of "common ground" in the ongoing conversation between the helper and worker .
Providing this shared visual context, however, can be difficult when the task involves the detailed manipulation or identification of objects in specific but disparate locations in a work area.
In surgery, for example, a detailed activity may occur in multiple areas of a patient's body.
In such scenarios, fixed-view "scene cameras" provide a useful overview, but little detail , while a camera mounted on the worker's head can provide greater detail, but constrains the helper's view to what the worker is focusing on .
While it is possible to simultaneously provide detail and overview by allowing the helper to control the camera or select between multiple shots , this has been shown to be potentially distracting, confusing and time-consuming .
An alternative approach proposed by Ou et al.
This suggests that a purely predictive approach may not be as effective as a hybrid one that attempts basic prediction as well as exploits the expected adaptation by the worker.
In this paper, we build on prior work by exploring the basic premise that worker hand position is a reasonable indicator of the helper's desired visual information.
We develop an automatic camera control system based on this premise, and provide empirical evidence indicating that it is highly effective in certain types of tasks when compared to a fixed camera.
There is a range of settings in which expert assistance may be required by a novice who is completing a complex realworld task.
Experts are not always physically close, however, so there is increasing interest in the use of collaboration technologies for tasks such as surgery in remotely located hospitals , repair of equipment in remote locations , and operation of scientific equipment .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Video systems necessarily constrain the range of cues that are available to do these things as compared with a face-toface environment, but have nonetheless been shown to be more useful than audio-only systems in completing collaborative tasks .
This is particularly true when the task in question is lexically complex - that is, when it involves elements that are difficult for participants to describe verbally, as was the case with the tartan plaid patterns used in Gergle's puzzle studies .
First is the identification of difficult to describe pieces.
Second is in their placement, when placement requires detailed manipulation or difficultto-describe orientation.
Moreover, work in this area has found that while there is typically not a strong need to use visual cues to monitor partner comprehension, this may be different if some component of the task requires face monitoring  or if users do not share linguistic common ground .
In most cases, however, video images of the shared workspace are more valuable than images of ones partner's face.
Thus, the most valuable cues seem to be those used for monitoring partner actions, task status, and establishing a joint focus of attention.
It therefore resulted in no performance benefits when compared with a static space.
It was potentially helpful, but underused and resulted in no performance benefits.
Gaver  also experimented with media spaces providing multiple views, and reported that cutting between views could be confusing and distracting.
Much of the work mentioned above suggests that there are some clear benefits to dynamic visual spaces, but helpers generally seem unlikely or unwilling to control the camera or select shots themselves.
While there are several possible reasons for this, the important point is that it has been shown repeatedly that they do not do it.
This suggests two potential approaches: third party human camera operation and automatic camera operation.
Human operators, when they are aware of the task and adept at camera operation, can be effective , but they can also be costly in financial terms and must be trained.
With this in mind, there has been substantial recent interest in automatic operation.
The goal in this work is to use some cue or combination of cues to predict the helper's desired focus of visual attention at any given moment, and use that prediction to drive camera operation or shot selection.
Prior studies have examined automating camera control in lecture rooms and meeting rooms using speaker tracking, detection and cinematography rules .
However, automatic camera control has not been explored in the context of a collaborative task as discussed here.
More elaborate models relating speech patterns to desired and actual visual information have also been developed .
In addition to speech, worker activity is another likely indicator of the helper's desired visual focus.
In observing pairs performing a Lego construction task, Ranjan et al.
They found a substantial correlation between the worker's dominant hand location in the workspace and the field of view of the operator's camera shot.
At the same time, however, they also found that pairs, consciously or not, used the workspace and shared visual space differently when it was dynamic than they did when it was static.
This suggests that predicting the helper's desired focus of visual attention is a slippery problem, in that what the helper wants to see at any moment depends, in part, on what the helper can see at that moment.
Shared visual spaces can be either static or dynamic in nature.
Static visual spaces provide the helper with a fixed view of the worker's work area, typically via an overhead or over-the-shoulder camera view .
Static visual spaces can be effective for monitoring the progress of tasks that take place in a constrained workspace, or that do not require very detailed observation of task actions or joint focus on minute details.
The capacity to establish a joint focus of attention can be augmented somewhat via systems that facilitate gesturing , but these do not allow for zooming in for detail.
Dynamic visual spaces, on the other hand, provide a range of views to the helper, either via a movable camera  or cutting between shots from multiple cameras .
This was useful, but substantially constrained the helper's range of view, and did not result in performance benefits over a static visual space.
First, we explore the extent to which the worker's hand position can be used as a predictor of the helper's desired focus of visual attention in a collaborative remote repair task.
Second, we are interested in developing insights for the design of automatic systems that have roots in prediction, but that exploit adaptations in user behavior.
All were required to have normal or corrected-to-normal color vision, and to use English as their primary language of communication.
Participants were paid $10, and were recruited via posted flyers and email lists at our university.
We use a full-factorial 2x2 within-participants design to compare the performance of pairs of participants - a "worker" and a "helper" - performing Lego construction and identification tasks at two levels of complexity, and in two camera control  configurations: Static camera: A camera above the worker's left shoulder provided a wide shot of the entire workspace.
Automatic camera: A single pan-tilt-zoom camera was located above the worker's left shoulder.
The camera shot was adjusted  based on the position of the worker's dominant hand.
As with the PC-based puzzle tasks used by Gergle , these tasks involve elements common to a range of realworld, collaborative remote repair tasks: piece identification, piece movement, piece manipulation and placement, and verification of correct placement.
The helper and worker were located in the same room, so they could hear each other, but separated by a 5-foot-high partition wall.
The worker was seated at a desk  divided into 6 discrete regions.
Five of these regions, referred to as "work regions," were marked with green Lego base plates.
The sixth, referred to as the "pieces region," was where the unattached pieces were placed, with white markings to define its rectangular boundaries.
With regard to the effect of camera configuration on task performance, we hypothesized that: 1.
Participants would complete all tasks faster with the automatic camera than with the static camera.
Participants would make fewer errors in the automatic camera configuration than in the static configuration.
The benefit of the automatic camera would be greater for lexically complex tasks than for simple tasks.
We also expected differences in satisfaction with the visual information provided and with system experience overall: 4.
Participants would be more satisfied with their performance in the automatic camera configuration.
Participants would value the automatic camera more for detailed views of pieces than awareness of partner activity in the workspace.
Motion Tracking- The workers wore partial-finger gloves  that had wireless, passive reflective markers attached to them.
We tracked the location of these markers with sub-mm precision .
Due to very slight shifting of the markers on the gloves themselves, the exact precision of whole-hand tracking was slightly less than this, but still adequate for our purposes Camera- A Sony SNC-RZ30 pan-tilt-zoom camera was positioned on a tripod 30 cm behind the worker's space, and above the worker's left shoulder.
The camera was connected via analog coaxial cable to the worker and helper monitors.
The camera was positioned so that it could capture all six regions of the workspace.
Displays- A 20-inch LCD monitor was located 20 cm in front of the worker's desk.
It displayed the camera output so that the worker was aware of what the helper could see.
The helper's space consisted of a desk with a 24-inch LCD monitor that displayed the camera output.
A Sony Mini-DV camcorder was located just outside the worker's space, and recorded all sessions for later analysis.
Based on Ranjan et al.
Hand movements towards the camera will be less in the automatic camera configuration.
The use of the dominant and non-dominant hand will differ significantly across camera conditions, i.e.
The overall task was for the worker to use Lego bricks to construct three four-layer "columns" in specific regions of the workspace, based on instructions from the helper.
Helpers were given a paper map of the workspace indicating which regions the columns were to be built in.
The columns were built one layer at a time, so a layer in all the columns had to be finished before moving on to the next layer.
In order to assess the value of visual information for different tasks, we used two types of tasks in each condition.
Two of the layers involved primarily "identification" of difficult-to-describe pieces, while the other two primarily involved "construction," which included detailed placement and manipulation of pieces.
In identification tasks, workers were provided with three similar, but not identical, pre-constructed Lego pieces .
Simple identification pieces were composed of three smaller parts.
Complex identification pieces were composed of 10-12 smaller parts.
Helpers were provided with an exact duplicate of each piece, one at a time.
The goal was for the helper to get the worker to pick up the correct piece, and place it in the correct region.
After each camera condition, the helper and worker both completed questionnaires that evaluated their perceived performance, the utility of the visual information for examining objects and tracking partner location, and the ease of learning to use the system.
The questionnaire items were developed for this study and validated by pilot data.
The automatic camera control system was based on data from Ranjan et al.
There were seven distinct shots that could be selected from: six were close-up views of each of the six regions and one was the overview shot of the workspace.
The overview shot was included to allow the helper to see where in the workspace the worker was, to be sure the tasks were taking place in the correct work regions.
Close-up shots were included to show detailed views of the construction and pieces as the tasks were underway.
In construction tasks, workers were provided with several smaller pieces with which to construct the layers of three columns.
In the simple construction task, each layer consisted of 10-12 square- or rectangle-shaped pieces.
In the complex construction task, a similar number of pieces was used, but the pieces were irregular in shape and orientation.
Helpers were provided with an exact duplicate of each completed layer, one at a time.
The goal here was for the helper to instruct the worker in constructing the next layer of each column, which included identifying pieces and placing them correctly.
Participants were permitted to talk to each other, but could not see each other.
They indicated to the experimenter when they thought each layer was complete, but were not permitted to move on until all errors had been corrected.
In order to more closely replicate activities  where detailed activity must take place in specific, discrete regions of a workspace, workers were not permitted to have more than one unattached piece outside of the pieces area at a time.
In other words, construction had to happen in the target region and be completed one piece at a time.
The position of the worker's dominant hand was constantly tracked in 3D using the motion capture system.
This information was used in real-time to determine the workspace region in which the worker's hand was located.
This, in turn, was used to determine the appropriate camera shot according to the following rules.
In these rules, the current work region location of the worker's dominant hand is called the "current work region," and the previous work region location is the "previous work region."
These are both distinct from the "pieces region," which is referred to by this name.
There were, essentially, four possible movement types and each resulted in a unique system response: 1.
Movement: The dominant hand enters a "current work region" that is different from the "previous work region."
System Action: Go to the overview shot.
Rationale: Moving to a new region meant that the helper was likely to need awareness information about where the worker was now located in the overall space.
Movement: The dominant hand stays in the "current work region" for at least 3.5 seconds after Movement 1.
System Action: Show close-up of current work region.
Rationale: Close-up of a work region shown only after it has been selected for construction and to avoid quickly changing views during the region selection process.
Movement: The dominant hand moves to a "current work region" that is identical to "previous work region" .
System Action: Immediately move to close-up of the current work region.
Rationale: Moving from the pieces area to a work area typically indicated that detailed work was about to occur.
Movement: The dominant hand moves to the pieces region and stays there for at least 2 seconds.
System Action: Show close-up shot of the pieces region.
Rationale: In prior work, most moves to the pieces region were extremely brief and having the camera simply follow the hand was confusing due to quickly changing views.
It is only when the hand lingers in the pieces area that a close-up is required.
The exact wait time of 2 seconds was decided after several pilot trials and on the basis of data from prior work .
Figure 3 shows a state diagram of the automatic camera control.
The states represent camera shots and the transitions represent possible movements.
These transition rules were developed iteratively, and we experimented with both continuous tracking and discrete, region-based tracking.
In the final design, even though the camera moves were guided by continuous movements of the dominant hand, the camera was programmed to make only discrete moves from one preset to another, as opposed to continuously following the hand over the entire workspace.
Discrete moves provided stable views of the regions despite significant hand movements inside the region.
The order of difficulty and camera condition were counterbalanced across all participants.
Participants were randomly assigned  to "helper" and "worker" roles, and were shown to their separate workspaces on arrival.
The task was then explained to them, and they were told that their goal was to complete it as quickly and as accurately as possible.
Workers then put on the gloves and participants completed simplified practice identification and construction tasks to ensure that they understood the details of the task.
In the automatic camera condition, the basics of the operation of the system were explained to the participants.
They were told that the camera movements were guided by the position of the dominant hand of the worker.
They were not given any specific detail of the algorithm controlling the camera.
However, as we will discuss later, the participants quickly understood the basic principle behind the automatic camera control, and some consciously made use of this understanding to "manually" control the camera.
The pieces for the first task were then placed in the pieces region, the helper was given the first model block  and the workspace map, and the pair was permitted to begin.
The completion of each layer, or subtask, was determined first by the participants, who reported to the experimenter when they believed the subtask was complete.
If, after examining their work, the experimenter determined that there were no errors, they were permitted to move on to the next subtask.
If errors were found, participants were informed that there was at least one error , and required to fix it.
Video of each session was analyzed to track and extract the completion times and the number of errors made.
Completion time was defined as the time from start to finish for the complete layer, as reported by the participants.
We considered only errors that were in place when the participants reported to the experimenter that they were done.
Errors made prior to self-reported completion were not tracked because it was not clear how these should be classified or when one would be considered an error .
Where there were errors, the number at the completion of each layer was counted, and the time taken to detect and correct errors was recorded separately.
Reliability of the questionnaire items was assessed using Cronbach's , which is a measure of the extent to which a set of scale items can be said to measure the same latent variable .
All of the scales used here except one had  values between .7 and .9, which is within the range considered acceptable for well-established scales .
The one remaining scale had an  value of .62, which is acceptable for exploratory work.
Confirmatory factor analyses indicated that each scale loaded on a single factor.
The study involved two independent task types: identification and construction.
Each task had two task complexity levels: simple and complex.
Each task was performed under two camera conditions: static and automatic.
Two-factor repeated-measures ANOVA models were run separately for the two tasks using task complexity and camera condition as independent variables.
Dependent variables were completion time and number of errors.
Participants also filled out questionnaires on completion of each camera control condition.
Questionnaire data were analyzed using repeated measures ANOVA models, including each term as a within-participants factor, and participant role  as a between-participants factor to test for interaction effects.
For the identification tasks, there was not a significant main effect for camera condition overall, but there was a significant interaction between task difficulty and camera condition =7.03, p < .05.
A trend similar to that in the construction task completion time can be seen here, though paired samples t-tests showed that the result is not statistically significant.
It should be noted that identification task completion times are substantially shorter than construction because the task involved fewer discrete steps.
We hypothesized above that the automatic camera condition would result in faster performance for all tasks , but that the benefit would be greater for complex/difficult tasks .
For the construction tasks, there was no statistically significant main effect for camera condition on completion time, but a significant interaction was found between camera condition and task difficulty =15.41, p<0.01.
No significant asymmetric transfer was observed between the two camera conditions.
This combination of results supports Hypothesis 3 and suggests that the automatic camera assisted task performance to a greater degree when the task was complex than when it was simple.
The left half of Figure 4 shows mean completion times under various conditions for the construction task.
The error correction times are shown on top of the bars.
We were also interested in the errors participants made in performing these tasks, for two reasons.
First, a reduced number of errors would suggest that an automatic camera system could be particularly useful in mission-critical settings where errors are costly or fatal .
Second, the situations in which participants made errors give us a potentially useful sense of the strengths and weaknesses of both camera conditions.
Only seven errors were detected upon the completion of all subtasks across all pairs of participants, and they were all in the construction task.
Six out of seven errors were detected in the static camera condition.
This suggests that the automatic camera system enabled participants to perform the tasks more accurately.
This was further reflected in the analysis of the number of dominant hand moves to and from the pieces area, where a larger number of moves in the completion of a task under one camera condition would indicate a larger number of misidentified pieces.
Even after standardizing the number of moves by dividing by the total number of minutes taken to complete each task, there were more moves to and from the pieces area in the static camera condition  than in the automatic camera condition  =3.76, p<.1.
These results support Hypothesis 2.
Errors caused by incorrect description or interpretation of color or other piece attributes  are considered piece identification errors.
Participants evaluated the quality of their performance as a pair, and their individual performance of the tasks.
Individuals rated their performance as more effective in the automatic camera than in the static camera condition =5.44, p<.05, supporting Hypothesis 4.
Moreover, there was a marginally significant interaction between participant role and self-reported individual effectiveness =3.95, p<.1.
While helpers reported slightly higher performance in the automatic camera condition  than in the static camera condition , there was no such difference for workers.
Somewhat surprisingly, particularly given the performance data presented above, there was only a small and marginally significant difference in perceived pair performance between the two conditions.
As can be seen in Table 2, perceived pair performance was slightly higher in the automatic camera condition than in the static camera by a relatively small, but still marginally significant amount =3.66, p<.1.
This supports Hypothesis 5 and suggests that the static camera condition was adequate for providing this information , and that the main difference between conditions was in participants' ability to examine detailed components of the task objects.
Participants also assessed the utility of both systems, in terms of how useful the video information was in performing the tasks, their ability to examine objects in detail, and their awareness of where in the visual space their partner was working.
In all of these cases, workers were assessing the perceived utility of this information to their partners, since they themselves were not relying on the video view.
As Table 2 shows, participants generally did not find the video useful  in the static camera condition, but did find it to be useful in the automatic camera condition =45.86, p<.001.
This suggests that there was value in the detailed view provided by the automatic camera condition, but that participants were able to adequately describe things verbally when this view was not available.
Combined with the completion time results presented earlier, however, these descriptions seem to have taken longer when the task was complex.
When we consider participants' self-reported ability to examine objects in detail, it is not surprising that they reported that they were substantially less able to do so in the static camera condition than in the automatic camera condition =81.04, p<.001.
Finally, participants were asked about the ease of learning to use and work with the two systems, where a higher score on this construct indicates an easy to learn system.
Again, there was no statistically significant difference between conditions.
This, combined with the fact that both mean scores were above the midpoint on the scale, suggests that the automatic camera system was not difficult for participants to learn.
It is not surprising that the static camera condition was easy to learn.
We were interested in the extent to which workers' physical movement in the workspace varied across camera control conditions.
To do so, we analyzed the motion capture data in which left and right hand positions were tracked for the duration of the experiment.
We first examined the vertical height of the worker's hands relative to the workspace.
In the static camera condition, holding a piece up towards the camera could be a way to distinguish that piece and provide a sort of primitive `zoom' capability.
If the automatic camera condition was effective, we would expect to see less vertical movement in this condition than in the static camera condition.
While the difference in means is small , it should be noted that the range of vertical movement is substantially greater in the static camera  than in the automatic camera condition .
This helps to explain the statistically significant finding and shows that the workers' hands were lifted substantially higher above the workspace in the static camera condition.
These results support Hypothesis 6.
We were also interested in user adaptation to the camera control system .
We were particularly interested in whether participants used their dominant and non-dominant hands differently in the two camera conditions.
While statistical analyses yielded no overall patterns in this regard, one worker did show signs of adaptation and we have analyzed his behavior here.
This participant made 94 dominant hand moves and 31 nondominant hand moves to the pieces region under the static camera condition, but only 40 dominant-hand moves and 74 non-dominant hand moves under the automatic camera condition.
By analyzing the video, we observed that this worker used the dominant hand to keep the camera focused on a particular region by leaving the dominant hand in that region, and using the non-dominant hand to get pieces from the pieces region.
This led to more frequent moves of the non-dominant hand to the pieces region.
This observation, though not common, has some design implications as we will discuss later.
Not surprisingly, hand type  had a significant main effect on the number of moves made to the pieces region =6.9, p<0.05, with the dominant hand making more moves than the non-dominant hand.
Moreover, the amount of movement by the dominant hand relative to the non-dominant one gives us some sense of the reliability of dominant hand movement as an indicator of changes in visual focus.
As can be seen in Figure 5, the mean number of times the camera moved to the pieces region for simple construction tasks is less than half the times the dominant hand moved to that region.
Since our automatic camera was programmed to follow all trips to the pieces region longer than 2 seconds, the fact that more than half of the trips were not followed shows that those trips were short.
On the one hand, the presence of numerous such short trips that were not followed by the camera explains why the percentage of time the dominant hand was in the camera view was lower for simple tasks; on the other hand, it restates our earlier assertion that visual information is not critical for simple tasks.
This indicates our camera control system succeeded, at least to some extent, in providing the information only when it was critically needed, which was one of the intents of our initial system design.
In order to evaluate the performance of our automatic camera system in capturing dominant hand activity, we examined the percentage of time the worker's dominant hand was inside the camera view.
For all the tasks combined, this percentage was 78.8%, indicating that the visual information about the dominant hand was presented to the helper a reasonable percentage of the time.
Further, for complex tasks the dominant hand was in the camera view more often than for simple tasks .
We began this study with the goal of exploring the value of worker hand location as a predictor of the helper's desired focus of visual attention in a collaborative remote repair task.
We developed an automatic camera control system that selected and adjusted camera shots based on the location of the worker's dominant hand, and hypothesized that this system would improve pair performance in terms of completion time and the number of errors, with possibly greater benefits for complex tasks.
The results show that our system had a substantial impact on reducing completion time and errors, but the benefits were not seen for both levels of task complexity.
Completion times were improved by a statistically significant margin only for complex tasks, but not for simple ones.
This partly reinforces Gergle's  finding that a shared visual space is more helpful for lexically complex tasks than for simple ones, but suggests further that the shared visual space must provide sufficient detail to allow for monitoring and discussing specific task elements.
This ability, however, is not unique to our study.
Prior systems, such as head-mounted cameras  or helper selection between multiple shots , have allowed for detailed task monitoring, but did not result in performance benefits.
This leaves the question of what it is about our system that yielded the benefits seen here.
We believe our use of hand tracking plays a significant role in this story.
Selecting camera shots via hand tracking has two significant benefits over prior systems.
First, compared with a head-mounted camera, hand tracking allows for looser coupling  of movement to shot change.
A head mounted camera can be described as extremely tightly coupled in that the camera necessarily changes focus every time the worker does - even when the changes are rapid or irrelevant .
This is potentially both intrusive for the worker and distracting for the helper, since the visual information is constantly changing.
Our system allows for the loosening of this relationship on both of these dimensions.
Waiting periods can be programmed so that the camera does not follow the worker on very rapid hand moves, and the camera can be restricted to task-centric regions  such that the worker's every glance is not taken to indicate a change in focus.
Second, our system requires less effort than those relying on manual operation by the helper or a third party operator.
Our participants indicated that the system was easy to learn, and its use required little, if any, conscious effort.
A few participants did, however, somewhat adapt their behavior to consciously control the camera.
This brings us to our final point of theoretical interest, which is the extent to which a system allows for and exploits behavior adaptation.
Clearly, a head-mounted camera allows for very little adaptation since the worker only has one head, and it must move if focus is to change.
Our system, however, allows for adaptation in that hand location is a reasonable predictor of focus, but the hand can also be easily moved to another region to "draw" the camera there, even if hand activity is not required in the new region.
Moreover, the non-dominant hand can also be used if camera movement is not desirable, as we saw with some of our participants.
The approach of tracking the objects serving such dual purposes could also be extended to other scenarios.
For example, in Gaver et al.
We observed that the static camera was as effective as the automatic camera for simple tasks, and was also efficient in conveying the information about where the task was being performed.
This suggests a potential role for static views as a fallback view for automatic systems in case of failures.
One of the reasons previous attempts to create a shared dynamic visual space using head-mounted cameras failed was unstable and shaky views .
In this study, special attention was paid to making the views stable in the system via region-based tracking and by introducing pauses at various transitions.
This strategy was specifically useful in the simple construction task in which the worker's dominant hand was moving frequently to the pieces area but the camera was not following it tightly.
This indicates that automatic systems must make provisions to balance the rate of showing visual information and the rate at which humans can process this information as excessive changes can potentially create a confusing visual space.
The experimental task has both strengths and weaknesses.
Having a consistent set of construction tasks allows for valid comparison across pairs, and the task involves components of many real-world tasks, such as piece selection and placement, and detailed manipulation of physical objects.
However, the task is necessarily contrived and it relies on a remote helper with limited experience in the task domain.
A possible limitation from this is that the helper was relying more heavily on explicit directions than memory, which could impact desired visual information.
On the other hand, this limitation is common to many experimental studies in this area.
Since our task was serial in nature and involved a single focus of worker attention, one could imagine that the worker's hand location would be a less accurate predictor of desired helper focus in a case where there are multiple activities taking place in parallel, or where activity in one region is dependent on information from other regions .
While this limitation does not negate our results, it cautions as to the set of domains to which they apply.
Another possible limitation of this work is the effect of the participants having known each other beforehand.
It is, of course, possible that participants had a shared vocabulary that would make these results less applicable to pairs of strangers.
On the one hand, full automation of camera control seems theoretically possible by better understanding the visual focus of attention; on the other hand, manual override cannot be avoided in practice for various reasons including the adaptive nature of humans.
Various instances of manual override in this study indicate that adaptive systems should provide fluid techniques for manual override.
The integration of low-overhead manual control with an automatic system is a challenging problem.
We plan to continue investigating several areas.
First, the worker's non-dominant hand was tracked, but the tracking information was not used in the automation.
Considering the encouraging results based on the dominant hand only, a better understanding of the role of the non-dominant hand and its incorporation in camera control is one possible research direction.
We could also consider incorporating other parameters such as gaze and head position.
Finally, we are also interested in exploring the possibility of combining motion detection with other means of predicting desired helper focus, such as speech parsing .
