Despite growing awareness of the accessibility issues surrounding touch screen use by blind people, designers still face challenges when creating accessible touch screen interfaces.
One major stumbling block is a lack of understanding about how blind people actually use touch screens.
We conducted two user studies that compared how blind people and sighted people use touch screen gestures.
First, we conducted a gesture elicitation study in which 10 blind and 10 sighted people invented gestures to perform common computing tasks on a tablet PC.
We found that blind people have different gesture preferences than sighted people, including preferences for edge-based gestures and gestures that involve tapping virtual keys on a keyboard.
Second, we conducted a performance study in which the same participants performed a set of reference gestures.
We found significant differences in the speed, size, and shape of gestures performed by blind people versus those performed by sighted people.
Our results suggest new design guidelines for accessible touch screen interfaces.
Until recently, most touch screens provided few or no accessibility features, leaving them largely unusable by blind people.
However, both the blind community and technology manufacturers have made progress on this issue in recent years.
At the 2009 Consumer Electronics Show, musician Stevie Wonder and a group of blind engineers took designers to task for the inaccessibility of touch screens, and encouraged them to improve touch screen accessibility for blind and visually impaired people .
Later that year, both Google and Apple released basic screen-reading software for their touch screen-based mobile devices, and most Google Android and Apple iOS devices now ship with screen-reading software preinstalled.
However, accessible touch screens still present challenges to both users and designers.
Users must be able to learn new touch screen applications quickly and effectively, while designers must be able to implement accessible touch screen interaction techniques for a diverse range of devices and applications.
Because most user interface designers are sighted, they may have a limited understanding of how blind people experience technology.
We therefore argue that accessible touch screen interfaces can be improved substantially if designers can better understand how blind people actually use touch screens .
A designer who wishes to create a new accessible touch screen-based application currently faces several challenges.
First, while touch screen interfaces for sighted users are largely consistent due to now-familiar gestures such as tapping, swiping, and pinching, touch screen interfaces for blind users vary widely across platforms.
While touch screens were once rare, touch screen-based interfaces are now present across a wide range of everyday technologies, including mobile devices, personal computers, and public kiosks.
As touch screens have become mainstream, it is crucial that touch screen-based interfaces be usable by people with all abilities, including blind and visually impaired people.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In contrast, in Google's Eyes-Free Shell for Android2, a user can select a menu item by holding their finger down on the screen, dragging their finger in one of 8 directions to choose the item, and then releasing their finger to select it.
Both systems use completely different layouts and interaction primitives, and thus there is currently no lingua franca for touch screen interactions for blind people.
Second, there exist very few examples of how to extend accessible touch screen interfaces to devices other than smartphones.
Touch screens may appear on devices of many different sizes, from jewelry-sized displays  to wall-sized interactive installations .
However, most commercially available and accessible touch screenbased devices are smartphones.
There still exists little information about how to best design accessible touch screen interfaces for tablets and other large touch screens.
Third, a designer who wishes to provide gestures in their application must consider whether the gestures will be appropriate for a blind user.
Although blind people may use the same hardware as their sighted peers, it is possible that they will prefer to use different gestures, or that they will perform the same gestures differently than a sighted person.
Sighted people perform gestures differently when they lack visual feedback , and it is reasonable to assume that a blind person may also perform gestures differently than a sighted person.
These challenges raise fundamental questions about how blind people use touch screens: What types of gestures are the most intuitive and easy to perform for a blind person?
And if blind people perform gestures differently than sighted people, how do their gestures differ?
In this paper, we address these questions through two user studies that explore how blind and sighted people interact with touch screens.
First, we conducted a gesture elicitation study  in which blind and sighted participants invented gestures for performing common computing tasks on a touch screen-based tablet PC.
Second, we conducted a gesture performance study in which both blind and sighted participants repeatedly performed a set of standard gestures on a touch screen.
Our results show that there are indeed differences in the types of gestures preferred by blind and sighted people, as well as differences in how gestures are performed by blind and sighted people.
Based on these results, we provide suggestions for the design of future touch screen-based applications and devices for blind users.
Providing blind people with access to touch screens has been a concern since the creation of the earliest touch screen systems.
In the 1990s, the emergence of touch screen kiosks in public places such as airports and shopping malls prompted investigation of how touch screen hardware could be made more accessible .
In recent years, researchers have explored accessible interaction techniques for mobile touch screens , and commercial manufacturers have begun to incorporate screen-reading software into their mobile devices .
While some earlier systems such as the Talking Fingertip Technique  and the Talking Tactile Tablet  used physically adapted hardware, most of these systems have used software alone to enable accessible interactions on a touch screen, typically by accepting gestures as input and providing speech and audio as output.
Despite the diversity of the touch screen-based devices that have been adapted for use by blind people, most of these systems have used one of a small set of underlying interaction techniques.
As no formal taxonomy of these interfaces has yet been published, we refer to these techniques as menu browsing, discrete gestures, and fixed regions.
These techniques are described briefly below.
In menu browsing, the user moves a cursor through a list of menu items and receives speech or audio feedback describing each item.
The user then performs a gesture to actuate the currently selected item.
The user may move the cursor through continuous touch gestures, in which the user strokes their finger across the screen to navigate the list, or by using discrete gestures or taps to move the cursor.
The list of menu items typically changes based on the current application state.
Menu items may be laid out spatially across the surface of the screen, either in their original arrangement or rearranged to optimize non-visual exploration.
Other applications use a series of predefined discrete gestures to perform actions.
Common actions are associated with a specific, predefined gesture, such as swiping one's finger in a specific direction or drawing a shape gesture.
For example, swiping a finger from the top of the bottom of the screen may change the currently playing track in a touch screen-based music player.
Systems that use discrete gestures include the Adaptive Blind Interaction Technique for Touchscreens , mBN , McGookin et al.
Some accessible touch screen applications map specific regions of the screen to predefined functions, as if the screen were in fact a set of discrete hardware buttons.
In these applications, the user initiates an action by performing a gesture over the designated screen region.
For example, a user might double tap the lower left corner of the screen to start an application.
Systems that use fixed regions include Mobile Messenger for the Blind  and Mobile Speak3.
Our research takes a technique-agnostic approach to understanding touch screen interactions for blind people.
In our gesture elicitation study, participants could choose any type of gesture they wished to perform a given command.
The results of this research show support for using a variety of touch screen interaction techniques.
In that study, participants were shown the outcome of an action and asked to demonstrate gestures that would accomplish that action.
The gesture elicitation study presented here is based on methods introduced by Wobbrock et al.
However, while that study focused on visual touch screen interactions for sighted users, the current study examines gesture preferences among both blind and sighted participants.
As touch screen interfaces become more common, it is important to provide equal access to these interfaces for blind people.
However, in some cases, touch screen-based interfaces may in fact be preferable to interfaces that use fixed buttons, even for blind people.
Prior research shows that blind people, even those who are born blind, may have substantial spatial and tactile abilities.
Blind people use the regions of the brain designated for visual processing when reading Braille and performing other spatial tasks .
Other studies have shown that both early-blind  and late-blind people have higher tactile sensitivity in their fingers than sighted people , and that late-blind adults can trace tactile shapes faster and more accurately than sighted adults .
Several studies have also examined the ability of blind people to draw or trace shapes using various technologies.
Kamel and Landay  performed a study in which blind users drew shapes using both tactile swell paper and a keyboard-driven, grid-based drawing program.
Crossan and Brewster  combined a haptic controller with audio feedback to enable blind users to trace simple shapes.
This research confirms that blind people are capable of performing gestures and drawing shapes on a screen, and suggests that gestures may in fact be an effective interaction method for some blind people.
For the purposes of this research, we define blind participants as people who typically use a screen reader to access a computer.
Blind participants were recruited via local blind organizations and via word of mouth.
Sighted participants were recruited via local mailing lists and bulletin boards.
Nine of the sighted participants and 6 of the blind participants regularly used some touch screen-based device.
All of the sighted participants and 8 of the blind participants were right-handed.
Three of the blind participants were early-blind and had become blind by the age of 2 years old.
Participants executed the experimental tasks using a 10.1 inch Lenovo S10-3t multi-touch tablet PC .
The tablet PC ran Windows 7 and a custom C application that recorded all screen touches with millisecond timestamps.
Touch information was stored in a JSON-formatted log file.
The experimenter used a wireless keyboard to begin and end experimental trials.
A video camera captured the tablet PC and the participants' hands, as well as participants' spoken comments and think-aloud data.
Another limitation of current accessible touch screens is the lack of user participation in their development.
Although some researchers have incorporated user feedback into the design of accessible touch screen systems , the resulting systems have still been largely created by their designers.
However, past research has shown that users often prefer gestures that were created by groups of potential users to those created by a single designer .
Beaudouin-Lafon  and Liu et al.
The study protocol was based upon the user-defined gesture study by Wobbrock et al.
Participants were seated at a desk in front of the tablet PC.
The tablet PC was initialized to a blank screen, and participants were introduced to the device.
Blind participants were given an opportunity to touch the screen and bezel in order to orient themselves.
Sighted participants were shown that the screen tracked touches and visualized them as trails on the screen.
Once participants were ready to proceed, the experimenter began the session.
At this point the application's blank screen was replaced by a "shapes world" containing labeled squares and circles.
This neutral layout was chosen, as in Wobbrock et al.
The shapes on the screen provided both visual and audio feedback: for example, touching a square would cause the program to speak the word "square," and a white noise loop played while the participant held his or her finger over the shape.
Each participant invented 2 gestures for each of the 22 commands.
In this section we analyze differences between the gestures invented by blind participants and the gestures invented by sighted participants, including participants' preferred methods for entering text on a touch screen.
The experimenter informed participants that their task was to invent gestures that could be used to execute a set of computing commands.
Commands were derived from Wobbrock et al.
However, commands that had a primarily visual function  were replaced by commands that applied to both visual and non-visual interfaces .
The commands used in the study were: menu, help, undo, task switch, move down in hierarchy, move up in hierarchy, previous page, next page, accept, reject, move object, open, close, duplicate, delete, cut, paste, select single, select group, move insertion point, select text range, and enter text.
Because showing the outcome of the command visually would not be accessible to all participants, the experimenter read a standardized description that described the outcome of the command.
For example, for the next page command, the participant was told: "Next page.
You move from the current page of content to the next page."
Commands were presented to each participant in random order.
For each trial, the experimenter read the description for the command.
The participant was then asked to invent 2 different gestures that could initiate the command, and to think aloud while doing so.
Once the participant decided upon a gesture, they described the gesture verbally to the experimenter and demonstrated it 3 times using the tablet PC's touch screen.
Once the participant had demonstrated the 2 gestures that they had invented, the experimenter prompted them to rate each of the gestures using scales from Wobbrock et al.
The first scale, referred to here as good match, read: "The gesture I picked is a good match for its intended purpose."
The second scale, referred to here as easiness, read: "The gesture I picked is easy to perform."
Both questions used Likert-type scales that ranged from 1  to 7 .
For the good match question, blind participants gave the gestures they created an average score of 5.54 , and sighted participants gave the gestures they created an average score of 5.15 .
For the easiness question, blind participants gave their gestures an average score of 5.77 , while sighted participants gave their gestures an average score of 5.72 .
Logistic regression showed that blind participants rated the gestures they created as significantly better on the good match question =13.69, p<.001.
We examined differences in the properties of the gestures invented by blind and sighted participants, including the total number of strokes, the location of the gesture, and the use of multi-touch.
A Wilcoxon rank-sum test revealed that blind participants' gestures contained significantly more strokes .
We analyzed whether participants' gestures used either an edge or corner of the screen.
Of the 880 gestures, 270  used at least two simultaneous contact points at some point during the gesture.
Blind participants invented 166  of these multi-touch gestures.
A Chi-Square test showed that blind participants were significantly more likely than sighted participants to invent multi-touch gestures =20.54, p<.0001.
Although both blind and sighted participants invented multi-touch gestures, the groups used multi-touch differently, and many of the multi-touch gestures invented by blind participants were different than the multi-touch gestures used in current gesture-based user interfaces.
In particular, many of the multi-touch gestures performed by blind participants involved a virtual mode key, in which the participant held one finger down on a specific area of the screen while performing the gesture with a second finger or hand.
This use of a mode key was common among blind participants: 63 of the gestures invented by blind participants during the study used a mode key to activate the gesture, while only 10 of the gestures invented by sighted people used a mode key.
We included the enter text command in this study to elicit ideas about how a user might enter text using a touch screen.
We were particularly interested in suggestions for text entry methods from blind participants, as entering text without visual feedback can be slow and laborious.
Participants offered the following ideas for entering text:  On-screen QWERTY keyboard: suggested by 7 blind and 9 sighted participants;  Handwriting: Suggested by 2 blind and 3 sighted participants;  Perkins Braille: Two blind participants suggested using the multi-touch screen to enter Braille using the Perkins technique, a two-handed chording technique used on physical Braille typewriters;  T93: Two blind participants suggested using the T9 predictive text method found on 12-key phone keypads.
Although participants were asked to keep an open mind and be creative, 10 participants were unable to invent a second text entry method, and instead chose an alternative form of their original method.
Of these participants, 9 chose a variation of QWERTY and 1 chose a variation of T9.
We identified participants' rationale for the gestures that they created by analyzing their think-aloud comments and gesture descriptions.
We classified gesture rationale using the nature dimension from Wobbrock et al.
Gesture nature considers a gesture's underlying explanation as either symbolic , physical , metaphorical , or abstract .
We assigned a nature to each gesture produced during the gesture elicitation study.
Figure 3 shows the gesture natures produced during this study.
Examining the results more closely, we note a higher number of symbolic gestures invented by sighted participants, and a higher number of abstract and metaphorical gestures invented by blind participants.
Within these categories, participants in both groups provided similar explanations for their gestures, with one notable exception: 95 of the metaphorical gestures produced by blind participants involved an interaction in which the participant touched areas of the screen in a way that was analogous to pressing keys on a physical keyboard.
For example, one participant demonstrated the paste command with a CONTROL-V gesture, in which she tapped an area of the screen near where the CONTROL key would be on a QWERTY keyboard, and then tapped an area of the screen near where the V key would be.
These keyboard metaphors accounted for 21.6% of the gestures invented by blind participants during the study, and were used at least once by 9 of the 10 blind participants.
Of the 9 blind participants who used such a gesture, 5 regularly used some touch screen-based device, suggesting that this idea was popular even among participants who were familiar with traditional touch screen interfaces.
No sighted participants performed gestures based on a physical keyboard layout.
The gesture elicitation study provided us with new insight about the types of gestures that blind people may wish to perform.
However, the open-ended nature of the study made it difficult to ascertain whether blind people and sighted people actually perform gestures differently, or whether they simply prefer different types of gestures.
To determine if there were significant differences in how blind and sighted people performed the same gestures, we conducted a second study in which all participants performed the same set of standard gestures.
This study featured the same participants as the previous gesture elicitation study.
Participants used the same Lenovo tablet PC and logging software as the previous study.
The tablet PC ran the same logging application, which recorded all contacts with millisecond timestamps.
During each trial, the application initially showed a blank screen, and drew trails that visualized the user's touches on the screen.
The experimental procedure was similar to the gesture elicitation study.
However, instead of inventing new gestures, participants performed specific gestures as specified by the experimenter.
For each gesture, the experimenter read the name and a brief description of the gesture.
Once the participant had practiced the gesture, they performed the gesture 3 times, and each trial was recorded in the log file.
After completing each gesture, the participant rated the gesture using a variation of the easiness scale from the first study: "The gesture is easy to perform."
For this study, we chose 40 gestures that represented common interactions on current touch screen platforms, and which included unistroke, multi-stroke, and multi-touch gestures.
Gestures were divided into 5 categories: tap , flick , multi-touch gestures, shape , and symbol .
The following categories and gestures were used:  Tap: single tap center, single tap left, single tap right, single tap top, single tap bottom, single tap top left, single tap top right, single tap bottom left, single tap bottom right, double tap, triple tap;  Flick: flick left, flick right, flick up, flick down;  Multi-touch: 2-finger pinch, 2-finger spread apart, 2 finger rotate clockwise, 2-finger rotate counterclockwise;  Shape: square, circle, triangle;  Symbol: A, B, C, D, E, F, L, X, Z, 1, 2, 3, 4, 5, question mark, check mark, 5-pointed star, scratch out.
The gesture set contained some symbols used in printed English writing, including the numbers 1 through 5 and the letters A through F, L, X, and Z.
Although many blind people do not typically use handwriting, and thus may be unfamiliar with these symbols, we chose to include the symbols to increase the overall number of glyph-like gestures, as well as to explore how familiar blind people actually are with these symbols.
Participants were allowed to skip a gesture if they were not familiar with it.
However, there was a significant effect of gesture category =68.85, p<.0001 on easiness, which shows that participants' ratings were influenced by the gesture category.
There was also a significant interaction between blindness and gesture category =18.61, p<.001, indicating that a gesture's category affected its rating differently for blind and sighted participants.
In addition to soliciting ratings for each gesture, we examined various properties of the gestures that participants performed to determine whether there were significant differences in how blind and sighted people performed the same gestures.
We measured the overall size of each gesture using the area of the bounding box for that gesture.
A Wilcoxon rank-sum test found a significant difference in the size of gestures produced by blind and sighted participants , indicating that blind participants tended to create significantly larger gestures than sighted participants.
In addition to calculating gesture size, we also examined the size variation between multiple instances of the same gesture created by a single participant.
We calculated size variation using the standard deviation of the gesture size for a given participant and gesture.
There was a significant difference in the standard deviation of sizes between blind and sighted participants , indicating that blind participants produced gestures with greater variation in size when performing the same gesture multiple times.
We calculated the aspect ratio of each gesture's bounding box .
The average aspect ratio for blind participants was 1.64 , and the average aspect ratio for sighted participants was 1.44 , suggesting that blind participants tended to create wider gestures.
Each participant was asked to perform 40 gestures 3 times each, for a total of 40 x 3 x 20 = 2400 gestures performed.
However, 2 blind participants skipped a total of 15 gestures because they were unfamiliar with the gesture.
In addition, 20 gestures were not accurately captured by the touch screen, leaving a total of 2335 recorded gesture instances and 785 gesture ratings.
Participants rated each of the gestures in terms of easiness.
Overall, blind participants gave the gestures they performed an average score of 5.71 , while sighted participants gave the gestures they performed an average score of 5.76 .
Ratings by gesture category are shown in Figure 4.
Looking specifically at gestures in which participants drew some glyph , we did find that blind participants drew significantly wider gestures than sighted participants .
This difference was significant , showing that, on average, blind participants took approximately twice as long to perform the same gestures.
In addition to the aforementioned properties, we were interested in some specific issues that had been observed during our pilot testing.
Because these issues did not apply across all gestures, we examined these features using specific subsets of the gesture set, rather than the entire set.
In general, participants were able to perform gestures at any location on the screen.
However, nine gestures in the tap category referred to specific screen locations, including the corners, edges, and center.
For these gestures, we calculated the distance between the centroid of the performed gesture and the specified screen location.
For blind participants, the average distance from the specified location was 110.97  pixels.
During the study, some blind participants mentioned that it was difficult to target locations that were away from the screen corners.
Looking only at gestures in which participants tapped the corners, we found that blind people's gestures were still farther from the intended targets than sighted people's gestures .
Another issue that we observed during initial testing was the difficulty of connecting the various parts of a gesture without visual feedback.
However, it is difficult to operationalize this feature for an arbitrary gesture, as different gestures connect in different ways.
To examine this issue quantitatively, we measured its effects on the circle gesture alone.
We chose the circle gesture because it was likely to be completed in a single stroke, and because the start point and end points coincide in its canonical form.
For the purposes of this analysis, we defined the metric form closure distance as the Cartesian distance between the circle's start point and end point.
During pilot testing, we noted that the lines of some blind participants' gestures seemed to be less steady or more "wavy" than those created by sighted participants.
Prior studies have attempted to quantify the steadiness of a gesture by measuring parameters of a single specific gesture, such as angular deviation when drawing a straight line or eccentricity from a reference shape .
However, these approaches are limited to specific gestures and do not generalize.
Furthermore, because participants in this study could not see a reference shape on screen, matching their gestures to a reference shape would be inappropriate.
To model this waviness quantitatively, we introduce a generalizable stability metric for drawn gestures, the average angular acceleration metric, shown in Equation 1.
This metric approximately measures how much the path changes direction over the course of the entire gesture.
For example, a gesture that is drawn in a wavy or jagged fashion will continually be changing direction, and thus will have a higher average angular acceleration value.
Figure 6 shows a wavy gesture with high average angular acceleration and a steady gesture with low average angular acceleration.
However, examining different participants as they perform the same gesture allows us to approximate the overall steadiness or waviness of their lines using this measure.
For demonstration, we calculated this value for the square gesture only.
For blind participants, the average angular acceleration was 0.47 , while for sighted participants the average angular acceleration was 0.06 .
We have described a number of differences in how blind people and sighted people perform gestures.
It seems likely that these differences would also affect gesture recognition accuracy for gestures performed by a blind person.
To explore this question, we compared gesture recognition results for a subset of the gestures collected in this study.
We used the $N multi-stroke recognizer  to recognize gestures.
We analyzed only gestures from the shape and symbol categories, as many gesture recognizers, including $N, are designed to handle glyph gestures such as these, but not taps or directional flicks.
For each of the shape and symbol gestures collected in this study, we performed a recognition test using 3 sets of gestures:  all other shape and symbol gestures created by that participant,  shape and symbol gestures created by blind participants, and  shape and symbol gestures created by sighted participants.
For  and , the creator of the gesture being tested was excluded from the set.
Recognition was considered correct if the correct gesture was the top recognition result.
Table 1 summarizes the results of this analysis.
A ChiSquare test revealed that gestures from sighted participants were significantly more likely to be recognized correctly =56.19, p<.001.
Recognition accuracy also differed based on the recognizer's training set.
For sighted participants, recognition accuracy was higher when tested against gestures from other sighted participants than when tested against gestures from blind participants =35.96, p<.0001.
Surprisingly, recognition accuracy was also higher for blind participants when tested against sighted gestures =7.33, p<.01.
This result seems counterintuitive, as we would expect that a blind person's gestures would be more similar to gestures from other blind people, and thus that recognition accuracy would be higher when the recognizer was trained with blind gestures.
A closer examination reveals that recognition accuracy for blind participants' symbol gestures was much higher when tested against sighted gestures than when tested against other blind gestures.
This difference may be due to blind participants' unfamiliarity with some of the symbol gestures.
Because these gestures were less familiar to the blind participants, there may have been greater variation in how they were performed.
The overarching goal of this research is to set future directions for the design of touch screen applications, and to promote accessible touch screen interaction techniques that work equally well for both blind and sighted people.
The studies described here address two primary questions: first, given the choice, would blind people prefer to perform different gestures than sighted people?
Second, do blind people perform gestures differently than sighted people even when performing the same gestures?
In response to the first question, we discovered significant differences in the gestures chosen by blind and sighted participants.
Blind participants in our study showed strong preferences for gestures that used screen corners, edges, and multi-touch.
Furthermore, when asked to invent gestures, blind participants in our study adopted two techniques that are rare in most current touch screen user interfaces: using a second finger or hand to begin a mode, and touching areas of the screen that correspond to keys on a QWERTY keyboard.
These gestures were used by a majority of blind participants in our study, including participants who had experience with other touch screen-based devices.
While it remains to be seen whether these gestures would be useful or efficient in a real world application, their popularity in this study suggests that they may have potential.
In answering the second question, we uncovered a number of performance characteristics that differentiate gestures produced by blind people from those produced by sighted people.
Gestures produced by blind participants were larger, slower, and featured greater variation in size than those produced by sighted participants.
Although some of these differences may seem intuitive, identifying these differences in real performance data, and quantifying them, deepens our understanding of how blind people perform gestures.
We have identified several important metrics related to gesture performance by blind people, including location accuracy, form closure, and line steadiness.
A relevant outcome of the second study is that some blind participants did not know how to perform some of the gestures used in our protocol, including letters, numbers, and other symbols.
Even some blind participants who knew these symbols sometimes pointed out that other blind people might not know them.
This is not surprising when we consider that many blind people have never learned how to print, but it is an important and non-obvious consideration when choosing gestures for an application.
The present work has uncovered several promising directions for creating new accessible interaction techniques for touch screens.
In particular, the QWERTY keyboardlike interaction technique was very popular among our blind participants, and thus merits further investigation.
In addition, our participants suggested a number of possible avenues for improving text entry on a flat touch screen, including handwriting Braille characters, using the Perkins Braille chording technique, or using variations of a telephone keypad.
These techniques may hold promise for both blind people and sighted people in eyes-free situations.
A second area of opportunity is in improving gesture recognition accuracy for blind people.
Our results show that blind participants experienced significantly reduced gesture recognition accuracy when using a traditional recognizer even when the recognizer was trained with gestures from other blind participants.
We envision several possibilities for improving gesture recognition accuracy for blind people, such as by preprocessing blind users' gestures before recognition, creating a modified gesture recognizer for blind gestures, or identifying a subset of gestures that can be recognized reliably when performed by blind people.
Based on the results of these two studies, we offer preliminary advice on how to design future touch screenbased applications for both blind and sighted users: Avoid symbols used in print writing.
Blind users may have limited knowledge of symbols used in print writing, such as letters, numbers, or punctuation.
Even when these symbols are known, users may not be used to them or may not be comfortable performing them.
If performing symbol-based gestures is an important part of the user experience, the user should be trained in how to perform these gestures or should be able to choose alternative gestures.
Favor edges, corners, and other landmarks.
Locating precise spots on the touch screen surface can be very difficult for a user who cannot see the screen.
The physical edges and corners of a touch screen are useful landmarks for a blind person.
Placing critical functions in these areas will improve accessibility and reduce the likelihood that the user will trigger these functions accidentally.
Reduce demand for location accuracy.
Blind users may be less precise in targeting specific areas of the screen, including edges and corners.
This problem can be reduced by increasing target size or by allowing approximate targeting methods, such as allowing a user to touch near a target and then explore with their finger to locate it more precisely.
Blind people may perform gestures at a different pace than sighted people.
Thus, using the gesture's speed as a recognition feature or as a parameter  may result in increased errors for blind users.
Reproduce traditional spatial layouts when possible.
Objects with familiar spatial and tactile layouts, such as a QWERTY keyboard or telephone keypad, are instantly familiar to many blind people.
Reproducing these layouts may make it easier for a blind person to learn and use a new interface.
In this paper, we explored two issues related to the design of touch screen user interfaces for blind people.
First, we examined blind and sighted participants' preferences for gesture-based commands on a tablet PC by asking them to invent their own gestures.
We found that blind participants did in fact suggest gestures that were different than those suggested by sighted people.
Blind participants favored gestures that occurred on the edges of the screen, and suggested new gestures that utilized spatial layouts that were familiar to them.
Second, we examined differences in how blind and sighted people perform the same set of gestures, and presented metrics for describing how gestures produced by blind people differ from gestures produced by sighted people.
As touch screens are now one of the most common ways of interacting with computers, it is not only important that blind people can access touch screens, but also that they can do so effectively and efficiently.
The present work provides new information about how blind people currently think about interacting with a touch screen, and about how they perform gestures on a touch screen as compared to a sighted person.
We believe that this work will bring us closer to the creation of robust and usable touch screen interfaces that work equally well for blind and sighted people.
We thank Nicole Torcolini for her valuable feedback.
This work was supported in part by National Science Foundation grant IIS-0811063.
Any opinions, findings, conclusions or recommendations expressed in this work are those of the authors and do not necessarily reflect those of the National Science Foundation.
