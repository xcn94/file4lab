Customer reviews on commercial sites such as amazon.com, or dedicated review sites such as epinions.com, contain reviews that are not just summary assessments or recommendations, but also self-reports of the end users experiences, in their own words, in the wild.
The aim of this paper is to quantify the amount of UUX information and dimensions in online reviews from the specific domains of software and video games.
We also implement and test a machine-learning-based classifier that tags sentences in reviews according to whether they contain usability or UX-related information and according to the dimensions of usability or UX they pertain to.
The primary aim of the classifier is to automatically extract the pertinent vocabulary of end users associated with the various dimensions of UUX.
A secondary aim is to investigate the feasibility of using such a classifier to automatically catalogue UUX information found in databases of thousands of reviews, too large for qualified human analysis.
We hope to aid the understanding of which dimensions of product use motivate laymen reviewers, and in the future potentially use this understanding when re-designing a product.
The scope of the present work is to provide a tool to UUX researchers; future work will explore the automatic identification and extraction of specific actionable outcomes for practioners.
In order to process information from many different reviews, our approach focuses on extraction of information from individual sentences, rather than entire texts.
This is somewhat at odds with approaches in focusing on obtaining a holistic understanding of interaction , but as a review may incorporate both good and bad experiences relating to many different dimensions of UUX, we believe that a sentencebased bottom-up approach will yield more precise information about the "typical" vocabulary associated to specific dimensions of UUX.
Internet review sites allow consumers to write detailed reviews of products potentially containing information related to user experience  and usability.
Using 5198 sentences from 3492 online reviews of software and video games, we investigate the content of online reviews with the aims of  charting the distribution of information in reviews among different dimensions of usability and UX, and  extracting an associated vocabulary for each dimension using techniques from natural language processing and machine learning.
We  find that 13%-49% of sentences in our online reviews pool contain usability or UX information;  chart the distribution of four sets of dimensions of usability and UX across reviews from two product categories;  extract a catalogue of important word stems for a number of dimensions.
Our results suggest that a greater understanding of users' preoccupation with different dimensions of usability and UX may be inferred from the large volume of self-reported experiences online, and that research focused on identifying pertinent dimensions of usability and UX may benefit further from empirical studies of user-generated experience reports.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
User experience has been studied by soliciting user narratives  where information is manually extracted from user-generated texts.
The volume of texts studied has been substantial , but still small enough for dedicated researchers to process manually, and the users have been specifically asked to write the texts, unlike the typ-
Similarly, studies in asynchronous usability testing and reporting  have studied user-generated problem reports that are only later reviewed by experts or researchers.
Outside UUX, classification and information extraction from user-generated text is a vibrant research area, both for full texts, and at sentence or short message level, e.g.
Some pertinent examples: Gamon et al.
Secondly, unlike most studies in sentiment analysis, the outcome of the classification is primarily a means to an end, namely charting the UUX content of reviews and the vocabulary used by reviewers to describe UUX-related phenomena.
For the purpose of this paper a review is a piece of text detailing pros and cons of a product and possibly an assessment of it and recommendations for potential buyers, written by a user of the product who has been in possession of said product and used it for some time.
It may be written either by a professional reviewer or an ordinary end user.
We concentrate on reviews assumed to be written by ordinary end users on dedicated web sites, for example epinions.com or amazon.com.
An example of an online review is shown in Figure 1.
Consider the following sentence from a review of the game Gears of War for the Xbox 360: "You'll be a little creeped out while playing this game almost all the time."
The above sentence clearly contains information that is hedonic in nature: Being scared due to the horror elements in the game, and there is a-much less clear-element of the satisfaction usability aspect: the sentence communicates a positive experience by the user.
From a communication perspective, user reviews may be viewed as word of mouth communication: informal communication between private parties concerning evaluation of goods and services ; reviews from review sites, online fora, and blogs are clearly examples of such informal communication, and are accordingly called eWoM  in the literature .
This was confirmed for online reviews by Hu et al.
Due to the bimodal distribution, the average score for these products may be misguiding.
In addition, the information extracted from online reviews may not be indicative of the experience of the average user, but may rather represent those experiences that add or deduct so much from certain users' experience that they are motivated to write a review.
For  word of mouth communication, extremely dissatisfied customers also engage more in word of mouth than very satisfied customers, though in a sizeable case of their data the differences were not significant; it seems plausible that the same phenomena occur for online user reviews.
There is evidence that potential buyers put more emphasis on reviews with low satisfaction than those with high satisfaction as they have a bigger impact on product sales than that of positive reviews and word of mouth .
Usability is a way to measure a products ability to help a user solve a given task adequately.
It is dependent on the product, task, user and circumstances , and has been the object of intense academic scrutiny.
In usability, traditional studies focus on short term product use  and conducted in lab settings; few studies stretch across longer time periods and then only weeks .
In contrast, most UX research concerns open use situations  and controlled task  experiments, only 20% of papers contain studies based on user-initiated use .
No UX research covers longer time periods of months or years which is the expected life span of most products but instead covers at most only a few weeks .
In contrast to traditional studies, reviews describe a users opinion and experiences after more protracted use.
In addition, and unlike narratives written for UX studies, customer reviews on Internet sites appear to be written because the reviewer is motivated by his or her own use of the product, usually in conjunction with some small reward .
Furthermore, some reviews may be fake.
Finally, the bimodal distribution of satisfaction present in word of mouth communication leads us to conjecture that in terms of satisfaction, the average user is underrepresented among reviewers, and that reviews may not always yield a representative description of the typical experiences among the user base.
However, satisfaction extremes are well represented, it should thus be possible to extract information about situations where the product under review performs both bad and good.
Usability: The extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use.
The definitions of the above five dimensions vary somewhat in the literature, and some studies use only a subset of the above .
In addition, some studies use a precise and limited definition of measures, and others such as the ISO definition  take a broad view of the measures.
Unlike usability, there seems to be much less consensus on the definition of the notion of user experience and its segmentation into meaningful aspects.
The ISO 9241-210 definition states: User experience: A person's perceptions and responses that result from the use and/or anticipated use of a product, system or service.
We may interpret the above as being covered by the Satisfaction dimension of the ISO 9241-11 definition of usability , but the literature contains many more nuanced interpretations: For example, Bevan  describes four dimensions, called satisfaction measures, and Ketola and Roto  describe a study at Nokia among relevant senior staff who were asked which UX data they found useful, which Bevan later grouped into dimensions .
Similarly, Hassenzahl  divides UX analysis into the three partially overlapping approaches beyond the instrumental, emotion and affect, and the experiental, but in later work  shifts focus to the subjective side of product use and relates it to Self Determination Theory  and Flow .
Bargas-Avila and Hornbaek  systematically collect a sample of 51 publications from 2005-2009 reporting empirical studies on UX and describe a number of  dimensions together with the percentage of studies from their sample that pertain to the identified dimensions.
Usability and User Experience are central terms in humancomputer interaction.
Their precise definition, and their subdivision into dimensions such as Efficiency, Learnability, Hedonic quality, and so forth is still debated , in the case of UX hotly so , and there seems to be no universal consensus about whether UX is an aspect of usability or vice versa.
We are particularly interested in the way researchers have subdivided UUX into various dimensions that pertain to specific aspects, viewpoints, or phenomena within UUX.
We briefly review existing research below.
Based on the literature above, we elected to use the 5 standard dimensions of usability, and chose the sets of dimensions from 3 of the studies of UX that had both precise definitions of the dimensions and clear demarcations of the differences between them .
A summary is shown in Table 1; we briefly describe the dimensions below.
The dimensions of CLASSICUA  are: Errors/effectiveness: The number of  errors made by users on their way to completing a task or the quality of task outcome.
Efficiency: The speed or other measure of cost associated with performing the task for users at a given experience level.
Satisfaction: A subjective rating of satisfaction with product use or liking of the product or features.
Learnability: The amount of time it takes to learn to use the system, how difficult it is for a first time user or development over time.
Memorability: How well users retain information gained about or through the system.
The dimensions of BEVAN  are: Likability: The extent to which the users are satisfied with their perceived achievement of pragmatic goals, including acceptable perceived results of use and consequences of use .
Pleasure: The extent to which the users are satisfied with their perceived achievement of hedonic goals of stimulation, identification, evocation and associated emotional responses.
Comfort: The extent to which the users are satisfied with physical comfort.
Trust: the extent to which the users are satisfied that the product will behave as intended.
The dimensions of KETOLA  are: Anticipation: What did users expect, what is the anticipated use?
Overall usability: Was the user successful in taking the product into use or upgrading from a previous product.
Hedonic: Fulfillment of inner needs such as pleasure, enjoyment, or things preventing this such as frustration.
Detailed usability: Going into details on which functions are used, ordinary usability problems and performance satisfaction/problems.
User differences: Differences between users such as previous product experience, how they access features and differences between the actual buyers and target user group.
Support: Aspects with the human- or software-support available and how it affects user satisfaction, possible product returns, or user wish lists.
Impact of use: If and how the new device change the usage patterns of the users.
The dimensions of FREQUENT are: Affect and Emotion: Affect and emotion induced by using the product, including other aspects such as Enjoyment, fun and Frustration.
This dimension fully encompasses Enjoyment, fun and Frustration, and would be considered encompassed by the Hedonic dimension.
Enjoyment, Fun: How entertained is the user while using the product?
This is also an affect and emotion,
Aesthetics, Appeal: Appreciation of beauty or good taste.
Typically associated with graphics or sound.
Engagement, Flow: How engaged is the user in using the product forgetting everything else?
Also includes challenge versus skill balancing needed for achieving flow state.
Motivation: What motivates the user in using the product ?
Enchantment: Being "both caught up and carried away" in the experience forgetting everything else, and causing a disorientation associated with a pleasurable sense of fullness and liveliness that charges attention and concentration.
Frustration: Frustration or hardship induced by using the product.
This is also a negative hedonic dimension.
Hedonic: Defined the same way as in KETOLA.
We discarded the two dimensions Generic UX and Other as reported by  as no clear definition was available.
To see whether randomly sampled reviews contain sufficient UUX-related information to warrant further study, we performed a pre-study among usability experts who were given a sample of Internet reviews and a free-form exercise asking them to mark sentences containing information about usability or UX.
24 reviews were sampled on January 5 2012 from 12:00 - 14:00 from the website epinions.com, collecting the 6 most recent reviews of each of the four categories "Digital Cameras", "Headphones", "Software" and "Video games".
3 reviews were discarded, all from the software category , and 2 were reviews of printed children's books.
Each review was randomly assigned to 2 distinct participants.
Each participant was asked to read and comment on six different reviews in total.
The participants received only written instructions asking them to free-form annotate text in the reviews that they found interesting concerning  their own perception of usability, and  user experience.
Participants were neither given definitions of usability or user experience, but encouraged to use their own perception of these terms.
Each participant was asked to use at most 2 hours in total on all six reviews, including time to read and to annotate.
We collected the annotated texts and post-processed them in two ways: Raw containment of UUX: Each review was manually split into sentences and was marked with the identity of a participant if the participant had marked part of or the entire sentence as relevant.
Due to the level of annotation performed by most experts, no distinction was done between dimensions or UX and usability based on the experts comments.
Content If you like multiplayer strategy games, buy this with confidence.
Multiplayer is excellent, but the single player campaign isn't.
Most of the inter-mission story telling happen in this mode, which tend to be awkward and clumsy.
Most of the missions are enjoyable, and each one has optional goals which add replay value.
The dimensions of motivation and enchantment, both popular dimensions in empirical user experience research being represented in 8% and 6% of papers respectively , were not encountered at all in the pre-study.
In total, 13 % of all sentences were marked as relevant to usability or UX by both participants assigned to each review, 36 % as relevant by one, but not both assigned to each review, and 51% of all sentences were unmarked .
There was great diversity in the understanding of UUX and annotation volume per participant.
One participant specifically noted that he had given up marking user experience data as it "virtually encompassed everything", and only a single participant consistently annotated UX and usability information as two distinct categories.
Presence of usability or UX dimension: The results are summarized in table 3.
For the classic usability measure as seen in Table 3, almost all sentences in the dimension errors/effectiveness were describing quality of task outcome , but a few classic error counts were also present .
In summary, 13% + 36% = 49% of all sentences were marked as relevant by at least one of the two participants annotating each review.
Some confirmation bias may be present as participants were specifically asked to look for information relevant to usability or user experience, but based on the results we concluded that the volume of text in a review relevant to usability or UX, and the dispersion of text across UUX dimensions were both substantial enough to warrant a largerscale annotation experiment.
Based on the promising results of the pre-study, we opted to harvest a larger sample of reviews and annotate them.
We decided to keep the per-sentence annotation of the pre-study and concentrate on only 2 product categories as it would allow us to annotate more sentences in each product category while retaining the ability to make comparisons across categories.
We collected reviews from the two product categories Software  and Video games  on the epinions.com website on July 5th, 2012.
All public available reviews in the two categories were collected.
We split each review into sentences using a routine from the Python NLTK  which came pre-trained on the British National Corpus.
We then drew sentences randomly from the pool of all sentences above and performed manual annotation on each drawn sentence.
This was followed by four hours of individual annotation where the student annotators could freely consult the senior annotator for questions.
Inter-rater agreement was computed at the end of the individual annotation with Krippendorff's  for all dimensions in the  range.
Each graduate student annotator then continued individually for 22-25 hours over the course of two weeks, and the senior annotator for 10 hours during one week.
All annotations were performed in a custom-built tool built by a research programmer not otherwise involved in the study.
Some examples of sentences from reviews and their annotations: * "Once again, the way sound distorts during the slowmotion sequences adds a nice touch to the experience.".
This sentence mainly describes Aesthetics and subsequently the effect on Engagement while playing the game, yet also expresses Satisfaction with the effect.
As this example illustrates, several dimensions are often encountered together with Satisfaction.
This sentence describes displeasure, relevant to the dimension Pleasure; this is also a measure of Satisfaction with the sound.
It is striking that there are quite few sentences annotated by dimensions in the FREQUENT classification  compared to the CLASSICUA, BEVAN and KETOLA categories .
This phenomenon is due to the FREQUENT classification's lack of a "catch-all" category such as CLASSICUA's Satisfaction, BE VAN 's Likability and KETOLA 's Detailed usability.
Second, the four sets of dimensions of UUX we consider differ very much in the balance of dimensions: Clearly, FREQUENT, the model containing the most categories, has the most even distribution of sentences across the various dimensions, whereas the other sets of dimensions seem to have greatly skewed distributions towards the "catch-all" categories described above.
Table 5 shows the fraction of sentences in each product category annotated by the various UUX dimensions.
The table confirms the observation from the pre-study that some dimensions are hardly used at all: In CLASSICUA, Memorability does not occur at all in the software category, and only in 0.37% of sentences among video games.
Likewise, Efficiency occurs very rarely .
Among the dimensions from BEVAN, Comfort and Trust are again absent, but Likeability is prominent, as expected from the pre-study.
The dimension Pleasure is more prevalent among video game reviews than reviews of software.
In KETOLA, Impact is almost completely absent, and Support rare, but more prevalent among software products, possibly reflecting that this dimension is more valued among users of software products than video games.
Conversely, the dimension Hedonic is present in 7.77% of all sentences sampled in the video games category, more than twice as often as in the software category.
Based on the results of the first study, we wish to investigate the vocabulary employed by users when conveying information relevant to the dimensions of UUX.
One straightforward way of extracting such a vocabulary is to construct a machine learning classifier that discriminates between dimensions based on words or other features of the text that are automatically computed during the training of the classifier.
An added benefit is that the constructed classifier, if precise, may be used for automatically tagging a sentence with the UUX dimensions it pertains to.
This tagging task may be viewed as a set of binary classification tasks: For each dimension, and for each sentence, does the sentence pertain to that dimension, or not.
Such a tagger may be used to either aid future researchers in manual annotation, or in lessening the amount of sentences to be studied .
For each UUX dimension a binary classifier using a bag-ofwords feature set was trained and evaluated in a sequence of steps as follows: Preprocessing step: Each sentence was tokenized, words from the NLTK stop word list  removed and the remaining words stemmed using the Snowball stemmer.
Data split step: The total dataset is split into a five-fold stratified cross-validation 3 scheme.
Training step: For each cross-validation split, the training set is used for creating feature vectors with TFxIDF weighting and 2  ranking is subsequently used to discard the 10% worst discriminating features.
A linear kernel Support Vector Machine   is then trained using the feature set.
Distribution of dimensions in sentences within Software and Video Games reviews.
The "Any dimension" rows indicate the percentage of sentences annotated with at least one dimension.
Each sentence can be annotated with more than one dimension, hence "Any dimension" is not the sum of the other numbers in the same column.
Differences between the Software and Video Games categories were tested for significance using the non-parametric two-tailed Wilcoxon rank-sum test  and significance at p < .05 is indicated in boldface.
Extraction of important words for each dimension: For each dimension, we extracted the most informative words by selecting the word stems having the largest distance in descending order to the separating hyperplane afforded by the SVM.
Aside from the classic bag of words approach as described above, we also experimented with the following feature sets commonly used in text classification tasks: binary bag of words, word di-grams and tri-grams, a combination of triagrams and a feature set consisting of all possible Wordnet synsets , and Wordnet synsets with automatic part of speech  tagging.
All of the alternative feature sets had slightly worse average performance than an ordinary bag of words approach, hence were discarded.
To avoid drawing erroneous conclusions from unreliable data, we elected to only consider the extracted word stems from the dimensions where the classifier performed better than random chance.
This was tested against a baseline classifier that always assigns to the majority class with significance at p < .05 .
Impact, Motivation and Enchantment that all have precision and recall at zero flat.
For the more commonly occurring dimensions, the classifier performs better than the baseline of assigning all sentences to the majority class, but it is clearly quite conservative: Precision values are generally high, but recall values low .
In short, for these dimensions, the sentences tagged as being relevant to a dimension will be relevant with high probability, but the classifier will miss many relevant sentences.
For dimensions that commonly occur in the data, the classifier works well, as should be expected: precision, recall and F1 are all high for Satisfaction and Likeability and-again with the exception of recall-for Enjoyment, Fun and Affect and Emotion, Frustration, and Hedonic.
Differences in both data domain and classification tasks preclude us from directly comparing to other studies, but for all but the very sparsely represented dimensions, the performance of the classifier is on par with studies conducting sentence-based classification: Gamon et al.
To evaluate the quality of the classifier, we use the classic information retrieval metrics precision, recall, and F1 .
Precision is the fraction of sentences correctly classified as relevant for a dimension among all sentences classified as relevant for it; recall is the fraction of sentences actually relevant for a dimension that are also correctly classified as relevant for it; F1 is the harmonic mean of precision and recall .
Table 8 holds the 30 most important word stems for each dimension where the difference in precision, recall and F 1 between the classifier and the baseline was significant.
As an example of top word stems associated with a dimension not included in the table are "sooth", "cute", "reliev", "handhold" and "exist", all associated with the dimension trust.
For the dimension Frustration with word stems "frustrat", "incompatibilit", "hardest", "perpetu", "insult" what seems like less relevant words also made it to the top 30, for instance "babysit".
This is due to reviews containing text such as "And that is of course an AI partner controlled friend, there is nothing that can ruin a good RPG then a partner that is supposed to be helping you but instead makes you feel like your babysitting a 5 year old with mental problems, on top of battling blood thirsty monsters."
This particular sentence also illustrates the intricacies of our task: Clearly, the use of "babysit" is figurative, not literal, hence signals frustration.
Results are checked for significance against a baseline classifier that assigns to the majority class.
Significance is calculated using the non-parametric McNemar's test with Yates' correction for continuity .
Significant results  are marked in boldface.
Other spurious word stems in Table 8  can be attributed to two phenomena:  the word stems in the dimension have low discriminatory power, whence the classifier was barely able to distinguish relevant/irrelevant sentences, and  a word stem may be considered important if it by chance occurs in the training corpus of the classifier in a small number of sentences, all of which are relevant to the dimension.
Dimensions such as Hedonic, Pleasure and Affect and emotion which fully or partially encompas other dimensions tend to share most of the top places of the included subcategories.
For example, nine of the ten most important word stems from the dimension Enjoyment and Fun were found among the top 15 important word stems in the encompassing dimension Effect and Emotion which also rated other word stems such as "scari" and "frustrate" related to emotion, but not enjoyment highly.
The notable exception to this phenomenon is the dimension Detailed usability that encompasses all measures of classic usability as well as mention of specific usability problems; this dimension has many highly ranked word stems relating to Satisfaction, but fewer highly ranked words stems also occurring among the other classic usability dimensions.
The set of word stems extracted shows that some dimensions  have associated vocabularies containing words closely related to the words used to describe the dimensions in the literature , but other dimensions such as Errors/effectiveness instead have vocabularies related to specific problems and errors such as "lag", "glitch", "imprecise" and "bug".
This illustrates the varied vocabulary used by reviewers when describing specific dimensions of interaction, and that the vocabulary is more varied for some dimensions than others.
Our results strongly suggest that more complex information about how users express their feelings and experiences with situations and problems related to UUX can be extracted from reviews and other narratives.
The results also suggest that the task of mapping the users' utterances to specific dimensions of UUX is only partially possible to do in an automatic fashion and that some of these dimensions are associated to a complex vocabulary.
When using sets of dimensions of UUX for practical purposes-for example for gauging which dimensions of a product are perceived to be important by users-then sets with many complementary dimensions such as FREQUENT appear to be more fine-grained and informative than other such sets.
It may be possible for the UUX community to settle on a small, possibly domain-dependent set of dimensions, simply by performing empirical investigations such as ours, or in more traditional settings such as usability tests.
We have unearthed a significant difference between software and video game reviews in terms of which UUX dimension they frequently mention.
With the exception of satisfaction software reviews emphasize classic usability measures more than video game reviews, which in term put more emphasis on dimensions such as Hedonic, Affect and emotion, Pleasure and Enjoyment and fun.
The notable exception to this difference is Frustration for which the difference between software and video games were not significant.
The automatic classifier works well on commonly occurring dimensions, but tends to be too conservative even for these dimensions.
There are ways of improving such classifiers, but our experiments suggest that simple off-the-shelf machine learning solutions are insufficient.
When sifting through large amounts of material, it is easy to miss infrequent, but potentially important information, for instance the presence of information pertaining to Enchantment or Trust, and the classifier clearly is unable to assist a human expert in this regard.
While our results shed light on the general UUX concerns of end users, the anatomy of the reviews we considered, possibly Internet reviews in general, does not seem to contain much detailed information about specific situations of use, or of measurements.
No reviewer writes "The number of mouse click to navigate from the start screen to the functionality I want is 7, and that this is annoying".
While it is conceivable that a professional software reviewer, or an end user taking a conscious interest in usability, would write such a sentence, we did not encounter these.
Thus, it seems highly unlikely that mining Internet reviews can supplant traditional usability testing or UX studies.
Finally, the sentence-based annotation has acted as a convenient proxy: If a user spends 10% of the sentences in a review discussing matters related to Enchantment, it is likely evident that enchantment is a major part of his view of the product; but there may be other measures that more precisely reflect how much the user is occupied with different dimensions of UUX.
A., and Hornbaek, K. Old wine in new bottles or novel challenges: a critical analysis of empirical studies of user experience.
Bevan, N. Classifying and selecting ux and usability measures.
In International Workshop on Meaningful Measures: Valid Useful User Experience Measurement , 13-18.
Bevan, N. What is the difference between the purpose of usability and user experience evaluation methods.
Bird, S., Klein, E., and Loper, E. Natural Language Processing with Python.
The effect of task assignments and instruction types on remote asynchronous usability  k, testing.
Castillo, J., Hartson, H., and Hix, D. Remote usability evaluation: Can users report their own critical incidents.
Chevalier, J., and Mayzlin, D. The effect of word of mouth on sales: Online book reviews.
Constantine, L. L., and Lockwood, L. A. D. Software for use: a practical guide to the models and methods of usage-centered design.
Cortes, C., and Vapnik, V. Support-vector networks.
The data we considered were limited to two specific domains , and the volume of data, while respectable, was insufficient to establish a vocabulary for all usability dimensions.
Future studies must extend our work to more domains, and must consider a very large volume of data .
In addition, the idea of extracting vocabularies and associating features of texts or other utterances to dimensions of UUX can be applied to other domains, including spoken words at traditional lab-based usability studies.
It seems worth to investigate the difference across product domains of the distribution of sentences among UUX dimensions found in this study.
Likewise, it is interesting to link the presence of sentences pertaining to the UUX dimensions to attributes of the reviews that can be inferred otherwise, for example negative vs. positive reviews, or the helpfulness of reviews as voted upon by other users.
Filtering and grouping of dimensions may be examined in greater detail in follow-up studies also investigating actionable outcomes.
Finally, a better-performing classifier, or human annotation of a larger amount of complete reviews instead of isolated sentences may allow for analyzing the distribution of the dimensions we consider, on a per-review basis.
Csikszentmihalyi, M. Flow: The psychology of optimal experience.
A framework for capturing the relationship between usability and software architecture.
Gamon, M., Aue, A., Corston-Oliver, S., and Ringger, E. Pulse: Mining customer opinions from free text.
In Advances in Intelligent Data Analysis VI, vol.
3646 of Lecture Notes in Computer Science.
Hassenzahl, M. User experience : towards an experiential perspective on product quality.
In Proceedings of the 20th International Conference of the Association Francophone d'Interaction Homme-Machine, IHM '08, The ACM Press , 11-15.
Hassenzahl, M., Diefenbach, S., and G oritz, A.
Needs, affect, and interactive products.
Hassenzahl, M., and Tractinsky, N. User experience-a research agenda.
Hennig-Thurau, T., Gwinner, K., Walsh, G., and Gremler, D. Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet?
Hollander, M., and Wolfe, D. Nonparametric Statistical Methods, 2nd ed.
Hornbaek, K. Current practice in measuring usability: Challenges to usability studies and research.
Can online reviews reveal a product's true quality?
Ergonomic requirements for office work with visual display terminals -part 11: Guidance on usability.
International Organization for Standardization, 1998.
Human-centred design process for interactive systems.
Ketola, P., and Roto, V. Exploring user experience measurement needs.
In 5th COST294-MAUSE Open Workshop on Valid Useful User Experience Measurement .
Kim, S.-M., and Hovy, E. Automatic identification of pro and con reasons in online reviews.
In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL '06, Association for Computational Linguistics , 483-490.
Korhonen, H., Arrasvuori, J., and  n V aa anen-Vainio-Mattila, K. Let users tell the story.
Krippendorff, K. Content analysis: an introduction to its methodology.
Lutz, R. Changing brand attitudes through modification of cognitive structure.
Manning, C. D., Raghavan, P., and Sch utze, H. Introduction to Information Retrieval.
McNamara, N., and Kirakowski, J. Functionality, usability, and user experience: three areas of concern.
Miller, G. Wordnet: a lexical database for English.
Olsson, T., and Salo, M. Narratives of satisfying and unsatisfying experiences of current mobile augmented reality applications.
In Proceedings of the ACL-02 conference on Empirical methods in natural language processing Volume 10, EMNLP '02, Association for Computational Linguistics , 79-86.
Preece, J., Rogers, Y., Sharp, H., and Carey, T. Human Computer Interaction, 1st ed.
Ryan, R., and Deci, E. Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.
Seffah, A., Donyaee, M., Kline, R., and Padda, H. Usability measurement and metrics: A consolidated model.
Shackel, B. Usability-context, framework, definition, design and evaluation.
Shneiderman, B., Plaisant, C., Cohen, M., and Jacobs, S. Designing the User Interface: Strategies for Effective Human-Computer Interaction, 3rd ed.
Sriram, B., Fuhry, D., Demir, E., Ferhatosmanoglu, H., and Demirbas, M. Short text classification in Twitter to improve information filtering.
In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR '10, The ACM Press , 841-842.
