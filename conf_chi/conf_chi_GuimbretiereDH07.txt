We present ExperiScope, an analytical tool to help designers and experimenters explore the results of quantitative evaluations of interaction techniques.
ExperiScope combines a new visualization incorporating aspects of the KLM and the three-state model with an interface helping users to rapidly cluster similar patterns of interactions.
The tool makes it easy to identify and compare key patterns of use encountered during data collection.
This promotes a deeper understanding of the results of a given evaluation.
We illustrate the advantages of this tool by revisiting the data collected for an experiment conducted by Hinckley et al.
Our results show that our tool complements the previously reported results by offering insights about error behavior and the impact of mode switching on user performance.
By providing a more fine-grained analysis of the data gathered during empirical evaluations, we hope that our tool will improve researchers' understanding of existing and newly developed interaction techniques.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Because analyzing each individual interaction is practically impossible, only gross outliers are usually identified.
Once collected, the data is averaged and statistical tools are used to infer which interaction techniques or classes of techniques are faster in a given experimental context.
While this approach is well understood and widely accepted, it provides limited insight about the underlying reasons for specific results and does not fully capture the diversity of user behavior.
Like Vicente , we believe that this approach fundamentally limits the progress of the field.
First, it makes it extremely difficult to notice that some interaction patterns are more common than others or that errors fall into different categories.
More importantly, it also discourages researchers from capturing interaction data "in the wild" where patterns of interactions are more varied and, as a result, more difficult to analyze.
Instead, when designing experimental protocols, researchers struggle to properly balance the need for external validity with the need to study interactions that are simple enough to be easily analyzed.
Despite such efforts, controlled experiments are often dismissed as unrealistic.
Starting from a log of the interactions performed during an experiment or in actual use, our tool creates a synthesis of all the interaction patterns encountered during the experiment, highlighting for each class of interaction, the most common patterns of use as well as typical error scenarios.
To simplify the analysis of these patterns, we developed a new visualization technique shown in Figure 1.
It combines aspects of the Keystroke-Level Model  , to describe the atomic tasks required by an interaction  and the Buxton threestate model , to describe the current state of the input system and to represent a user's level of engagement  with his or her input devices.
We believe that this approach allows designers to understand the relationship between specific user actions and the input vocabulary offered by the input device at any given point.
Thus, our visualization helps designers appreciate what Buxton calls pragmatic considerations , such as deciding if it makes more sense to ask users to click a button at the periphery of the screen or to press a command button in order to enter a marking mode.
It is also flexible enough to accommodate a large variety of techniques including two-handed interactions  and pressure-based interactions .
ExperiScope allows designers to rapidly identify key patterns that users exhibit during data capture.
Experimenters can easily compare and, if needed, form higher order clusters of patterns which are equivalent from a user's point of view.
For example, while users may have perceived that they pressed a modifier key and a mouse key "at the same time", the event stream will record that one of the two was pressed earlier than the other .
Regardless of the exact sequence, the two patterns would be considered semantically equivalent.
Designers may also use our difference tool to identify and analyze timing differences between patterns.
We illustrate the potential of ExperiScope by revisiting the data collected for an experiment conducted by Hinckley et al.
Our analysis of the error patterns confirms the advantages of using the so-called "SpringOnce" as compared to the original "Springboard" design .
Our timing analysis also reveals that under certain circumstances, marking on a "lagoon" at the bottom of the screen can be as fast as marking in place.
Our visualization suggests possible reasons for this result.
The three-state model  generalizes the states sensed by input devices .
This model has been used extensively to describe and analyze new interaction techniques, such as a two-handed touch sensing technique  and Tracking Menu .
Unfortunately, three-state models describe all possible interactions simultaneously, making it difficult to understand typical syntactical constructs encountered by users.
This neglects important aspects of command selection design such as phrase structure and chunking .
Like an oscilloscope  presents a temporal description of the behavior of a complex state machine, our notation provides a description of the evolution of input states of a given three-state model in the context of the sequence of interactions performed by users.
This approach will help designers to better understand how users actually employ novel interaction devices or input techniques and thus pave the way for creative insights into how techniques could be improved or extended.
Our approach also extends the three-state model to more complicated state models, including multi-level buttons  or pressure sensing techniques .
The keystroke-level model   approximates the time it takes a user to perform an interaction using a series of pre-defined atomic steps, or operators, with constant values representing average task times.
KLM operators include P , H , and M , among others.
KLM has been applied to studies of command selection .
In the present system, we use KLM notations to provide a high level description of user interactions.
We present this information in the context of the interactive system state model to make it easier to identify specific pragmatic considerations  that might influence the design.
Several notations and simulators have been proposed to study possible designs of interactive techniques.
The GOMS model proposed by Card et al.
Its successors, including CPM-GOMS,  and EPIC , were used in increasingly more complex scenarios.
These models and their corresponding implementations  focus on simulating users' behavior assuming known interaction patterns.
The tool described here is complementary as it helps researchers to better understand real users' behaviors.
It can also be used in conjunction with GOMS-based models to test hypotheses or calibrate the models for a given experimental setting although this possible extension is not considered in the present paper.
Our work is also related to recent work comparing usage behaviors for single and multiple monitors .
Our visualization design uses KLM as a starting point.
This is a natural choice since the KLM has been used extensively to analyze the results of experiments and convey the structure of command interactions .
In the KLM, a given interaction is decomposed into a series of discrete events representing actions such as pointing at a button, marking a command, and pressing a key or a button.
This notation is easily extensible beyond the original set of events; Figure 2 presents the set we currently support in our system.
While very powerful, the purely textual KLM descriptions are often difficult to read.
In particular, when looking at a given event sequence, it is challenging to understand the system input stream state and the pragmatic aspects of the interaction.
For example, it is often difficult to determine readily which hands are being used or which button is pressed without retracing the sequence step by step.
This can be very tedious and error prone.
To address this problem, we rely on a two-dimensional notation presented in Figure 3.
On the horizontal axis, we present the phraseology of the interaction using the KLM action symbols  as they appear over time.
On the vertical axis, we represent the pragmatic aspects of the interaction itself including the locus of interaction and the state of each device at a given time.
In most current graphical user interfaces, the screen can be partitioned in two main areas: the Task Area which contains the users' work, such as the drawing area of a drawing application or the text area of a word processor; and the Command Area dedicated to command selection mechanisms such as the menu bar and different toolbars.
Our notation segregates activities in these areas into two different "bands" .
Transitions between Task and Command Areas are highlighted in light blue indicating a shift in visual attention.
It is important to note that this information can be inferred without the use of eye-tracking technology by simply noting where pointing interactions are taking place.
However, this may not always be reliable.
For example, novices may look at the keyboard to execute a shortcut.
In each of the two bands, we represent the state of each input stream with a notation similar to a timing diagram.
Many command selection techniques use simple two-state buttons.
However, multi-level buttons  or pressure sensing techniques  require that our notation represents more than three levels of interaction.
For multi-state selections we add additional levels to the "3-state" representation .
For continuous sensors we adopt the Card et al.
Multiple buttons can be represented by different line styles and colors, allowing even complex chording interactions to be represented.
To visually emphasize changes in levels of engagement, the area beneath the line is lightly shaded .
Following Sellen et al , we adopt a notion of engagement which reflects both the haptic and visual channels.
For each step, our visual representation illustrates the user's level of engagement:
Let's consider a simple "Connect the Dot" task in which users are asked to select a color using a tool palette and then asked to connect the two dots using a simple rubber band interaction .
The visualization of this interaction is shown in Figure 3 bottom.
Then, they switch their attention back to the drawing area  and aim  at the first dot before pressing the pen tip .
Next, they use their dominant hand to place the pen on top of the first dot .
The goal of our visualization was to make key aspects of interaction design more salient.
The representation of elapsed time along the horizontal axis makes it easy to pinpoint parts of an interaction sequence that slow down users.
On the vertical axis, we emphasize the engagement level required at any given time, and highlight key aspects influencing users' engagement including the need to switch one's locus of attention, or the need for synchronization between two hands.
It is clear that a systematic manual analysis of a log could provide similar results.
We believe, however, that our visualization will help novices to rapidly pinpoint problems, and allow experts to conduct analyses that are more fine-grained than the ones based on currently available models.
So far we have only considered one-handed techniques, but, of course, many interactions such as ToolGlass  or simply the use of modifier keys involve both hands.
In twohanded interactions, both the dominant and the nondominant hand may travel between the task area and the command area .
We present the diagrams for the dominant and nondominant hand alongside each other.
For each hand, the band for the task area is shown closer to the baseline  than the band for the command area.
In Figure 4, we illustrate the use of our notation for two-handed techniques using the example of a "Connect the Dot"-task performed with ToolGlass.
The non-dominant hand controls a semi-transparent tool palette.
To select a tool or, in this example, a color, users need to click through the corresponding color on top of the starting point of the line to be drawn.
They then connect the two dots using a simple rubber band interaction .
Our visualization of this interaction is shown in Figure 4, bottom.
The visualization presented above is very useful to explore and compare a small set of interactions.
Of course, given the sheer number of interactions recorded, it is impractical to observe each interaction individually.
ExperiScope therefore combines the visualization described above with a hierarchical visualization of all the patterns encountered during an experiment.
For each sequence, the system first constructs a string representing the sequential succession of events during that interaction.
This string is then matched against existing patterns to determine if such a sequence has already been seen.
If so, the new sequence is placed in the corresponding bin for further processing before visualization.
If not, a new bin is created and assigned the corresponding pattern.
While the use of a serial representation may be limited when compared to more complex representations such as Partially Ordered Sets , it was sufficient to describe all the cases we encountered in the present example.
To further limit screen clutter, our tool only shows patterns which appear more than 5% of the time for a given interaction type.
Less frequent patterns are grouped into a folder labeled "infrequent" .
If necessary, the tool also creates a subfolder called "outliers" for sequences with a total completion time more than three standard deviations above the mean for the corresponding experimental cell.
For each pattern and folder, statistical information such as average time and frequency is presented to the right of each item.
The data from the "outliers" folder is not considered when computing these values.
As shown in Figure 5, this approach allows for a quick overview of the general trends of an experiment but also provides more details if needed.
To use ExperiScope, experimenters must first describe each interaction sequence using ExperiScope's simple XML format.
This format describes the type of the interaction sequence  as well as key events occurring during the interaction.
These include three-state model transitions , KLM events , error conditions, and any other user defined annotations that can help to interpret the corresponding interaction .
These annotations can be either generated automatically or created manually when automatic detection is difficult .
Each interaction record can also include "private" data which will be ignored by ExperiScope, but will be kept with the corresponding record during all processing.
Upon reading the data from an experimental run, our tool first creates a synthetic hierarchical overview of the interaction patterns encountered during the experiment.
An example of this overview  is shown in Figure 5.
The first levels of the hierarchy reflect the structure of the experimental design.
They are built automatically using the interaction type data provided by the experimenter.
In this example, the analysis considered 2 different tasks , 4 task structures , and two command selection techniques .
Within each experimental cell, the system further groups correct trials and trials containing any errors.
Sometimes, patterns with a slightly different serialization of events are considered equivalent in practice.
This is best illustrated by interactions which involve pressing a modifier key while clicking the mouse button.
In such interactions, it is often difficult for users to respect a strict order .
Therefore, interfaces often adopt a lax synchronization policy which does not require a specific ordering.
Our system offers the option to cluster apparently different but functionally equivalent patterns into one group by a simple drag and drop operation.
Upon clicking on any pattern, the pattern is shown in a new window using the visualization described above.
For simple patterns, the time between events is computed by averaging the corresponding times across all sequences that match a given pattern.
For sets of functionally equivalent patterns that were grouped manually, the visualization reflects possible variations in the sequence of events by creating hatching marks in areas where the different patterns disagree .
Currently, our system assumes that the main interaction patterns  are identical within a set.
More complex composition methods are possible and will be considered in future work.
The Alternation condition is shown.
The five targets that users are asked to circle are shown in the middle of the screen.
The lagoon is shown in the lower left corner.
The purple line represents the activity of the pen tip, the green line the activity of the pen button.
The top pattern represents the aggregate view of the bottom patterns using hatching where patterns disagree.
The tool creates grey bands between timelines to simplify comparison.
To simplify pattern comparison, users can also open several patterns in the same window - either by selecting several patterns to open at once, or by simply dragging a new pattern on top of an existing window.
In such cases, our tool graphically highlights the timing differences between each segment of the main interaction patterns  as shown in Figure 6.
To further simplify comparisons, our system also offers an alignment tool based on instrumental interactions .
The tool consists of a vertical ruler that can be set at any position on the time scale.
Once set, any location on a diagram can be snapped to the ruler by using a snapping tool.
To simplify the analysis of successive segments, users can use the horizontal arrow keys to shift all the patterns that are currently aligned by one transition to the right or left.
If needed, any diagram can be edited to reflect information that was not captured during the logging process but may be helpful for data analysis.
This includes adding text tags to identify events of interest such as the name of the currently selected tool.
Users may also create a diagram from scratch as a reference point.
This feature can be useful to compare expected patterns to observed patterns.
This makes it easy for example, to restrict a statistical analysis to a specific set of interaction patterns or to interactions annotated with a certain flag.
While it would have been possible to include advanced statistical capabilities in ExperiScope, we found it more convenient to conceptualize our tool as a filter between the collected data and traditional data analysis packages such as SPSS.
We used the tool presented above to revisit data from Hinckley et al.
The Springboard technique  explores the use of springloaded modes  in pen interfaces.
Springboard interactions require users to press a command button with their non-dominant hand in order to switch between inking and command gestures.
This approach can be used in local settings where the command selection is performed at the locus of attention , or in non-local settings where a special area for command selection , becomes active at the lower left of the screen upon pressing the command button.
Upon activation, the lagoon area can be used to mark commands .
To evaluate the potential for the Springboard technique, Hinckley et al.
All tasks were derived from a simple basic task: Using a pen to circle five large dots shown on the screen.
In the basic reference task, all the dots were to be circled using standard ink.
A different tool was required for each dot.
This task was designed to simulate commands interleaved with inking.
In the Repeat condition, the 3 center dots required participants to select the same tool, once, for all three targets.
This task examined the influence of the amortization of command selection .
While all tasks used marking menu  as a menu system, in the Marking setting, the menu could be triggered at any location of the screen simply by pressing a command button.
In the Lagoon setting, the menu could only be triggered inside a lagoon .
In the Persist setting, the command button only needed to be pressed during the tool selection, and the tool selection persisted until a new tool was selected.
This reflects the behavior of a modal system.
In the Once setting, the command button only needed to be pressed during the tool selection, and the tool selected was available for one stroke.
Thereafter, the pen reverted to inking.
In the Springboard setting, the command button needed to be pressed as long as the tool was needed .
Once the command button was released, the pen reverted automatically to the inking.
Finally, in the SpringOnce setting, users could either keep the button pressed as long as the tool was needed, or, if they released the button, the tool stayed active for at least one stroke.
This mode was introduced because for simple commands, users sometimes released the command button too soon.
The log of one participant was not available to us, and the logs did not contain pen locations, so the transitions between task and command areas were inferred.
Next, we used our visualization tool to analyze the data.
In the following, we illustrate how our tool can aid the analysis and interpretation of this type of dataset.
Because we focus on the use of the visualization tool, we did not carry out any analyses to evaluate the statistical significance of the findings.
In practice, of course, such analyses should be performed to validate the insights gathered by ExperiScope.
Our first task was to verify that user behavior reflected the instructions provided during the experiment.
For example, we examined user behavior in the Alternation Lagoon condition and compared interaction sequences for different modal settings.
Our tool illustrated  that the most dominant pattern of interaction for the SpringOnce setting  was identical to the most dominant pattern in the Springboard condition .
This confirmed that users did not mind keeping their finger pressed during the full duration of the command.
At the same time, the SpringOnce design appeared to help users to reduce their errors.
For SpringOnce, the error rate was only 10% .
When examining the 12 different error patterns, we found that 37% of them were caused by Springboard users releasing the button too soon.
It is interesting to note that the pattern seen in the Lagoon condition is not repeated in the Marking condition.
Here, only 58% of the patterns observed in the SpringOnce setting resembled Springboard patterns.
Left: first command invocation; Right: second command invocation.
In both cases, the alignment tool  is used to align the graph at the end of the previous interaction .
One of the key findings of Hinckley et al.
Our tool provides some interesting findings in that regard.
First, we found that for the most common interaction pattern  in the Alternation Springboard condition, the average interaction time for Lagoon was about 150 ms faster than for Marking.
Given the opposite pattern for the average completion times across all interactions in this condition , this suggests that the slower performance of Lagoon was due to the cost of error correction.
This is not surprising, since in the Lagoon setting, an error could cause a full round trip between the task area and the lagoon area.
Our tool also shows that the two techniques do not proceed at the same speed, but distribute the interaction times differently.
First we observed that the in the Marking condition, the first interaction occurred about 300 ms later than in the Lagoon condition, probably reflecting a longer initial thinking time.
We then carried out a side by side comparison of the timing for the two techniques during the first  and second command selection .
We used our alignment tool to synchronize the two patterns at the point where the user completes the previous interaction .
It also shows that for the first command selection, which takes place at the left of the screen , there is virtually no difference in travel time for the dominant hand between both conditions .
This is somewhat surprising since the two paths are very different.
In the Marking condition, participants first needed to mark anywhere on the screen and then go to the target.
In the Lagoon condition, participants first needed to go to the Lagoon and then to the target.
By the time the target is circled, Marking is ahead of Lagoon, yet most of this speed advantage is lost during the release of the command button, which seems to incur a higher cognitive load.
This reflects the influence of Fitts' law  on the overall performance of the task.
We also used our tool to explore the cost of pressing a button.
Experimental protocols often require several iterations to be successful.
For this reason, we consider it a very important feature of our tool to allow for seamless transfer of analytical approaches and insights from one round of analysis to the next.
For example, during the early phases of experimental design, researchers may identify patterns of interactions which are equivalent and subsequently cluster them together to create a filter.
Although this often represents tedious work, it may help researchers to gain key insights about the data set.
It is therefore important that the information gathered by manual modifications of the clustering can be easily transferred from one dataset to the other .
We are currently exploring solutions to export cluster patterns from one cycle of analysis to the next.
This mechanism can also help researchers to create a library of similar patterns to simplify the analysis of future experiments.
In the Lagoon-Once condition, no button press is necessary to activate the Lagoon , so it serves as a useful reference.
A comparison of the menu invocation times for Marking-Once  and Lagoon-Once , shows that for Marking-Once, the ability to invoke the menu at a convenient location may completely offset the cost of pressing the command button.
The Marking-Once pattern also exhibits a faster command selection time and a short delay between command selection and drawing.
It is important to note that in the Marking-Springboard condition  the exact same actions take considerably longer.
This may highlight the higher mental cost of the Springboard approach as compared to the simpler Once approach.
Since the pattern of use for the Once condition is so regular, it can probably be chunked completely  into a broader pattern, while the Springboard requires a small amount of mental preparation that cannot be chunked.
So far, we have presented experimental analysis as the main focus of ExperiScope.
In addition, we believe that it has great potential as a teaching tool.
We foresee that it can be used both by teachers to illustrate key aspects of interaction techniques and by students to carry on their own investigations.
We are planning to integrate this tool into future HCI curricula and will evaluate its impact on students' understanding of key concepts in human computer interaction design.
These examples illustrate how ExperiScope lets researchers quickly inspect their data to discover the most common patterns of use and major causes of errors.
It helps designers gain a more in-depth understanding of their data to better comprehend how users distribute their time during a given task.
As a result, researchers will be able to answer more complicated questions and conduct more meaningful statistical tests.
The tool also illustrates the diversity of user behavior for any given technique.
In this paper we have presented a new visualization technique and a new tool for experimental analysis designed to help users rapidly review and analyze data captured during research on human computer interaction.
We have demonstrated how ExperiScope can be used to revisit data presented Hinckley et al.
The design of our tool makes it easy to use in a wide variety of contexts.
We believe that it will help both interaction designers and experimenters to evaluate and compare interaction techniques more accurately and provide a deeper understanding of experimental results.
It can also be used in teaching to illustrate key aspects of interaction techniques.
This work was supported in part by NSF Grant IIS-0414699 and by the Microsoft Research Center for Interaction Design and Visualization at the University of Maryland.
We would like to thank Corinna Lockenhoff, Ben Bederson, and Mary Czerwinski for providing many useful comments.
Nick Chen provided the data used in Figure 6.
Refining Fitts' law models for bivariate pointing.
Anderson, J.R. and M. Matessa, An Overview of the EPIC Architecture for Cognition and Performance With Application to Human-Computer Interaction.
Beaudouin-Lafon, M. Instrumental interaction: an interaction model for designing post-WIMP user interfaces.
Pad++: a zooming graphical interface for exploring alternate interface physics.
Toolglass and magic lenses: the see-through interface.
Buxton, B., Lexical and Pragmatic Considerations of Input Structures.
Buxton, B. Chunking and Phrasing and the Design of Human-Computer Dialogues.
Proceedings of IFIP World Computer Congress, pp.
Buxton, W. Three-State Model of Graphical Input.
Moran, and A. Newell, The psychology of human-computer interaction.
Robertson, A morphological analysis of the design space of input devices.
ACM Transactions on Information Systems, 1991.
The effect of reducing homing time on the speed of a finger-controlled isometric pointing device.
Fitts, P.M., The infomation capacity of the human motor system in controlling amplitude of movement.
Journal of Experimental Psychology, 1954.
Fitzmaurice, G., A. Khan, R. Pieke, B. Buxton, and G. Kurtenbach.
Atwood, Project Ernestine: Validating a GOMS Analysis for Predicting and Explaining Real-World Task Performance.
Grossman, T., K. Hinckley, P. Baudisch, M. Agrawala, and R. Balakrishnan.
Hover widgets: using the tracking state to extend the capabilities of pen-operated devices.
Guimbretiere, F., Fluid Interaction for High Resolution Wall-size Displays, PhD thesis, Stanford University.
Guimbretiere, F., A. Martin, and T. Winograd, Benefits of Merging Command Selection and Direct Manipulation.
Transactions on Human-Computer Interaction, 2005.
Hinckley, K., M. Czerwinski, and M. Sinclair.
Interaction and modeling techniques for desktop twohanded input.
Hinckley, K., F. Guimbretiere, P. Baudisch, R. Sarin, M. Agrawala, and E. Cutrell.
The springboard: multiple modes in one spring-loaded control.
Hutchings, D.R., G. Smith, B. Meyers, M. Czerwinski, and G. Robertson.
Display space usage and window management operation comparisons between single monitor and multiple monitor users.
Wood, K. Abotel, and A. Hornof.
GLEAN: a computer-based tool for rapid GOMS model usability evaluation of user interface designs.
Kurtenbach, G., The Design and Evaluation of Marking Menus, PhD thesis, University of Toronto.
Concurrent bimanual stylus interaction: a study of non-preferred hand mode manipulation.
Experimental analysis of mode switching techniques in pen-based user interfaces.
Luckham, D. and J. Vera, An Event-Based architecture Definition Language.
IEEE Transaction on Software Engineering, 1995.
Which Interaction Technique Works When?
Floating Palettes, Marking Menus and Toolglasses support different task strategies.
Ramos, G., M. Boulos, and R. Balakrishnan.
Raskin, J., The Humane Interface: New Direction for Designing Interactive Systems.
Sellen, A., G. Kurtenbach, and B. Buxton, The Prevention of Mode Errors Through Sensory Feedback.
Torenvliet, The Earth is spherical : alternative methods of statistical inference.
Theoritical Issues in Ergonomics Science, 2000.
Zeleznik, R., T. Miller, and A. Forsberg.
Pop through mouse button interactions.
