ABSTRACT This study investigates the use and e ectiveness of an advanced information retrieval  system .
64 novice IR system users were studied in their use of a baseline version of INQUERY compared with one of three experimental versions, each o ering a di erent level of interaction with a relevance feedback facility for automatic query reformulation.
Results, in an information ltering task, indicate that: these subjects, after minimal training, were able to use the baseline system reasonably e ectively; availability and use of automatic relevance feedback increased retrieval effectiveness signi cantly; and increased opportunity for user interaction with and control of relevance feedback made the interactions more e cient and usable while maintaining or increasing e ectiveness.
KEYWORDS: information retrieval, user interfaces, evaluation, empirical studies, relevance feedback INTRODUCTION We are experiencing in our work and home environments a dramatic explosion of information sources that become available to a exponentially growing number of users.
This has resulted in a shift in the pro les of users of online information systems: more users with no or minimal training in library science and/or information retrieval have gained access to tools that were once the almost exclusive domain of librarians who served as intermediaries between end-users with their particular information needs and the information retrieval tools.
This situation has stimulated increasing interest in computerized tools that support end-users in their information seeking tasks.
One particularly important such situation is the information ltering  task, in which streams of information  are automatically ltered by a program based on speci cations that are directly or indirectly obtained from the user.
How these speci cations should be obtained and used, and in particular, whether such programs should be autonomous or interactive are unresolved and controversial issues, which are explicitly addressed in the study reported here.
This paper describes an experiment investigating the information seeking behavior of 64 novice searchers who used one of four versions of an advanced IR system to formulate routing queries for two given search topics.
Each version o ered a di erent level of interaction with a query formulation support mechanism called relevance feedback.
The paper is organized as follows: we rst present the rationale for using an interactive, best-match, rankedoutput, unstructured input, full-text information retrieval  system, and we discuss relevance feedback as a support tool for query reformulation.
We then detail the design of the four di erent systems/interfaces employed in our study, and describe the experiment which we conducted.
The major portion of the paper focuses on a comparative description of the informationretrieval behavior and e ectiveness in these systems.
We conclude with a some general recommendations for the design of e ective interfaces for information retrieval suggested by our results.
SUPPORTING END USERS Users in all types of IR systems face the central di culty of e ective, interactive formulation of queries which represent their information problems.
Professional searchers using commercial IR systems have de-
Conversely, the di culties faced by end-users with no training or experience in the use of these systems have been well documented.
These belief values are based on the terms that are shared by the query and the full text of the document, and on the operators used to combine these terms.
The system returns a ranked list of documents with documents that best match a given query being ranked at the top.
We therefore restricted queries to simple lists of terms.
The only  operator allowed was the concatenation of terms to form multi-term phrases such as \automobile recall".
One particularly interesting and promising tool to support  query formulation in the context of these systems is relevance feedback: documents that the user has indicated as being relevant are used to revise or expand a user query or pro le with the goal of retrieving and ranking highly those documents that are similar to the \relevant" documents.
Our concern is with determining how a relevance feedback component impacts the information seeking behavior and e ectiveness of novice searchers in an interactive environment, and therefore with relatively few relevance judgments.
A central question for the design of interactive systems in general is the amount of knowledge a user is required or expected to have about the functioning of the system and the level of control a user can exert.
We share the \task-centered" view that interfaces for the occasional user should hide as much as possible of the inner workings of a system and should instead present users with a view that focuses on the user's task.
At one extreme, the existence of such a tool can be completely hidden from the user: the set of \relevant" documents could be determined by some algorithm that takes as input a user's behavior such as the viewing, saving, or printing of documents.
The other extreme would be a system that provides the user with complete control over the feedback mechanism: a user could provide lists of \good" documents to the mechanism, manipulate the query modi cations  suggested by the relevance feedback component, and even adjust internal parameters such as belief thresholds.
Between these two extremes there is a large space of possible designs; the goal of this study was to explore this space through the design of four systems/interfaces described in the next section.
THE SYSTEMS We designed and implemented  an baseline interface to the INQUERY  retrieval engine, RU-INQUERY,  that allowed users to enter queries and to view the results.
Users entered single or compound terms into the term entry window.
Terms were checked against the database and rejected if they did not appear in the collection.
Words were stemmed and common words that appeared on a stop word list were ignored.
Subjects submitted a query for retrieval by hitting the Run Query button.
The total number of retrieved documents and the titles of the top ve  ranked retrieved documents were displayed.
Users could scroll through the entire list of titles and look at the full text of any document by double-clicking on its title.
A check box next to each title allowed keeping track of documents between iterations.
A single-step undo mechanism allowed users to return to the previously run query.
The system blocked most inappropriate user activities in order to prevent error episodes.
Subjects could view the results of a query, and then reformulate the query by manually adding or deleting terms.
Performance in this baseline system was compared with one of three experimental versions, each o ering  a di erent level of interaction with a relevance feedback facility for automatic query reformulation.
The opaque interface/system treated relevance feedback as a black-box, a \magical" tool that hid the functionality of the relevance feedback component from the user.
Searchers were simply told that marking documents as relevant would cause the system to retrieve additional documents that were similar to the ones marked as relevant and/or that similar documents would be ranked higher.
Figure 1: The RU-INQUERY Interface: Base Version Without Relevance Feedback on the evaluation of documents rather than on the resulting reformulation of queries.
The transparent relevance feedback system/interface had the same functionality as the opaque version with the following addition: after a relevance feedback query had been executed searchers were shown the list of terms that were added to the user-entered terms by the relevance feedback component.
This additional information could be used to develop a more accurate model of the tool.
Feedback terms could also be used as a source for future queries.
The penetrable relevance feedback system/interface  took the transparent version one step further: in addition to providing information about the functioning of the relevance feedback mechanism it provided the ability to manipulate the output of the relevance feedback component prior to query evaluation.
The execution of the query was interrupted and the user was presented with the list of terms suggested by the feedback component.
Users had the opportunity to add none, some, or all of the suggested terms prior to the continuation of the query evaluation.
For example, the user might select only terms that appear to be central to the search topic.
THE EXPERIMENT Given the research results and considerations discussed above, our experiment was designed to investigate the following questions: Can best-match, ranked-output, full-text retrieval systems combined with an operator-free query language be used in an e ective way by end-users with little training?
Is relevance feedback e ective?
That is, do users using one of the three systems with relevance feedback perform better on the routing task compared to subjects who use the baseline system without relevance feedback?
Is user knowledge about the output of the relevance feedback system helpful?
If so, users in the transparent and penetrable conditions should perform better than subjects using the opaque relevance feedback system.
Is user control over the operation of the relevance feedback system helpful?
If so, users in the penetrable condition should perform better than subjects using the transparent feedback system and better than users using the opaque system.
Tobacco company advertising and the young A document will provide information on what is a widely held opinion that the tobacco industry aims its advertising at the young.
Narrative: A relevant document must report on tobacco company advertising and its relation to young people.
A relevant document can address either side of the question:  Do tobacco companies consciously target the young, or  As the tobacco industry argues, is this an erroneous public perception.
The "young" may be identi ed as youth, children, adolescents, teenagers, high school students, and college students.
Figure 3: Search Topic De nition For Topic 165 problem has become available.
For our experiment, we used a subset of the TREC test collection, consisting of 74,520 articles from the Wall Street Journal between 1990 and 1992.
Two search topics  were selected from the set of TREC search topics.
Each topic consisted of a title, a short description, and narrative that spelled out what would constitute a relevant article .
Each of the unique 2000 retrieved documents was rated by the rst author as being relevant or not relevant to the topic on hand.
These ratings were compared to TREC relevance judgments made by the originators of the topic descriptions that were available for a subset of about 560 of the 2000 retrieved documents.
The inter-rater agreement between the experimenter and the TREC evaluators was almost perfect  for topic 162 and very good for topic 165 ; cases of disagreement were resolved by careful reexamination of the documents.
The task for this study was a \routing" task, i.e.
Consequently, we provided users in our experiment with a routing scenario but asked them to focus on the current collection.
Speci cally, we asked subjects to devise a single nal query that retrieved at least 30 documents from the collection, of which as many documents as possible in the top 30 were relevant to the provided topic.
A focus on the top 30 articles seemed to mirror realistic retrieval situations in which users are only interested in a small number of documents.
A short  interactive tutorial was integrated into the interface.
It guided subjects through a sequence of exercises using the baseline system without relevance feedback in the form of a sample search.
Figure 2: RU-INQUERY Interface: Penetrable Relevance Feedback Version  How do di erent levels of interaction impact the information seeking behavior of users such as the number of queries developed and the way these queries are formed?
None of the subjects had any formal training in library or information science.
Their IR searching experience was limited to the occasional use of a computerized library card catalog.
In addition, a few instances of Psych-Lit searches and WorldWideWeb browsing were reported.
The INQUERY system and the RU-INQUERY interfaces were installed on a SPARC 2 workstation and users interacted with it via a networked SUN 3/50 with monochrome monitor, standard keyboard, and mouse.
Experimental Design and Procedure Subjects performed two  searches: all subjects used the baseline system without relevance feedback for their rst search, followed by a second search on a di erent topic using either one of the three relevance feedback systems or continuing the use of the baseline system .
The order of topics was counterbalanced between searches, leading to eight  di erent conditions based on the topic order employed  and the type of system used during the second search .
Subjects were assigned to one of the eight conditions in a blockrandomized fashion.
After giving their informed consent and lling out the online questionnaire subjects worked through the online tutorial at their own pace.
After subjects ended the tutorial or after the allotted time of 45 minutes had expired they were given twenty minutes to formulate a routing query for their rst topic.
At the twenty minute mark subjects were told that time was up and that they should wrap up the current action.
After a short break subjects worked for up to ten minutes either through one of the three relevance feedback tutorials  or returned to a review of the original baseline tutorial .
Next, subjects were again given twenty minutes to formulate a routing query for the second topic, using the system and interface they just had learned about.
All interactions with the system were automatically recorded by the system, creating a timed log of user and system actions.
Subjects were instructed to think aloud during their two searches and the utterances were video-taped along with the current screen image.
Independent of actual performance, subjects were told that they did well.
A short debrie ng phase that also asked subjects to state their opinion about relevance feedback concluded the experiment.
The two standard measures of retrieval e ectiveness are precision, the number of relevant retrieved documents over the total number of retrieved documents, and recall, the ratio of relevant retrieved documents to the total number of  relevant documents.
A major problem with both measures is that they assume the existence of relevance ratings for documents in the collection, and in particular computing absolute recall requires relevance judgments on all of the documents in a collection with respect to any query.
A further problem is that neither precision nor recall,
For the purposes of this paper we use precision at 30 retrieved documents, the cut-o level determined by the task that we set for our searchers in the experiment.
We used the non-parametric KruskalWallis and Mann-Whitney U tests with corrections for tied ranks to analyze precision results since the normality and homogeneity assumptions for an ANOVA were violated.
We report medians , the interquartile range , means , and standard deviations  for descriptive purposes.
Training Searchers trained on the baseline system on average about 26 minutes; the fastest subject nished the tutorial in 9.5 minutes whereas one subject had to be stopped after 45 minutes.
There were no di erences in training time between conditions.
There was no correlation between training times and performance on the rst search topic .
Baseline Search There were no di erences in mean search-times for the rst search by topic or experimental condition.
Topic 162 on \Automobile Recall" had 2.5 times as many relevant documents in the collection compared to topic 165 on \Tobacco Advertising And the Young" .
The average and median performance on the rst search did not di er for the four experimental groups that searched for topic 165 in their baseline search.
Among the groups that were given topic 162 rst, only the group that would later use the opaque relevance feedback version performed on a level that was borderline signi cantly lower than the other groups.
Subjects in the control group nished their review on average in 6.5 minutes; the opaque feedback group who had only to learn about the marking of documents for relevance feedback but not about interaction with the output of the relevance feedback mechanism nished equally fast .
Subjects in the transparent and penetrable condition used the allotted 10 minutes to nish the tutorial.
Figure 4: Retrieval Precision at 30 Documents for Final Queries of Second Search .
Given are median precision , mean , interquartile range, and minimum and maximum precision  for each of the four conditions .
Subjects who used relevance feedback had 17% to 34% better performance than subjects who continued in the control condition.
Subjects in the penetrable feedback condition performed as a group 15% better than subjects in the opaque and transparent feedback conditions.
Pairwise comparisons were not signi cant except for the di erence between the penetrable and baseline conditions  due to large within-group variations.
Figure 4 also demonstrates another interesting e ect of the transparent and penetrable feedback systems: even in the worst case subjects still were able to retrieve and rank 13 relevant documents in the top 30.
The same general pattern held when data were analyzed separately for both topics.
We can compare the performance on the two topics by considering R-precision, the proportion of relevant retrieved documents at the number of relevant documents in the collection.
Topic 162 had 83 relevant documents compared to 33 relevant documents for topic 165.
The median R-precision for all subjects searching on topic 162 was .39 as compared to .48 for all subjects working on topic 165.
Thus, subjects who searched on topic 165 were able to retrieve a larger proportion of those relevant documents known to be in the collection.
Although the task speci ed 30 documents as the cuto level, it is instructive to note the performance at other cuto s as well.
At 100 retrieved documents the systems' relative performances mirror the results from the top 30 cuto : the control group did worst , the penetrable group did almost 50% better , and the groups in the opaque and transparent condition fell in between .
If one only considers the top 5 and top 10 ranked documents, an even more dramatic di erence in favor of relevance feedback materializes: typically 5 out of 5 and 9 out of the top 10 documents were relevant for subjects in the feedback conditions whereas subjects in the control managed to manually design nal queries that had 3 out of the top 5 and 7 out of the top 10 being relevant.
A potential limitation of our study is the use of relevance feedback as a simple query expansion tool without a reweighting of user terms.
It remains an open issue whether or not the more massive query expansion through automatic, opaque relevance feedback may do better under particular term reweighting schemes.
Interactive query formulation is an iterative process of query design and entry, query execution, and query evaluation.
The number of iterations for each condition is depicted in gure 5.
There were signi cant di erences in the number of iterations .
Figure 5: Number of Iterations  for Second Search .
Given are median number of iterations , the mean , and the interquartile range for each of the four conditions .
The penetrable feedback group needed less iterations  to develop equally good or better queries.
The four di erent systems/interfaces shaped how subjects constructed their nal queries over the course of the interaction.
Table 1 provides a summary of the analysis of nal queries and their constituent terms.
The rightmost column lists the total number of query terms for each of the four conditions and for the two search topics separately and combined.
Users in the baseline condition without relevance feedback entered on average 6.4 terms , 1.8 of which were compound terms of 2 or 3 words each.
The use of relevance feedback led to a dramatic increase in query length.
In the opaque condition, the nal query comprised on average 35.5 terms .
This was not a result of users entering more terms , but a result of the automatic query expansion through relevance feedback which added on average 28.2 terms .
This expansion was caused by searchers having marked on average 12.6 documents as being relevant when running the nal query.
Table 1: Number and Sources of Query Terms in Final Queries for Second Search, Both Topics.
Given are the mean number of query terms for each of three possible sources: user entry, copied by user from relevance feedback output, and automatic query expansion through relevance feedback.
User queries had on average about 11 terms.
On average, only 3.8 of these terms were entered by the user, almost twice that many  were terms the user had copied from the display of automatically added terms or through relevance feedback expansion during prior iterations.
Finally, the average length of nal queries by subjects in the penetrable feedback condition was 21.7 .
This number is lower compared to the other feedback conditions because subjects in the penetrable condition selectively copied only 16.9 terms  suggested by the feedback component although the  number of 14.25 documents marked relevant shows that the number of terms suggested by the feedback component was comparable to the number of terms added by other feedback conditions.
That is, subjects were quite selective in using feedback terms; an initial content analysis of copied terms suggests that users primarily copied those terms which had a clear and central semantic relation to the search topic.
Given that relevance feedback depends on the marking of relevant documents and that topic 165 was much harder than topic 162, it is not surprising that there were signi cant di erences in query length between the two topics  = 16 3 001.
However, table 1 shows that the pattern described above also holds for each topic in isolation.
Subjects used our system and interface quite e ectively and very few usability problems surfaced.
Users had little problem formulating their queries and the observed retrieval e ectiveness of nal queries supports the view that interactive best-match, ranked-output, unstructured input, full-text retrieval systems are suitable tools for end users with limited search experience.
Users clearly bene ted from the opportunity to revise queries in an iterative process.
Overall, relevance feedback is a bene cial mechanism that improved performance by 10%  at 30 retrieved documents.
Even larger gains were observed if one only considers the top 10 or top 20 documents retrieved, a situation quite common in many application domains.
All users declared their preference for the relevance feedback mechanism over the baseline system.
Subjects that interacted with the penetrable version of relevance feedback did best in our study, but individual di erences in performance precluded statistical significance for most di erences between feedback systems.
One clear e ect was the diminished variability of results in the transparent and particularly in the penetrable condition: these versions seemed to insure at least moderate retrieval success for all users.
Perceived performance, trust in the system, and subjective usability are important issues, in particular for such sensitive domains as the ltering of one's personal email or news.
Subjects really \liked" the penetrable version that allowed them to manipulate the list of suggested terms.
Indeed, subjects in the opaque condition and in another study currently in progress routinely expressed their desire to \see and control" what the feedback component did to their queries.
People commented that using the feedback component as a suggestion device made them \lazy": the task of generating terms was replaced by the easier task of term selection.
Furthermore, users in the penetrable condition needed fewer iterations to achieve results comparable to, or better than the other, less interactive, feedback conditions.
Although a number of issues need to be further addressed, these results strongly suggest that interfaces for IR systems for end users should be designed to support interactive collaboration in query formulation and reformulation between the users and the \intelligent" programs which support them.
This work is supported by NIST Cooperative Agreement 70NANB5H0050.
Thanks to the Rutgers University Center for Cognitive Science  for providing equipment loans and infrastructure support and to Jamie Callan, Bruce Croft, and Steve Harding of the Center for Intelligent Information Retrieval at the University of Massachusetts at Amherst for their unstinting support of our use of INQUERY.
Nicholas J. Belkin and W. Bruce Croft.
James P. Callan, W. Bruce Croft, and Stephen M. Harding.
E ectiveness of weighted searching in an operational ir environment.
Overview of the second text retrieval conference.
Proceedings of the Third Text REtrieval Conference , Washington, DC, 1995.
Jurgen Koenemann, Richard Quatrain, Colleen Cool, and Nicholas J. Belkin.
New tools and old habits: The interactive searching behavior of expert online searchers using inquery.
In Donna Harman, editor, TREC-3.
Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Gerard Salton and Chris Buckley.
Improving retrieval performance by relevance feedback.
Natural language vs. boolean query evaluation: A comparison of retrieval performance.
