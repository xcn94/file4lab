People routinely rely on Internet search engines to support their use of interactive systems: they issue queries to learn how to accomplish tasks, troubleshoot problems, and otherwise educate themselves on products.
Given this common behavior, we argue that search query logs can usefully augment traditional usability methods by revealing the primary tasks and needs of a product's user population.
We term this use of search query logs CUTS--characterizing usability through search.
In this paper, we introduce CUTS and describe an automated process for harvesting, ordering, labeling, filtering, and grouping search queries related to a given product.
Importantly, this data set can be assembled in minutes, is timely, has a high degree of ecological validity, and is arguably less prone to self-selection bias than data gathered via traditional usability methods.
We demonstrate the utility of this approach by applying it to a number of popular software and hardware systems.
In this paper, we argue that search engine query logs can be filtered and transformed into forms that usefully complement and augment data collected via traditional usability methods.
We demonstrate this potential by introducing an automated process for harvesting, ordering, labeling, filtering, and grouping search queries to understand the common tasks and needs of a user base .
We call this process CUTS--characterizing usability through search.
Importantly, the labeled, ordered data produced by CUTS can be assembled in minutes, is timely, has a high degree of ecological validity, and is arguably much less prone to selfselection bias than traditional means of collecting data from users.
As an example of the utility of this approach, an approximation of this process can be illustrated using Google Suggest, the service that provides query completion suggestions for a given input.
Given the phrase "firefox how to", Google Suggest produces a list of 10 suggested completions .
As we will show later, these suggestions closely correspond to the 10 most popular queries matching that input.
From the list of top 10 Firefox "how to" suggestions , it is immediately clear that users have a number of pri-
For example, users submit search queries to locate tutorials, troubleshoot problems, or learn how to use specific features of an application.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Finally, common questions or issues are often expressed using a number of different query phrasings.
To cope with this variability, we introduce a transformation that enables minor differences between queries to be ignored.
The rest of this paper is structured as follows.
We first present related work, then describe our method for harvesting and ranking search queries using publicly available services.
We then introduce our two classification schemes and show how they can be used to label and filter search queries.
The final step of the process, grouping queries, is discussed, and a set of strategies are introduced to assist with this process.
We then present a series of examples illustrating the overall utility of this approach, and conclude with a discussion of the limitations of the technique.
However, the eighth item  is particularly interesting.
An inspection of the Firefox user interface , reveals that the top-level menu bar is easily hidden by deactivating the "Menu bar" item in Firefox's "View  Toolbars" sub-menu.
However, once this action is taken, it is not easily reversed: The top-level menuing system is now hidden, removing the very means the user would employ to attempt to re-instate the menu bar.
What is noteworthy about this example is that we quickly moved from data derived from query logs to a testable hypothesis regarding the usability of the software.
The contributions in this paper lie in expanding this manual process to the automated one shown in Figure 1.
While seemingly straightforward, automating this process requires overcoming a number of challenges: Raw query logs are not made publicly available; there is a need to automatically determine query intent for the purposes of labeling and filtering queries ; and differently phrased queries on the same topic should be reduced to a common canonical form.
Our specific contributions, outlined below, address these challenges.
To address the problems of obtaining and ranking search queries, we demonstrate how publicly available query suggestion services  and web-based tools for advertisers can be employed to create reasonable approximations of raw query logs.
We also introduce two new query classification schemes to address the need to label and filter queries.
The first classification scheme is a taxonomy that extends previous search query taxonomies to include categories relevant to interactive systems.
For example, this new taxonomy differentiates between queries issued to troubleshoot a problem and those seeking a tutorial.
The second classification scheme considers how a query is phrased.
We show that how a query is phrased closely corresponds to the categories of our specialized taxonomy.
CUTS exploits the relationship between these two classification schemes to ascribe query intent from query phrasing.
In recent years, researchers have demonstrated the potential for search engine query logs to model and predict real-world phenomena and events.
For example, Jeremy Ginsberg et al.
In this latter research, "health-seeking behaviour" is automatically detected by monitoring search terms associated with influenza .
This allows the Google Flu Trends application4 to estimate the prevalence of influenza infections on a week-to-week basis.
The resultant models closely agree with data released by the Center for Disease Control , though they exhibit much less lag: Models built using query logs show a 24 hour lag in tracking flu trends, compared to the week lag of the CDC.
More generally, Richardson  argues that query log analysis could quickly become an indispensable tool for researchers working in such human-centric fields as anthropology, sociology, psychology, medicine, economics, and political science.
He notes that query logs function as if "a survey were sent to millions of people, asking them to, every day, write down what they were interested in, thinking about, planning, and doing."
Accordingly, he argues that "taken as a whole, across millions of users, ... queries constitute a measurement of the world and humanity through time" .
To demonstrate his point, Richardson describes a common search pattern that unfolds over the course of three to six months, starting with a user's search for "mortgage calculators".
Within a week, these same users search for "realtors".
About one month later, they search for legal services , and three months later, their searches include those for home furnishing .
As with Google Flu Trends, this latter example shows the potential for query logs to describe real-world phenomena.
Within the realm of interactive systems, the research literature contains many accounts of search query logs being used to improve information interfaces--interfaces in which finding or accessing information is a user's primary task.
For example, Zhicheng Dou et al.
Our work broadens these previous uses of query logs, demonstrating their utility in understanding users' needs with any publicly available interactive system.
While prior work in interactive systems has focused on using query logs to improve information interfaces, work by Brandt et al.
Through studies of developer practices, Brandt et al.
Our work more generally considers how people employ search to support their use of interactive systems.
Importantly, all of the aforementioned research has been conducted by institutions or companies with direct access to search query logs.
Access to these query logs is highly guarded, especially after the privacy problems encountered when AOL released  an anonymized sample of their query logs .
Lacking direct access to raw query logs, Bar-Yossef and Gurevich have demonstrated how statistics of these logs can be approximated using an importance sampling technique .
This technique estimates the popularity of certain keywords by using parameterized models  and by sampling query completions provided by query completion suggestion services.
Our work is inspired by this research, as we also use query completion suggestion services.
However, we perform more exhaustive searches of the query suggestions for a specific topic , and we supplement query completion suggestions with data harvested from web-based advertising tools.
There are also a number of research endeavors which are related to CUTS, but which do not use query logs as a primary data source.
In particular, many researchers have explored how logs of user interface events can be leveraged to learn about a system's usability , or to help characterize a system's user community .
In this space, David Akers et al.
Similarly, Amy Hurst et al.
We view query log analysis as complementary to the analysis of user interface event logs, and as providing a higher-level view of the issues encountered by users.
Moreover, query logs tend to more directly showcase issues regarding feature discoverability, as evidenced by the "firefox how to get the menu bar back" example noted in this paper's introduction.
Finally, online social media sites such as blogs, customer support forums, and sites hosting customer product reviews are other possible sources for understanding user needs.
These sources are heavily utilized in sentiment analysis research  .
When applied to the evaluation of interactive systems, sentiment analysis can determine if a product's reviews are generally positive or negative, and can determine which aspects or features of a product typically lead to a positive or negative review.
While sentiment analysis is a promising area of research, its effectiveness and scope as a tool for understanding user needs may be impacted by the effort required for users to post content to these social media outlets .
Conversely, query log analysis provides a view of the issues encountered by the broader user community who may not be regular contributors to the online discussion of interactive systems.
In summary, previous work demonstrates the general utility of query log analysis: it yields timely, highly ecologically valid data that can quickly lead to significant insights when studying a wide range of phenomena.
However, query log analysis has not been previously applied to the problem of characterizing the overall usability of interactive systems.
In the rest of this paper, we demonstrate its specific utility in the realm of interactive systems by detailing each step of the CUTS process.
The core of the CUTS process lies in the analysis of search query logs.
When access to raw query logs is not possible, search queries can be harvested using publicly accessible interfaces: Modern search engines provide indirect and privacy-preserving access to their logs through their query completion suggestion services .
In this section, we describe a process for systematically harvesting queries related to a particular interactive system using these services.
We also provide evidence that the results of this method can be considered a representative sampling of the raw query logs.
Query completion suggestion services operate as if backed by a prefix tree .
When viewed in this way, the characters making up a partially entered query define a path through the tree starting at the root, passing through numerous nodes.
Each node contains a listing of popular queries whose prefix matches the path taken thus far.
Query completion services follow the paths prescribed by partially entered queries, and return the suggestions listed at the ends of these paths.
Given the tree-like structure of these services, a standard depth-first or breadth-first tree traversal can be performed by expanding partial queries one character at a time, starting with the name of the system under investigation .
A leaf  is reached when the completion service returns no suggestions for the given prefix.
Google also states: Our algorithms use a wide range of information to predict the queries users are most likely to want to see.
For example, Google Suggest uses data about the overall popularity of various searches to help rank the refinements it offers.
Some search providers, such as Google, vary their query suggestions depending on the position of the caret in the search query input box .
More specifically, Google provides a list of the top 10 completions that either begin or end with the phrases on the left or right side of the cursor.
Given this behaviour, the whole tree traversal procedure can be repeated to uncover query suggestions that end with a particular suffix, providing a more complete sampling of the query logs.
By executing a systematic search of the query completion tree, many queries can be collected for a given topic.
For example, on June 19th , 2010, we recorded 74,795 unique queries when performing a systematic search for queries incorporating the term "Firefox" in Google Suggest's query auto-completion database.
Similar results were obtained for other systems for which we collected data .
To identify trends and new issues as they arise, it is desirable that query suggestion services emphasize recent searches over those performed in the more distant past.
To study the timeliness of Google's query suggestion service, we monitored the query completion suggestions for a range of products and software applications for a period of approximately three months .
Suggestions were sampled on Monday, Wednesday, and Friday on each week during this timeframe.
An analysis of the collected data reveals that Google updates its auto-completion database approximately once every 14 days.
These results indicate that Google is actively maintaining its query suggestion database.
Knowing the frequency with which these services are updated is advantageous, but is not sufficient for determining the extent to which current search trends are represented in query suggestions.
To investigate this question, we can examine when a noteworthy event begins to appear in query suggestions.
A prime candidate for exploring this question is provided by the release of the iPhone 4 on June 24th , 2010.
Almost immediately, there were reports of significant signal degradation when the phone was held in a certain way .
The first evidence of this issue was spotted in the query suggestions on July 14th , 2010.
On this date, the partial query "iphone d" resulted in Google suggesting "iphone death grip", while "iphone a" yielded "iphone antenna", and "iphone how to h" yielded "iphone how to hold".
None of these queries appear in the suggestions sampled on previous dates.
This corresponds to a lag of about 20 days, suggesting that the query completion services place sufficient weight on recent queries.
In harvesting these queries, our working assumptions are that  query completion services are derived from the raw query logs,  a given query's prevalence in these logs will have some bearing on its ranking in the list of suggestions, and , the suggested completions are timely.
By "timely," we mean that query completion services assign more weight to queries performed within a recent window of time.
In the following subsections, we briefly provide evidence that these assumptions are sufficiently valid for our purposes.
After harvesting queries, the next step is to assign an importance rank to each query.
When queries are sampled from query suggestion services, detailed query frequency information is not made available .
We substitute this missing data in two ways.
First, we complement our data set with data collected from advertising and market research tools, such as the Google AdWords Keyword Tool .
Second, we examine the structure of the synthesized prefix tree to obtain a partial ordering of the queries not covered by the market research tools.
We describe each technique in turn.
Google provides a set of tools that can be directly applied to the problem of ranking queries.
The Google AdWords Keyword Tool , intended to help marketers valuate keywords for advertising purposes, can be configured to report the estimated average global monthly search volume for any exact phrase.
As such, it is possible to directly rank many query suggestions using this tool alone.
In doing so, we again find that there is good correspondence between the harvested queries and their estimated search volume.
While many queries can be directly ranked using the Google AdWords tool, not all queries can be ranked in this way; Google AdWords provides no data for queries whose monthly search volume is below some threshold, and this threshold is reached well before the list of query suggestions has been exhausted.
For example, on June 19th , 2010, we harvested 74,795 unique query suggestions for the Firefox web browser.
However, Google AdWords provides search volume data for only 15,057 of those queries.
In short, the search volume of about 80% of the Firefox queries falls below the threshold reported by AdWords.
Accordingly, we must employ another means of ranking the remaining queries, as described next.
Given a ranked list of queries, the next step is to automatically classify queries according to the likely intent of the individual performing the search .
Once labeled, query logs can then be filtered to select entries that are potentially related to user tasks and usability issues.
Before queries can be automatically labeled and filtered, we first need to understand the range of system-related queries that users submit to search engines.
While previous work has developed a number of taxonomies for general classification of search queries  , we found these taxonomies too broad for our purposes.
Instead, a classification scheme specialized for the domain of interactive systems is needed.
Additionally, we need to understand what features of a query can be used to support automatic labeling.
In this section, we address both of these needs: We introduce a taxonomy of query intent specialized for interactive systems, and a second classification scheme that describes how a query is phrased.
As we will show, in this domain, query phrasing is strongly related to query intent.
Based on the aforementioned intent-phrasing relationship, we present a set of heuristics for automating the process of labeling and selecting queries of interest.
While query suggestion services do not return the frequency with which each suggested query is performed, we have argued that they operate by returning the most popular queries for a given input.
We can use this behaviour to derive a partial ordering of the query suggestions.
The key insight is this: We know that the 10 query suggestions returned for a given prefix are more popular than all other queries later harvested that also begin with that same prefix.
An example illustrates this point.
Returning to the earlier Firefox example, the suggestion "firefox menu bar missing" appears in Google's top 10 suggestions for the prefix "firefox m".
Thus, we can infer that the "firefox menu bar missing" query is more popular than the 2362 other suggestions occurring in the data set that also share the prefix of "firefox m".
We say that this query has 2362 subordinates in order to convey this relationship.
This ranking technique provides only a partial ordering because we can only perform comparisons of a node with its ancestors and descendants in the prefix tree.
We cannot directly compare suggestions occupying separate branches of the tree.
Nevertheless, for queries whose search volume falls below the AdWords reporting threshold, a search volumebased ranking will be crudely approximated by simply sorting those queries according to their number of subordinates.
These first two steps of harvesting and ranking queries provide us with a suitable, privacy-preserving, publicly accessible replacement for raw query logs.
In the remainder of the paper, our technique assumes only that one has access to a ranked list of search queries relating to the interactive system of interest.
Following the basic methods of grounded theory , we developed our query taxonomy by performing open coding on 200 randomly sampled queries regarding the GIMP software application.
From this initial coding, we identified a set of common, higher-level themes, which led to our taxonomy.
The resultant taxonomy includes six separate classes of interactive system queries, synthesized from the perspective of query intent: Q UERY I NTENT: * Operation Instruction Would the query be used to find instructions for performing a specific operation?
In parallel with developing the former taxonomy, we also developed a classification scheme that describes how individual queries are phrased.
The motivation for developing this scheme arose during our open coding sessions: In considering the range of queries, it appeared that how a query was phrased was very much related to the intent of the user.
As we will show, there is indeed a relationship.
Based on the open coding of the queries, the following highlevel categories of query phrasing were identified: Q UERY P HRASING: * Noun phrase  * Imperative statement  * Question  * Statement of fact  * Present participle  * Other In the next section, we show that raters are able to achieve a high degree of inter-rater agreement when using the intent and phrasing taxonomies to label search queries.
This agreement lends support to the overall utility of the taxonomies as instruments for labeling search queries.
The classifications of the 195 labeled queries are summarized in Table 3.
The categories we find interesting for usability analysis coincide with the first two listed in the table and the taxonomy: "Operating Instruction", and "Troubleshooting".
In our sample, about half of all query suggestions fall within categories that are of interest to HCI researchers and practitioners, demonstrating the overall richness of query logs when studying interactive systems.
To establish the inter-rater reliability of these two classification schemes, two researchers applied both schemes to a set of 195 queries sampled from the GIMP and Firefox datasets.
The GIMP and Firefox datasets were collected from Google Suggest on May 23rd , 2010 and June 19th , 2010 respectively.
Selection of the 195 sample queries proceeded as follows: For each application, the top 50 queries  were selected, followed by an additional 50 randomly selected queries.
The resulting set of 200 samples shared 5 queries in common with the set used for the initial open coding and were thus excluded from our validation process.
In labeling this data set, we achieved an overall inter-rater reliability rate of  intent = 0.76 for query intent, and  phrasing = 0.79 for query phrasing, using the Cohen's kappa measure of rater agreement.
Inter-rater reliability across the 4 sources of queries is listed in Table 2.
The observed agreements are considered to be substantial .
Before describing how the query phrasing classification scheme can be used to identify query intent, we first show how queries are distributed across these two classification schemes.
These query distributions lend additional arguments for the overall utility of this approach.
There are a few noteworthy observations to make in this table.
As can be seen, in our sample set, if a query is phrased as an imperative statement, there is a 90% chance that the query is seeking operating instructions.
A similar probability  applies if the query is phrased as a question.
Finally, if a query is phrased as a statement of fact, then it is almost certainly being used for troubleshooting.
These relationships provide us with a set of strategies for automating the labeling of queries, which we describe next.
In the previous section, the relationship between query phrasing and intent was established by examining labels that were manually assigned to queries by a pair of human raters.
Automating the CUTS process requires mechanization of the query labeling step.
Through further inspection of the data, we have found that certain keywords or patterns are highly indicative of each of the different phrasing types.
For example, queries containing the phrase "how to" indicate questions.
Once a query's phrasing has been established, we can then infer its intent using Table 4.
A partial list of phrasing patterns is presented in Table 5.
These patterns were generated through a manual inspection of labeled data, and serve as basic heuristics for labeling different types of queries.
Many queries will not match any pattern, and will thus go unlabeled at this stage of processing.
In the next section, we describe a technique for grouping related queries.
When queries are grouped, labels for the individual queries are extended to the group, increasing the coverage of the labeling.
More importantly, we found that a canonical group's cardinality  is directly related to the popularity of the group's overall topic or concern; compared to less popular topics, those experiencing high search volume yield logs that contain a more complete sampling of the alternative phrasings with which those queries can be expressed.
Consequently, those high-volume queries tend to form groups of higher cardinality.
To illustrate this point, Table 7 lists the cardinality of the canonical groups associated with the top 10 "firefox how to" queries already mentioned in the Introduction.
All but the last of these queries fall within the top 99.6th percentile of group sizes, thus reinforcing the popularity of these concerns.
The final step in CUTS is to reduce the variability with which queries are expressed in the data set.
In query logs, common questions or issues are expressed using a number of different query phrasings.
As an example, GIMP users may search "how to draw a circle in gimp", or they may simply type "gimp draw circle".
Given this variability, it is desirable that similar queries be grouped, and their weights or rankings combined, in order to better estimate the prevalence of a given issue.
To group similar queries, we transform queries to a canonical form where inconsequential differences are ignored .
This transformation applies the following rules: * Convert inflected word forms to common word lemmas.
We use the WordNet lexical database  to perform this transformation  * Remove all instances of stop words 
Using this technique, it is possible to achieve a modest reduction in the size of the data set.
The output of CUTS is a categorized and ranked list of query groups relating to the system under investigation.
A sample of this output, for the Firefox application, is presented in Table 8.
The final ranking of groups is determined by summing the search volumes of each group's member queries, and then sorting those groups accordingly.
When search volume information is not available, a sum of subordinate counts is used instead.
In this section, we apply our technique to a number of different interactive systems.
Our goal here is to demonstrate the wide range of insights that can be gained using this approach.
We structure this section by showing how issues related to language, desired functionality, and poor affordances can all be detected using this technique.
Query logs provide an excellent view of the vocabulary and terminology with which users conceive their use of interactive systems.
However, this terminology does not always match that which is used by their systems.
When such discrepancies arise, the associated systems can be considered to be in violation of Jakob Nielsen's "Speak the User's Language" usability heuristic .
We provide two examples of this problem that we identified using our technique.
One popular class of queries related to Apple's iPhone product inquires about the possibility of selectively blocking unwanted calls from specific telephone numbers.
While this feature is not currently supported by the device, users search for information on performing this task at least 5,800 times a month, or once every 7.5 minutes.
The consensus among the user community is that the issue can be resolved by associating a silent audio clip as the ringtone of unwanted telephone numbers.
That this issue is so popular suggests users would be well-served if provided with a sanctioned means of achieving this same behaviour.
The GNU Image Manipulation Program  is an open source raster graphics editor offering similar functionality to Adobe's Photoshop application.
Analysis of the GIMP data set reveals that the terms "black" and "white" co-occur in 93 distinct queries.
In each case, the queries inquire about converting color images to black and white.
According to the Google AdWords tool, the query "gimp black and white" is searched an average of 590 times a month, or about once every 74 minutes.
Inspecting GIMP's interface  reveals that there are at least three alternative methods for converting a color image to grayscale.
These methods are labeled as "grayscale", "desaturate", and "channel mixer".
Such technical terms may not be familiar to a sizeable portion of GIMP's user base, as evidenced by the vocabulary used in the harvested queries.
Given this finding, one could create a "black and white" command that aggregates into one command the many methods of transforming a color image into a grayscale image.
Another example of identifying desired functionality emerges when analyzing the searches specific to Amazon's Kindle eBook reader.
Specifically, query log analysis reveals 89 distinct phrasings of the query "how to change your kindle screensaver".
The Kindle device ships with a few dozen stock images that are displayed by the device when not in use.
However, these images cannot be customized by the end user.
Again, the popularity of these searches suggests that such a feature would be welcomed.
Inkscape is an open source vector graphics editor similar to Adobe's Illustrator program.
Interestingly, the 8th highest volume query was "inkscape crop", with an average of 480 searches performed each month.
However, being a vector graphics application, Inkscape does not have a "cropping" tool; cropping is specific to raster graphics.
The equivalent operation for vector graphics is to "clip".
This very popular query suggests that new Inkscape users are relying on Google to translate knowledge from one domain  to another domain .
This behaviour closely resembles similar behaviour exhibited by programmers' use of Google .
Recognizing this issue, Inkscape could provide a "crop" command or a help entry that assists users in setting the clip for their document.
Finally, an analysis of the GIMP query data set reveals many queries related to drawing primitive shapes: Roughly 130 unique queries inquire about drawing various types of lines, 80 unique queries inquire about drawing circles, 40 queries inquire about drawing rectangles, 20 queries inquire about drawing squares, and 14 queries inquire about drawing ellipses.
Moreover, the suggestions "gimp how to draw a line", and "gimp how to draw a circle" appear in the top 10 suggestions for the prefix "gimp how to", and the Google AdWords tool reports that the query "gimp draw circle" is performed an average of once an hour, each and every day.
These queries are noteworthy because GIMP provides no explicit tools for drawing simple shapes.
Dedicated tools for these functions would likely find great use by GIMP users.
Ubuntu is currently one of the most popular GNU/Linux distributions.
For reasons of security, Ubuntu disables the "root" superuser account by default, requiring users to issue the "sudo" command to gain superuser privileges.
The root account has otherwise been present and used in UNIX and UNIX-like systems for decades.
While Ubuntu's policy is arguably a positive change for security, the operating system may not be adequately com-
An analysis of the queries related to Ubuntu reveals nearly 130 distinct query phrasings all asking about how to access the root user account.
The specific query "ubuntu login as root" is performed 720 times a month, or about once an hour.
Similarly, a search for the error message "su authentication failure ubuntu" occurs about once every 7 hours.
These findings suggest that users would be well served by a more helpful or detailed error message which could communicate the proper course of action when attempting to login as the root user.
This additional information can be obtained using standard evaluation techniques involving users or expert evaluators.
Since many methods  require representative tasks to be identified for evaluative purposes, CUTS can assist by supplying a ranked list of common tasks and needs.
A ranked list of common queries can also be used to assign importance to existing lists of known usability issues.
The benefit of using the results of CUTS is that this ranking is derived from the search behaviour of thousands, if not millions, of users.
This ranked list may also be more exhaustive than existing lists tracking usability issues.
Software producers with limited resources, including volunteer-driven open source products, could thus benefit from this additional means of identifying potential usability problems.
In this section, we more broadly discuss issues related to using query logs to understand the needs of users of interactive systems.
We begin by comparing the output of CUTS to manually curated "frequently asked questions" .
We then discuss how query log analysis can factor into existing usability practices, and enumerate various issues that may affect the rankings produced by our method.
CUTS reveals search queries that are frequently performed by a system's users.
As such, its output is directly comparable to lists of frequently asked questions  commonly provided as documentation for many software applications.
However, standard FAQs are curated by individuals, and require continual maintenance and individual judgement regarding inclusion of content.
Accordingly, we expect CUTS to more accurately represent the needs of the user base over time.
A comparison of our results with the GIMP FAQ lends support to this notion.
The GIMP FAQ5 contains 25 questions/answers in the section entitled "Using GIMP."
Sixteen of these issues overlap the top issues found using CUTS.
The FAQ issues that don't overlap with our results tend to be quite specialized .
Since GIMP's user base primarily consists of casual users who perform relatively simple tasks , very few users will benefit from the answers to these specialized questions.
In contrast, CUTS reveals a more representative set of questions related to the simple tasks users have been found to perform .
Again, the CUTS data suggests many potential usability issues not directly addressed by the Firefox KB .
The key point in both instances is that CUTS provides a data-driven view of user concerns that is continuously updated.
A growing body of research  examines user search behaviour.
One of the practices observed is that people reformulate their queries when search results do not match their expectations or needs.
As an example, a user might start with queries consisting of a few words, and then pose more detailed questions as they fail to find relevant documents in the search results .
As a result of this query reformulation strategy, it is conceivable that the analysis proposed in this paper artificially inflates the importance of issues for which relevant information is scarce.
Reflecting on this, we note that query popularity has been observed to follow a Zipf's law distribution .
As a result of this exponential relationship between query rank and frequency, it is unlikely that any reformulation behaviours would grossly distort the ranking of popular queries.
However, the effect may be more pronounced among queries with lower search volume, suggesting a need for more work in this space.
A similar problem is encountered by products whose names are now synonymous with a class of operations or applications.
For example, an altered digital image is often described as being "Photoshopped," regardless of which software application was used for image manipulation.
In these problematic cases, we have found our filtering techniques  are often enough to filter out the less desirable, off-topic queries.
We also suspect that it is possible to differentiate between the uses of a word by analyzing the results that search engines return for those queries.
Search engines are designed to return relevant documents, and often refine their relevance rankings by observing which pages users visit after performing searches .
The query-document associations recorded by search engines provide a wealth of untapped information that can further guide analysis of query logs.
Use of these associations constitutes a promising area of future research.
In the interest of preserving user privacy, query auto-completion services are unlikely to suggest queries unless those queries have been performed many times, and by many different individuals.
As such, the quantity of data available for analysis by CUTS is related to the popularity of the interactive system being studied.
Related to this, while CUTS is ideal for assessing usability in general, we cannot currently filter results according to particular sub-groups of users, such as novices or experts.
Indeed, sub-groups in the minority are likely to be underrepresented in CUTS' output.
When faced with difficulties or questions relating to the use of interactive systems, many people routinely turn to Internet search engines as a first line of support.
In this paper, we have introduced CUTS: characterizing usability through search.
This process takes the name of an interactive system as input and outputs a ranked and categorized list of potential issues that users encounter with that system.
These data are assembled by sampling from the query logs of top-tier Internet search engines.
Importantly, the results of this process have a high degree of ecological validity, and can directly inform more formal evaluation methods by suggesting particular tasks or issues to study.
