Consuelo Valdes1, Diana Eastman1, Casey Grote1, Shantanu Thatte3, Orit Shaer1, Ali Mazalek2, Brygg Ullmer3,4, Miriam K. Konkel5 Dept.
Such affordances are well suited for sensemaking .
Research also indicates that tabletop interfaces support active reading  and promote reflection .
Considering these qualities of tabletop interaction, as well as other factors such as improving display quality, increasing availability, and falling prices of commercial hardware platforms, tabletop interfaces provide unique opportunities for learning and discovery with large and abstract data sets.
Multi-touch and tangible interfaces provide unique opportunities for enhancing learning and discovery with big data.
However, existing interaction techniques have limitations when manipulating large data sets.
Our goal is to define novel interaction techniques for multi-touch and tangible interfaces, which support the construction of complex queries for big data.
In this paper, we present results from a study, which investigates the use of gestural interaction with active tokens for manipulating large data sets.
In particular, we studied user expectations of a hybrid tangible and gestural language engaging this space.
Our main results include a vocabulary of user-defined gestures for interaction with active tokens, which extends beyond familiar multi-touch gestures; characterization of the design space of gestural interaction with active tokens; and insight into participants' mental models, including common metaphors.
We also present implications for the design of multi-touch and tangible interfaces with active tokens.
Multi-touch and tangible interfaces are a growing area of human-computer interaction  research.
Recent studies have focused on their use in entertainment, formal and informal learning environments, as well as collaborative search.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
However, designing multi-touch and tangible interfaces that support learning and discovery in data-intense areas requires going beyond the application of existing interaction techniques .
While direct touch is currently a standard input method for multi-touch and tangible interfaces, in data-intense applications, representations are typically small .
Finger size and occlusion make it difficult to interact directly with small targets through touch .
Furthermore, in data-intense applications, WIMP-style control elements provided by various toolkits, such as scrollbars, sliders, and text fields, may often be either too small for effective and accurate touch interaction, consume expensive screen real-estate , or lead to cognitive dissonance with the broader interaction paradigm.
Several studies have considered novel multi-touch interaction techniques for data driven multi-touch and tangible applications; e.g.
However, while providing advantage over touch interaction with WIMP-style controls, multi-touch gestures often suffer from low discoverability and lack of persistence .
In this paper, we investigate an alternative approach: exploring large data sets on multitouch and tangible interfaces using tangible interaction with active tokens.
Our goal is to identify novel interaction techniques for multi-touch and tangible interfaces, which support the construction of complex queries for large data sets.
Our work draws and expands on Tangible Query Interfaces  , which introduced tangible interaction techniques for querying large databases.
TQI utilized systems of tokens embodying query parameters.
These were manipulated, interpreted, and graphically augmented on a series of physical constraints.
Technological advances and massmarket offerings such as Sifteo Cubes  have opened up the possibilities to revisit this approach and consider novel tangible and gesture-based interaction techniques for the manipulation of large data sets that use active tokens .
Active tokens are programmable physical objects with integrated display, sensing, or actuation technologies .
Thus, they can be reconfigured over time, allowing users to dynamically modify their associations with datasets or controls.
The use of active tokens could support the exploration of large data sets by expanding the design space of token and constraints interaction , which leverages physical expression of digital syntax.
Combining interactive multi-touch surfaces with active tokens could facilitate the presentation and manipulation of big data while preserving the benefits of tangible interaction  such as support for two-handed interaction, co-located collaboration, and strong affordances.
We focus on a sub-class of active tokens that can be manipulated using gestures independently from global constraints.
Gestural interaction with active tokens blurs the boundaries between tangible and gestural interaction, between manipulations and gestures ; and fits within the area defined as "tangible gesture interaction" .
Active tokens also enable the expansion of tangible interaction with multi-touch and tangible interfaces beyond interaction on the surface into less explored areas such as tangible interaction onbezel and in-air, hovering above or in front the surface.
While technological advances facilitate the recognition and tracking of on-surface, in-air, and on-bezel interactions with active tokens, the gesture language for such interactions has yet to be explored in a task-driven scenario.
What kinds of gestures should be employed to manipulate and explore large data sets?
What gestures might typically or best be performed on-surface, on-bezel, and in-air?
In users' minds, what are the important characteristics of such gestures?
Are there guessable gestures for navigating a data set and for constructing complex queries?
How consistently do different users employ gestures for the same commands?
On the one hand, we regard the design of a hybrid tangible and gestural language as a highly complex task, which pushes the skills of even expert tangible and gestural interaction designers.
On the other, gestural interaction with smart phones and tablets is now routine for many millions of people.
To understand and respect the expectations of users, we see it important to explore a cross-section of users' first anticipations of new systems in this space.
In this paper, we present findings from a user study with 19 users.
The study investigated user expectations of a hybrid tangible and gestural language for multi-touch and tangible interaction with active tokens.
Our findings include a vocabulary of user-defined gestures for active tokens, and insights into the mental models and experiences that shape users' expectations.
Based on our findings we characterize the design space of gestural interaction with active tokens and discuss implications for the design of multi-touch and tangible interfaces with active tokens.
With the increasing importance of and access to large data sets by diverse prospective users , several multi-touch and tangible interfaces have been created to support learning and understanding in data intense areas in informal and formal settings.
For example, Block et al.
Involve  is a precursor of DeepTree that utilized Vornoi maps to visualize phylogenetic data.
Both applications allow users to zoom, pan, and find a particular element in a visualization of the Life on Earth database.
These multi-touch interfaces support collaborative and playful interaction with large data sets, which is appropriate for informal learning settings, but limit exploration through designer-defined guided discovery scenarios and visual representations.
G-nome Surfer  and GreenTouch  are tabletop multi-touch applications for collaborative exploration of genomic and phenology databases.
Both applications target college-level inquiry-based learning and support openended data investigation.
However, their expressivity is limited as users cannot define and set query operators directly.
While application domains for search interfaces varied from personal data to large data-
Facet-Streams  is a recent tabletop search interface that explores the design space of tangible-tabletop interaction.
The interface allows users to construct expressive queries using passive tokens that are augmented with content and functionality when placed upon the tabletop surface through the display of a surrounding circular menu.
Similarly to the TQI , Facet-Streams utilizes passive tokens that could only be used on top of a tabletop interactive surface.
While simplifying the construction of complex queries, this system also has some limitations such as physical clutter, separation of query formulation and result browsing, and lack of persistency for query parameters.
Our focus is on the use of active tokens that, when combined with multi-touch and tangible interfaces, could be manipulated on-surface, on-bezel, and in-air.
Such interaction remains unexplored within search interfaces.
Multiple tokens can be combined with a logical AND or negation through vertical stacking.
Interaction with a single stackable token is limited to using its sliders and button.
Stackables provides a step in the direction of using active tokens for search.
Our focus is on identifying possibilities for rich gestural interaction with active tokens combined with multi-touch and tangible interfaces.
We seek to explore gestural interaction that extends beyond the surface into less explored areas such as on-bezel and in-air, hovering above or in front of the surface.
Several tangible interfaces have explored the use of tokens for query formulation.
Navigational Blocks  is an early tangible user interface for querying a database where each block represents one query parameter, its six faces represent the possible values.
Depending on whether two parameter values would yield information, the blocks attract or repel each other, providing users with actuated feedback.
The number of blocks and their fixed values limited the search space to a predefined number of queries.
DataTiles  combines tiles - graspable windows for digital information - on top of a horizontal display.
While DataTiles provides a highly expressive physical language, interaction is constrained to a horizontal surface.
One employed "parameter wheels" for fixed query parameters; the other, "parameter bars" that can be dynamically assigned to represent various parameters.
Such token and constraint interaction, while expressive, makes the tokens less portable and limits possibilities for collaboration.
In later evolutions, cartouches  build on DataTiles and TQI toward generalizing representation of digital information across diverse systems.
Examples include systems of tokens and constraints that could be combined with systems ranging from interactive walls to information kiosks.
We extend this work by investigating the design space of active tokens, which as active physical objects could be manipulated using gestures independently from mechanical constraints.
While most tangible query interfaces utilize horizontal arrangements , Stackables  explores the use of tangible objects that are arranged vertically.
Each stackable token represents a query parameter; multiple tokens are stacked to express a query.
The use of active tokens has been explored in various contexts.
For example, the Tangible Video Editor  employed active tokens to represent video clips.
To guide users' interaction, video clip tokens were embedded in cases that resembled jigsaw puzzle pieces.
Video clips were associated by connecting the puzzle pieces on a surface or inair.
However, while the affordances and physical constraints provided a clear association pattern, they also restricted the use of the active tokens.
Siftables , a precursor of Sifteo Cubes , explored interaction gestures based on reality-based  metaphors such as grouping cubes to tag them with a common element; shaking to express yes or no; and "sugar pack snap" to clear cube contents.
Sifteo  implements various gestures including shaking, flipping, tilting, neighboring, and touching.
However, these gesture are expert-designed and were not evaluated with users.
SixForty by Four-Eighty  is an art installation that uses a set of gestures for manipulating "tile" displays.
SynFlo  is an interactive installation for introducing Synthetic Biology concepts to non-scientists.
It utilizes Sifteo cubes to simulate a wet-lab bench and evoke gestures such as pouring and shaking.
The installation incorporates a Microsoft PixelSense, which serves as a test bed for a virtual experiment.
Though these systems present explorations into the design space of gesture-based interaction with active tokens, they neither tackle the big data context nor evaluate the gesture set in a task-driven scenario.
User-elicitation studies are a specific type of participatory design methodology that involves end-users in the design of gesture-sets .
Such studies have been used to design gesture interfaces of various types including multitouch gestures on small and large surfaces  and multi-modal interactions .
There is evidence that user-defined gesture sets are more complete than those defined solely by experts .
In a user-elicitation study, users are shown referents  and are asked to demonstrate the interactions that result in a given referent .
In this work, we draw upon the user-elicitation methodology to identify user expectations and suggestions for active-token interaction with multi-touch and tangible interfaces for big data explo-
Our intention is not to present a complete userdefined gesture language, but rather to identify and understand user expectations when transitioning from familiar multi-touch interaction into a new space.
We examine when users use on-bezel and in-air interactions, and how users associate active tokens with data displayed on top of multitouch and tangible interfaces.
We identify common gestures for search and query formulation tasks and discuss users' perceptions of the benefits and drawbacks of each.
Participants used the think-aloud protocol.
Video from these sessions was recorded.
Participants also supplied subjective preference ratings.
By using a think-aloud protocol, we obtained rich qualitative data that illuminates users' mental models.
The logged data and questionnaires supplied quantitative measures regarding gesture performance, timing, and user preferences.
The results provide a detailed description of user-defined gestures for the manipulation of large data sets, which combine active physical tokens with multi-touch interaction, and insight into the mental models accompanying them.
Personal genomics engages the study of individual  genomes.
We chose personal genomics as an application area for several reasons.
First, user interfaces for personal genomics could prospectively benefit from the novel HCI techniques we investigate here .
Second, we were motivated by the increasing public interest and media coverage of personal genomics, broadening its relevance to both experts and lay people.
Third, the genes chosen for the study task, BRCA1 and BRCA2, received extensive news coverage reflecting actress Jolie's medical decision around the time of the study.
While personal genomics was chosen as a context for this study, the tasks investigated were selected from taxonomy of search interactions and could be generalized to various contexts.
The chosen application area did not pose observed difficulties to our participants .
Participants typically did not have domain knowledge beyond general familiarity with basic genetic concepts.
Users where asked to perform eight distinct commands for exploring the human genome, with selection based on a taxonomy of search interactions .
Four of the commands were repeated throughout the task.
These included select an item from a list; display information from the cube onto the surface; modify and define a range; associate information to a cube; create a query; zoom in; make a query more specific; replace part of a query, and create a compound query.
Table 2 lists the eight commands for which participants were asked to choose a gesture.
The order of the commands was determined by the task, which mirrors a real-world scenario of genomic investigation.
To avoid restricting users' gesture interactions, we presented users with a prompt asking them to perform a gesture for a particular command .
Only upon confirming the gesture, we displayed the effect of that command.
To further avoid bias , no elements specific to a particular platform  were presented.
We conducted a within-subjects design with 19 users.
The task was identical across both conditions, with the order of conditions counterbalanced to account for learning.
A within-subjects design was employed to solicit gestures for both form factors , and to explore the impact of a particular form factor on user-defined gestures.
We decided to compare horizontal and vertical form factors.
This was motivated by observations from pilot studies where we noticed that cubes tend to stay on the surface, occluding the workspace when a horizontal surface is used.
Our goal was to test for differences between these two form factors.
To elicit task-driven gestures for querying large data sets using an interactive surface with active tokens, we created a casual genome browser application .
The application enables users to search the human genome for a particular gene and find related scientific publications; we prompted users to search for BRCA1 and BRCA2 genes.
The task began with the selection of a chromosome on a cube .
Results were displayed on the surface.
Next, participants zoomed in to a gene of interest, and saved this back to a cube.
The task was then repeated for another gene on another chromosome.
Next, the participant was guided through a series of steps to formulate a publication query by retrieving publications on a particular gene, defining the publication date parameter, adding a Boolean operator and another gene to the query, and saving the results to a cube.
Following the guided query formula-
This task was designed to examine eight commands for the manipulation of large data sets and query formulation.
We selected these commands based on existing taxonomies for search interactions .
Users manipulated both discrete and continuous query parameters.
Table 2 shows the eight commands with corresponding user task description.
Participants were greeted, briefed about the goal of the study, and given a standard consent form.
Users were then introduced to the Sifteo cubes with the official demo video  followed by five minutes of "free-play."
The video provides users with additional possibilities for action beyond gestures users are familiar with based on their experience using phone and tablet interfaces.
Playing with Sifteo cubes prior to the experimental task allowed users to explore gesture possibilities independently of the task context.
Participants were introduced to and walked through the process using a simple demo task.
Following introduction, the interactive surface and active physical tokens displayed still images, serving as a starting point for the task.
Prompts were read from a standard script for consistency.
Users were instructed to think aloud and confirm their gesture for each subtask verbally.
The surface application did not respond in real-time to any touch or gesture, and the Sifteo application provided only basic visual feedback: offsetting the image depending on the tilt, rotating the image when a cube was rotated, and magnifying the image if the screen was clicked.
Upon confirming their gesture, participants were shown two 5-point scales and asked to rate the gesture goodness and ease.
The effect of the gesture would then be presented on both the surface and the Sifteo cubes: a still image illustrating the state of the application following the user's action.
The complete experimental task was comprised of 11 outlined sub-tasks that tested eight query commands.
Upon completion of the experimental task, users were asked to repeat the task on a different condition.
Condition order was counterbalanced to account for learning.
For each session, log and video data were collected.
An observer tracked the gestures generated by participants while a second observer transcribed user comments.
The study was conducted using six Sifteo Cubes that served as active physical tokens.
A first generation Microsoft Surface measuring 30 inches at 1024 by 768 resolution was used as a horizontal surface.
A Microsoft PixelSense measuring 40 inches at 1920 by 1080 resolution was used as a vertical surface.
Our decision to use different platforms  was based on the availability and ease of setup of the devices.
The surface interfaces were resized accordingly for each resolution to maintain a comparable user experience.
We used Sifteo Cubes to accelerate realization of an initial prototype, as early commercial reprogrammable examples of active tokens.
Figure 1 shows the experimental setup.
The Sifteo application gave users some feedback for interaction such as offsetting the image depending on the tilt, rotating the image if two are neighbored, or magnifying the image if the screen is clicked; but no menu structure was presented so as not to affect the users' mental model of information association on the cubes.
It also collected accelerometer data, touch events, and neighboring events for gesture analysis.
The Sifteo application was implemented using Sifteo 1.0 ; the surface application was identical for both conditions.
It displayed still images as feedback for user gestures.
It was built on the Surface 1.0  and Surface 2.0 SDK  and was written in C#.
We used the formula introduced by Wobbrock et al.
We also calculated and compared the means of the goodness and ease scores obtained from user preference questionnaires.
Table 3 shows agreement, goodness, and ease scores for each of the eight commands used in the elicitation study per condition.
We did not find significant differences between conditions in terms of agreement, goodness, and ease of gestures.
Overall, we found that when simple gestures were mapped to simple commands they had more agreement and higher goodness.
In order to better understand the design space of active tokens with multi-touch and tangible interfaces, we manually classified each gesture along three dimensions: space, flow, and cardinality.
While there are numerous gesture taxonomies in the literature, our focus is on characterizing dimensions that are mostly relevant for the manipulation of active physical tokens within multi-touch and tangible interfaces.
Interaction space describes where a gesture was performed: on-surface - upon the interactive workspace, on-bezel - in the area surrounding the interactive surface, or in-air - above or in-front the interactive surface.
Some gestures have a hybrid interaction space; for example initiated in-air and concluded on-surface.
To accomplish more complex tasks, some gestures had combined flow; for example, one hand was sliding along the screen while another was clicking a cube screen.
Cardinality indicates the number of hands and tokens involved in a gesture.
We regard atomic gestures as performed by one hand on a single token, and compound gestures as comprising a sequence of atomic gestures and involving one or two hands interacting with multiple tokens.
Parallel gestures are bimanual and consist of two atomic gestures performed at the same time.
We intend to continue to explore the design space of two-handed interaction with active tokens and multi-touch interfaces.
Figure 3 shows percentage of gestures in each category per dimension.
For example, one popular gesture was to swipe a finger on top of a cube's display, other common gestures included pinching-in and out with two fingers on the face of a cube.
Users also suggested that a keypad or keyboard appear on the cube, as well as various buttons such as `ok' and `save.'
One user described, "Instinctively, I just want to zoom in like that  and it is probably because I have a Mac."
Another user said, "Try to swipe the cube like on an iPad or on a phone."
These observations are aligned with findings from previous studies indicating that users' creativity in suggesting gestures for new form-factors are impacted by their experiences with existing interface technologies .
Reasons include hand fatigue, effort, and persistence.
For example, one user said, "I thought it was easier on the horizontal.
I felt like with the vertical, I couldn't put the cubes down."
Another user mentioned, "I like the table better than the PixelSense.
It felt weird tapping a cube to an upright screen.
You can drag things around and leave them there.
A different user described, "The horizontal surface feels more personal, it's like you own space.
When it moved to the vertical surface, I was more reluctant to use more than one cube at a time because that requires more effort ..." Possibilities for action -The minimal visual feedback  combined with the simple and clean design of the cubes provides only minimal real and perceived affordances.
Several users commented on the difficulty of coming up with new ideas for interaction.
In the words of one user, "It's hard to come up with a way to use this cube"; and in the words of another user, "I really wouldn't use these cubes at all.
I don't find them intuitive."
One explanation to these views is that the form of the Sifteo cubes did not provide users with sufficient clues regarding possibilities for action.
Common Metaphors - the `think aloud' protocol provided us with insight into participants' mental models.
We identified several common metaphors that were suggested by multiple participants.
Several users treated the cubes as physical knobs for information presented on the surface.
For example, one user described: "put  on the BRCA1 gene and then rotate the cube to bring up different data sources..." Some participants used active tokens as tools for indirect touch, such as a pen or a marker; for example: "I will circle 2007 with the drawing edge of the cube."
In this section, we describe the implications of our results for the design of multi-touch and tangible interfaces with active tokens.
While we elicited user-defined gestures for active physical tokens within a particular model world of personal genomic investigation, the design considerations we discuss below could apply to many domains; For example, Twitter data mining, or the exploration of medical records.
This suggests interaction above, in-front, or next-to an interactive surface might profitably be considered by designers along multi-touch interaction.
Designing interactions beyond the surface could help designers to overcome challenges common to data-intense multi-touch and tangible interfaces such as finger size, occlusion, and clutter.
In our study, participants employed in-air and on-bezel gestures for selecting and setting query operators, modifying and defining ranges, and formulating queries.
Continuous interaction - roughly 40% of gestures operating active tokens were continuous.
Among these, hovering above the surface, rotating a cube, and scrolling a cube were all common interactions.
This suggests that users might value immediate response and continuous control over information.
The use of parallel and compound gestures, combined with on-bezel and hybrid interactions  suggests that system designers might also consider using active tokens in control panel-like structures .
Such structures could enable users to gain fine control of the displayed information while avoiding occlusion and fat finger challenges.
Gesture reuse - we found that participants occasionally reused gestures for multiple commands.
For example, the gesture of placing a cube on the surface was suggested for each of the commands; however, the target on the surface was different for each command.
For example, placing a cube on a chromosome start region was suggested to bind modifying functionality to the cube.
Alternately, placing a cube on the gene BRCA1 was suggested to cause the gene to be saved to that cube.
Gesture reuse and consistency are important for increasing learnability and memorability.
Multi-touch gestures often suffer from low memorability, which could be improved using active tokens combined with contextual constraints to disambiguate gestures.
The notion of interpreting an action based on a combination of token and constraints is discussed in the TUI literature  and should be applied when using active tokens with multi-touch and tangible interfaces.
Affordances, metaphors, and constraints - we observed that the design of the cubes combined with the lack of feedback in our study did not provide users with sufficient affordances.
Affordances denote the possibilities for action; inviting and allowing specific actions .
The power of tangible interaction lies in providing both real and perceived affordances .
Variations in size, shape, and material of a token as simple as a cube can affect the ways in which users handle it.
We found that with the lack of sufficient affordance users' expectations were deeply influenced by traditional GUIs and multi-touch interfaces.
For example, some users used the cubes as means for indirect touch.
Users also suggested gestures that draw upon reality-based  metaphors such as naive physics, containers, and knobs.
Constraints , which restrict and provide context to possible actions, work in tandem with affordances .
Our study has several limitations that point toward future work.
First, we aimed to gain insight into users' behavior and preferences without the bias caused by computationally-mediated feedback.
Hence, we removed the dialogue between the user and the system; instead creating a monologue, in which users' behavior is always accepted.
This approach has several weaknesses; such as the absence of guiding cues, which makes it difficult for users to imagine possible gestures, and the lack of opportunities for users to reflect back on their gestures and change the interactions they proposed.
In the words of one user: "It was really weird trying to build something from the ground up because I'm more used to relying on the feedback of whatever I'm interacting with to know what my next move is."
Also, the use of active physical tokens offers opportunities for supporting collaborative work with big data around an interactive surface or between distant surfaces; however, in this study we did not investigate a collaborative scenario.
Rather, we focused on the engagement of a single user.
It is possible that a collaborative scenario would highlight additional classes of gestures - e.g.
These issues are worthy of investigation, but are beyond the scope of the current work.
In the future, we intend to draw on our findings to design, implement, and evaluate a multi-touch and tangible interface for querying genomic data.
In this paper, we conducted a study with 19 users, which investigated gestural interaction with active-tokens.
We intend to apply our findings in the design of tangible interface for the exploration of large genomic data sets.
Asymmetric division of labor in human skilled bimanual action: The kinematic chain as a model.
Visualizing biodiversity with voronoi treemaps.
Collaboration and interference: awareness with mice or touch input.
Data visualization on interactive surfaces: A research agenda.
Realitybased interaction: a framework for post-WIMP interfaces.
Materializing the query with facetstreams: a hybrid surface for collaborative search on tabletops.
Blended Interaction: understanding natural human-computer interaction in post-WIMP interactive spaces.
Stackables: combining tangibles for faceted browsing.
PaperPhone: Understanding the Use of Bend Gestures in Mobile Devices with Flexible Electronic Paper Displays.
When the fingers do the talking: A study of group participation with varying constraints to a tabletop interface.
Siftables: towards sensor network user interfaces.
Analysis of natural gestures for controlling robot teams on multi-touch tabletop surfaces.
Web on the wall: insights from a multimodal interaction elicitation study.
