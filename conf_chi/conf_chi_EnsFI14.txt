As wearable computing goes mainstream, we must improve the state of interface design to keep users productive with natural-feeling interactions.
We present the Personal Cockpit, a solution for mobile multitasking on head-worn displays.
We appropriate empty space around the user to situate virtual windows for use with direct input.
Through a design-space exploration, we run a series of user studies to fine-tune our layout of the Personal Cockpit.
In our final evaluation, we compare our design against two baseline interfaces for switching between everyday mobile applications.
This comparison highlights the deficiencies of current view-fixed displays, as the Personal Cockpit provides a 40% improvement in application switching time.
We demonstrate of several useful implementations and a discussion of important problems for future implementation of our design on current and near-future wearable devices.
Unlike their handheld counterparts, HWDs need not be limited by physical display constraints.
Instead, designers can leverage the 3D capabilities of such devices to appropriate the abundance of space around the display wearer.
Multiple virtual windows can appear to `float' in surrounding space, remaining easily accessible, but without unwanted occlusion.
As in real-world multi-monitor environments , we can use virtual windows to partition information by task and by users' current needs.
For instance, an on-the-go multitasker might place a map below his line of sight, looking down to consult it only as directions are required.
Later, while waiting for the bus, he may place his calendar and a real-time bus schedule sideby-side, viewing each at his leisure with a turn of the head.
We explore the design space for such a mobile and userconfigurable arrangement of multiple, floating displays we call the Personal Cockpit .
The recent proliferation of lightweight, low-cost, transparent head-worn displays  makes it possible for users to view and interact with information content at all times.
However, the practical scope of these current interfaces is narrow due in part to limitations of display configuration.
Content is fixed to a single view location, restricted to the periphery, or occluding the wearer's view of his surroundings .
These conditions will also inhibit task switching, as is the case with mobile devices, where nearly 30% of tasks involve multiple applications  and the costs of switching are severe .
B. Ens, R. Finnegan and P. Irani.
The Personal Cockpit: A Spatial Interface for Effective Task Switching on Head-Worn Displays.
In CHI '14: Proceedings of the SIGCHI Conference on Human Factors and Computing Systems , 10 pages, to appear, ACM, 2014.
This is the author's versio n of the work.
It is posted here by permission of ACM for your personal use.
The Personal Cockpit  leverages an empiricallydetermined spatial layout of virtual windows.
We investigate its design space, including field of view constraints of wearable displays.
Our design is a shift from current interfaces , in which content remains fixed in the user's forward view.
Our work is inspired by past designs for HWDs, such as Feiner's implementation  of world- and body-fixed virtual windows.
Subsequent studies  have indicated that leveraging users' perception of surrounding space may provide advantages over virtual navigation techniques.
Our research builds on this prior work by exploring in depth a HWD interface that provides the benefits of proprioception and spatial memory.
We craft a 2D layout customized to utilize head-motion with the constraints of a HWD's limited field of view .
To fully exploit the potential of a spatial layout, we tune our design for use with direct input, akin to a personalized arrangement of floating `touchscreens'.
Our study shows users can interact with the Personal Cockpit more effectively than with existing HWD navigation methods using view-fixed displays; when switching between a set of everyday applications, participants completed an analytic task 40% faster.
Our work is the first to apply such a rigorous design approach to body-centric HWD interfaces.
Our thorough design space exploration provides an example for future research on interfaces for varying hardware configurations and lays several steps toward a user-configurable, multiwindow management system.
Spatial constancy in multi-window layouts can improve memorability and reduce switching time .
We explore the design space for an interface ideally suited for multi-tasking on HWDs, the Personal Cockpit.
Our work is inspired by a number of interfaces that leverage spatial memory to bridge the gap between real and digital worlds.
Much of this work can be traced back to Fitzmaurice's information spaces , which map information to its associated physical locations in the real world.
Feiner  later implemented a HWD interface with virtual windows mapped to world- and body-based reference frames.
In Billinghurst's following work , we see the potential of head-tracking for improving interaction with multiple displays.
Many similar world- and body-centric concepts followed on other platforms such as spatially aware mobiles  and projectors .
We build on these prior works by pinpointing relevant design issues that we use to guide our design process.
Given that the Personal Cockpit requires head movement, we consider the effects of angular separation between the multiple displays.
The range of human neck motion for a normal adult is relatively large: about 85 for rotation to either side, 50 for vertical flexion  and 6070 for vertical extension  .
However, the effective range for task switching is smaller.
For example, Su and Baily  found that two displays on the same vertical plane can be displaced by up to 45 before negative effects on a docking task.
Display size can influence task performance, although the effects are dependent on viewing distance.
When viewing distance is held constant, we refer to display size as angular width.
Ball and North  argue that the affordance of physical navigation has a greater effect on task performance than display size.
Similarly, physical motion could prove advantageous for multitasking.
We are interested in how FoV limitations impact the Personal Cockpit.
The human visual field spans about 200 horizontally and 130 vertically, however the detailoriented foveal region of the eye spans only about 3 .
A wide FoV contributes to a user's sense of `presence' in a virtual environment  and a limited FoV is known to hamper tasks relying on the user's peripheral view , such as navigation .
Due to limitations of technology, weight and cost, the display field of existing HWDs does not cover the entire human range.
The impact of FoV on performance is gender dependent .
For tasks relying mainly on the foveal region, a 40 width may suffice .
As our design of the Personal Cockpit includes direct user input, window distance is a primary design factor.
For virtual displays, the impacts of depth are numerous.
The minimum comfortable distance of binocular convergence is about 0.25 m , although ergonomics research recommends placing desktop monitors at a distance of at least one metre .
Tan and Czerwinski  found that performance is negatively impacted by mixed display distances.
Thus our Personal Cockpit design should keep the working set of windows at a single depth.
Estimation of depth is known to be impaired in virtual environments , due in part to FoV restrictions .
A well-understood phenomenon and cause of simulator sickness is vergence-accommodation mismatch.
This effect occurs when the proprioceptive cues of focus and vergence become decoupled in stereoscopic environments .
One further design consideration on HWDs with limited FoV is binocular overlap.
As illustrated in Figure 2, the viewing frusta of both eyes typically overlap exactly at the distance of the display's virtual image plane.
A device can be designed to allow a wider FoV by only partially overlapping the frusta.
This choice comes at a trade-off in performance  due to monocular regions on the sides of the viewing region.
Binocular overlap is also reduced when a large virtual object appears wider than the available viewing region.
For example, the lower window in Figure 2 is cropped to a different region for each eye.
One particular item of interest we explore is how the distance of a virtual display affects the interpretation of its contents.
Because HWDs are easily portable, we explore the impact of different reference frames on direct input with the Personal Cockpit.
Their study shows that performance with search and route tracing is 30% faster on the curved layout.
This result may suggest that task switching is more efficient on a curved layout, which is well suited for reaching with an extended arm.
Accordingly, we use a curved layout for the Personal Cockpit.
We refine our design of the Personal Cockpit as an advanced interface for multi-tasking on HWDs through four user studies.
In the first 3 studies we fine-tune the design parameters  of display size , distance, reference frame and angular separation.
In the last study we compare the Personal Cockpit against standard methods for task switching on view-fixed HWD interfaces.
Whereas the direct manipulation metaphor allows intuitive interaction with virtual objects , our Personal Cockpit design must take into account several issues inherent to `touching the void': Depth perception of virtual objects is difficult and the depth estimation of a virtual surface is made more problematic by the lack of a tangible surface .
Furthermore, when distance is overestimated, the user's penetration of the surface can cause double vision, or diplopia .
Also, interactive objects must remain within average maximum reach, about 50-60 cm to the front and 70-80 cm to the dominant side .
HWDs present additional challenges for direct input.
In a wearable system, head-tracking and registration relies on body-fixed sensors.
Thus, robust tracking and motion stabilization are required to create a convincing illusion of spatially situated objects.
Also, since the display is located physically between the viewer and the locations of situated objects, a virtual display will occlude the user's hand as it reaches the surface.
To make direct input feel natural, the system should detect the reaching hand and make it appear to occlude the virtual display.
We circumvent these issues in our studies by emulating a HWD in a CAVE setting.
As we focus on human factors limitations in our design, we run our studies in a projection-based CAVE environment.
The low display latency and high-precision optical tracking enable us to explore designs not practical on today's hardware, to control for confounding background clutter and to examine previously untested design factors.
We emulate the restricted FoV of a HWD by clipping the viewing frusta of users to 40 x 30 .
As with actual see-through HWDs, the FoV restriction only affects virtual content; the real world view remains unobstructed.
To facilitate direct input, we explore visual output within reach of the user.
We emulate an image plane distance  of 1m, about the expected limit for use with direct input .
As with FoV, this choice serves as a worst-case setting in which we evaluate the human-factors aspects of our design.
Our environment does not take all possible issues into account, for example vergence-accommodation mismatch  or the problem of unwanted hand occlusion .
However it allows us to examine issues related to FoV restriction such as the effects of binocular overlap and the efficiency of navigating to displays that are hidden out of view.
Within each condition, participants complete ten trials consecutively.
To measure performance we record trial time and the number of incorrect selections.
We collected 5 angular widths x 4 distances x 10 trials x 10 participants = 2000 data points.
After each set of ten trials, participants provided perceived effort  by answering the question "How hard did you have to work  to accomplish your level of performance?"
We analyzed data of recorded trial times and subjective scores of overall effort.
In this study and those that follow, we remove outliers greater than 2SD from the mean.
Trial Time: We removed the first trial from each set  to eliminate learning effects.
We ran the univariate ANOVA for our analyses.
Mean times for angular width and distance are shown in Figure 4.
Post-hoc comparisons with Bonferroni corrections show significant differences between all angular width conditions  except for 50 vs. 75 % .
Our first study explores size and distance placement for a virtual display.
These values depend on the FoV and distance limitations for direct input.
Displays that appear wider than the FoV width are not fully visible from a single head position and may be difficult to interpret due to a reduction of the binocular overlap region .
We expect participants will be more efficient when the virtual display's angular width is equal to or less than the field of view.
We recruited 10 university students  from our local campus.
We screened participants using a binocular depth test, which required them to differentiate between virtual displays placed at close , intermediate  and far  distances.
As a result of this test, we had to turn away 2 participants.
We implemented a visual search task to examine the effects of display width and distance.
We use a conjunction search , in which the target and distracter objects share multiple properties.
In our case, objects can share the same shape  or colour .
The display is partitioned by a vertical line, with a target object appearing on left .
The participant must search an array of randomly generated objects on the right side of the line and count the number with the same shape and colour as the target object.
Participants report their count by pressing one of four buttons on a handheld wand device.
Virtual displays appear directly in front of the participant, centred at eye-height.
Participants are asked to complete the task as quickly and as accurately as possible.
Effort: Participants provided scores after each condition for their perceived level of overall effort.
We ran Friedman's ANOVA tests for each factor followed by post-hoc Wilcoxon tests.
Mean scores are shown in Figure 4.
We find that task time is directly influenced by the ratio of the display width to FoV.
Task time is optimal when the virtual display is roughly 3/4 the size of the FoV, likely due to reduced head motion.
We see a small change from 100 to 75%, but no improvement with the smaller 50% ratio.
Interestingly, perceived effort scores, in response to display width, follow an identical pattern to task time.
We find that participants perceive increased discomfort at the nearest display distance , but task performance is unaffected by distance.
This result leaves open the possibility for direct input, as this latter factor is limited by the user's reach.
Whereas Study 1 focused on visual output, Study 2 explores direct input.
Our first goal is to determine which display distances best facilitate target selection.
Our second goal is to see how the choice of spatial reference frame affects input that relies on reaching.
In combination with Study 1, we can determine the ideal balance of design parameters to support both output and input.
We expect that participants will benefit from proprioception with body-fixed or viewfixed windows, leading to lower targeting error.
From a resting position, participants were asked to quickly and accurately `touch' the centre of a 10 cm diameter bullseye target with their right hand .
The target is placed at one of 5 locations on a virtual window.
Based on the outcome of the previous study, we chose a window width smaller than the FoV .
The target provided colour feedback to indicate correct or incorrect  selections.
Participants began the next trial by returning their hand to the resting position.
Input detection is provided by a Vicon tracking system.
Fatigue: Participants rated fatigue on a 12-point Borg scale.
As the Borg CR10  scale was designed to be a linear mapping between perceived and actual intensity, we treat the resulting scores as scalar, using a univariate ANOVA.
We used a 3x3 within-participants design.
The factors are: spatial reference frame ; distance of the display  and target location .
Body-fixed and view-fixed displays appeared at a set distance from the participant's body, as determined by the distance condition.
World-fixed displays are initially set at the same distance, but are fixed to world-coordinates and do not move with the user.
Distance and reference frame are presented in a random order to reduce learning effects.
Within each condition, participants complete 5 blocks of trials.
Within each block there is 1 trial at each location, presented in random order.
To measure performance we record trial time and target selection error.
Participants provide ratings of perceived fatigue for each combination distance and reference frame by answering the question "What was the level of fatigue from this task?"
We collected 3 distances x 3 reference frames x 5 target locations x 5 trials x 12 participants = 2700 data points.
We were surprised to find that target selection is clearly more precise in the world-fixed reference frame.
Any benefits of proprioception in the other two reference frames were overshadowed by unintentional motion of the target window caused by the pointing motion.
Although distance did not influence pointing speed, there was an unexpected effect of distance on pointing error.
This effect was strongest in the body-fixed frame, i.e.
Error was greatest at 60 cm, where participants' arms were near full extension.
Precision was particularly bad in the top and left target locations, which required a slightly greater  reach.
We analyzed task completion time, pointing error and subjective ratings of fatigue.
We found no effects of time.
Pointing Error: We define error as the distance between the detected selection and the target centre.
For error analysis, we included all correctly completed trials.
We compared error distances using a 3 x 3 x 5 univariate ANOVA.
Having refined the distance parameter for direct input and visual output, we now investigate layouts of multiple windows, with target selection between two windows.
Study 1 showed the best task performance when the window fits fully within view.
Multiple tasks, however, are likely to occupy separate windows that span beyond the user's FoV.
The ideal placement range is limited by human factors including the range of neck motion for a typical user and performance of direct input.
As study 2 showed negative effects on pointing error from even subtle body motions, we use a world-fixed frame for optimal input.
However, a curved layout has a natural focal point.
To determine if windows are best centred directly around the user, or offset to the side of the dominant pointing arm, we include focal point as a study factor.
The centre focal point is symmetrical to the participant whereas the right focal point coincides with the right shoulder.
All windows are placed at an equal distance  from the current point of focus.
Multiple windows are offset radially by a given separation angle .
We recruited 8 university students  from our local campus.
Participants are presented with a two small windows .
One window contains a start button and is placed at shoulder height directly in front of the focal point .
The second window contains a bullseye target, and is displaced either horizontally or vertically from the start window.
The participant begins by `touching' the start button, then moves quickly and accurately to the target.
We use a 4x5x2 within-participants design.
For each focus, participants complete 10 consecutive blocks of trials, where 1 block contains all combinations of direction and angle.
Trials in a block are presented in random order to prevent learning effects.
The order of focus presentation is balanced between participants.
We collected 4 directions x 5 displacement angles x 2 points of symmetry x 10 trials x 8 participants = 3200 total trials.
Trial Time: Time is measured from the moment the start button is tapped until a selection is detected.
We removed the first trial from each condition  to reduce learning effects.
Mean trial times are shown in Figure 6.
Post-hoc tests with Bonferroni corrections showed significant differences between all pairs of angles and directions  except for up vs. left .
Pointing Error: As in study 2, error is the distance between the detected selection and the target centre.
Mean values are shown in Figure 6.
Time and error are both higher for targets in the down direction than for up .
Despite this finding, several participants preferred the down direction to up, as it reduced arm fatigue .
Pointing time generally increases with angle, as expected, due to increased travel distance.
However, there is a steep increase in around the 35 mark, when the start button and target both fit barely within view.
Although focus doesn't affect pointing time, there is a significant reduction in error when the centre of curvature is shifted to align with the right shoulder.
As a result of this finding, we explored various options for rightoffset layouts before implementing the final study.
We envision the Personal Cockpit as a versatile, configurable window manager that will be useful for many scenarios, including on-the-go multitasking.
However, since study 2 showed that body-fixed windows are prone to target error, we use a world-fixed reference frame for our study.
To keep windows within easy reach of the user, we chose a curved layout for the Personal Cockpit .
Using the best input/output distance from studies 1 and 2, and the right-offset from study 3, we place each windows 50 cm from the user's right shoulder.
To keep a 4x4 array within head range , we use a separation angle of 27.5.
To prevent window overlap, we reduce their width to 22 cm .
Once the window position is determined, we set each window's orientation to face the user's point of view.
Finally, based on results from study 1, we correct the window viewing distances.
Since the rightshoulder focus causes some of the windows on the user's left to be placed uncomfortably close, we displace windows along the line of sight so each is a minimum of 50 cm viewing distance .
The applications are randomly placed among empty desktop windows within an array of either 9 or 16 windows.
The windows are laid out in space according to our Personal Cockpit design and the user switches applications by moving his head .
We recruited 12 university students  from a local campus.
Participants are presented with a set of windows showing everyday applications, representing ones that might be used on a real HWD.
The goal is to scan the windows for information needed to answer a question .
The windows present all of the information required to select the correct answer, thus the participant must navigate between windows, but need not pan or scroll within the applications themselves.
An example task goes as follows: the participant begins a trial by pressing the button on the Start window, triggering the appearance of icons on the Question and Map windows.
The participant navigates to the Question window to find out who he is meeting.
Next, he finds the message next to that person in the Messages window.
It looks like he is meeting for pizza, so he navigates to the Map window to locate the pizza icon marked with the letter `a'.
Finally, he returns to the Question screen to select the correct answer, `a', ending the trial.
In addition to our Personal Cockpit design, participants must navigate using two baseline techniques with viewfixed displays: one with direct input and the other with indirect input .
In these techniques, the same application windows are arranged in a flat array, but the participant can only see those that fit within the 40 FoV.
With the direct input technique, the user switches applications by panning the intangible, view-fixed surface .
This technique is analogous to panning on a typical smartphone.
To assist direct input, we provide visual feedback to indicate whether the reaching finger is above, on, or behind the window surface.
Based on previous work showing difficulties with depth judgement  and pilot testing, we provide a substantial `surface' depth of 8cm.
The indirect technique uses a wireless trackpad, with which participants control a cursor on the view-fixed display .
To switch applications, the participant must select a home icon at the bottom of the display, which leads to an overview of the entire array .
From the overview, he can select any window in the array, which brings the corresponding window into full view.
For consistency, all application windows are sized to 22 cm width and placed at 50cm viewing distance for both view-fixed techniques.
We use a 3 x 2 x 2 within-participants design: technique ; complexity  and question type .
Within each technique, participants completed 4 sets of questions, 1 for each combination of complexity and question type.
For each new set, applications were moved to new random window locations, but with a minimum of one application for each row and column in the layout array.
Each set of 4 questions was completed using the same window layout.
Techniques and complexities were fully balanced between participants.
Type I questions always preceded type II.
Several participants commented that the Personal Cockpit was "easy to navigate".
One participant said, "I liked the speed of navigation - I was able to move around quickly and in such way it reduced the amount of work."
Others mentioned that it was "productive" and "the most natural".
The Personal Cockpit is also scalable.
Whereas the panning technique  shows a large increase in time with a greater number of application windows , the Personal Cockpit shows only a small increase, as with the indirect method .
Despite the use of direct input and necessity of head motion, participants rated the Personal Cockpit on par with the indirect interaction technique .
Our results are positive but come with some limitations.
We tested only 2 baseline techniques.
Further study with additional tasks is required for generalization, however our results are in line with those of prior research .
Further studies with actual HWD hardware are required for ecological validity.
Window Overview: Although the Personal Cockpit user can access many applications quickly and easily, there may be times when an overview  of all open windows is useful.
With a command gesture, the user can shrink the Cockpit layout into a palm-sized sphere , which fits easily into view.
Attached to the non-dominant hand, the user can manipulate the sphere for convenient viewing.
Changing Frames of Reference: The Personal Cockpit is as mobile as the HWD device and can be designed to follow the user on the go with a body-fixed reference frame.
When at work or at home, the Cockpit windows can be fixed to a wall or other available space.
In this demo, a user can switch between a flat, world-fixed layout and a curved, body-fixed layout with a tap on the HWD .
Manual Arrangement: Our Cockpit design in Study 4 demonstrates a customized automatic layout.
Depending on the situation, the user may want to rearrange the windows manually.
In this demonstration, the user can grab, move and resize windows at his leisure using in-air pinching gestures.
To open a new application window, the user grabs an icon from a task-launcher window and places it in an empty location, where a new window springs into view .
A hand-fixed reference frame is convenient for bimanual interaction techniques.
Window Intercommunication: In multitasking situations, two or more windows may be tied to the same task.
For instance, many tasks can have peripheral information or tool palettes in a secondary display.
We demonstrate a colour-picker tool, in which the user can select a colour to tint a photo in a separate application window .
Transfer to Wearable Technology: Our emulation of the FoV limitation in a CAVE environment provided us with several advantages in implementation and tracking.
Further research is required to discover the limitations of applying a functional Personal Cockpit interface on current and nearfuture hardware with variations such as different image plane distances.
We also must answer questions about the effectiveness of transparent displays in real situations, such as with objects moving in the background or while walking.
Our next step is to demonstrate that Personal Cockpit's design advantages transfer to a real-world HWD.
As display and tracking technologies advance, systems will be able to support fully-mobile implementations.
We outline some important challenges for this realization.
Body-Fixed Stabilization: In Study 2, we found that naively fixing windows to body-fixed coordinates leads to selection inaccuracies with slight body motions.
Based on this finding, we envision a hybrid between world- and bodyfixed reference frames for mobile use.
When the user is standing still, the layout becomes purely world-fixed.
When he begins moving again, the system detects this and brings the Cockpit along.
Other approaches include using a lowpass filer to smooth and stabilize motion.
Pseudo-Occlusion: An important problem we discussed earlier  is that a HWD lies between the viewer and the input space.
This causes the display image to occlude any outside objects, including the user's hands.
We propose the concept of pseudo-occlusion to solve this.
The system would accurately track the hands' position in nearby space.
When the hand is placed between the HWD and a virtual window,
Windows can be placed as close as 50cm, even with a 1m distant virtual image plane.
Greater window offset angles, and thus greater window numbers, lead to increased head motion.
This can lead to longer task switching times.
We have explored the design space of the Personal Cockpit, a design concept for fast task switching on head-worn displays.
We refined our design based on the outcomes of 3 user studies and tested our final design against techniques using direct and indirect interaction with view-fixed displays.
We work towards a window management system for HMDs by demonstrating a set of interactions using the Personal Cockpit.
We lay out several challenges that must be addressed in translating our design to a wearable device.
In future, we plan to move forward by implementing a wearable prototype system.
We will also continue exploring techniques for multi-window management  as well as features for enabling direct input, by better understanding limitations of perception  and user fatigue .
