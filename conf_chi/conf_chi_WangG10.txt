Communication is more effective and persuasive when participants establish rapport.
Tickle-Degnen and Rosenthal  argue rapport arises when participants exhibit mutual attentiveness, positivity and coordination.
In this paper, we investigate how these factors relate to perceptions of rapport when users interact via avatars in virtual worlds.
In this study, participants told a story to what they believed was the avatar of another participant.
In fact, the avatar was a computer program that systematically manipulated levels of attentiveness, positivity and coordination.
In contrast to Tickel-Degnen and Rosenthal's findings, high-levels of mutual attentiveness alone can dramatically lower perceptions of rapport in avatar communication.
Indeed, an agent that attempted to maximize mutual attention performed as poorly as an agent that was designed to convey boredom.
Adding positivity and coordination to mutual attentiveness, on the other hand, greatly improved rapport.
This work unveils the dependencies between components of rapport and informs the design of agents and avatars in computer mediated communication.
Recent research in virtual environments has demonstrated the possibility of translating these findings into computer-mediated  and humancomputer interactions  where embodied communicated behaviors can not only be reproduced but altered in novel ways to perhaps amplify their interpersonal consequences  .
Tickle Degnan and Rosenthal  define rapport as a subjective feeling of connectedness and argue that it arises in face-to-face interaction from the expression of these essential components of nonverbal behavior: mutual attention, positivity and coordination.
The positivity correlates of rapport are behaviors, such as smiling and head nodding, that indicate participant liking and approval of one another.
The coordination correlates, on the other hand, are behaviors that signal that the participants are "with" one another, functioning as a coordinated unit, such as postural mirroring and interactional synchrony.
Forward lean and orienting body towards one another are behaviors indicate mutual attention.
However, one of the most important indicators of mutual attention is gaze.
As we grew up, we were taught by our parents to "look someone in the eye" when we speak.
During initial interaction, mutual gaze signals interest, a precondition to the continuation of the interaction.
Later, gaze signals the unity of the dyad members, both in terms of the unity of their experience and the mutuality of their relationship goals.
Although rapport was developed to explain properties of face-to-face interaction, recent work has examined TickleDegnan and Rosenthal's theoretical framework  within the context of CMC  and HCI .
Indeed, rapport bears close similarity to the CMC notion of social presence  and subjective measures of rapport index many of the same concepts as social presence scales.
In our own work, we have examined how the nonverbal behaviors of virtual characters can influence subjective and behavioral correlates of rapport in interactions within virtual worlds .
This work emphasized the positivity and coordination components of the model.
In this paper, we examine the impact of mutual attention component by manipulating the gaze behavior of a virtual agent.
This is part of a large scale in-
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The results of this article not only deepen understanding of the role of rapport in interactions in the virtual environments but also raise important cautions in directly applying theories of face-to-face communication to computer-based interaction.
Other studies also show that people high in traitanxiety are more engaged speaking with the Rapport Agent than they are speaking with a stranger face-to-face .
A more recent study showed that people who are more agreeable established more rapport with the Rapport Agent and suffered less speech disfluency .
Eye contact is an invitation to communicate.
It clearly signals a person's "availability" for communication and usually produces positive perceptions in receivers.
Goldberg, Kiesler, and Collins  found that people who spent more time gazing at an interviewer received higher socioemotional evaluations.
Increased eye contact was also associated with greater perceived dynamism, likability, and believability .
Gaze also serves to facilitate the learning process and enhance task performance.
During instruction, gaze helps learning, in that college students had higher performance on a learning task when the instructor gazed at them than when the instructor did not   also showed that students learned better when looked at more by a virtual teacher.
Mutlu  found that increased gaze from a storytelling robot facilitated greater recall of story events.
The amount of eye contact in a human-human encounter varies widely.
Argyle  found that in dyadic conversations, the listener spent an average of about 75% of the time gazing at the speaker.
Kendon  reported that a typical pattern of interaction when two people converse with each other consists of the listener maintaining fairly long gazes at the speaker, interrupted only by short glances away.
Evaluations of the effects of gaze on the quality of interactions in mediated conversation have shown that improving the gaze behavior of agents or avatars in human-agent or human-avatar communication has noticeable effects on the way communication proceeds     .
However, most of the evaluation of gaze in mediated communication had been with human-human conversations in video-conferencing and not, to any great extent, with conversations between human and autonomous embodied conversational agents .
In this paper, we continue our investigation of mutual attention using the virtual Rapport Agent.
The Rapport Agent was designed to establish a sense of rapport with a human participant in "face-to-face monologs" where a human participant tells a story to a silent but attentive listener.
In such settings, human listeners can indicate rapport through a variety of nonverbal signals 
The Rapport Agent attempts to replicate these behaviors through a real-time analysis of the speaker's voice, head motion, and body posture, providing rapid nonverbal feedback.
Creation of the system is inspired by findings that feelings of rapport are correlated with simple contingent behaviors between speaker and listener, including behavioral mimicry  and back-channeling, e.g., nods .
Rapport Agent uses a vision based tracking system and signal processing of the speech signal to detect features of the speaker and then uses a set of reactive rules to drive the listening mapping displayed in Figure 1.
The architecture of the system is also displayed in Figure 1.
To produce listening behaviors, the Rapport Agent first collects and analyzes the speaker's upper-body movements and voice.
For detecting features from the participants' movements, we focus on the speaker's head movements.
Watson  uses stereo video to track the participants' head position and orientation and incorporates learned motion classifiers that detect head nods and shakes from a vector of head velocities.
Other features are derived from the tracking data.
Thus, we detect head gestures , posture shifts  and gaze direction.
Acoustic features are derived from properties of the pitch and intensity of the speech signal, using a signal processing package, LAUN, developed by Mathieu Morales.
Speaker pitch is approximated with the cepstrum of the speech signal  and processed every 20ms.
Audio artifacts introduced by the motion of the Speaker's head are minimized by filtering out low frequency noise.
Speech intensity is derived from amplitude of the signal.
LAUN detects speech intensity , range , and backchannel opportunity points .
Recognized speaker features are mapped into listening animations through a set of authorable mapping language.
This language supports several advanced features.
Authors can specify contextual constraints on listening behavior, for example, triggering different behaviors depending on the state of the speaker , the state of the agent , or other arbitrary features .
One can also specify temporal constraints on listening behavior: For example, one can constrain the number of behaviors produced within some interval of time.
Finally, the author can specify variability in behavioral responses through a probability distribution of different animated responses.
These animation commands are passed to the SmartBody animation system  using a standardized API .
SmartBody is designed to seamlessly blend animations and procedural behaviors, particularly conversational behavior.
These animations are rendered in the Unreal TournamentTM game engine and displayed to the Speaker.
The study adapts a common paradigm used for studying the impact of listener behavior on speech production  .
In this "quasi-monolog" elicitation, one participant, the Speaker, has previously observed some incident, and describes it to another participant, the Listener.
In this study, the Listener corresponds to some experimental manipulation of the Rapport Agent.
It should be noted that restricting the study to quasimonologs potentially limits the generality of our results but we adopt this paradigm for several reasons.
First, it allows us to assess the effectiveness of our Rapport Agent which was designed to give feedback in such storytelling contexts.
Although limited, several potential applications of virtual human technology correspond to quasi-monologs including survey interviewing , story elicitation or psychotherapeutic applications.
Second, free natural language dialogue is beyond the capability of current dialogue systems but we wanted to avoid the common use of confederates or "wizard of Oz" designs.
Such designs should be avoided as the confederate/wizard can unconsciously recognize the experimental manipulation and introduces biases into their own behavior .
Following the standard setup adopted by McNeill , we designed the study as having a human participant watch a short cartoon and then describe it to a listening agent.
We designed three different virtual agents to play the listener role for the study.
Behaviors of these three virtual agents are listed in Table 1.
Based on the literature review, we hypothesize that: H1: Self-reported rapport will be the highest when the Rapport Agent provides feedback that reflects all three components of rapport.
When only mutual attention is expressed, the level of rapport level would be lower.
When none of the three components is included in the feedback, rapport would be the lowest.
H2: Human Speaker will speak most fluently when the Rapport Agent feedback reflected all three components of rapport.
The speaker will speak less fluently when only mutual attention is expressed by the Rapport agent.
The speaker will suffer the most speech disfluency when none of the three components is provided in the agent feedback.
The first virtual agent is a "good virtual listener" .
The agent exhibits attentive listening behaviors including head nods and posture mimicking that have previously been demonstrated to create self-reported feelings of rapport .
Agent's posture mimicking includes posture shifts  and head nods.
Between these attentive listening behaviors, the agent does idle-time behaviors including blinking and random posture shifts.
This agent gazes continuously at the speaker except when he is blinking and nodding.
The second virtual agent, a "gaze only listener" , gazes continuously at the speaker 100% of the time and exhibit random idle-time behaviors.
This agent's gaze behavior is shown at the bottom of Figure 2.
The study design was a between-subjects experiment with three conditions: Responsive , Staring , and Ignoring , to which participants were randomly assigned.
Speaker was told that the virtual agent on the screen represents the human listener.
The size of the agent is approximately the same size of the human listener sitting 8 feet away.
While the speaker spoke, the listener could see a real time video image of the speaker retelling the story displayed on the TV.
Next, the experimenter led the speaker to a separate side room.
The speaker completed a questionnaire about the contents of the video he/she saw before interacting with the virtual agent.
During this time, the listener  remained in the computer room and pretended to speak to the camera what he/she had been told by the speaker so that the participant would not suspect that the listener is a confederate.
Later, the speaker was led back to the computer room and watched remaining of the two videos.
The speaker then retold the stories portrayed in the clips to the listener.
After that, the speaker filled out another questionnaire about the contents of the video while the listener  remained in the computer room and spoke to the camera what he/she had been told by the speaker.
Then the speaker completed the post-questionnaire.
Finally, participants were debriefed individually.
No participants indicated that they believed the listener was a confederate in the study.
The participant first signed the consent form and completed the pre-questionnaire.
Then the participant was assigned the role of the speaker and the confederate was assigned to the role of the listener.
Next, the speaker was led to the computer room while the listener waited in a separate side room.
The speaker viewed one of two videos.
One of the videos was a Tweety and Sylvester cartoon.
The other video is taken from the Edge Training Systems, Inc.
The video clip, "CyberStalker," is about a woman at work who receives unwanted instant messages from a colleague at work.
Which one of the videos was shown was randomly decided.
After the speaker finished viewing the video, the listener was led back into the computer room, where the speaker was instructed to retell the stories portrayed in the clips to the listener.
The speaker was also told that the listener will later retell the story to the camera.
Speakers sat in front of a computer monitor and sat approximately 8 feet apart from the listener, who sat in front of a TV.
They could not see each other, being separated by a screen.
The speaker saw the virtual agent displayed on the computer monitor.
Two Videre Design Small Vision System stereo cameras were placed in front of the speaker and listener to capture their movements.
Three Panasonic PV-GS180 camcorders were used to videotape the experiment: one was placed in front the speaker, one in front of the listener, and one was attached to the ceiling to record both speaker and listener.
The camcorder that was in front of the speaker was connected to the computer monitor in front of the listener, in order to display video images of the speaker to the listener.
Four DELL desktop computers were used in the experiment.
The animated agent was displayed on a 30-inch Apple display to approximate the size of a real life listener sitting 8 feet away.
The video of the speaker was displayed on a 30-inch TV to the listener.
We constructed a 10-item rapport scale , presented to speakers in the post-questionnaire packet.
This scale was measured with an 8 point metric .
Sample items include: "I think the listener and I established a rapport" and "I felt I was able to engage the listener with my story."
For listener focus and distraction scale, we constructed 2 items for each scale, with Cronbach's alpha coefficient of .70 and .71, respectively.
We also constructed a 6-item agent naturalness scale, with Cronbach's alpha coefficient of .94.
All the scales were measured with an 8 point metric .
From Table 3 we can see that, Hypothesis 1 is partially supported.
Participants interacted with the Responsive agent reported significantly higher level of rapport than those interacted with the Staring and Ignoring agent.
However, there was no significant difference between Staring and Ignoring condition.
In addition to the scales listed above, the post-questionnaire packet also contained questions to examine speaker embarrassment, speaker's goals while explaining the video and listener traits.
Listener's traits were measured using items such as "likeable", "tense" and "trustworthy" taken from the dependent measure from Krumhuber study .
Other scales ranged from 1  to 8 .
Table 3: Results of Post Hoc analysis of self-report and speech disfluency.
For example, in each row, items sharing superscript "a" are statistically significantly different from each other and items sharing superscript "b" are statistically significant from each other.
Participants from all conditions did not differ significantly on their evaluation of the naturalness of the agent's appearance and behavior.
However, participants found the agent in the Staring and Ignoring condition more distracting than the one in the Responsive condition.
Participants found the Staring agent as distracting as the Ignoring agent.
We asked participants how much do they think they focused on the listener  when they interacted with him.
Participants focused significantly more to the Responsive agent than the Staring agent, according to their selfreport.
Interestingly, participants that interacted with the Staring agent rated the human listener more tense than those interacted with the Responsive and Ignoring agent.
We then conducted a Post Hoc analysis on each of these variables to how the conditions different from one other.
Table 3 summarizes the means of various self-report and behavior measures.
Items sharing the same superscripts are significantly different from each other in the Post Hoc tests.
For example, in each row, items sharing superscript "a" are statistically significantly different from each other and items sharing superscript "b" are statistically significant from each other.
Since the interactions are of various length , we divided the sum of the three types of disfluencies by duration and defined it as the disfluency frequency scale.
From Table 3, we can see that Hypothesis 2 is partially supported.
Participants in the Responsive condition spoke with less disfluencies than participants from the Staring and Ignoring condition.
Again, there was no significant difference between the Staring and Ignoring condition.
Thus, we annotated the instances when participants gazed at the agent during their interaction.
On the overall percentage of time the participants gazed at the agent, ANOVA test show that there was no significant difference between the Responsive, Staring and Ignoring conditions .
However, the means shows a trend that participants in the Responsive condition spent more percentage of time gazing back at the agent than the ones in the Staring and Ignoring condition.
And the gaze percentage from the Staring condition and Ignoring condition are almost identical.
An earlier study of human-human interaction in similar experiment setup showed that human speaker spent about 65% of the time gazing at a human listener .
We divided each interaction into 5 sections evenly to analyze how gaze duration changes over time.
Since each participant interacted with the agent twice, once explaining the Tweety and Sylvester cartoon and once explaining the Sexual Harassment Prevention Training video, we divided each interaction into 5 sections.
For each section, we then calculated the percentage of time the participant gazed at the agent.
Figure 3 shows how the average of gaze duration changes from the first interaction to the second interaction.
Note that first  interaction is defined as the first  time the participant interacted with the agent, regardless which video they are describing.
From Figure 3 we can see that, gaze duration in the Responsive condition remains relatively high throughout the two interactions, while gaze durations in the Ignoring condition remains low.
In the Staring condition, gaze duration is high toward the beginning and slowly decreases over time to the level similar to the Ignoring condition.
In this paper, we explored the psychological construct of rapport as an explanatory construct for guiding the design of virtual agents and avatars.
Consistent with the predictions of Tickel-Degnen and Rosenthal , participants experienced more rapport when the virtual representation of their conversation partner showed more attention, positivity and coordination: participants interacting with the Rapport Agent had greater subjective experience of rapport and exhibited more fluent speech when compared to an agent that only exhibited attention  or an agent that exhibited none of the constituents of rapport .
This is consistent with Hypothesis 1 and 2.
Unexpectedly, gazing alone  had surprisingly strong negative impact on user performance: an agent that simply stares is just as bad as an agent that conveys disinterest in terms of creating distractions, reducing rapport and disrupting speech production, despite the fact that all agents were perceived as equally natural.
This finding is significant in that many avatars and virtual agents convey attention to the user by staring.
Although inconsistent with the model of Tickle-Degnen and Rosenthal, the negative effects of staring are consistent with some theoretical perspectives on human interaction.
Several theories of nonverbal interaction emphasize that, although gaze communicates interest and intimacy, this intimacy may not be desired and its expression can have negative consequences in certain circumstances.
For example, according to discrepancy-arousal theory, the expression of nonverbal intimacy in face-to-face conversations can reduce participant gaze and create feelings of discomfort or embarrassment if the rate or magnitude of these nonverbal cues differ from what is expected .
To assess if negative arousal played some role, we asked the participants to evaluate how uncomfortable they were when interacting with the agent  in the postquestionnaire packet.
ANOVA test shows no significant difference between the three conditions.
However, the means show a trend that participants in the Staring condition felt more uncomfortable than participants in the Responsive and Ignoring condition .
A completely different explanation is offered by theories of conversational grounding .
According to this research, speakers in a conversation expect frequent and incremental feedback from listeners that their communication is understood.
We then conducted a General Linear Model  Repeated measure test to analyze the change over time.
The result shows that the main effect of condition  is not significant .
This means that the overall, the average percentage of gaze duration does not differ significantly between the three conditions.
However, the ANOVA test of gaze duration does show trend that participants in the Responsive condition spent more time gazing back at the agent than the ones in the Staring and Ignoring condition, while the Staring and Ignoring condition did not differ.
Such feedback can take the form of nods  but also complex patterns of making and breaking gaze .
Indeed, Nakano  found that listener staring can be interpreted by speakers a failure to establish mutual ground.
Thus, the poor performance of speakers in the Ignore and Staring conditions, according to grounding theory, can be explained by the failure of these agents to produce grounding cues.
Further research will attempt to distinguish between these theoretical perspectives.
Males and females can respond differently to nonverbal behaviors, particularly in the case of eye gaze cues .
Studies in Computer Mediated Communication have shown gender differences on interpretation of gaze and presence .
In this study, we did not find any significant main or interaction effect of participant's gender on self-reported and behavior variables.
This could partly due to the mismatch of the confederate  and the virtual agent's gender.
The participants were led to believe that the virtual agent's behavior was controlled by the human listener .
But the confederate of the study was female and the agent was male.
However, in our prior studies, when human listener and the virtual agent's gender were matched, we didn't find any significant effect of gender.
Harrigan  studied the nonverbal behavior of high/low rapport doctors and found that high rapport doctors engaged in less extensive eye-contact than low rapport doctors.
In their study, the low rapport doctors maintained mutual gaze with the patient throughout 85% of interaction.
The high rapport doctors maintained mutual gaze 70% of the time.
Gaze behavior can have different social implications in different social context.
The findings presented here are observed in a monolog setting.
Further studies need to be conducted to better understand how they generalize to contexts where agents/avatars can provide other forms of feedback.
The work presented here has intriguing implications to the design of agents and avatars in the collaborative virtual environments and virtual worlds in general.
Thanks to the recent technology advancements, collaborative virtual environments  where user can interact and collaborate via avatars in 3D worlds have become more and more common.
State of the art of the social behavior of the avatar is to be desired.
For example, even though in some of the virtual worlds, user could choose the nonverbal behavior accompanies the verbal signals, the default listening behavior of the avatar is gazing directly at the user or accompanied by random posture shifts.
Results from this paper show that during social interactions, including the ones in the virtual worlds, simply use directed gaze to establish mutual attention is not only not enough to create positive impression but could have negative social effect.
There are other nonverbal behaviors that can build mutual attention.
For example, forward lean and orienting body towards one another.
However, regardless which nonverbal behavior is used to express mutual attention, it is not that agents and avatars should not maintain eye contact with the user, but mutual attention should be accompanied by behaviors that indicate positivity and/or coordination to create positive interaction.
This material is based upon work supported by the National Science Foundation under Grant No.
This work was also sponsored by the U.S. Army Research, Development, and Engineering Command , and the content does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.
Argyle, M. Bodily Communication .
Argyle, M. and Cook, M. Gaze and Mutual Gaze.
Cambridge University Press, Cambridge, 1976.
Bailenson, J.N., Yee, N., Blascovich, J., Beall, A.C., Lundblad, N., and Jin, M. The use of immersive virtual reality in the learning sciences: Digital transformations of teachers, students, and social context.
Effects of eye contact, posture and vocal inflection upon credibility and comprehension.
Bevacqua, E., Mancini, M. and Pelachaud, C. A listening agent exhibiting variable behaviour, in 8th International Conference on Intelligent Virtual Agents, , Springer, 262-269.
B., Coates, L. and Johnson, T. Listener responses as a collaborative process: The role of gaze.
Bente, G., Eschenburg, F. and Aelker, L. Effects of simulated gaze on social presence, person perception and personality attribution in avatar-mediated communication, in 10th Annual International Workshop on Presence, 
Goldberg, G. N., Kiesler, C. A., and Collins, B. E. Visual behavior and face-to-face distance during interaction.
Gratch, J., Wang, N., Gerten, J., Fast, E. and Duffy, R. Creating Rapport with Virtual Agents.
A., Oxman, T. E., Rosenthal, R. Rapport expressed through nonverbal behavior.
Controlling the Gaze of Conversational Agents, in Natural, Intelligent and Effective Interaction in Multimodal Dialogue Systems.
Kallmann, M., & Marsella, S. Hierarchical Motion Controllers for Real-Time Autonomous Virtual Humans.
Does Contingency of Agents' Nonverbal Feedback Affect Users' Social Anxiety?
Agreeable People Like Agreeable Virtual Humans, in 8th International Conference on Intelligent Virtual Agents, , 253-261.
Some functions of gaze direction in social interaction.
Towards a common framework for multimodal generation in ECAs: The behavior markup language.
In 6th International Conference on Intelligent Virtual Agents, , 28-41.
Krumhuber, E., Cosker, D., Manstead, A. S. R., Marshall, D., & Rosin, P. L. Temporal aspects of smiles influence employment decisions: A comparison of human and synthetic faces, in 11th European Conference Facial Expressions: Measurement and Meaning, .
Wolfe, Revision of the SelfMonitoring Scale.
Journal of Personality and Social psychology, .
In S. Prevost, J. Cassell, J. Sullivan, and E. Churcill, eds, Embodied Conversational Characters.
McNeill, D. Hand and mind: What gestures reveal about thought.
The University of Chicago Press, Chicago, 1992.
Morency, L.-P., Sidner, C., Lee, C., and Darrell, T. Contextual Recognition of Head Gestures.
A Storytelling Robot: Modeling and Evaluation of Human-like Gaze Behavior.
In Proceedings of the IEEE-RAS International Conference on Humanoid Robots, , 518523.
Towards a model of face-to-face grounding.
In 41st Annual Meeting on Association For Computational Linguistics - Volume 1 .
Association for Computational Linguistics, Morristown, NJ, 553-561.
O'Connor, B., and Gifford, R. A test among models of nonverbal intimacy reactions: Arousal-labeling, discrepancy-arousal, and social cognition.
Oppenheim, A. V., and Schafer, R. W. From Frequency to Quefrency: A History of the Cep-strum.
Pelachaud, C. and Bilvi, M. Modelling gaze behavior for conversational agents.
In 4th International Conference on Intelligent Virtual Agents, , Springer.
Rosenthal, R. Interpersonal expectancy effects: A 30year perspective.
Carver, The Self-Consciousness Scale: A revised version for use with general populations.
The social psychology of telecommunications.
Swaab, R. and Swaab, D. Sex differences in the effects of visual contact and eye contact in negotiations, Journal of Experimental Social Psychology  45  129-136.
Tatar, D. Social and personal consequences of a preoccupied listener.
Stanford, CA, Stanford University: Unpublished doctoral dissertation .
Negotiating via information technology: Theory and application.
Thorisson, K. R. and Cassell, J.
Why Put an Agent in a Body: The Importance of Communicative Feedback in Human-Humanoid Dialogue.
Thorisson, K. R. Layered modular action control for communicative humanoids.
IEEE Computer Society Press, Geneva, Switzerland, 1997.
Thorisson, K. R. Natural turn-taking needs no manual.
Tsui, P., & Schultz, G. L. Failure of Rapport: Why psychotheraputic engagement fails in the treatment of Asian clients.
Vertegaal, R. The GAZE Groupware System: Mediating Joint Attention in Multiparty Communication and Collaboration, in Proceedings of Conference on Human Factors in Computing Systems, , ACM, 294-301, 61.
Ward, N., & Tsukahara, W. Prosodic features which cue back-channel responses in English and Japanese.
Yngve, V. H. On getting a word in edgewise, in 6th regional Meeting of the Chicago Linguistic Society, 1970.
