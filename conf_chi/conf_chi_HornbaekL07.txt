Such measures of usability play important roles in several areas of human-computer interaction: usability engineering has as its goal to use usability measures to improve computer systems; research comparing the relative merits of two interfaces often uses measures such as task completion times and errors; and practical summative testing of an application against a competitor's product also typically relies on usability measures.
While the quantitative measures of usability we discuss in this paper are not the only way to capture the usability of an interface, they are widely used and indispensable to many researchers and practitioners.
The literature on HCI, however, offers surprisingly little help in how to measure usability, in particular how to select measures of usability.
The papers investigating this issue have mostly looked at correlations between usability measures, but show mixed results .
Nielsen and Levy , for example, found that performance and preference were correlated in 75% of a selection of 57 studies, meaning that users in general preferred the application with which they performed best.
In contrast, Frokjaer et al.
In addition to addressing these differences in results, it has been suggested that analysis of correlations among usability measures would help understand better how usability can be measured .
We present a meta-analysis of usability measures by investigating how they correlate in 73 studies.
The aim of the analysis is to provide information about how measures relate, which will help  understand better what usability is and how to develop models of it, and  select measures for usability studies.
In contrast to earlier meta-analyses of usability we use the raw data of the studies, allowing calculations to be thorough and uniform across studies; we base our results on a comprehensive sample of journals and conferences, forming the largest sample we know of to be meta-analyzed with respect to usability; we investigate the role of moderator variables such as task complexity; and we present implications for both usability research and practical usability studies.
Understanding the relation between usability measures seems crucial to deepen our conception of usability and to select the right measures for usability studies.
We present a meta-analysis of correlations among usability measures calculated from the raw data of 73 studies.
Correlations are generally low: effectiveness measures  and efficiency measures  have a correlation of .247  .059 , efficiency and satisfaction  one of .196  .064, and effectiveness and satisfaction one of .164  .062.
Changes in task complexity do not influence these correlations, but use of more complex measures attenuates them.
Standard questionnaires for measuring satisfaction appear more reliable than homegrown ones.
Measures of users' perceptions of phenomena are generally not correlated with objective measures of the phenomena.
Implications for how to measure usability are drawn and common models of usability are criticized.
The literature on usability contains numerous definitions of usability and models of the dimensions or components of usability .
Shneiderman and Plaisant , for example, identified five usability measures: time to learn, speed of performance, rate of errors by users, retention over time, and subjective satisfaction.
The ISO 9241-11 standard  identified three aspects of usability: effectiveness, efficiency and satisfaction.
Their QUIM model incorporates more than 127 specific measures in 10 factors, including - in addition to the ISO aspects - factors such as safety, trustfulness and accessibility.
The main contribution from this line of work appears to be its fleshing out the meaning of the usability construct and its implications for how to measure usability.
In practice, however, choosing among measures appears difficult.
A recent review of usability measures used in HCI research listed more than 54 kinds of measure .
This diversity - and the desire to develop empirically based models of usability - has spurred studies of the extent to which usability measures are related.
Typically this is done by studying the correlations between usability measures .
Below we briefly review these studies.
Nielsen and Levy  performed a meta-analysis of 57 papers, investigating the relation between objective and subjective measures of performance.
They used published papers as their source of data, from which information about usability measures was extracted.
This information was analyzed so as to uncover whether objective performance measures and subjective preference measures showed similar results, that is, favored the same interface.
Nielsen and Levy found that in approximately three-quarter of the cases, performance predicted preference.
In contrast, Bailey  presented an early argument for separating measures of preference and performance.
Similarly, Kissel  found that subjective and objective measures of usability were only weakly related, but that this relation was affected by users' experience with computers.
A study by Frokjaer et al.
In an analysis of data from a single experiment, and from a selection of papers from the ACM's CHI conference, they found no correlation between the three aspects of usability identified by the ISO 9241-11 standard.
They recommended measuring all the three aspects - effectiveness, efficiency, and satisfaction.
However, recently papers have appeared that try to combine usability measures, in part on the assumption that they to some degree contribute the same information.
McGee  and Sauro and Kindlund  shared the goal of developing a single, standardized usability score.
These single scores have the apparent advantage of brevity.
However, the validity of McGee's master usability scale, with sole reliance on user perception, is constrained by how users interpret the definition of usability.
For instance, they may selectively and inconsistently focus on certain aspects when assessing the usability of an object.
Similarly, the validity of Sauro and Kindlund's summated usability score is limited by which usability metrics that are included in or excluded from their summation procedure.
For their approach to work, we still need to establish which aspects or metrics of usability that are valid.
A relatively recent and separate approach to understanding how usability measures are related departs from users' perception of product qualities and their relation.
They used multivariate analysis to arrive at five groups of usability aspects, including core usability , secondary usability , and satisfaction qualities .
While studies such as that of McGee et al.
In summary, correlation studies appear one of the most prominent sources for understanding usability and how it may be measured.
Yet, the findings of correlation studies are in contradiction and limited on at least four counts.
First, existing studies  base their conclusions on a limited number of data sets.
Second, papers that study correlations often do not have access to raw data, only summary statistics.
Already Nielsen and Levy  described how they could not use common techniques for meta-analysis because "the original papers did not report sufficient statistical detail about their results" .
Thus, relations between usability aspects can only be simplistically coded.
Third, studies of correlations rarely account for the variety of ways that, for instance, satisfaction may be measured and what this means for relations between usability measures.
Fourth, studies of correlations do not try to account for contextual factors  that may impact the relation between usability aspects.
The aim of the metaanalysis presented next is to address these limitations.
The goal of the meta-analysis is to study the relation among usability measures, using the raw data from a selection of published studies.
The overall phases of the meta-analysis are to select studies for inclusion, to attempt obtaining raw data for these studies, to code the studies, and to analyze the coded studies.
Below we go through these phases.
First, however, we outline the basic procedures and goals of meta-analysis.
This range was chosen because it yielded a substantial number of full papers for consideration  and because we expected it to become increasingly difficult to get in contact with authors of the papers if we chose a longer span of time.
As candidates for our analysis we focus on original research papers reporting usability measures concerning human interaction with user interfaces.
Let us expand on this focus.
First, we only looked at full-length papers reporting original research; we assumed that short papers, poster summaries, and session overviews would not contain the kind of detail needed to perform the meta-analysis.
Second, since the meta-analysis concerns correlations between measures, a candidate study had to report at least two measures.
We disregarded studies with no information on usability measures  and studies reporting only qualitative data.
The latter choice aimed to restrict the focus of the meta-analysis; it does not imply that we find studies of a qualitative nature  of lesser utility in HCI.
Third, because our focus is on human performance we excluded studies that did not have this as their primary focus.
Thus, studies of cognitive models were excluded, as were papers concerned with testing data collection methods or with exploring specific sociological or psychological research questions.
Fourth, because the focus is on interaction we excluded studies with no two-way exchange of information between the user and the computer; some studies of non-interactive reading to compare the legibility of different font sizes, for example, were excluded on this account.
Fifth and finally, we interpreted user interfaces somewhat narrowly in that we disregarded interfaces in support of driving and flying/aviation.
In general, meta-analysis is an organized way to summarize, integrate and interpret selected sets of empirical studies.
Through systematic procedures of coding, recording and computing, effects and relationships on which a set of studies converge  can be identified.
The most important concept supporting these activities is that of effect size, a quantification of the magnitude of a difference between conditions or of a relation among variables.
Effect size is related to the significance of a statistical test so that significance = number of subjects x effect size.
This relation makes it possible to combine effect sizes that may not be significant in individual studies to form a general and possibly significant picture of some phenomena of interest.
We base our work on the existing literature on meta-analysis, specifically the procedures of Glass et al.
In this study meta-analysis is a matter of aggregating the correlations of usability measures across studies, because correlations are one way of expressing an effect size .
As will be discussed below, the main difference between typical meta analyses, including meta-analyses in HCI , and our study, is that we have the raw data of studies available.
Domain was coded using the leaf-levels of the ACM Computing Classification System .
After extracting from individual studies all their dependent variables, each variable was classified according to ISO 9241-11 standard - the tripartition of usability into effectiveness, efficiency, and satisfaction .
Then, each variable was further classified using a taxonomy from a recent study of usability measures .
This taxonomy distinguishes 54 kinds of usability measures.
For example, asking study-participants to rank interfaces in terms of preference would first be categorized as regarding the ISO category of satisfaction, and next be classified as rank preferred interface, in the preference category of .
In several candidate studies, the same specific usability aspect is measured in several ways.
For example, ease of use may be measured by a series of questions on a post-task questionnaire.
Following , we code measures that are described as capturing the same construct as just one measure .
Standardized questionnaires  are considered one measure, independently of the actual number or phrasing of questions.
Note that in contrast to , we coded TLX as a standardized questionnaire.
To ensure that studies were reliably coded, both authors went over the coding of every study and resolved any differences in opinion by discussion and by consulting the paper and the raw data set.
Existing meta-analyses of usability measures have not had access to the raw data of studies; this is how meta-analyses are typically carried out, see .
When sufficient information is contained in the papers being analyzed, this is fine.
However, correlations between usability measures are typically not reported in the HCI literature.
Thus, only coarse coding of the dependent variables is possible.
This was, for example, what Nielsen and Levy  did, by noting whether performance and preference data suggested the same direction of differences between conditions.
However, we wanted to have the raw data of the studies available so that we could calculate correlations ourselves and, more importantly, so that we could quantify the effect size of relations among usability aspects.
To obtain the raw data, we contacted authors of the 386 candidate studies to inquire if they would share with us the original data of their studies .
The first author was contacted by e-mail; if we received no reply we followed up with an e-mail to the author that we perceived to be the senior researcher; in some cases we also mailed a letter to the address mentioned in the paper.
This procedure yielded responses from 184 authors; an overall response rate of 48%.
Among the authors who responded, 133  agreed to share their data; 92  of these authors actually sent in their datasets.
Fifty-one of the authors  declined our requests for data sharing for various reasons: being prohibited from sharing because of institutional review boards or ethics guidelines, data loss, no time to retrieve data, data currently under use, or no access to data.
Some authors sent us more than one data set, for example when a paper reported several experiments.
In those cases, we randomly chose only one of the data sets, as inclusion of both studies could bias our sample.
Nineteen of the 92 sent-in datasets were discarded because of incompleteness, inappropriate data format or unclear research methodologies.
Consequently, we processed and analyzed 73 sets of raw data, that is, 19% of the total number of candidate studies.
Studies were coded in part on their methodology  and in part on their so-called substantive dimensions .
For each raw data file we calculated correlations between the usability measures.
Note that correlations may be calculated at different levels, for example, at the task level for individual subjects, at the level of averages for a particular subject's measures for a specific condition, or at the level of averages for a particular interface.
We calculate correlations at the least-aggregated level possible.
When computing the correlation between two measures such as time and accuracy, the correlation will be based on the time and accuracy data for each subject's solution to each task.
Some measures are not available at the task level, but only per interface .
Correlations will then be calculated per subject per interface, that is, on each subject's average task completion time and satisfaction score for an interface.
In all cases, we carefully checked the sign of the correlation.
For checking whether conditions in the studies impacted the correlations, we used the residual correlations from multivariate analysis of variance.
As suggested by Glass et al.
Procedures for doing so are readily available in the meta-analysis literature .
In particular, error and preference are often dichotomous, meaning relations between them and other variables will be point-biserial correlations, which need transformation.
The product-moment correlations will be used as our effect size measure, with the usual interpretation that r2 signifies the variance explained  and that an r  .5 is a large effect, r  .3 is a medium effect, and r  .1 is a small effect .
The average number of tasks performed by each participant per study was 104 .
Ten of the 73 studies did not present data on task duration.
In some studies a task was a simple move-and-click with a mouse, in others a single task could consist of many complex sub-steps or last for days.
The average number of participants involved per study was 32 .
In 37 studies the participants were experienced with respect to the tasks required to be performed, and in 23 studies the participants were novice.
Two studies employed both types of subject, whereas 11 studies did not give any data in this regard.
The studies' research designs were directly related to the number of participants recruited: 44 studies employed within-subject repeated design ; 23 studies between-subject ; two studies mixed design , and in four studies the design was unknown.
When aggregating effect sizes across studies, we first transform effect sizes to standard values using Fisher's r to z conversion .
The z-transformed score has a standard error of 1/sqrt , where n is number of participants in the study.
The inverse of this error can be used as a weight for each individual z-transformed score, so that studies with smaller standard errors are given more emphasis.
In practice, this amounts to multiplying the effect size with n3.
After this weighting, studies can be aggregated by averaging their z-transformed scores: Rosenthal  suggests this as a conservative procedure.
Finally, ztransformed scores can be translated back to r values, which is what we report throughout the paper.
We first give an overview of the characteristics of the studies in our sample and how they measure usability.
Next, we discuss the correlations among measures.
Note that throughout the paper, effectiveness, efficiency, and satisfaction will be reported with the meaning that higher values are better: high accuracy and low error rates are thus both indicative of high effectiveness.
The measures taken in the 73 studies were categorized according to ISO 9241-11 and to the taxonomy developed by Hornbaek .
Note that a usability aspect, say efficiency, can be gauged by different measure types  and their subsuming measure tokens  .
A measure token  can be gauged differently depending on the specific tasks performed in a study.
Measure tokens of satisfaction, in particular, are difficult to classify because they are collected using a variety of questionnaires, scales and levels of granularity, which seem only bounded by the imagination of their authors.
Counted at the level of measure tokens, the overall average was 4.07 measures per study .
Table 3 shows the corresponding values for the three usability aspects.
As shown in Figure 1, 36 out of the 73 studies  had measures of all the three usability aspects; 30 studies  had measures of the combination of effectivenessefficiency, effectiveness-satisfaction or efficiencysatisfaction; seven of the studies  collected measures of only one usability aspect.
Domain Input devices and strategies Information presentation and navigation Information search and retrieval Interaction device, style and technique Graphical user interface Visualization Virtual environment Audio-based interaction Database management Distributed collaborative computing Evaluation/Methodology Others  Table 2.
For instance, in a study where the task was multidirectional point-and-click, two types of error  were registered; in this case, we counted the measure token - error rate - only once.
As shown in Figure 2, the distribution of the nine types of effectiveness can be represented by an exponential curve with the peak token  being followed by a series of less frequent tokens .
The same distribution can be seen for the measure tokens of efficiency , where the peak token is task completion time , followed by several specific tokens such as deviation from optimal path and percentage of preferred walking speed .
These findings indicate that some convergence concerning selection of usability measures exists.
Studies contain a variety of satisfaction measures.
Twentyfive measure tokens were identified when enumerating them at the finest  level of the taxonomy from .
When they were grouped into the coarsest  level, there were six groups .
In contrast, the measure type "specific attitudes towards the interface" is more diverse, including annoyance, confidence, control, discomfort, frustration, fun, learnability, liking, and wantto-use-again.
Further, the measure type "others" include tokens that are emerging , vaguely defined  or encompassing .
Figure 4 shows the average correlations between effectiveness and efficiency.
Across the 54 studies that include measures of both these aspects we find a correlation of .247 .
This suggests that more efficient performance, such as faster task completion, is associated with more effective performance, such as fewer errors.
According to Cohen, this is a small to medium effect.
In practical terms, 87% of the studies have a positive correlation between effectiveness and efficiency.
Among the studies with the highest correlations is a study of four navigation interfaces for a hand-held mobile device ; a study of authoring of privacy rules shows a negative correlation between errors and time usage, r = .23.
Though the average correlation is significantly above zero , it appears that there is quite a bit of variation in the data.
To uncover the sources of this variation, we may look at just the prototypical measures of effectiveness and efficiency, that is, at task completion rates and task completion time.
Thus, the general result of a correlation between effectiveness and efficiency appears not to be due to our inclusion of a broad range of usability measures.
Rather, the prototypical correlation is higher between time and error than the one presented above.
Task complexity does not seem to affect the relation between efficiency and effectiveness.
Figure 4 illustrates the relationship between variables across the task complexity categories.
It could be hypothesized, as done by for example , that more complex tasks would not show strong correlations between usability aspects.
For none of the three combinations of ISO aspects of usability is task complexity significant .
Thus, task complexity does not seem to attenuate or otherwise affect the relation between usability aspects.
Simple explanations related to task complexity seem difficult because the precise way effectiveness and efficiency are measured impacts their relationship.
Take as one example the difference in what errors are taken to mean.
Two interpretations may be found in the data: errorsalong-the-way and task-completion-errors.
Errors-alongthe-way are mistaken actions on the way to task completion: trying a wrong navigation path or miss clicks before hitting an object; task-completion-errors are errors in a task's outcome .
Errors-along-the-way has an average correlation to efficiency of .441  .125 ; task-completionerrors an average correlation to efficiency of .155  .08 .
This is the case, for example, in studies of input devices and interaction techniques that allowed only a correct selection of an object to end a task: missing the object would of course make the task last longer.
In addition to the above explanation, it seems that some of the more complex measures of effectiveness are not correlated to efficiency measures.
As suggested by Figure 2, most studies measure error rates and binary task completion; fewer assess quality  or use expert assessments .
None of these six studies show a significant correlation between efficiency and effectiveness.
Rather, the average correlation between efficiency and complex satisfaction measures is negative .
While task complexity does not in itself change relations between usability measures , complexity of measures does.
Figure 4 also shows the average correlation between measures of effectiveness and satisfaction.
Across the 39 studies that include measures of both these aspects we find a correlation of .164  .062.
According to Cohen, this is a small effect.
It is also the lowest of the three comparisons between ISO aspects.
Yet, 86% of the studies show a positive correlation between effectiveness and satisfaction.
The simplest example of these relations occurs between on the one hand task completion rates and satisfaction questionnaires and preference indications on the other hand.
Table 5 suggests correlations of .243 between error and preference and of .196 between errors and satisfaction questions.
Since preference is typically measured dichotomously, we may illustrate the difference concretely by the observation that for studies in Table 5, error rates are about 18% for the non-preferred interfaces  and 13% for the preferred ones.
Six studies measure both task effectiveness  and participants' own assessment of their effectiveness.
The correlation between effectiveness  and participants' assessment of their task solutions is on average r = .22 , not significantly different from a correlation of zero.
This observed inconsistency between objective and subjective measures could be due to cognitive and social bias, for example, the role of prior experience  and social desirability effect .
This remains to be explored.
Figure 4 also shows the relation between efficiency and satisfaction.
Across the 45 studies that report one or more measure of these aspects, the average correlation is .196  .064; a small to medium effect.
However, it appears relatively uniform across studies as 81% of them show positive correlations.
Again we can illustrate this correlation by appealing to the relation between prototypical measures, in this case between task completion times and preference/satisfaction questionnaires .
For these studies, a preferred interface is about 20% faster than a non-preferred one.
As suggested earlier, a number of studies measure both efficiency and participants' experience of the interaction.
This is done by questionnaire items like "rate your satisfaction with task completion time" and "how quickly did the system let you finish your tasks".
Interestingly, we find a correlation between such questions and objective task completion times that are indistinguishable from zero .
Again, correlations vary a lot, with two studies showing negative relations between time and subjective measures of the interaction.
Three studies in our sample measure satisfaction both at the level of an individual task and at an aggregated level, typically once for each interface.
For instance, in one study the simple three-question ASQ  was administered right after each task to capture users' instant reactions and the long 19-question CSUQ was administered after all the tasks had been performed to capture users' overall perception of the system.
Reassuringly, the correlations between individual and task level satisfaction measures are medium to large, with rs ranging from .38 to .70.
Finally, 10 studies measure both preference and some other aspect of satisfaction.
It appears relevant to look at how well satisfaction questionnaires filled out during a study predict the preferences expressed by participants.
Again the correlation is reassuringly large, with a mean r of .49.
A large portion of the studies uses several ways of measuring satisfaction, for example, by asking questions concerning specific satisfaction, using a standardized questionnaire, and asking for the interface that users preferred.
This opens the possibility of studying the relation among measures of satisfaction which we do next.
One issue that the raw data of the studies allow us to investigate is the reliability of satisfaction measures.
For all questionnaires where we had available the full questionnaire data, we calculated Cronbach's , a widely used measure of the reliability of questionnaires .
Table 6 gives these values for standard questionnaires and for homegrown ones, that is, questionnaires created ad hoc with more than one question and that purport to measure some aspect of satisfaction.
The table suggests that homegrown questionnaires have lower reliability and greater variation in reliability: six such questionnaires fail to reach the commonly accepted minimum reliability of .70.
Possibly some of the questionnaires attempt to measure several distinct constructs, but we suspect the drop in reliability may be caused by poor questionnaire design.
We have characterized how usability is measured across a selection of 73 studies.
Our study has shown an overall small to medium correlation between usability aspects; typical measures of usability are related with a Pearson correlation coefficient ranging from .164 to .247.
Factors involved in shaping these correlations are the use of complex usability measures, of prototypical or standardized measures, and of measures based on participants' perceptions.
Task complexity does not seem to influence the relation.
We find quite similar correlations across studies with clear differences in domains, interface types, procedures, and experimental conditions, suggesting that these differences matter less for relations among usability measures than commonly assumed.
On the one hand our data may be interpreted as showing only a low correlation between usability aspects .
Though the effect sizes are generally low to medium, they might be considered of little practical importance given the variation in the data.
This interpretation follows the literature that suggests weak to no correlation among usability measures .
Indeed the results that more complex measures of effectiveness and efficiency are not correlated, and that task-completion-errors attenuate correlations, suggest that in more complex study setups, correlations drop.
Under this interpretation, our analyses indicate that attempts to reduce usability to one measure  are bound to lose important information, because there is no strong correlation among usability aspects.
Apart from masking potentially interesting details, the use of a single usability score cannot support formative evaluation well.
Further, data redundancy is not necessarily undesirable: even when usability measures are highly correlated they convey information in different ways.
In system redesign, for example, developers may be more convinced or motivated to improve the system when usability measures converge.
The correlations found in our paper are lower than those presented by Sauro and Kindlund .
They found correlations between time and error of r = .5, and between satisfaction and task completion of about .5.
Possible reasons for their overestimation relative to our data include simple measures used consistently across studies , a much smaller data set , a specific kind of task , and the use of per-task satisfaction measures .
With correlations half the size of those in Sauro and Kindlund's study, we think the argument behind the one-measure usability score is seriously weakened.
Our other interpretation  is reflecting a surprise to see such a general correlation across studies.
In about 80-90% of the studies, variables in the main categories of the ISO classification are positively correlated.
The straightforward idea by Frokjaer et al.
Rather, an important factor in attenuating correlations is more complex measures of in particular effectiveness: these include quality of tasks, task-completion-errors, and so on.
The complexity of usability evaluation tasks are somewhat tied to the domain.
Our finding that task complexity does not affect the relationships among usability measures seems to imply that such measures are applicable across a wide spectrum of domains.
Further, the "ceiling" or "floor" effect  engendered by the particularities of tasks that are noncanonical, but tailor-made by usability specialists for evaluating a particular system, does therefore not appear threatening to the utility of task-based performance metrics.
Analogously, we find inconsistencies in classifying workload.
It is measured similarly to satisfaction measures and correlate strongly with such measures, but some authors  consider it an efficiency measure Second, we find a difference between users' experience of interaction/outcomes and objective measures.
For studies that collect both measures of the same phenomenon, we find negligible correlations.
While some models accept fundamental differences between subjective and objective measures , others do not .
Leveraging these differences for novel measures would be interesting.
This idea might be extended to measures other than time.
Third, the variation in usability measures suggests the malleability and extensibility of the notion of usability.
Among others, the user experience movement  has argued to broaden the notion of usability, rather than narrowing it.
We find mixed results related to this issue, because some aspects of users' experience seem orthogonal to performance measures and some shows substantial correlations.
Further work may investigate if correlational studies could help describe the relation between user experience indicators and traditional usability measures.
We suggest that usability studies describe correlations among the usability measures collected.
This would help interpret and compare outcomes of usability evaluations.
We also recommend that standard questionnaires be used when possible, given their higher reliability, and that the more complex effectiveness measures be used when feasible .
Our study suggests several conceptual problems in current models of usability.
First, our distinction between errorsalong-the-way and task-completion-errors indicates a particular relation between these measures and other usability measures.
We do not see this distinction in common models of usability , nor do recommendations on selecting measures for usability tests make this distinction .
Our method raises a couple of concerns.
First, we have extensively relied upon the ISO classification of usability.
As mentioned above we do not find it entirely satisfactory, but it has helped manage the complexity of our data set.
Second, in a few of the studies in our sample there are differences between the correlations found when aggregating data at the task level and at the user interface level; we currently cannot offer any good explanation for these differences.
Third, our meta-analysis has been barebones.
We consider using path modeling to investigate the relation among usability aspects promising, and would like to further analyze the role of continuous moderator variables, such as duration of use.
Fourth, it is not our wish to scorn arguments that usability is to a large extent shaped by context .
Studies of correlations among usability aspects appear a useful way of enriching our understanding of usability.
However, existing studies are in disagreement with each other and often calculate correlations from a limited collection of data.
We have investigated correlations in the raw data of 73 usability studies and find medium to low correlation among usability measures.
In addition a number of specific factors that affect correlations have been identified .
Our results suggest that some models of usability are problematic and that theory to speculate about the relation between measures is lacking.
