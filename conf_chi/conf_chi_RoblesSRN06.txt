This paper explores the relationship between display of feedback  by a computer system and the basis for evaluation  of that feedback.
We employ a social interpersonal context  in a controlled laboratory setting.
Participants  receive real-time performance feedback, either only about themselves  or about both participants .
Participant perceptions of monitoring, conformity, and self-consciousness about themselves and their dating partner, as well as perceptions of system invasiveness, system competence, and system support are assessed.
There is a consistent pattern of significant interaction between feedback display and basis for evaluation conditions.
Public feedback with an added, trivial basis for evaluation creates significantly lower perceptions of monitoring, conformity, self-consciousness, and system invasiveness, than do the other three conditions.
Additionally, there is a main effect for basis for evaluation with respect to system competence and supportiveness; the presence of a basis produces more positive assessments than its absence.
This research shows that reactions to being monitored and evaluated do not differ strictly along the dimension of public vs. private; basis for evaluation of feedback functions as a mediator and thus co-determines participant attitudinal responses.
The implications are discussed at several levels, and motivate a broader cultural explanation in terms of the theory of rationalization.
Issues concerning the utility of linking laboratory settings to larger cultural contexts in this and related fields of inquiry are presented.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
For the HCI and CSCW communities, developing technologies for group contexts necessitates concerns about designing systems that employ socially appropriate mechanisms of feedback, assessment and information sharing.
One of the richest dimensions of debate around the issue of socially appropriate computing has been the distinction between public and private information settings.
Respecting these boundaries can lead to adoption and use of systems to construct public, shared spaces of collaborative exchange .
Transgressing these boundaries creates privacy issues that negatively affect the user's satisfaction with a technology and ultimately degrades the potential for a technology's long-term adoption.
Technologies that serve monitoring, surveillance and automated assessment purposes are increasingly ubiquitous, leading to a pressing need for developing models of private and public information that can be applied in everyday social contexts .
HCI research has advanced the debate around the public and private sufficiently to suggest that the distinction between these ideas is socially determined and certainly not fixed or static .
By expanding the discussion beyond public and private, to include ideas about shared, personal, central and distributed information, the research seeks to iteratively refine models of how situational characteristics affect user perceptions of informational content.
However, this research, which points to a variety of social and technological elements, is represented almost exclusively through ethnographic methodologies which depend on deep contextual particularities of a situation.
Focusing on a particular site of inquiry, though affording the capacity for "thick description" , or even generating "sensitized concepts" , or analytical insights that prove generative to the research of other scientists, must be complemented by research at broader psychological and cultural levels in order to provide a more complete picture of how we, as social actors, interact with, respond to, and are shaped by technologies.
The need to situate particular ethnographic studies of users in larger cultural debates is perhaps most apparent around the distinction between public and private information sharing.
This is evinced by research that suggests two seemingly paradoxical trends.
First, people are concerned with protecting social boundaries about what is private and what is public.
They are deeply concerned about surveillance and monitoring, and these concerns hinder widespread adoption of technologies for use in group contexts, home, mobile, and ubiquitous computing.
Second, we live in a society of unprecedented self-disclosure and availability for public scrutiny and evaluation: surveillance, monitoring, and assessment have become banal rather than spectacular phenomena as evidenced by the widespread participation in and viewing of social networks, personal home pages, webcams, blogs, online dating, online "attractiveness" ratings, reality TV shows, etc.
Thus, an examination of the social weightings carried by information in public or private contexts is incomplete without a culturally and historically aware viewing of the nature of public vs. private itself.
The first trend suggests a need to investigate how people understand the appropriate dissemination of information, and then to design technologies that preserve people's understandings in new contexts, e.g., designing for privacy in a "networked" world .
The second trend suggests broader cultural shifts in the everyday notions of public and private, which are inextricably linked to the evolution of media and information technologies and computing discourse .
Reconciling these two trends requires identifying mechanisms that influence people's acceptance of being made available for public interaction, scrutiny, or evaluation.
This paper proposes that a powerful logic that systematically negotiates these trends is rationalization, as originally articulated by Max Weber .
Rationalization argues that in the process of modernization, societies have transferred practices from the personal sphere to the public sphere, thereby making these practices available for scrutiny and evaluation.
This process is accomplished by employing objective and standard rules and procedures for evaluation across individuals such that public scrutiny becomes normalized and accepted by all.
Information and computing technologies are key sites for the extension of rationalization.
In other words, it is not simply being watched, but being special, that inappropriately disrupts the social boundaries of public versus private information.
In this paper we demonstrate empirically, through an experiment in a controlled laboratory setting, that rationalized evaluation  mediates people's responses to public and private performance assessments in interpersonal contexts.
At first glance, an experiment seems a rather unconventional place to extend a fundamentally cultural argument.
However, we contend that the laboratory affords a space particularly appropriate for isolating the distinction between public and private feedback and systematically examining how the presence or absence of some basis for evaluation might produce the effects predicted by rationalization -- whereby public, rationalized evaluations about the self are made psychologically acceptable.
Our aims are to contribute to the ongoing discussion in the HCI community about what it means for information to be "public" or "private", to suggest that we need to integrate this research with broader cultural shifts which have transformed attitudes about being available for public scrutiny and evaluation, and finally to provocatively suggest a role for experimental methodologies in this discussion.
This research informs developers of groupware systems because they design and develop displays intended to disseminate information and feedback in a socially appropriate manner in interpersonal contexts.
Stewart, Bederson, and Druin  explore the advantages of SingleDisplay Groupware .
They note that using a single, public display improves user experience by maximizing common ground between group members.
Similarly, Huang and Mynatt  advocate the use of semi-public displays in the small group context.
They show that these displays foster increased awareness of all group members, and thus increase the ease of collaboration through shared, persistent, central information display.
Finally, Morris, Morris, and Winograd  show that increased collaboration can result when private, rather than public feedback is provided.
This work indicates that appropriately disseminating information for individuals in groups is a core issue for the design of a successful groupware system.
From a different perspective, researchers on mediaspaces  are also deeply concerned with developing technologies that maintain appropriate distinctions between the public and the private.
In this literature, this issue is construed as the need to develop a feasible model for user privacy in a networked world.
Bellotti and Sellen  build from insights gained from EuroPARC's RAVE project .
This line of research links to broader concerns related to the introduction of ubiquitous devices that use sensors to monitor and assess us daily.
Though the research acknowledges that "public" and "private" are socially negotiated definitions, it proposes no psychological or cultural explanations to account for these observations.
Finally, we consider the argument advanced by critical and cultural scholars that there is a fundamental relationship between rationalization and acceptability of being monitored and assessed.
Rationalization is the process by which society rationalizes through quantifying, objectifying, and informating subjective experiences .
This process makes formerly private experiences privy to political and economic reorganization in the public sphere, such as Taylorism, Fordist production practices, and standardized testing.
Computing plays an intimate role in the reproduction and extension of rationalization in modern social practices.
A common, now banal, example would be the widespread acceptability of scientific management practices in organizations, a trend stretching back to Frederick Taylor  in which technologies are embedded in organizations to facilitate monitoring and efficiency evaluations of worker productivity .
Workers thus accept surveillance of practices that would have, centuries ago, belonged in a private rather than public sphere, because they see each person as subject to those same practices and because the practices are justified through the application of standard quantitative assessment.
A similar logic underlies our culture's willingness to have individual knowledge and understanding rated through standardized testing procedures.
Rationalization does not just appear in formal environments such as work and school, but also in leisure pursuits.
For example, media theorist Mark Andrejevic points out that this logic underlies the popularity of the reality television genre .
Its success relies not just on participants who consent to twenty-four hour surveillance, a feature of many documentary productions, but the marriage of this surveillance with game-show style judgments and evaluations.
It is not just being watched, but being assessed by the watchers that rationalizes the surrender of the personal to the public.
Less spectacular examples of this phenomenon are also seen on sites such as amihotornot.com, where people upload photos and submit to being rated on attractiveness, or dating sites such as match.com where people allow algorithmic processes to determine, in part, appropriate potential romantic partners based on a standard set of variables.
We conducted a two-by-two, between-participants laboratory experiment designed to investigate the relationship between display  and basis for evaluation  of performance feedback during a speed-dating task.
We choose to employ a dating task for three reasons.
First, it represents an interpersonal domain with a skill set that is traditionally not quantified.
As such, it has functioned largely outside the rationalizing processes of more formal situations, e.g., work practices and competitive tasks.
Second, a standard definition of quality performance in a dating situation, as with many interpersonal skills, is sufficiently ambiguous to preserve the plausibility of our stimuli: random performance feedback.
Finally, dating features importantly in the lives of our college-age participant population.
Therefore, we assume that participants will quickly and authentically engage the goal of the task.
Showing that it may be the logic of rationalization that produces the effects rather than a cognitive, informational value to reason-giving requires that we perform a subtle separation of the form of reason-giving from the content of it.
To do this, we construct a variable called "basis for evaluation," which varies by its presence or absence.
Even when present, it is entirely "placebic", or devoid of substantive, informational value.
A placebic basis invokes the form and grammatical structure, but not the content, of reason-giving.
The effective use of placebic rationales first appears in Langer et al's classic study on compliance .
In this study, an experimenter approaches an unwitting participant using the Xerox machine at the local university library.
The experimenter asks for a favor in one of three forms.
The first was a salutation, and then request, with no reasoning offered: "Excuse me, may I use the Xerox machine?"
The second form provided salutation, request, and then an informative reasoning for the request: "Excuse me, may I use the Xerox machine because I am in a hurry?"
The final form employs salutation, request, and then placebic reasoning, or: "Excuse me, may I use the Xerox machine because I want to make copies?"
The rate of compliance increases from sixty percent where no reasoned basis is offered to near total  where some reasoned basis is offered.
Interestingly, there is no difference between conditions of informative and placebic bases.
Langer and colleagues conclude that invoking the schema of reasongiving, in this case adding a subordinate clause beginning with "because", is sufficient to create compliance.
Thus, it is not really the informational, but rather the ritual nature of the basis that provides much of its functionality in daily use.
Leveraging this finding, which has been replicated across diverse contexts in social psychology, we provide either no basis for evaluation during feedback or a placebic basis: "You should provide more personal information", versus "Linguistic analysis indicates you should provide more personal information", or "You should listen more attentively" versus "Conversational analysis indicates you should listen more attentively."
Finally, performance feedback is displayed either privately or in public.
In the private condition, a participant would only see feedback referring to that participant: "You should be more talkative."
In the public condition, each participant would see feedback about both of the participants: "Participant A should be more talkative and Participant B should allow longer pauses in the conversation".
Thus, we define public as the state where all feedback is available to all participants.
Private refers to the state where participants are privy only to feedback intended for them.
Forty-eight participants, aged 18-25, were recruited from a university.
Participants were randomly assigned to experimental condition, and gender was balanced across conditions.
All participants signed voluntary informed consent forms.
They were compensated with a $10 gift certificate for Amazon.com.
Over-the-shoulder screen positioning minimizes the need for participants to break eye contact or disrupt the flow of conversation during stimulus delivery.
Participant A can view the screen as well as participant B in the same field of view and vice versa.
After collecting voluntary informed consent forms from the participants, the experimenter leaves the room for a tenminute period.
During this time, the participants converse freely.
Approximately every ninety seconds, a stimulus in the form of feedback about the interaction appears onscreen.
The feedback phrases are essentially variations on talk less or talk more, like "listen more attentively", "allow longer pauses", "take a turn at directing the conversation", etc.
Stimuli are unique, randomly ordered and remain onscreen for fifteen seconds.
At the end of the session, participants are escorted to separate computer consoles where they complete questionnaires about their attitudes concerning the system, themselves and their partner.
They are then thanked for their participation and debriefed about the purposes of the study.
Participants were asked to complete an online "dating questionnaire" concerning relationship status and personality traits like extroversion, prior to the experimental session.
They were told that this information would be used by our computerized dating system to suggest a potential date whom they would meet at the session.
For each session, a male and female participant, of same experimental condition, arrive at the laboratory and are greeted by an experimenter.
The experimenter explains that we are testing a system that can understand and analyze spoken conversation.
Participants are told that the system uses this information to provide real-time feedback designed to improve the flow of interaction, in this case, to improve their dating skills.
Participants are also informed that they would be video and audio recorded throughout the session.
The laboratory setting contains numerous visible recording devices, many inactive, to provide a sense that the room is "wired for intelligent interaction" with users.
Participants are seated in two chairs, facing each other, approximately twenty-four inches apart.
Behind the shoulder of each participant is a rear projection screen used to provide feedback throughout the experiment .
Participant attitudes and beliefs were assessed via an online questionnaire administered immediately after the experiment session.
Participants were asked to rate how well a variety of adjectives  described both their own, and their co-participant's attitudes, beliefs, and perceptions during the experiment.
Each adjective was rated on a ten-point Likert scale, anchored by "Not at all" and "Very well".
Related adjectives were combined via Principal Component Analysis to construct a set of indices reflecting social judgments about the self, the co-participant, and the computer system.
This index consists of three items that assess to what degree the participant and co-participant feel monitored, on-stage, and watched during the experiment.
This index consists of three items that assess to what degree the participant and co-participant feel conformist, manipulated, and obedient during the experiment.
This index consists of seven items that assess to what degree the participant and co-participant feel inadequate, insecure, judged, restrained, self-conscious, uncertain, and inhibited during the experiment.
Additionally, three indices were constructed to reflect participant assessments of the computer system.
All indices were reliable: System invasiveness.
This index consists of six items that assess to what degree a participant feels the system was annoying, inappropriate, interrupting, invasive, irrelevant, and unwelcome .
This index consists of ten items that assess to what degree a participant feels the system was accurate, aware, competent, correct, insightful, intelligent, justified, knowledgeable, reasonable and reliable .
This index consists of three items that assess to what degree a participant feels the system was helpful, reassuring, and supportive .
This finding is counterintuitive because a system that senses, processes, evaluates, and provides reasoned feedback about the speech of both conversational partners should literally be the most monitoring of all four variants.
However, the results clearly suggest the opposite.
The same interaction pattern occurs with respect to participants' assessments of their co-participant, F = 6.80, p < .01.
Once again, when a basis for evaluation is present and feedback is public, participants report far lower levels of system monitoring of their co-participant.
A slight cross-over pattern does occur in the interaction but there are no statistical differences between the other three conditions .
The results for conformity reveal the same pattern as those for monitoring.
There is a significant interaction between feedback display and basis for evaluation with respect to how conformist participants felt, F = 5.54, p < .02.
Again, the basic interaction pattern is sustained by systematically lower feelings of conformity in the public feedback display condition when a basis for evaluation, albeit placebic, is provided .
A series of univariate analyses of variance  was conducted to assess the effects of feedback display condition and the presence or absence of basis for evaluation on each of the constructed indices.
Note that in all graphs referred in this section, the horizontal axes vary feedback display condition and the vertical axes represent additive scale values of the indices.
There was a significant interaction between feedback display condition and the presence of a basis for evaluation with respect to how monitored participants felt, F = 4.62, p < .03.
Participants reported the lowest feelings of being monitored when feedback was displayed in public and some basis for evaluation was offered .
The highest feelings of being monitored are reported where feedback is public but no basis for evaluation is provided.
However, there is no significant difference between this condition and the private conditions.
Analysis of conformity assessments for the co-participant reveal a significant interaction F = 11.54, p < .001.
Participants judge their co-participant to be least conformist when feedback is public, or about everyone, and a basis for evaluation is present .
These results, however, do show a greater spread between the other conditions.
The highest assessments of partner conformity occur when feedback is displayed in public and no basis for evaluation is provided.
In the private conditions no basis for evaluation shows slightly lower assessments of coparticipant conformity than when a basis for evaluation is provided.
Analysis of how self-conscious participants feel reveals an interaction between feedback display and basis for evaluation conditions, F = 5.13, p < .03.
Again, participants report feeling least self-conscious when feedback is delivered in public and a basis for evaluation is provided.
The highest reported feelings of selfconsciousness result in the public display condition when no basis for evaluation is provided .
Self-consciousness is rated similarly in both the private display conditions, with slightly lower self-consciousness reported when no basis for evaluation is provided.
Analysis for co-participant self-consciousness reveals an interaction for feedback display and basis for evaluation conditions that approaches significance, F = 2.13, p < .10.
The results mirror that of self-conscious assessments for the self with lowest levels in public with a basis for evaluation, highest levels in public with no basis for evaluation and intermediate levels in the private conditions .
Finally, we analyze assessments of the system including system invasiveness, competence, and supportiveness.
The lowest sense of invasiveness from the system occurs when all feedback is public and a basis for evaluation is provided .
The highest sense of invasiveness results from public feedback without a basis and intermediate levels occur in the private conditions, with higher assessments of invasiveness where a basis is provided than when it is not.
The second variation, a cross-over interaction, is exemplified by the results of the analyses regarding self-consciousness.
In these interactions, the public display condition with a basis for evaluation again gets the least negative assessments by a significant amount.
However, these interactions reveal more spread between the other three conditions: public display with no basis for evaluation creates the most negative assessments; private feedback with basis is more negative than private feedback without basis.
The results, taken together, may be interpreted at multiple levels that usefully inform design, discussion, and future research about the role of public and private information in social contexts, and additional mechanisms that mediate this role.
The first conclusion we derive from these results is that the public/private dimension alone cannot adequately account for people's judgments of acceptability of being watched and assessed.
By using a speed-dating task in the laboratory, we construct an interpersonal context where feedback display can be varied  independently with another content feature, basis for evaluation .
We show that the interaction of these two variables results in far less negative reaction to being monitored.
The most acceptable system is that which reveals everyone is being monitored and provides some basis for the feedback that is provided.
This finding applies to such scenarios as the design of error feedback for learning or collaboration systems.
When delivering error messages or performance feedback to a participant, it is important to consider whether others will be able to see the feedback as well as whether or not the system should give reasons for a message or not.
Reasongiving is demonstrated as one of many possible codetermining factors that go beyond just public/private for designing a socially appropriate system.
The second conclusion is that a basis for evaluation, or giving a reason, serves a social more than informational function.
This is evidenced by the results and the logic of the experimental design.
There are consistent findings that a basis for evaluation is particularly appropriate when giving feedback publicly, but it can be more damaging than providing none when in a private context.
This, coupled with an experimental design that employs placebic reasoning, provides strong evidence that people are affected more positively when criticized  in a public setting if given reasons  for the criticism.
Also, people assess others as less conforming when they have the same public experience of criticism as themselves, and again reasons are given.
Because there is no informational component to the reasons, and they are the exact same phrases as in private conditions, we can conclude that the increased comfort of participants derives from a social rather than cognitive role.
Providing a basis for evaluation, even though it is placebic, leads to a greater sense of system competence, M = 55.1, SD = 12.5, than does feedback without any basis, M = 46.4, SD = 16.8.
Finally, analysis of system supportiveness reveals a main effect for basis for evaluation, F = 1.06, p < .04.
As with competence, the presence of a basis for evaluation prompts higher ratings of system supportiveness, M = 13.4, SD = 3.4, than does its absence M = 10.7, SD = 5.0.
Results of the analysis for both system supportiveness and system competence suggest that any system providing feedback is essentially operating in a social manner.
All things being equal, providing even some placebic basis for the feedback is generally more socially appropriate than not.
The results from this experiment indicate that people's attitudinal responses to public and private feedback are differentially affected by the introduction of a basis for the evaluation that they receive.
Varying display  of feedback and basis for evaluation  systematically produces similar patterns of interaction across a variety of attitudinal measures.
Most striking is that for measures of monitoredness , conformity , and self-consciousness , as well as system invasiveness, participants show markedly less negative attitudes where feedback is delivered publicly with a basis for evaluation.
Secondarily, there are two slight variations in the results worth addressing.
First, for monitored  there is an interaction pattern where almost zero numerical difference indicates that delivering feedback in private is not categorically more appropriate than delivering feedback in public, contrary to what might be the intuitive presumption.
In private, however, these social needs would not be present, so the introduction of a basis for evaluation may be more nuisance than necessity.
The third conclusion is that monitoring and assessment of people is normalized, and thus rationalized, when made public and applied uniformly to all participants.
We often presume that being watched and judged are uniformly less palatable than not.
However, our results lend empirical support to the cultural argument that monitoring is rationalized, and thus made acceptable, when being watched and assessed is an apparently uniform process.
Everyone knows everyone is being watched, and with merely some trivial indication of an objective basis for watching and judging, the process is made significantly less spectacular.
Human-computer interaction, an arena for exchange between researchers and technologists concerned with the intersection of people, technologies, and culture, plays a central role in developing modes of investigation, understandings about, and designs for public and private experiences in a networked world.
Andrejevic, M. Reality TV: The work of being watched.
Bellotti, V. and Sellen, A.
Designing for Privacy in Ubiquitous Computing Environments.
Boyle, M., and Greenberg, S. The language of privacy: Learning from video media space analysis and design.
Christians, C. and Carey, J.W.
The logic and aims of qualitative research.
In Stempel, G. and Westley, B.
Research methods in mass communication.
Dourish, P. Culture and control in a media space.
Dourish, P., Harrison, S. Re-place-ing space: The role of place and space in collaborative systems.
Edwards, P. The Closed World: Computers and the politics of discourse in cold war America.
The affordance of media spaces for collaboration.
Geertz, C. Thick description: Toward an interpretive theory of culture.
In The Interpretation of Cultures.
Grudin, J. Groupware and social dynamics: Eight challenges for developers.
Privacy risk models for designing privacy-sensitive ubiquitous computing systems.
Huang, E. M. and Mynatt, E. D. Semi-public displays for small, co-located groups.
Lampe, C., and Johnston, E. Net communities: Follow the  dot: effects of feedback on new members in an online community.
This paper presents the design, analysis, and results of a controlled laboratory experiment in which participants' attitudes and beliefs about themselves and their interaction partners systematically differ as a result of both the condition of feedback display  and basis for evaluation .
From the experiment, we conclude that  the appropriateness of public/private feedback is mediated by a basis for evaluation,  providing a basis for evaluation plays a social role in this mediation, and  feedback in public is normalized through the process of providing even a trivial basis for evaluation.
The results generate a series of follow-up questions such as "would repetition of the experiment in different contexts confirm the results?"
This research also raises a series of methodological points for discussion about how we can braid psychological, social, and cultural approaches to foster interdisciplinary research between practitioners and researchers interested in the role that dimensions such as public/private play in the design of socially appropriate technologies.
These results and questions are relevant to current research contexts dealing with the social and psychological aspects of computing systems that enable collaboration, community, and shared experience.
Consideration of these aspects is evidenced in research on groupware , calendaring , and online communities .
These aspects include notions of membership, participation, and social hierarchy that are deeply affected by the ways in which participants are assessed or evaluated and how these evaluations are made public.
Ultimately, this project is about developing a richer, interdisciplinary language about the distinctions between and variations of the public and private experience made possible by computing technologies.
