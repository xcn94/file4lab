Usability comprises the aspects effectiveness, efficiency, and satisfaction.
The correlations between these aspects are not well understood for complex tasks.
We present data from an experiment where 87 subjects solved 20 information retrieval tasks concerning programming problems.
The correlation between efficiency, as indicated by task completion time, and effectiveness, as indicated by quality of solution, was negligible.
Generally, the correlations among the usability aspects depend in a complex way on the application domain, the user's experience, and the use context.
Going through three years of CHI Proceedings, we find that 11 out of 19 experimental studies involving complex tasks account for only one or two aspects of usability.
When these studies make claims concerning overall usability, they rely on risky assumptions about correlations between usability aspects.
Unless domain specific studies suggest otherwise, effectiveness, efficiency, and satisfaction should be considered independent aspect of usability and all be included in usability testing.
Effectiveness, which is the accuracy and completeness with which users achieve certain goals.
Indicators of effectiveness include quality of solution and error rates.
In this study, w e use quality of solution as the primary indicator of effectiveness, i.e.
Efficiency, which is the relation between  the accuracy and completeness with which users achieve certain goals and  the resources expended in achieving them.
Indicators of efficiency include task completion time and learning time.
In this study, we use task completion time as the primary indicator of efficiency.
Satisfaction , which is the users' comfort with and positive attitudes towards the use of the system.
Users' satisfaction can be measured by attitude rating scales such as SUMI .
In this study, we use preference as the primary indicator of satisfaction.
Although the importance of usability is gaining widespread recognition, considerable confusion exists over the actual meaning of the term.
Sometimes usability is defined quite narrowly and distinguished from, for example, utility , on other occasions usability is defined as a broad concept synonymous to quality in use .
We adopt ISO's broad definition of usability  as consisting of three distinct aspects:
While it is tempting to assume simple, general relations between effectiveness, efficiency, and satisfaction, any relations between them seem to depend on a range of issues such as application domain, use context, user experience, and task complexity.
For routine tasks good performance depends on the efficient, well-trained execution of a sequence of actions which is known to yield stable, high-quality results .
For such tasks high-quality results are routinely achieved, and task completion time may therefore be used as an indicator of overall usabilit y.
The efficient execution of the sequence of actions is of secondary importance.
Consequently, efficient execution of the actions may or may not lead to high-quality results, and diligence is not even guaranteed to lead to task completion.
This suggests that, at least for complex tasks, efficiency measures are useless as indicators of usability unless effectiveness is controlled.
Nielsen & Levy  analyzed the relation between efficiency and user preference in 113 cases extracted from 57 HCI studies.
Their general finding was that preference predicts efficiency quite well.
However, in 25% of the cases the users did not prefer the system they were more efficient in using.
The ambition of finding a simple, general relationship between efficiency and satisfaction is therefore questionable .
Studies of, for example, specific application domains may yield more precise and informative models.
With respect to the relationship between satisfaction and effectiveness, Nielsen & Levy  note that their very comprehensive literature survey did not encounter a single study that compared indicators of these two aspects of usability.
In this paper we investigate the connection between efficiency, indicated by task completion time, and effectiveness, indicated by quality of solution.
This is done by reanalyzing data from the TeSS-experiment  where 87 subjects solved a number of information retrieval tasks, using four different modes of the TeSS system and programming manuals in hard copy.
In analyzing the data we look for correlations between efficiency and effectiveness across retrieval modes, tasks, and individual subjects.
The purpose of this paper is to emphasize the importance of accounting for all three aspects of usability in studies that assess system usability, for example to compare the usability of different designs.
Effectiveness is often difficult to measure in a robust way.
This may be the reason why several studies involving complex tasks refrain from accounting for effectiveness and settle for measures of the efficiency of the interaction process .
These studies rest on the assumption that an efficient interaction process indicates that the user also performed well in terms of crucial effectiveness indicators such as solution quality.
The TeSS-experiment illustrates that this assumption is not warranted--unless it can be supported by an argument that effectiveness is controlled.
The first two sections present the method and results from the TeSS-experiment, establishing the argument that efficiency and effectiveness are weakly--if at all-- correlated.
Next, we discuss the general relationship between the three aspects of usability, exemplifying the impact of our findings by studies from the CHI Proceedings of the years 1997-99.
We then discuss the implications of our findings with regard to the selection of usability measures.
In the final section, we outline our main conclusions concerning the weak and context -dependent relation between the usability aspects.
To solve the tasks the subjects needed information concerning the development of graphical user interfaces in the X Window System.
Access to the necessary documentation  was provided through an experimental text retrieval system called TeSS and by means of manuals in hard copy.
TeSS can be operated in four different modes, each providing the user with a different set of retrieval facilities.
Thus, the experiment involves five retrieval modes: * BROWSE.
In TeSS, browsing can be done by expanding and collapsing entries in the table of contents and by searching the table of contents for specific strings.
The text itself is presented in separate windows.
A mode of TeSS offering conventional Boolean retrieval where queries are logical expressions built of query terms, ANDs, ORs, NOTs, parentheses, and wildcards.
In this mode of TeSS queries are expressed by means of a Venn diagram which replaces Boolean operators with a, supposedly, more immediately understandable graphical image of intersecting sets.
The whole of TeSS offering the combination of BROWSE, LOGICAL, and VENN.
In this mode searching is done in hard copies of the programming manuals, i.e.
The subjects were 87 students in their third year of a bachelor degree in computer science.
While the project was a mandatory part of the students' education, participation in the experiment by allowing the data collection to take place was voluntary and anonymous.
The subjects were first-time users of TeSS and had no prior knowledge of the programming tools on which the tasks were based.
In the TeSS-experiment each subject solved 20 information retrieval tasks.
As preparation, the subject completed two practice tasks.
The 20 tasks concerned whether and how certain interface properties could be achieved in a graphical user interface.
To answer the tasks the subjects had to identify the relevant user interface objects, e.g.
As the subjects were unfamiliar with the X Window System, the tasks involved a substantial element of learning in addition to the need for retrieving specific pieces of information.
Radio buttons are used in situations where exactly one option must be chosen from a group of options.
Which widget class is used to implement radio buttons?
The caption on the button "done" should be changed to "quit".
The experiment was explained to the subjects at a lecture, after which the subjects had ten days to complete the tasks.
The subjects received a manual for TeSS and a two page walk-up-and-use introduction.
The system itself was available on terminals to which students have access 24 hours a day.
The manual searching was done in the library where one of the authors was present three hours a day to hand out tasks and receive solutions.
Upon entering the library, the subjects received hard copies of the three manuals, a sheet with the proper task, and a log sheet with fields for starting time, finishing time, and solution.
The experiment employed a within-groups design where all subjects solved the tasks in the same sequence and each subject was required to use all retrieval modes.
To avoid order effects, the subjects were exposed to the retrieval modes in a systematically varied order.
The 20 information retrieval tasks were clustered into five blocks.
The first block was solved with one of the five retrieval modes, the second block with one of the remaining four retrieval modes.
Thus the permutations of the modes on the two first blocks divided the subjects into 20 groups.
The number of subjects did not allow all 5!
Rather, the order of the three remaining modes was kept the same within each group.
Finally, 4 subjects were excluded because they clearly did not take the experiment seriously.
Thus, 11% of the answers were not submitted or excluded.
The analysis is based on the remaining 1555 answers, the results of 648 hours of work performed by 83 subjects.
In this paper we focus on two aspects of the usability of TeSS: * * Efficiency measured as task completion time, which is extracted from the interaction log or the log sheets.
Effectiveness measured as the quality of the solution, which was assessed by one of the authors and expressed by a grade on a five-point scale, see Table 1.
As an example, a medium and a high quality solution to task 5  must identify toggle widgets as the relevant widget class.
A brilliant answer also explains the use of radio groups to cluster the toggle widgets.
The data collected in the experiment include a detailed log of the subjects' interaction with TeSS.
It also includes task demarcation and solutions reached, both obtained from a separate module governing the subjects' access to TeSS.
This Task Handling Module makes it possible to let the subjects work unsupervised while at the same time enforcing a strict experimental procedure.
The Task Handling Module presents the tasks to the subject one at a time, gives access to the retrieval mode to be used by that subject when solving that particular task, and records his or her solution.
For the PAPER retrieval mode, the subjects recorded their starting time, finishing time, and task solution on the log sheets.
The 87 subjects received 20 information retrieval tasks each, giving a potential total of 1740 answers.
The following analysis is restricted to the 20 information retrieval tasks--the bulk of our data.
Data concerning user satisfaction, measured as subjects' preference for one or the other retrieval mode, were collected for three implementation tasks, which followed the information retrieval tasks.
The preference data show that the subjects did not prefer the retrieval mo de with which they performed best.
Rather, they overwhelmingly preferred ALL, the retrieval mode where they did not exclude themselves from any of the search facilities available in BROWSE, BOOLEAN, or VENN .
This suggests that user satisfaction is not simply correlated with performance measures such as task completion time and grade.
Thus, the TeSS-experiment was another exception to the general finding of Nielsen & Levy  that users prefer the objectively best system.
Table 3--Correlation between time and grade in different retrieval modes.
The first column shows the retrieval modes, and the second and third columns the mean time in minutes and the median grade for each mode.
Task completion time for subjects receiving a certain grade varies much, as can be seen from the large standard deviations in Table 2.
An analysis of variance shows significant variation in task completion times between different grades .
However, we did not find any pairwise differences between grades using Tukey's post hoc test at a five-percent significance level.
The tasks in any of the five intervals of task completion times shown in Table 2 received markedly different grades.
Between time intervals there is significant variation in grades .
Pairwise comparisons of the five time intervals using Tukey's post hoc test show that the 20% fastest solved task receive significantly higher grades than the 60% slowest solved tasks.
Similarly, solutions to tasks in the P20-P40 time interval receive significantly higher grades than solutions in the time intervals P60-P80 and >P80.
Spearman's rank order correlation analysis shows that task completion time and grade are significantly correlated in tasks solved in the TeSS-experiment .
Using more time for completing a task is thus correlated with receiving a lower grade.
Table 2--Distribution of task completion time and grade for all tasks in the TeSS-experiment .
The column to the left shows the five grades given to the tasks, cf.
The next columns show the number of tasks in each of five intervals based on the 20, 40, 60, and 80 percentiles of task completion time.
The rightmost column shows the mean time in minutes for a certain grade and, in parentheses, the standard deviation.
According to  a correlation of this magnitude is negligible.
To control for interplay between the design of the experiment and the weak correlation found, we performed a partial correlation analysis of the TeSS data.
In the partial correlation analysis, the influence from different tasks and retrieval modes is removed from the correlation coefficient between time and grade .
This analysis also reveals a weak but statistically significant correlation between task completion time and grade .
These analyses show that at the general level efficiency and effectiveness are only weakly correlated.
In spite of this, time and grade could be correlated at a more detailed level of analysis, hereby undermining the conclusion at the general level.
In the following sections we therefore analyze whether time and grade are correlated for specific retrieval modes, tasks, or subjects.
The retrieval modes LOGICAL and VENN--the only retrieval modes requiring the subjects to formulate queries--do not show a significant correlation between time and grade .
The retrieval modes BROWSE, ALL, and PAPER all show a statistically significant but weak correlation between task completion time and grade .
The tasks solved in the retrieval mode PAPER have a numerically larger correlation between time and grade than the other retrieval modes.
Figure 1--Correlation between time and grade for different tasks.
The figure shows Spearman's correlation coefficient  for each of the 20 information retrieval tasks.
Each task has been solved by between 69 and 81 subjects.
Time and grade are significantly correlated for tasks 11, 13, and 17.
These tasks appear as squares in the figure.
Figure 2--Average time and grade for each of the 83 subjects included in the data analysis.
The horizontal line indicates the overall mean grade , the vertical line the overall mean time .
Subjects with a significant correlation between time and grade appear as squares, other subjects appear as triangles.
These correlation coefficients are all negative, suggesting that more time spent is correlated with lower grade .
For 86% of the subjects, time does not predict grade at all.
It is difficult to find a common denominator for the subjects where time and grade are correlated.
The average time and grade of those subjects vary above and below the mean time and grade for subjects .
However, there is a significant difference between the grade for subjects with a significant correlation between time and grade and those without .
The correlation between task completion time and grade varies somewhat across the tasks .
For 85% of the tasks there is no correlation between time and grade.
For these tasks between 5% and 15% of the variation in grade can be predicted from time, where more time spent is correlated with lower grade.
The description of these tasks given to the subjects specifies in detail some of the central interface objects of the tasks .
For task 17 it is only the relation between time and grade that is significant, individually neither time nor grade differs significantly from the other tasks.
Our analysis of the TeSS-experiment shows that efficiency  and effectiveness  are either not correlated or correlated so weakly that the correlation is negligible for all practical purposes.
For the individual retrieval modes, a weak correlation is found for three of the modes, while two of the modes do not show any significant correlation between task completion time and grade.
Task completion time and grade are not correlated for 85% of the tasks.
Finally, only 14% of the subjects display a significant correlation between time and grade--for the large majority no correlation is found.
One interpretation of our results is that users are more attuned to qualitative aspects of the interaction."
The number of automatic speech recognition rejects contributes the most to user satisfaction.
This result was contrary to the authors' initial hypothesis and illustrates the importance of measuring efficiency, effectiveness, and satisfaction independently, as opposed to basing conclusions about one of them on measures of the others.
We now extend the discussion of correlations between aspects of usability by including studies of computer support for complex tasks published in the CHI Proceedings for the years 1997-99.
A total of 19 studies investigate aspects of usability in sufficient detail to enable an analysis of their choice of usability measures, see Figure 3.
Eight  of the 19 studies cover all three usability aspects.
The other 11 studies, implicitly or explicitly, rely on assumptions of correlations between the different usability aspects, or seem confident that their choice of only one or two aspects of usability is sufficient to capture overall usability.
Two CHI-studies concerning computer support for complex tasks, entitled "Time-compression: systems concerns, usage, and benefits"  and "Effects of awareness support on groupware usability" , do not include any measure of the quality of the outcome of the users' interaction with the system.
Below we comment on these two studies, and show how their conclusions about overall usability are jeopardized by their incomplete choice of usability measures.
In the first study, Omoigui et al.
An experimental time -compression system was used for comparing different granularities of the time -compression  and differences in the latency  experienced by users after adjusting the degree of time-compression.
As already mentioned, no effectiveness measures we re employed, although effectiveness could have been measured as the accuracy and completeness of the subjects' verbal summary of each video.
In the concluding remarks, Omoigui et al.
This conclusion neglects the satisfaction measures, which indicate that real differences might exist between the experimental conditions: "... several subjects commented in post-study debriefing that the long latency and discrete granularity conditions had affected their use of the time compression feature.
The subjects felt that they made fewer adjustments and watched at a lower compression rate when long latency and discrete granularity were used."
An analysis of the correlations between the efficiency and satisfaction measures might have shed further light on the differences between conditions, as might solid measures of effectiveness.
Of the eight studies including measures of all three usability aspects, only the study by Walker et al.
Let us summarize their study, so the reader can see that the correlation analysis pays off.
The study measures effectiveness by qualitative measures such as automatic speech recognition rejects, efficiency by number of dialogue turns and task completion time, and user satisfaction by a multiple -choice survey.
The results show that even though the mixed-initiative dialogue is more efficient, as measured by task completion time and number of turns, users prefer the system-initiative dialogue.
In the second study, Gutwin and Greenberg  analyze whether enhanced support for workspace awareness improves collaboration.
In an experiment, they compare users' performance on two real-time groupware systems where workspace miniatures were used to support workspace awareness.
The basic miniature shows information only about the local user, the enhanced miniature about others in the workspace as well.
Efficiency is measured by task completion time and communication efficiency; satisfaction is measured as preference for one or the other system.
The correlations between the measures are not analyzed, and no measure of effectiveness is employed.
The overall conclusion of the study is that workspace-awareness information reduces task completion time, and increases communicative efficiency and user satisfaction.
The support for this conclusion is weak.
For one out of the three task types, task completion time was not reduced.
For two task types out of the three, the communicative efficiency was not increased.
All 38 participants preferred the awareness-enhanced system, suggesting that the employed measures of usability are incomplete: "The overwhelming preference for the interface with the added awareness information also suggests that there were real differences in the experience of using the system, but that our measures were insensitive to these differences."
These differences might have been more explainable if the study had included measures of effectiveness, making possible an analysis of how users' preferences were affected by the quality of the outcome of their activities.
This requires a firm understanding of how tasks, users, and technology interact in constituting the use situations within the particular application domain .
The study by Su  is an illustrative example of the kind of work needed to distinguish and refine performance measures.
Su investigated the correlation between 20 measures of information retrieval performance in an academic setting, and suggests a best single measure  and best pairs of measures of information retrieval performance.
Such work may lead to the development of reliable, domain -specific collections of critical performance measures.
Third, effectiveness measures oriented toward the outcome of the user's interaction with the system are gaining attention in usability evaluation , although two of the CHI-studies discussed earlier did not include such measures.
The development of valid and reliable outcome measures i s a prerequisite for assessing overall system usability and is necessary for working systematically with improving the usability of systems supporting users in solving complex tasks.
We believe that the weak correlation between effectiveness, efficiency, and satisfaction has three implications regarding the choice of measures in evaluations of system usability.
First, it is in general recommendable to measure efficiency, effectiveness as well as satisfaction.
When researchers or developers use a narrower selection of usability measures for evaluating a system they either  make some implicit or explicit assumptions about relations between usability measures i n the specific context, or  run the risk of ignoring important aspects of usability.
In our analysis of the CHI-studies we have shown how interpretation of experimental data based on only one or two usability aspects leads to unreliable conclusions about overall usability.
Given that the three usability aspects capture different constituents of usability--we have not seen arguments to the contrary for complex tasks --there is no substitute for including all three aspects in usability evaluations.
Second, at the moment no clear-cut advice can be given about which usability measures to use in a particular situation.
The relations between efficiency, effectiveness, and satisfaction--the three aspects of usability--are not well understood.
We have analyzed data from a study of information retrieval and found only a weak correlation between measures of the three usability aspects.
Other studies imply that for complex tasks in other domains, a similarly weak correlation between usability measures is to be expected.
In general, we suggest that efficiency, effectiveness, and satisfaction should be considered independent aspects of usability, unless domain specific studies suggest otherwise.
Studies that employ measures of only a subset of the three usability aspects assume either that this subset is sufficient as an indicator of overall usability or that the selected measures are correlated with measures covering the other aspects of usability.
As we have exemplified with an analysis of studies from previous CHI Proceedings, such assumptions are often unsupported.
Hence, these studies jump to conclusions regarding overall usability while measuring, say, efficiency only.
This is a problem for the HCI community, since more than half of the last three years of CHI-studies concerning complex tasks do not measure all aspects of usability.
Usability testing of computer systems for complex tasks should include measures of efficiency, effectiveness, and user satisfaction.
Morten Hertzum was supported by a grant from the Danish National Research Foundation.
The design and implementation of TeSS as well as the design and execution of the experiment were done in collaboration with Jette Brolos, Marta Larusdottir, Kristian Pilgaard, and Flemming Sorensen.
We wish to thank the students who participated in the experiment as subjects, and the CHI-reviewers.
We are indebted to Per Settergren Sorensen for his support on statistical issues and to Peter Naur for many judicious proposals for clarification.
Performance vs. preference, in Proceedings of the Human Factors and Ergonomics Society 37th Annual Meeting , HFES, 282-285.
Bevan, N. Measuring usability as quality of use.
The keystrokelevel model for user performance time with interactive systems.
Cohen, J. and Cohen, P. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences.
Lawrence Erlbaum Associates, Hillsdale NJ, 1975.
Gutwin, C. and Greenberg, S. Effects of awareness support on groupware usability, in Proceedings of CHI '98 , ACM Press, 511-518.
Hertzum, M. and Frokjaer, E. Browsing and querying in online documentation: A study of user interfaces and the interaction process.
Ergonomic requirements for office work with visual display terminals  - Part 11: Guidance on usability .
