This may include considerations like the expert's availability, interests, and previous performance with similar tasks.
It has been suggested that well-informed expertise selection directly influences the effectiveness of finding an appropriate expert .
Despite the importance of this stage, with few exceptions , expertise selection has largely remained an unexplored topic within HCI.
In this study, we address this gap by demonstrating that additional information revealed about experts leads to quicker and better-informed expertise selection and identifying which information may be most useful for various expertise selection tasks.
Our investigation was driven by two research questions: 1.
How does additional information about people surfaced in an expertise location system affect selection?
What information is most useful for people to select the right people for different, common expertise selection tasks?
Expertise selection is the process of choosing an expert from a list of recommended people.
This is an important and nuanced step in expertise location that has not received a great deal of attention.
Through a lab-based, controlled investigation with 35 enterprise workers, we found that presenting additional information about each recommended person in a search result list led participants to make quicker and better-informed selections.
We also asked participants to rate the type of information that might be most useful for expertise selection on a paper prototype containing 36 types of potentially helpful information.
We identified sixteen types of this information that may be most useful for various expertise selection tasks.
These results focus attention on a currently understudied aspect of expertise location--expertise selection--that could greatly improve the usefulness of supporting systems.
Expertise location plays a critical role in the workplace, leading to better project outcomes through a greater diversity of ideas and flow of information between company divisions .
The process of finding and consulting people with desired expertise is only partially understood, since most research efforts have focused on the first step: finding appropriate expertise.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
To answer these questions, we extended an existing expertise location system deployed within IBM, by revealing additional information about each person recommended by the original system.
We conducted a comparative study, involving 35 participants who used the existing system and our augmented prototype for two expertise location tasks typical to their jobs.
Since our augmented prototype could not include all possibly useful information about the people, we then conducted a second procedure.
We asked each participant to respond to a paper prototype with 36 types of potentially useful information, specifying which information would be most useful in supporting expertise selection in their work.
The main contributions of this paper are twofold:  an empirical demonstration of the benefits of revealing additional information about people in a recommender system's search results, and  identifying which information about people is most useful to the seeker for a set of typical expertise selection tasks.
These results focus attention on a currently understudied aspect of expertise location--expertise selection--that could greatly improve the usefulness of supporting systems.
In the rest of the paper, we start by presenting related work on finding help in the workplace.
We then give a brief description of the existing expertise location system used in our experiment and explain the augmentations made for our study.
We then describe our participants and study design.
Finally, we present our findings and discuss their implications for recommender system design.
Expertise selection is currently a difficult task because choosing a person to contact is an inherently nuanced and context-sensitive problem .
Expertise location systems do not support the task well, since it is often difficult to explain why a particular person is suggested, which is key to a user's ability to evaluate the recommendation .
Furthermore, existing systems have no concept of what the user's goals are in selecting a person in the first place.
As Yimam-Seid and Kobsa  pointed out, many expertise location systems lack the support "to rank and contrast experts using different user-selected criteria ".
Although there is little work on expertise selection, two related pieces of research did explicitly look at how users evaluate the recommendations provided by an expertise location system.
Shami  provided participants with 10 potential experts, each of which had a photo, company division, job title, and any common social connections with the participant.
The participants were asked to choose experts that they would like to know better.
The study found that rank order in the search results and presence of common social connections increased the likelihood that a user would select an expert's profile.
In a related study, Shami et al.
Among the most important factors were the expert's social connections, use of social software such as forums, and self-described expertise.
We build on these two studies by considering what information may be most important in selecting an appropriate expert.
However, we significantly expand on this work by providing a controlled comparison between systems that show differing amounts of information and by evaluating the quality of the selection results for a variety of expertise selection tasks.
Furthermore, we offer insights into additional information beyond what was identified previously, which may be useful for expertise selection.
HCI has a rich history of exploring expertise recommender systems.
In general, a user enters one or more search terms and a recommender system returns a list of people whose attributes match with the user's terms.
Sometimes the system allows the user to filter based on additional criteria, such as location or social connections .
These systems can be distinguished by the different sources of technical and social data used to provide recommendations.
Sources explored in the past include an employee's previous source code contributions , user-volunteered files and communications , social network information , and combinations of technical and social criteria .
In general, all of these systems have been concerned with presenting the right list of people, but little thought has been given to the process users must take to select an appropriate person from the list.
In this work, we focus on expertise selection, which is an important aspect of deciding among the results returned by an expertise recommender system.
Nevertheless, this step may not be needed in all help-finding systems.
Depending on the application, one type of expertise location system automatically matches a user's request to an appropriate expert.
One example is the IM-an-Expert system, which automatically sends a user's question out to several possible experts who are willing to receive questions and are currently online .
Another approach involves letting experts self-select.
For example, a user may post a question in his or her status message on internal social networking sites  or post a question to an online forum or Q&A site such as Stack Overflow .
This allows experts who are qualified and available to pick which questions to answer, rather then expecting the user to select an appropriate expert.
While these systems provide effective ways of avoiding the problems of expertise selection, they work best for straightforward tasks that could be easily carried out with minimal expert involvement.
However, they are less suited to forming long-term collaborations or fostering relationships with experts in complex situations.
For example, a customer engagement may require specific experience beyond general expertise, which only one or a very small group of experts posses; a contract negotiation may require the involvement of experts who not only have technical expertise but also have decision-making power to be effective in the nuanced context.
For these reasons, Reichling & Veith recommend that "push" and "pull" help finding systems coexist to complement each other in large companies .
We therefore pursue expertise selection as a topic of investigation on the assumption that certain help-finding problems require users to make a choice from a list of potential experts.
We used an existing expertise recommender system, called SmallBlue Find as the control system for our study.
SmallBlue Find is one of four expertise recommender systems deployed internally within IBM.
We selected this tool for our study because it performed best during our trial use of all four systems.
We also extended SmallBlue Find to create the experimental condition system.
Given one or more keywords, SmallBlue Find searches the social network within an enterprise and uses multiple sources of information, to recommend matched experts .
These sources of information include both public information within the company's intranet, such as forum posts, publicly shared presentations, and user-defined profiles, as well as personal communications, including emails and chats.
Personal communications were collected from about 15,000 volunteer IBM employees.
While the volunteers represent only a small portion of the entire employee population, through their contact with non-volunteers, the system has information about almost all IBM employees.
SmallBlue Find also uses these information sources to recommend introduction paths to each expert .
Like most other expertise recommender systems, SmallBlue Find returns a sorted list of matched experts on a particular topic.
The result list presents each expert's name, photo, company division, job title, and a link for more information .
This link leads to a separate page where the user can find additional information, such as contact information, a recommended social path to the expert, and a list of most recent publicly shared documents.
We presented SmallBlue Find to participants in our lab study as the control condition.
One of our research goals was to evaluate how presenting extra information in the results of an expertise recommender system could support expertise selection.
Thus, we extended SmallBlue Find to provide users with additional information about each expert.
Findings from previous studies have suggested several types of information that may facilitate expertise selection, including an expert's social connections, his or her use of social software , and his or her selfdescribed expertise on a specific topic ; location information ; and status message as a proxy for availability .
In our experimental system, we thus parsed a number of company internal data sources and LinkedIn, and extracted the following information to describe each recommended expert : * * * Current status message on the company's internal social networking site.
Link to profile page in the corporate directory.
Self-defined expertise summary from corporate directory profile page.
Time working in current position at the company.
Total connections on LinkedIn, degrees of separation from the study participant, and any shared connections.
Link to all of the expert's publically shared documents from any one of 22 different sources, including groups, forums, bookmarks, papers, patents, etc., gathered from an internal enterprise social search service .
All 35 participants were IBM employees.
Table 1 describes each participant's job role and how long they had been with the company.
Since the lab study needed to be conducted in person, all of the participants were from one of two local branches of the company.
Our goal was to include a diversity of job roles, especially those that often require working across the company, such as project management, business development, HR, and communications.
To meet our goal, we selectively recruited people in each role through our personal contacts.
Moreover, we tried to include participants with varied levels of experience within the company.
In addition to using our personal contacts, we also set up an information table outside the cafeteria at one of the sites to recruit people whom we may not have otherwise reached.
Overall, 25 of the participants were recruited through personal contacts and 10 signed up at our cafeteria table.
Despite our efforts, the participant population presents potential limitations of this study.
First, since all participants were from the same company, our results may be skewed by specific properties of the organization, such as its nature of business and culture.
For example, a number of professions, such as healthcare and legal, where expertise location may be critical, were not represented here1.
Second, our participants may have been more technically savvy than the average business worker, since they are all employees of a technology company.
Note that not all information was available for each expert .
We displayed only the information that was available.
We used a GreaseMonkey script to inject the additional information as a table directly below each recommended expert in the result list produced by SmallBlue Find.
Figure 1 shows two entries in our extended SmallBlue Find system.
Our approach allowed us to keep the interface consistent between the control and experimental conditions, with the exception of the single variable of interest: the presence of additional information about each expert in the result list.
Building on the insight of Terveen and McDonald , who noted that very little expertise finding work is evaluated in the context of real participant goals, it was important for us to ground our study scenarios by the typical needs of the participants.
As the first step, we asked each person to keep a two-week diary of his or her expertise location needs in the workplace.
We collected and analyzed 248 diaries to summarize different types of expertise location tasks and topics.2 Here, a task describes the overall goal of seeking an expert, such as to get help gathering information or solving a problem.
A topic specifies the subject matter content that the sought expert is supposed to provide, such as information about a specific group or a domain.
We followed up with each participant in a 30-minute interview to examine our classification of each of their diaries and to correct any misclassifications.
The results were 8 tasks and 5 topics covering all of the expertise location needs our participants had recorded in their diaries.
We then used these tasks and topics to tailor the lab study scenarios for each participant.
This empirical gounding lends considerable realism to our procedure.
We conducted a within-subjects lab study with 35 participants to compare their performance on expertise selection tasks using the control and experimental systems.
We also gathered subjective feedback on the types of information participants wanted when performing expertise selection tasks typical to their jobs.
For the lab study, we included scenarios for all but one of the expertise location tasks identified in the diary study : * Collaborate - make contact with an expert in order to work together or initiate a longer-term collaboration.
Our lab study scenarios involved three topics out of the five found in the diary study3, which could be supported by our experimental systems: * * * Logistics - seeking a person who can help with a specific logistical issue or internal process.
Group - seeking an expert who knows about a specific group, project, department, or company.
Domain Knowledge - seeking an expert with specific domain knowledge, such as of a scientific field , specialized technology , or method 
We then combined one search term and one intended task-topic crossing to generate a scenario description for each participant.
Here are two example scenarios, showing how one topic, described by the search term "Fortran," was written differently for two different tasks:
You want to find somebody who might be able to help solve this bug."
Each participant was invited into a conference room for the study.
The participant was assigned two scenarios, one using the experimental system, the other using the control system.
The order of conditions was balanced to avoid order effects.
All sessions were video recorded.
For each scenario, the participant was allotted 10 minutes and asked to select the "best person" to help from the list returned by SmallBlue Find.
The participants were told that they could also use any other online resource to help judge which expert was the best .
After the participant decided on a particular expert, we asked them to explain why they chose this person.
The information we added to the experimental system was influenced by suggestions from prior research, but also by the type of information we could readily access online.
Nonetheless, we also wanted to learn what kind of information might be useful in expertise selection without the latter constraint.
Therefore, after completing the scenarios, each participant was given a paper prototype that provided them with a list of experts similar to Figure 1.
However, in this list an expert was associated with 36 pieces of information, instead of the 4-10 pieces of information shown by our experimental system.
This allowed us to get feedback on the usefulness of information that may not be currently available or may be particularly hard to extract using today's technology.
Table 2, first column, lists all of the information types included in our paper prototype.
Each participant was asked to rate the usefulness of each information type on a scale of 1 to 5, where 1 was least useful and 5 was most useful in addressing the task-topic crossing in his or her two scenarios .
Finally, the participant was asked to respond to the paper prototype again, this time thinking about all of the possible task-topic crossings they might experience in their work.
Each participant was provided with a summary of their diaries to remind them about the kind of task-topic crossings they had faced in the past.
Our next goal was to understand the quality of the experts selected by the participants in each condition.
For each participant, we created two isomorphic scenarios that fit into a single task-topic crossing.
We chose to keep task-topic crossings the same for a single participant's two conditions to avoid introducing an additional variable within subjects.
Task-topic crossings were chosen using the following criteria: 1.
The specific task-topic crossing must have appeared at least once in the participant's diaries.
Each task-topic crossing that appeared in the diaries must have at least two participants.
If the above conditions are met but there are still multiple task-topic crossings for a participant, choose the one that appeared most frequently in their diaries.
The task-topic crossings for each participant are listed in Table 1.
To generate a scenario for each participant, we first decided on a topic by crafting two to three search terms, such as "Fortran" , "Smarter Cities Initiative" , or "Smarter Commerce Global Summit" .
The information presented on the paper prototype, rated for usefulness to expertise selection .
The GEN column is the average usefulness reported on the paper prototype filled out by each participant regarding their expertise location problems overall.
Each of the other columns corresponds to the averages per type of task or topic.
The top 10 most useful pieces of information for each category  are shaded.
They were asked three questions related to each scenario and given the answer options of "likely," "somewhat likely," and "unlikely" to each question.
They were also told that the scenarios were hypothetical and part of a research study.
How likely would you have responded to them had they sent you an email about this topic?
How likely is it that you are the right person to ask about this?
Each of the 46 selected experts received between 1 and 8 such questionnaires, depending on how many participants chose them as the best expert.
42 experts  replied with answers to the questionnaire.
Though we were asking experts to evaluate their expertise through self-report, it did provide an indication as to whether the participant's selection was appropriate.
I looked at his connections on LinkedIn and he had 260 and so I figured he knew a lot of people outside of  ... he's in Minnesota so only a 2-hour differential in time.
He's been in the position for at least a year and 8 months ... so that's why I went with him."
We compared the time it took each of 31 participants4 to select an expert between the experimental and control conditions.
The completion time in the experimental condition was 26%  faster on average than the completion time in the control condition, a statistically significant difference .
Thus, more information about experts helps seekers make faster people selection.
In order to understand whether the experimental condition led to the selection of better quality experts, we compared expert questionnaire responses between the experimental and control conditions.
Since 42 out of 46 selected experts replied to our survey, we were able to analyze expert quality data for 32 of the participants.
We found non-statistically significant evidence that the experimental condition led to better quality choices: on average, experts selected in the experimental condition were more likely to respond to the inquiry, more likely to consider themselves an appropriate expert to address the query, and more likely to be able to introduce the participant to others who may be able to help with the inquiry .
Note that while none of these measures reach the level of significance allowing us to reject the null hypothesis, the results do provide evidence that additional information in the search results may be the factor responsible for improved performance.
Since the number of participants in this study was low for a quantitative investigation, more conclusive results might be seen in a larger investigation.
We combine these three measures into one that describes whether or not the expert was helpful.
We define the concept of a "strikeout," where the contacted expert is unlikely to answer, or is neither an appropriate expert nor can introduce the participant to an appropriate expert.
In these situations, both the information seeker and expert's time would be wasted.
In this study, 9 out of the 32 control condition selections , compared to only 3  of the experimental condition selections were "strikeouts."
While we did not find that the difference on any single question was statistically significantly better for the experimental than the control condition at the 0.05 level, these results provide strong evidence that the experimental condition led to better selections.
This was coded to cite 4 sources of information : location, expertise summary, LinkedIn connections, and time in position.
Most participants looked at more information in the experimental condition and cited a statistically significant, greater number of sources when explaining their decisions .
This is particularly remarkable given that all of the information was available in both conditions but appeared in different places.
In the experimental system the cited information appeared in the result page, while in the control system, this information may have required additional navigation to find .
Overall, these results show that additional information about experts in the result page leads to significantly faster, better informed expertise selection, with better quality experts chosen.
We compared the strategies participants took to select an expert.
To do so, we observed participants interacting with the study systems and also asked them to explain how they arrived at their expertise selection decisions.
In both conditions, strategies generally involved narrowing down the search results, gathering information about the smaller intermediary list, and then picking an expert from the smaller list.
However, the criteria for this process were more arbitrary in the control condition:
Differences between the control and experimental conditions on measures of task completion time, three expert evaluation questions, and the number of sources of information considered in the decision.
Interval data was compared using a paired 2-tailed t-test; ordinal data was compared using a rank sum Wilcoxon test .
In the control condition, at least 8 participants placed a lot of emphasis on finding an expert in a similar division or line of work, for example, the MBA intern said he would most prefer to talk to somebody in marketing, the communications specialist preferred another external relations person, and six researchers explicitly stated that they gave strong preference to others who were in research.
Though the majority of participants tried to find additional information about each expert on internal and external web sites, several expressed their hesitance to do so in the control condition:
The other site broke down more details on the first page, instead of actually having to go find the details.
So, it's just one step less."
To understand what kinds of information may be most useful for expertise selection beyond current constraints , we asked participants to rate potentially useful pieces of information on a paper prototype.
Table 2 shows the average rating of each piece of information in terms of what would be useful for expertise selection in the participant's work overall and the average rating of each piece of information in terms of a specific task or topic.
Note that the GEN column averages responses from all 35 participants, while the specific task or topic columns average between 2 and 16 responses, depending on the number of participants assigned to that task or topic.
In the experimental condition, participants on average decided on an expert more quickly .
We noticed three differences in how participants went about making this decision.
First, because all the information was on the same page, rather than requiring additional navigation, participant were able to more quickly scan through the expertise summaries and status messages for matched search terms.
Second, several participants used the browser find function to search for specific keywords within the results page, which quickly highlighted some experts to examine first.
Lastly, having all the information on the same page, reduced the effort to find each piece of evidence for selecting an expert and allowed for easier comparison between suggested experts.
This helped participants to consider and cite more sources of evidence for their choice without spending more time to identify this information.
Participants found it more difficult to evaluate recommendations in the control condition.
In the control condition, four participants explicitly commented on this difficulty because the model used by the expertise recommender was not transparent:
This list should be considered cautiously, since there was a great deal of individual difference in the ratings of any particular item .
In fact, for every item on the list, at least one person rated it as very useful  and at least one person rated it as not useful at all , so there was no universal agreement as to which items were absolutely necessary or absolutely useless.
Furthermore, the type of information that was rated most useful for expertise selection varied by task and topic.
Most of this information is feasible to collect automatically.
Five of these items, time at current position, company division, job title, public documents, and work location, are often available in the internal employee profile pages, company directory, or intranet.
It is also relatively straightforward to gather and keep such information up to date.
Additionally, some companies may already have searchable archives of publicly shared documents related to the search term.
Our findings confirm that such archives are useful.
Another 4 items, top 5 tags, expertise, social network status message, and job responsibilities currently exist in multiple data sources, but may be challenging for a system to automatically extract and maintain.
Ongoing investigations in HCI such as semi-automated profile generation  and gamification of social tagging  may help maintain a complete and accurate listing of these items.
Our results further motivate such research.
Obtaining the remaining information would require the support of advanced information systems that may be beyond what is currently available.
It may be fruitful for future research in data mining and social analytics to investigate obtaining information such as approachability ratings, ratings of answer quality in general and for specific searches, availability for a 1 hour meeting, past experiences, and past/current activities.
Our results show distinct advantages to including additional information about each expert in the recommender's search result page.
Expertise selection took less time in the experimental condition because more information was readily available without any additional navigation or searching.
In addition to scanning, this enabled users to perform an inpage keyword search across all experts on a page to quickly find relevant information.
Our data also provides insight into why the experimental condition led to better expert selections : participants considered more sources of information before making a decision and used more content-based heuristics in narrowing their search.
An important question to ask is whether these results are significant in practice .
Is it important to save the user an average of 1.5 minutes in finding an expert?
It is important to consider that expertise location is a ubiquitous task across different divisions and job roles in the enterprise .
For many of our participants, expertise location was not only a daily need, but also a task they performed multiple times a day.
Across all employees in a large company, the time saved would be significant.
However, the bigger win may be in helping users choose a better expert to contact.
There are several factors to consider when assessing how important an improvement more accurate expertise selection could be.
One important factor is the time spent waiting for a response.
If the expert selected never responds, the information seeker's time spent on locating the expert and waiting for a response is wasted.
In our study, 19% of experts in the control condition reported they would not respond versus only 9% in the experimental condition.
If the contacted person does not consider themselves an appropriate expert on the topic, the information seeker then needs to ask for a recommendation or find an alternative.
In our study, 41% of experts in the control condition reported not being the right person versus 16% in the experimental condition.
This may introduce another significant delay as the information seeker waits for the next potential expert to respond.
Another important factor to consider is that the improvements cited above resulted from basic additions using currently available information.
Incorporating the most useful information for expertise selection as found in our paper prototype study may lead to even better improvements.
Our work offers several implications for designers of expertise recommender systems.
First, we demonstrated advantages to including additional information about the experts presented in a search result page.
In hindsight, it seems like it should be obvious that including more information would support users in selecting an expert, but we are not aware of any existing expertise recommender systems that currently provide additional information similar to our experimental system.
Second, making a system's model for recommending particular experts transparent seems important to its user experience.
Several of our users reported being frustrated while trying to understand why a particular person was suggested as an expert or how the expert list was sorted.
The additional information shown in the experimental condition reduced such frustrated comments.
While this result has previously been demonstrated with media recommendations , it is useful for designers to know it applies to expertise recommendations as well.
Finally, the most useful information to consider when deciding whom to contact varies depending on the user's task, the topic of the inquiry, and individual characteristics of the participant.
If a user explicitly specifies the task and topic of his or her search, a targeted system should provide the best information to support the user's particular expertise selection task.
For example, it may be beneficial for designers to re-conceptualize the profile page from a static display describing a colleague in general terms, to one that considers the specific needs and goals of the information seeker: foregrounding the most relevant information for the query task and topic and highlighting occurrences of relevant search terms.
Expertise selection is an important, but under-investigated stage of expertise location.
We found that providing support for expertise selection by revealing additional information about each recommended expert led to quicker and betterinformed selections by our study participants, shown particularly by the selection time dropping 26% and the "strikeout" rate dropping from 28% to 9%.
Participants also rated the importance of different information items for a set of typical expertise selection tasks in the workplace.
Most of the highly rated information can feasibly be gathered automatically or inferred through social analytics.
Our results suggest that future expertise recommender systems can be significantly improved by incorporating this additional information about each recommended expert.
