With P IXELT ONE, users speak to edit their images instead of hunting through menus.
P IXELT ONE associates the tag "shirt" with the selected region.
Photo editing can be a challenging task, and it becomes even more difficult on the small, portable screens of mobile devices that are now frequently used to capture and edit images.
To address this problem we present P IXELT ONE, a multimodal photo editing interface that combines speech and direct manipulation.
We observe existing image editing practices and derive a set of principles that guide our design.
In particular, we use natural language for expressing desired changes to an image, and sketching to localize these changes to specific regions.
To support the language commonly used in photoediting we develop a customized natural language interpreter that maps user phrases to specific image processing operations.
Finally, we perform a user study that evaluates and demonstrates the effectiveness of our interface.
Photo editing can be a daunting task with a steep learning curve.
Not only are interfaces for photo editing often complex, but they also expect the user to learn the language of image processing.
Users must understand image properties such as hue, saturation, levels, and cropping, and learn how they are changed and combined to achieve a desired effect.
To add complexity, effective image edits are often localized to a specific region, e.g., to brighten a face, recolor an eye, or make a sunset more vivid; this task usually requires sophisticated direct manipulation.
Such manipulations are easier with the large displays available in most desktop environments.
However, the reality is that image editing is now frequently performed on small, portable devices such as camera phones, which makes complex interactions even more challenging.
Speech interfaces can make complex tasks more accessible because they allow users to simply state goals without first learning an interface.
Research on integrating speech interfaces into software applications starting in the 1980s  gave rise to today's systems.
Popular speech interfaces like Apple's Siri  allow users to efficiently perform complex operations .
However, image editing is hard to perform with speech alone, since people are not good at describing spatial locations; previous work has shown that visual tasks benefit from a combination of speech and direct manipulation interfaces .
If we look at how professionals communicate desired photo edits , we find a combi-
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Notably, even professionals have very limited shorthand for describing the changes they want to make and frequently resort to more natural instructions .
In this paper, we explore the effectiveness of a multimodal interface that combines speech and direct manipulation for photo editing.
To guide our exploration, we distill a set of design principles for multimodal photo editing applications by observing existing editing practices.
We implement these principles in P IXELT ONE, a multimodal photo editing application for a tablet device that allows end-users to express desired changes in natural language and localize those changes by sketching.
For example, a user can point to a person in an image and say "this is Bob," and then say "make Bob brighter."
P IXELT ONE detects Bob's face and associates it with the tag "Bob."
When the user refers to Bob, P IXELT ONE applies the image editing operations to Bob's face.
To make P IXELT ONE effective, we developed a natural language interpreter for the photo-editing domain that maps user phrases to image processing operations.
Our interpreter segments each user phrase into parts-of-speech components and then maps the parts-of-speech components to predefined phrase templates.
P IXELT ONE handles ambiguous terms by looking for synonyms or close matches for the ambiguous word that are part of its vocabulary.
When P IXELT ONE is not able to interpret the user's intent, it offers a gallery of options that may be appropriate.
This graceful fallback allows the user to learn the vocabulary supported by the system while successfully editing an image.
To evaluate the benefits of a multimodal interface for image editing, we collected feedback from fourteen people.
Participants used the different modalities offered, preferring the speech interface when they knew what they wanted to do, but using a more traditional gallery interface to explore available commands.
In the process of building P IXELT ONE, we observed that a two-tiered approach that first tries a domain-specific method before resorting to a general purpose one worked best.
For example, we combined a very constrained local system with a cloud-based general purpose system for more accurate speech recognition.
We also combined a more precise interpreter using parts of speech pattern matching with a bag-of-words approach for improving speech interpretation.
Additionally, we also learned that adjusting our vocabulary model towards the photo editing domain made the synonym-matching component of P IXELT ONE more robust, which we achieved by mining online photo editing tutorials.
This research makes the following contributions: * a set of design guidelines for integrating a speech interface into image editing applications,
While natural language interfaces have been studied extensively in a number of domains, including databases , automating web tasks , command lines , mobile phones , car interfaces , and home media systems , natural language interfaces for image editing have received little attention.
There are techniques that use natural language to correct colors in images ; however, they focused on global operations and limited the user interaction to natural language only.
Our language parser supports a much wider variety of commands, including tonal corrections, localized edits, and object tagging.
Furthermore, previous findings comparing speech-only and multimodal interface show that a multimodal interface is more effective for visual tasks .
Researchers have also studied how to integrate natural language interfaces into sketching tasks.
For example, Speak'n'Sketch  lets artists issue commands, such as "group", "rotate", or "thicker brush"' as they are sketching.
This frees up the artist to continue sketching on the canvas rather than switching between the menu system and sketching canvas.
Pausch and Leatherby  showed that adding voice to a drawing application reduced time to completion by up to 56%, with results showing an average reduction of more than 21%.
Researchers have combined image capture and the hardware capabilities of mobile devices  to create new applications for mobile visual computing.
In addition, Xiong and Pulli  presented an image blending algorithm that can be applied for panaromas and image compositing on mobile devices.
Consumer mobile applications such as Instagram1 , Camera+2 , Photoshop Express3 , Photosynth4 , SnapSeed5 , and Vapp6 have made it easier for users to edit and share images.
However, these applications mostly support global image edits , and none of them include a speech interface as an interaction component.
To understand how natural language fits into the editing process, we studied how professional photographers communicate photo edits, and carried out two studies: one small lab study, largely to test prototype features, and one larger online study on Amazon Mechanical Turk, to help us create an initial dictionary of phrases .
We recruited five participants for a pilot experiment .
We asked the participants to tell us how they would improve a given image .
Next, we showed them two images  and asked them to tell us what they would say if they had to direct another person to transform image A into image B .
Finally, we asked the participants to edit images using an early prototype of the system that mapped keywords to commands .
Each participant performed all 15 image tasks.
Historically, professional photographers and art directors annotated their proofs with instructions on how the images should be modified or printed .
Because edits were to be performed by another person, most often a retoucher or color/printing professional, the description of changes needed to be specific and unambiguous.
Annotations typically include a circled region and description of changes.
Despite this high-level norm, the subjective nature of images and the language to describe them has made a formal notation system difficult to achieve.
What developed instead was a mix of shorthand for common tasks  with a significant use of "long form," unstructured, natural language, descriptions of desired changes .
Instructions can range from the very specific  to extremely subjective and open .
Additionally, we note the use of highly metaphorical language: "too hot" , "too cool" , "flat" , or "can be opened"  instead of direct references to image features .
Finally, the example images we studied demonstrate the tactics of localization either by circling or referencing an object in the text .
Conventional wisdom for photographer-retoucher workflow is that ambiguity is to be avoided.
For example, one would prefer to say, "remove wrinkles" instead of, "lighten wrinkles," which has two meanings  .
This tension between expressive descriptions, range of editorial control , and demand for low ambiguity have led to reduced use of shorthand and increased use of unstructured natural language.
To gather additional natural language phrases, we used Amazon Mechanical Turk.
Our experiment was very similar to the one performed in the lab.
Although our experiment on Amazon Mechanical Turk did not use a speech interface, the images themselves were the same as the lab study, and the Turkers entered descriptions as unstructured text.
The Turkers were paid $0.05 for each image or image pair.
We showed 10 individual images and 14 image pairs and collected 10 responses for each task .
Users saw each image and image pair once.
After filtering out bad data , we had 211 valid responses from 35 unique workers.
We analyzed and coded the data using an affinity diagram.
One of the key findings from these experiments was that while users had a common language for describing changes, discoverability of the lexicon terms was difficult without guidance.
After trying out our early prototype, users in the lab study noted that it was hard to remember the commands.
Participants used prior editing experience to guide them on what to say.
For example, more experienced users performed more adjustments, and used more advanced terminology .
Consistent with previous findings , users mentioned that specific, localized and elaborate changes were challenging to describe, while global operations were easier.
Users also naturally referred to objects within an image, but they varied in their degree of precision , and they also pointed to specific regions of the image .
Through both the lab and crowdsourcing experiments we found consistent use of both imperative and declarative language to describe desired changes.
Imperative phrases contain verbs and act on the entire image or portions of it .
Declarative phrases indicate a problem in the image needing correction without specifying the operations that corrects it .
P IXELT ONE makes use of face detection to associate the tag "John" with the face on the right.
Here the midtones in the sky are sharpened.
In addition to design directions that were consistent with previous studies , our analysis of professional annotations and study findings helped us distill key design guidelines for incorporating natural language into image editing interfaces.
Both professional and novice photographers often refer to objects in an image when using natural language .
Systems should leverage object recognition and offer tagging as a way to reference objects in an image.
Support declarative and imperative sentences.
When users know how to improve an image, they tend to use imperative phrases that describe what should be done.
But users, especially novices, don't always know how to improve an image.
They simply know what's wrong with it.
In such cases, they are more likely to use declarative phrases that state what they don't like about an image.
Guide users with available commands.
Due to the diverse and large set of possible utterances in the image editing domain, extra care is necessary to properly address discoverability and learnability.
Users may want to edit only part of an image, so a system should facilitate selecting a variety of regions including spatial regions , tonal regions , and colors .
Different modalities may be used to support different types of referents.
Eve is presented with a slider, and she uses it to finetune the degree of brightness.
Once done, Eve taps the "Save" button and saves changes to her newly enhanced photo.
Next, Eve notices that specific areas in the photo need some enhancements.
She says "sharpen the bottom" and P IXELT ONE applies a "sharpness" filter to the bottom region of the image.
Next, Eve wants to improve the dark areas in the image.
Eve says "make the shadows more contrasty" and P IXELT ONE enhances the contrast on the low-intensity regions of the image.
Finally, Eve draws a circle around one of the grassy areas of the photo and says "make this greener," and P IXELT ONE applies a "green tint" to the region Eve selected.
Users may not always know how to improve an image.
For instance, when Eve wants to enhance one of her skyline photos but she is not quite sure what to do, she can say "make this better", and P IXELT ONE finds an effect that approximates her goal.
In this case, P IXELT ONE applies the "auto-color" operation to Eve's photo.
When P IXELT ONE does not understand what the user wants and cannot find a suitable approximation, it offers a fallback interface which offers a list of image operations .
When Eve wants to enhance a recent photo of her friends, she loads the image in P IXELT ONE, points at a person in the photo, and says "This is Roslyn."
P IXELT ONE identifies "Roslyn" as one of the names in Eve's contact lists, uses face recognition to find a person in the region where Eve pointed, and tags it as "Roslyn" in the photo.
Then, Eve says "add soft-focus on Roslyn," and P IXELT ONE applies a "soft focus" filter to the region in the image with Roslyn's face.
We implemented P IXELT ONE on a tablet device , and used a client-server architecture to distribute computationally intensive processing functions on remote servers.
A user interacts with P IXELT ONE through a combination of voice and direct manipulation.
A speech recognition component converts users' voice into text; an interpreter combines natural language processing and keyword matching techniques to map a user's input phrase into an image processing request; and an execution engine combines the interpreted command with user's direct-manipulation input to execute an image processing operation.
We use existing speech recognition technology to convert voice into character strings that P IXELT ONE can process and understand.
Neither implementation was satisfactory in isolation, but we found through iterative testing that a multi-pass approach worked well for our purposes.
We extend their approach by segmenting an input command into parts-of-speech tags, matching tags against a repository of phrase templates, and then remapping keywords into functions and parameters.
This technique allows us to scan for functions and parameters to specific words that play a "grammatical" role within an input phrase.
For example, if we know that a verb in a phrase pattern maps to an image operation , we can match for operations that fit with that verb and not to other parts of the sentence.
If the interpreter cannot find a matching phrase template, our system ignores the parts-of-speech tags and scans for keywords by treating the input phrase as a bag-of-words.
This approach works well when word order is jumbled or when a sentence is awkwardly phrased .
The bag-of-words approach also allows users to not follow strict sentence structures.
In the following sections, we describe the algorithm in detail.
In the first pass, we employ a local speech recognition engine.
We use data gathered from our user study and crowdsourcing experiments as the corpus for the local speech recognition component.
For utterances that fall within the corpus, this approach yields high accuracy and a relatively fast response.
P IXELT ONE uses OpenEars  for local speech recognition.
If the local speech recognition component encounters an "out-of-vocabulary" utterance , P IXELT ONE sends the recorded voice data to a remote speech recognition server.
Although this approach is slower compared to local speech recognition, it provides P IXELT ONE with access to a more generalized vocabulary when needed.
We use the iSpeech HTTP API  for remote speech recognition.
First, we tokenize an input command into constituent words.
For each word, we determine its part-of-speech tag using a customized Brill transformation-based tagger .
In our current implementation, we use a two-level tag hierarchy for each input command.
The lower portion of the tag hierarchy consists of Penn Treebank tags , and the upper level consists of verb, noun, or adjective expressions formed by groups of tags for each phrase element.
Phrase Level At the phrase-level, an input phrase is tagged into broader "phrase expressions."
For example, the sentence "make the shadows on the left slightly brighter" is tagged for Verb, Noun, and Adjective Expressions  as:
Once speech is converted to text, the interpreter  analyzes and maps the input command into an action request.
Our approach uses a variant of the keyword-command technique proposed by Little and Miller .
Their approach breaks an input command into tokens and recursively builds a function tree by scanning for keywords that match functions and data types.
We use lexicon ontologies such as Wordnet for term disambiguation similar to SenseRelate , based on earlier work by Banerjee et.
We developed a modified shortest path distance function to compute the semantic similarity between two or more terms with ties resolved by closeness of the nearest common hypernym.
Using noun, verb, adjective, and adverb lexicon ontologies, we compute the shortest path distance within the ontology between any two terms.
Terms that are synonyms in the same synset ring  have path distance zero; non-synonym terms that have the same hypernym parent have distance 2.
Terms further away within the ontology will have higher path distances.
Word-level parts of speech tags are needed to differentiate the types of words that make up a phrase element.
Phraselevel tags identify potentially complex subject, predicate, and objects.
Our phrase templates work at the phrase-level.
Word tags are needed to determine the elements within the phrase.
Next, we define a model for how an input phrase is mapped into a valid image processing request.
Based on data gathered from our initial user study and crowdsourcing experiments, we segment the user's input phrase into three main components: Image Operations, Masks, and Parameters.
An Image Operation is the effect or filter that a user wants to apply to the image.
A Mask defines the target of the effect.
This could either be global , within a selected region , within a geometric area , within a specified tonal region , within a particular range of colors , or through object references .
Finally, the user specifies Parameters which indicates the degree to which the user wants the effect applied .
P IXELT ONE uses a canvas layer overlaid on top of the displayed image to allow users to make arbitrary selections for localizing effects.
For example, a user can draw arbitrary scribbles in the images, and P IXELT ONE applies an effect only to that particular selection.
In most cases, users want to adjust the "degree" of a particular effect.
P IXELT ONE provides sliders to fine-tune an effect.
Sliders prevent the user from repeating particular words like "more, more" or "less, less" to tweak an effect.
Once an input phrase is categorized into parts-of-speech components, we use its phrase-level parts-of-speech tags to find any matching phrase templates.
Phrase templates are simple "rulesets" that map words and phrases into the different components of an image processing request.
Here is the phrase template mapping for the example above: Production Rule Match for pattern  "VX NX AX" Adjective in AX  image operation Nouns in NX  masks Adverbs  parameters Require  presence of JJ Example see example above "brighter"
P IXELT ONE uses object detection to find an object within an image.
For example, a user can point to a person in an image and say "This is Kurt."
In our current prototype, we use face detection  to identify faces within an image, and we combine this information with the point location to resolve the face whose bounding box falls within the neighborhood of the pointed region.
The bounding boxes of the objects are used as masks, and the captured tag  is used as the identifier for the reference.
P IXELT ONE also makes it possible to store arbitrary selections within the image.
For example, a user can draw a selection on a specific part of the image and store that selection by saying "This is the background."
When P IXELT ONE detects a previously named reference from an input phrase, it retrieves the stored selection and applies the effect to that region.
For example, a user can say "brighten Kurt" or "blur the background" and P IXELT ONE applies the particular effects within the region specified by those tags.
Finally, the execution engine component processes the interpreted command and combines it with users' directmanipulation input to execute an image processing operation.
If a mask is specified, the execution engine localizes the operation only to that region.
In addition, the execution engine blends multiple masks in cases where more than one mask is specified .
We compared two interfaces: P IXELT ONE, and P IXELT ONE without the speech interface.
We recruited 14 users  from an open e-mail list at a large public university.
The participants had diverse experiences with photo-editing, and six users selfreported having novice experience with photo-editing software.
In addition, the age of the participants ranged from 19 to 47, with eight participants between the age of 22 to 34.
Four users were non-native English speakers.
At the beginning of the study, we trained our participants on how to use our prototype.
We read a written script, and walked them through relevant features of the system, including the speech interface and the gallery mode.
Afterwards, we allowed participants to interact with the prototype.
During that period, we also gave them a hand-out that showed examples they could try.
We asked participants to complete 16 tasks, which were segmented into two parts .
In each part, we randomly assigned either P IXELT ONE or P IXEL T ONE without voice.
To decrease learning effects, we gradually increased the tasks' level of difficulty.
At the beginning of each task, we also asked users for a high-level description of their intended actions.
The images were counterbalanced across interfaces and tasks.
We defined task success using a scale of 1 to 5 .
We designed a method to assign a success score for each task.
For example, a user who applied "hue" for a task that involved improving an underexposed image got a score of 3, while a user who applied "brightness" for the same task got a score of 5.
To normalize expertise effects, we gave users one minute to do whatever they wished to accomplish for each task.
After one minute, everyone received a "guide sheet" that showed them the operations for completing the task.
For example, the guide sheet for a task could include the operations "brightness", "sharpen", and "vibrance."
We debriefed the participants after they completed their tasks.
We asked them for their overall impressions, the differences they encountered between the two types of interactions, and their overall feedback.
In addition, we also asked the participants to complete a short questionnaire.
Success rate for both interfaces were identical.
We did not observe a significant difference in success rates between the two interfaces.
The success rate of novice users with the multi-modal interface was slightly higher  in comparison to the no-speech interface , but this difference was not statistically significant.
Since the baseline interface already allowed a high success rate, the additional speech modality was not able to provide a significant improvement in rates of success.
Users preferred the multimodal interface.
Consistent with previous findings , users rated the multi-modal interface as the one that they liked more  compared to the interface without speech .
Number and complexity of utterances varied from user to user.
In one extreme, we observed a couple of users who successfully completed tasks using the speech interface alone, whether or not the gallery mode was available to them.
In the other extreme, we also observed some users who rarely used the speech interface.
However, we observed that the percentage of image operations invoked using the speech interface was higher among novices  than with users having advanced imageediting experiences .
Similarly, native English speakers used the speech interface more often .
We approximate the complexity of utterances using the mean length of utterances  measure  .
From a total of 386 utterances, the number of words used ranged from 1 to 6, and the average MLU across 14 users was 1.89 words per utterance .
Similarly, native English speakers average 2.04 words per utterance  compared to 1.61 words per utterance for non-native speakers .
This finding among native english speakers vs. non-native speakers was statistically significant  using a standard t-test.
Speech engine implementation obtained high accuracy.
We transcribed all user utterances and compared them with P IXELT ONE's speech-to-text output .
The accuracy was calculated by comparing the Levenshtein distance  between P IXELT ONE's speechto-text output and the actual transcribed utterance.
From a total of 386 utterances across 14 users, our two-tiered speech recognition implementation obtained an average accuracy of 84% .
For native English speakers, we obtained an accuracy of 90%  compared to 70% for non-native speakers  This finding was statistically significant  using a t-test.
Our participants were overall very positive.
Almost all users  said they would recommend P IXELT ONE to their friends.
Participants found the combination of speech, gallery-mode, sliders, and sketched-based selections easy to use.
One user  said "It's possible that my mom and grandma would be able to use this."
Another user  was impressed about the performance of the speech interface: "Initially I had doubts about the speech interface because my experience tells me that speech doesn't really work in other apps.
But this one, it worked well.
I think it's better than Siri."
13 out of 14 participants said they preferred the multimodal interface because it allowed them to accomplish tasks in a way that met their needs.
One user  mentioned that the multi-modal interface allowed choosing the best approach for a particular task: "For the tasks that were more complicated, it was easier to pick out using the gallery menu.
For simple tasks like brighten, voice makes it easy."
Users use the speech interface when they have a good idea of what they want to do.
The participants mentioned that they used the speech interface to do simple things very quickly.
In fact, the majority of the users  mentioned that when they knew exactly what they wanted to do, they used speech as their first choice because it saved them time.
Users use the gallery mode when they want to explore options and compare different effects.
When users were not exactly sure what to do, they often used the gallery mode as an exploration tool.
Users mentioned that unlike the speech interaction, the gallery mode allowed them to have a visual representation of the effects, which they said was particularly useful when comparing options to accomplish a task.
Users use direct manipulation to fine-tune and explore.
Thirteen out of fourteen users found the sketch-based selection particularly useful.
They frequently used it for finetuning the edits they made on their images.
One user  said "even without the speech, it was fun just to do the scribbles."
However, users also mentioned that sometimes they unknowingly "tap" the screen and make small scribbles, unintentionally localizing effects to undetected areas.
To mitigate this issue, the system should provide more feedback on whether a local mask was applied to the image.
In addition, we observed that users utilized the sliders to tweak effects.
The sliders also served as a feedback mechanism for exploring the capabilities of the system .
Non-native English speakers with accents used speech interaction much less.
Non-native English participants often had a preconceived belief that their accent would impede their performance with the speech interface.
Thus, they tended to use the speech interface less frequently.
Moreover, they also reported being more self-conscious about their pronunciations, and the presence of a native English speaker  could have limited their inclination to use the speech interface.
To have a more robust and reliable speech interface, the speech recognition system must account for individual differences, including nuances in diction and pronunciation.
It should also consider the diversity of non-native English speakers.
Our studies suggest that for most users the speech interface must work accurately in the first three or four tries or it will impact their perception of its reliability and interfere with future use.
Supporting non-native English accents was out of the scope of this project, and we leave the exploration of these techniques to future research in the speech recognition domain.
However, it may be worthwhile to explore how commercial speech recognition systems such as Dragon  and Siri  account for the diversity of its users.
In addition, future work should explore how to create a feedback mechanism to automatically train P IXELT ONE to learn from the vocabulary and nuanced speaking styles of users.
In its current form our prototype has a limited vocabulary .
We mitigate this in part by using Wordnet for synonyms and disambiguation as well as mining the user's contact list to find names.
The current prototype performs well with simple commands , and a limited set of highlevel commands that require several image editing operations in a row .
These high level commands are manually created and P IXELT ONE may be better served by learning from examples  or mining online tutorials.
We have only begun to explore the different ways in which people use natural language in the context of photo editing.
Most of the phrase templates we designed were in the imperative form since they tend to be more common.
However, the image-editing domain affords the use of declarative sentences , and users may learn to use this form as they become more comfortable.
Unlike imperative sentences, some ambiguity is introduced when interpreting declarative sentences.
For example, the sentence "the image needs to be bright" might denote brightening the image , while "the image is too bright" might denote darkening the image .
Two future enhancements are possible in this area: supervised learning of additional image editing operation vocabulary from tutorials, and assigning correct word sense to the learned vocabulary words to improve disambiguation of unknown words.
Finally, we chose the tablet as our initial platform for P IX ELT ONE for practical reasons .
It would be worthwhile to explore how our findings can translate to other platforms such as desktop computers.
In this paper we introduce P IXELT ONE, a multimodal system to support image editing tasks through speech and direct manipulation.
The system is motivated by a number of formative studies on how both professional and novice users make use of natural language and annotation to indicate the changes they would like to make.
We found that the multimodal interface more naturally captures both existing work practice and desired functionality.
Image editing is an incredibly difficult task, and the shift to mobile  devices makes this task harder.
Additionally, the language of image manipulation is varied, ambiguous, and subjective.
By interpreting speech through a combination of local and remote speech recognition and customized natural language parsing, P IXELT ONE provides users with a powerful mechanism to obtain desired results.
While we identified future improvements, our user study found that users preferred the multimodal interface overall and were able to use it effectively for a realistic workload.
Our focus thus far has been mostly on direct manipulation, but we believe that adding gestures could bring further richness to the interaction.
We have started experimenting with some gestures for cropping and specifying gradients.
To let the user easily crop an image, P IXELT ONE can use the stored touch points to determine the "bounding box" of an arbitrary selection.
And to specify gradients, the user can draw a line.
It remains future work to explore which types of gestures make sense in this context and whether users would be interested in learning them.
Additionally, P IXELT ONE asks the user to use the different interface modalities one at a time  and does not integrate them.
This means that the user can't sketch and talk at the same time.
While this seems sufficient for now, a future goal would be to support integration  to offer a more flexible interface.
Androutsopoulos, L. Natural language interfaces to databases - an introduction.
Banerjee, S., and Pedersen, T. An adapted lesk algorithm for word sense disambiguation using wordnet.
Bolt, R. A. Put-that-there: Voice and gesture at the graphics interface.
Brill, E. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.
Our current implementation supports 12 phrase templates that were manually written in code.
A full implementation might require defining more phrase templates to allow users to fully express their intentions.
Since these templates are based on grammatical structures of sentences, it is possible to semi-automate the process of defining templates by analyzing grammatical patterns on large corpora of users' utterances.
Alternatively, users can train the system directly by verbalizing what they are doing as they are doing .
Currently, P IXELT ONE supports a limited number of image processing operations.
We picked them based on the most commonly used operations, similar to those supported by consumer-type photo editing applications such as iPhoto .
Milota, A. D. Modality fusion for graphic design applications.
Oviatt, S. Multimodal interactive maps: designing for human performance.
Pajari, M. Color mark up terminology.
Patwardhan, S., Banerjee, S., and Pedersen, T. Using measures of semantic relatedness for word sense disambiguation.
Pausch, R., and Leatherby, J. H. An empirical study: Adding voice input to a graphical editor.
OpenEars: speech recognition and speech synthesis for the iPhone.
Samad, T., and Director, S. W. Towards a natural language interface for cad.
Sedivy, J., and Johnson, H. Supporting creative work tasks: the potential of multimodal tools to support sketching.
Woolfe, G. Making color adjustment accessible to non-experts through the use of language.
Xiong, Y., and Pulli, K. Gradient domain image blending and implementation on mobile devices.
In Mobile Computing, Applications, and Services.
Zhao, Y., Bala, R., Braun, K. M., Langford, Z., Rolleston, R. J., and Stevens, M. T. Language-based color editing for mobile device.
