Collection recommender systems suggest groups of items that work well as a whole.
The interaction effects between items is an important consideration, but the vast space of possible collections makes it difficult to analyze.
In this paper, we present a class of games with a purpose for building collections where users create collections and, using an output agreement model, they are awarded points based on the collections that match.
The data from these games will help researchers develop guidelines for collection recommender systems among other applications.
We conducted a pilot study of the game prototype which indicated that it was fun and challenging for users, and that the data obtained had the characteristics necessary to gain insights into the interaction effects among items.
We present the game and these results followed by a discussion of the next steps necessary to bring games to bear on the problem of creating harmonious groups.
Regardless of the domain, some items work well together and others do not.
These co-occurrence effects are one of the most important factors in the success or failure of many collections.
It can be a complex task to evaluate co-occurrence effects.
Even two items that both have high individual item ratings may not work well together.
Someone might have a deep love for chocolate and also for pickles, but not for the two together.
This is a rather intuitive effect when considering pairs, but gets more complicated when considering the quality of larger sets of items such as a triple.
For example, chocolate bars and graham crackers are a fine combination; marshmallows and chocolate bars are also; and marshmallows and graham crackers are as well.
None of these pairs are poor but neither are they exceptional.
However, the combination of all three into a "s'more" makes a much beloved snack for many people.
The combination of all three items is better than would be indicated by looking at the three pairs.
On the other hand, three items that are very good pairwise can make a bad triple.
Consider building a research team of two professors and one graduate student.
The professors may work well together, and each may work well with the student.
However, all three may have trouble working together.
The presence of a student may bring out some tension between the faculty members about who is in control, and the student may have trouble balancing work or contradictory instructions from the faculty.
Similar scenarios can be made moving up from groups of three to four, and so on.
Even with extensive data on users' preferences for items and groups of items, this space is vast enough that general rules will almost certainly be necessary for collection recommender systems to be successful.
To derive these rules, too, will require a large set of data.
Collection recommender systems  are similar to existing recommender systems but instead of recommending individual items to the user, they recommend groups of items that work well together as a whole unit.
There are many factors to consider when creating a collection.
The size of the collection, diversity, potential order, and, quality of items all have an impact.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Just as human users have been able to successfully label millions of images with descriptive tags , they can also build small collections that can be used to learn patterns about what works well together and what does not.
There have been GWAPs designed around eliciting user preferences , where users see two items and select the best, but eliciting preferences about what items go well together requires a new game design.
In this paper, we present a new class of games, collection games, for creating combinations of items that work well together.
We then present a prototype game and results from a pilot experiment.
We then outline future steps for implementing a large scale game for understanding collection preferences.
Games with a Purpose have been used successfully for many tasks that humans can easily solve but computers cannot.
Perhaps the greatest success and most widely used example is the ESP game / Google Image Labler where hundreds of thousands of players have contributed tens of millions of labels .
Describing the content of media has indeed been the focus of these games.
The ESP game generates labels for images; TagATune  gathers labels for songs; Peekaboom  has players identify objects within an image.
Users' preferences, the focus of our work, have also been addressed in some games.
Matchin  is the best example.
This game presents users with two photos and awards points when they agree on which is "more beautiful".
This game has been very successful and has yielded interesting data.
Most relevant to this work, the authors also used a variation on the SVD algorithm to produce an image recommender system based on preferences users express in the game.
Many of the insights from this work are applicable to our task.
However, our domain has different requirements.
To apply the Matchin game technique directly to collection recommenders would involve showing users two collections and having them pick the better one.
The number of combinations is so vast that it is unlikely even a popular game would produce useful results.
For example, with only 1,000 base items to group together, there are 1 billion combinations of three.
Direct comparison as a ranking mechanism on a set of 1 billion would require far more game play than is reasonable.
We require a game that considers many more combinations at once and where we can gather data both about the combinations people make  and what combinations are not made.
This problem requires a new class of game to gather data about what items work well in groups.
However, combining items without constraint - the closest parallel to labeling GWAPs - would, in most domains, lead to frustrating game play.
Making a match would be difficult, and even the simple task of selecting a combination to make would be daunting if all items are considered.
Thus, we have created a class of games that allows for multiple matches on a constrained set.
In collection games, players see sets of items that they must combine into groups.
The items are drawn from a large pool of possible items.
Players group the items together into collections and submit their choices.
They are awarded points for collections that match .
Curator is a two-player game output agreement game played online.
It has been designed as a prototype collection game.
Users connect to a lobby where they are randomly paired with another player who is also waiting in the lobby.
The players are then taken to the first round of the game.
In the round, they are shown two sets of items and asked to group them into collections.
When both players have finished making collections, they move to the scoring phase.
Both players are awarded points for each matching collection.
At the end of each round, players see which collections matched and which did not.
For both, they see their choices and their partner's choices.
To score well, players must not only submit collections that match their own preferences, but also consider the preferences of their partner.
This review phase helps players gather some insights about their partner's tastes without direct communication.
At the end of each round, players can earn a bonus.
Although they may have put the collection together, that does not mean they thought it was a particularly good combination, but perhaps it was the best given the options in the round.
For each set of matching ratings, the players receive extra points.
The other player's ratings are never shown in the bonus round to prohibit development of strategies for cheating in this phase.
Checks are also used to make sure players do not always assign the same rating to every combination.
There are five rounds in a game after which high scores are recorded and displayed on a leaderboard.
Our current prototype has approximately 100 items in each category, though a fully implemented game would have more.
All together, users with 70 unique usernames played.
Many of these game rounds were played in a one hour game session held in our lab.
All together, players had 1,031 opportunities to match, i.e.
There were 157 matching selections from these opportunities, a match rate of 15.2%.
Most matches were made only once, but seven matches were made twice and two pairs were made three times.
Figure 2 shows these most common matches from the shoe-and-handbag version of the game.
All pairs made by players are stored and even if the players do not agree, a frequently created but rarely matched pair may be a good combination.
The most common match made was between the pink shoe and bag combination shown in figure 2.
These two items were combined 7 times - 6 of those instances occurred in the three rounds where both players made the combination together.
There were four combinations that were made 6 times, and that includes the other combination from figure 2.
Every time a player made that combination, his or her partner also made it.
Not only are these two matches the most frequently made in the system, but, they are put together almost every time by both players which indicates that they are an exceptionally strong match.
While our pilot results are insufficient for statistical analysis, they provide a preliminary indication that the game can indeed produce useful, interesting, and meaningful combinations.
In a game like our prototype, with 8 items to be paired with any of 16 other items, there are 16!
By requiring players to create combinations they think will work well, we gain data in two ways.
First, and most obviously, we can see which combinations they make and which combinations they agree on.
These can be analyzed with methods traditionally used for output of GWAPs.
Once a combination has been made enough times, it can be considered a valid and interesting combination for study.
The number of times it is made is also interesting.
Ranking mechanisms and collaborative filtering techniques, such as those from  discussed above, could also be used on the sets of combined items.
However, the second data points we have are equally useful.
While each player will only make eight pairs, and each round will result in sixteen pairs at most, we know that the remaining pairs will not have been made.
On one round of data, that is not meaningful, but over time it can also lead to insights about pairs that do not work well.If we have not seen a pairing after two items have been in the same round many times, it indicates the items may not work well together.
The more rounds that are played, the more information we obtain about items that have never been paired.
These insights are just as valuable as the combinations which are frequently made.
For a GWAP to be successful, it must be fun to play and yield useful data.
To test this in our prototype game, we ran a pilot study with users playing our pilot game, Curator.
For this first prototype, players only created pairs of items that work well together.
In future implementations, the fixed set of items on the left will be small collections rather than single items, and players will add an additional item to each set to build larger collections.
Players chose from two versions of the game.
In one, players match shirts and ties as shown in figure 1.
In the other, players match shoes and handbags.
Qualitatively, subjects in the pilot provided interesting insights about the matches made and expertise.
Interestingly, male subjects with self-confessed ignorance of what made good shoe-and-handbag pairings scored rather well.
Much of the nuance behind creating a good match was lost even though the scores were better.
This indicates that players should accurately represent the target users of the recommender system and share their understanding of the nuances behind good matches if the data from their games is to be helpful as guides of the system.
Subjects also reported that the game was fun and challenging.
While making many matches was difficult, they were enthusiastic during play when they did make matches.
The leaderboard was particularly motivating as subjects reported being very proud of appearing there after a successful game.
This version of the game is an initial demonstration of the idea behind collection games.
The pilot test demonstrates the potential of our technique.
However, we have chosen one of the simplest types of collections and game play - matching pairs of items.
To use this technique for collection recommender systems will require more sophisticated forms of game play to handle larger collections.
There are also scaling issues that need to be addressed.
As mentioned above, the number of possible combinations expand exponentially as collections include more items.
This makes it increasingly unlikely that users will create matching sets if they are picking each item from a list.
These larger collections are important, though, so there should be ways to play to create these collections.
One approach would be to have users take collections of size n and add an item to create collections of size n + 1.
The size n groups could come from earlier game play, leveraging user preferences, or these groups could be automatically generated by the system to explore an additional space.
Another game option would be to create collections, present them to the user, and have players remove one item and replace it with another.
This would help identify items that create poor group interactions.
If the initial collections are chosen wisely, this type of play could also be used to highlight items that work well pairwise but do not work well together in larger groups.
One final alternative could take advantage of the technique used in  and have users rank collections by choosing the best one.
The limits of the game are also important to explore.
The technique used in our prototype is not likely to scale up to very large collections because the combinatoric space is too large.
However, even with large collections, a game like this could be used to test ordering effects and other local features.
From the recommender system perspective, the game also gets at only part of the data space.
Because users need to agree with one another to score points in the game, the users' personal preference is not necessarily reflected by their choices.
This preference data would need to come from another source.
Indeed, the importance of personal preference, diversity, interaction effects, and other factors varies from collection to collection and person to person.
This is discussed in greater detail in .
In this paper, we have presented a new class of games, collection games, where users create collections of items.
We have shown that the structure of the game provides data in several ways that will be useful for developing rules for collection recommender systems.
Through a pilot study with a prototype game Curator, we found that the game was fun, challenging, yet not frustrating for users.
A preliminary analysis of the data showed that some pairs were very commonly used while others were never put together.
More game play is necessary to obtain significant results that can be used in recommender algorithms, but these preliminary steps indicate that a GWAP can be a successful method for gathering data to help create collection recommender systems.
Future steps will require fine tuning the game play, scoring mechanism, and deploying in a domain and an environment that will receive attention and participation.
User testing will also be required before final launch.
Once data starts flowing in, we will begin analysis by hand as well as work with machine learning algorithms and existing recommender system techniques to discover the best ways of utilizing this data in the eventual development of collection recommender systems.
