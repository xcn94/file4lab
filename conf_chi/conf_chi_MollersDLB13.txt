Touch interfaces provide great flexibility in designing an UI.
However, the actual experience is often frustrating due to bad touch recognition.
On small systems, we can analyze yaw, roll, and pitch of the finger to increase touch accuracy for a single touch.
On larger systems, we need to take additional factors into account as users have more flexibility for their limb posture and need to aim over larger distances.
Thus, we investigated how people perform touch sequences on those large touch surfaces.
We show that the relative location of the predecessor of a touch has a significant impact on the orientation and position of the touch ellipsis.
We exploited this effect on an off-the-shelf touch display and showed that with only minimal preparation the touch accuracy of standard hardware can be improved by at least 7%, allowing better recognition rates or more UI components on the same screen.
Also, on larger interaction areas, people have more freedom for touching the surface--different body, arm, finger orientations are possible.
Touch interfaces are currently the dominant design in mobile computing, and touch-enabled tabletops might become a major computing platform for knowledge workers in the future.
However, bad touch recognition often leaves users with frustration regardless of the surface size as not only small targets but also long distances make touching hard .
One can increase the target size to compensate for this effect.
Another option that even preserves the precious real estate is to correct the user's input for errors.
Existing research already tells us how we can do this for single touches on small surfaces , yet the correction of touch  on large-scale surfaces has not been investigated.
Several solutions were proposed to solve the problem of inaccurate input as well as occlusion from the finger itself.
Some use indirect manipulation by having an input area act as a proxy for the area which is manipulated.
This proxy can be located close to the actual input area  or farther away, e.g., back-of-device interactions .
Other methods record more than just the touch point to increase accuracy.
Wang and Ren  also employed the contact ellipsis.
TapSense  used the sound of the impact of an object to identify the touch.
TouchID Toolkit  used gloves with fiducial markers on the finger tips, knuckles, etc.
The Ridge Pad is able to track yaw, roll, and pitch from the fingerprint .
None of these systems take touch sequences into account.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
A first, middle, and last button are displayed simultaneously, i.e., PE and SE should occur.
B first and second are displayed directly at the beginning, then third after the second has been touched, i.e., PE should occur, but SE not as the participant does not know the successor when touching.
C first is not displayed at all, middle and last are displayed directly at the beginning, i.e., PE does not occur as there are no different predecessors , but SE should happen.
D first is not displayed at all, middle is displayed directly at the beginning, the last after the middle has been touched, i.e., PE and SE should not happen.
Conditions A and B presented sequences of three touches each.
Conditions C and D presented sequences of two touches each.
Users started with doing one condition, performed latin square balanced order of all touch sequences in this condition, repeated 5 times, and then went to the next condition.
This order of the conditions was also latin square balanced among the users.
People who work frequently with the same application know which input is required for achieving a goal.
They turn this into a sequence of actions, which on a touch device usually consist of a sequence of touches.
We assume that individuals plan these sequences of touches by predicting comfortable final limb postures .
To evaluate our hypotheses, we let people perform touch sequences on a tabletop in a controlled setting and analyzed our data regarding the PE and the SE.
The northernmost button was reachable with no arm stretching or leaning forward.
We used round buttons  and the ring had a diameter of 40 cm.
Dependent on the study condition participants performed up to three touches in a sequence.
Only buttons used in the sequence were displayed.
Users were asked to move their finger to a pre-defined position after performing one touch sequence: a 10 cm by 5 cm wide box on the left half of the table, easily reachable with the right hand.
Most touch sequences started on the ring, went to its center and back to the ring.
We only look at touches aimed at the center.
We record the location and orientation  w.r.t.
First, the experimenter gave instructions on how to perform the task and participants were told that their speed and accuracy was a non-important factor.
All users were sat at the same predefined position in front of the table and were instructed to use their index finger for touching.
We started the test with a whack-a-mole game for about five minutes in advance of each experiment.
This way, participants were able to familiarize themselves with the setting.
Then the first touch sequence was presented and participants were asked to press the lit buttons according to their occurrence.
Participants took 30 minutes including preparation.
For accurate touch tracking, we use a single camera beneath the table that can be rotated along its axes to be pointed at a specific spot on the screen.
This gives us an input resolution of 140 dpi, i.e., sub-pixel accurate tracking for our 59 dpi output resolution.
We collected demographic data before the test using a questionnaire: gender, age, height, and prior experience with touch devices .
Our system returned the touch as a contact ellipsis.
We record the angle of its main axis and the center of the ellipsis.
We had concerns how the participants height would impact the body posture which might lead to different touch angles.
However, arm length scales with height and seemed to counterbalance this effect: we could not see any effect of height.
But we didn't see an effect of succeeding -20 touches.
This might be due to the fact that this effect is very small or does not exist.
In both cases, we actually 0 60 120 180 240 300 welcome this result: We do not need to predict user behavior to get very accurate touches.
We used a 27" Perceptive Pixel horizontal display with sub pixel accurate tracking on a 110dpi screen.
As warm-up, we sat the participants in a fixed spot in front of the display, displayed a round button, and asked them to touch the button 30 times with fixed pitch  and roll 15px -7px.
Between each touch, they should Successor  Predecessor with Successor  Predecessor with Successor  move their finger to a resting position above the table.
Users touched a button on the ring first and then 12px -9pxtouched the middle button.
After this, they needed to move their hand to a resting position in front of them in the air.
The 11px -10pxorder of the six buttons was counterbalanced and the full set was repeated 15 times.
We measured touch location ~ t and angle , also per participant.
9px -11px Similar to 2 other machine learning the data was 5 6 1 2 3 4 5 6 1 3 4 5 approaches, 6 split randomly and used in two ways: Two thirds were used Figure 4.
X offset depending on the predecessor.
It variates similarly in to generate a model, the other third was used for evaluation.
They indicate how people typically over- or under-shoot.
We chose to use median instead of the average as the former is more outlier-resistant.
The correction term can also be based only on per participant data to account for individual behavior.
This results in two simple look-up tables.
The questionnaire revealed that participants had experience using small touch devices such as smartphones  and little experience using table-sized touch systems .
Each participant performed the study in less than 10 minutes.
We create two datasets based on the touch data: corrected for the predecessor as well as the corrected for predecessor and user.
For comparison, we calculate the size of a minimal rectangular button that covers at least 95% of the touches, i.e., its width and height is equal to about 1.96 standard deviations of the mean of the x and y data.
We evaluated how touch sequences have an effect on the accuracy of their single touches.
We saw that the location of the previous touch affects the location and orientation of the following touch.
We applied this knowledge to an off-theshelf touch table and were able to improve its accuracy with less than 5 minutes of per participant setup.
It is quite likely that even this short learning phase can be performed during systems usage, making no setup necessary while improving touch accuracy for any large touch surface.
P. Albinsson and S. Zhai.
High precision touch screen interaction.
Back-of-device interaction allows creating very small touch devices.
The information capacity of the human motor system in controlling the amplitude of movement.
C. Harrison, J. Schwarz, and S. E. Hudson.
TapSense: enhancing finger interaction on touch surfaces.
C. Holz and P. Baudisch.
The generalized perceived input point model and how to double touch accuracy by extracting fingerprints.
N. Marquardt, J. Kiemer, D. Ledo, S. Boring, and S. Greenberg.
Designing user-, hand-, and handpart-aware tabletop interactions with the TouchID toolkit.
Constraints for action selection: Overhand versus underhand grips.
Lawrence Erlbaum Associates, Inc, Mahwah, New Jersey, 1990.
D. Vogel and P. Baudisch.
Shift: a technique for operating pen-based interfaces using touch.
F. Wang and X. Ren.
Empirical Evaluation for Finger Input Properties In Multi-touch Interaction.
Our explanation is that individual differences in touch behavior overshadow the predecessor effect.
However, taking individual behavior into account, we can shrink the button by 7%-- and we only needed to capture five minutes of user interaction.
We would like to vary more of our parameters: button sizes and shapes, relative user location, targeting speed, etc.
We also want to investigate which time is the right threshold to decide whether a touch can affect the next one.
Also, more predecessors could explain the sine shape of Fig.
In a real application, we would extend our 6-ring to a model that stores offset per relative direction  and relative distance to every predecessor.
This model could even be stored per each pixel to account for potential differences due to the relative location to the user.
As this would be a rather big model and thus hard to calibrate beforehand, we want to explore ways to learn this model during the systems usage, similar to what current smartphones do to correct for touch errors and personal touch behavior.
