To anticipate the main result, we found that standard Fitts' law  does not adequately model performance with magic lens interfaces, because the conditions of the visual feedback loop change during the movement, whereas it does adequately model the case in which no visual context is given outside the device display, i.e., when the handheld device acts as a dynamic peephole  or spatially-aware display .
When camera phones are used as magic lenses in handheld augmented reality applications involving wall maps or posters, pointing can be divided into two phases:  an initial coarse physical pointing phase, in which the target can be directly observed on the background surface, and  a fine-control virtual pointing phase, in which the target can only be observed through the device display.
In two studies, we show that performance cannot be adequately modeled with standard Fitts' law, but can be adequately modeled with a two-component modification.
We chart the performance space and analyze users' target acquisition strategies in varying conditions.
Moreover, we show that the standard Fitts' law model does hold for dynamic peephole pointing where there is no guiding background surface and hence the physical pointing component of the extended model is not needed.
Finally, implications for the design of magic lens interfaces are considered.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In order to explain the observed difference between these two types of selection tasks with camera phones, we present a two-part modification of Fitts' law that improves prediction of cameraphone-based selection performance in the magic lens pointing case.
A key idea of the model is to split interaction in two parts, one for initial targeting by direct observation and the second one for targeting through the magic lens.
For high-precision touch-screen pointing Sears and Shneiderman  proposed a two-stage model with five parameters that includes a term for gross arm movement and a term for fine-tuning motions of the fingers.
However they write that the analysis of their modification was inconclusive and do not provide any experimental data.
Target acquisition or pointing is a fundamental gesture in today's human-computer interfaces and has thus been thoroughly researched in numerous studies and for a wide range of input devices .
As a tool for predicting the time for directed movements, Fitts' law  has been used extensively in human-computer interaction.
An excellent overview of Fitts' law research is provided by MacKenzie .
According to Fitts, the movement time for a target pointing task involves a tradeoff between speed and accuracy: The larger the distance to be covered and the smaller the size of the target, the higher the movement time.
While Fitts' experiments only examined one-dimensional movements, Fitts' law has also been shown to hold for two-  and three-dimensional movements .
When visual feedback on the movement cannot be directly observed, but is mediated by some sensing mechanism, lag and update rate play a role.
The effects of lag and update rate in mediated visual feedback have been evaluated by Ware and Balakrishnan , Graham and MacKenzie , and others.
Magic lens pointing, which we investigate in this paper, has unique characteristics in that during the first phase of the interaction the target and the hand's movement towards the target can be directly observed, while during the second phase the target is occluded by the magic lens and can only be observed through the display, which introduces some non-negligible delay in the visual feedback.
Camera-based interaction with the above mentioned interfaces can be understood in terms of a Fitts' task.
For both magic lens and dynamic peephole pointing the fundamental components of interaction are rapid precise movements towards a point target or a spatially extended target.
Consequently, according to Fitts' law movement time in such a task depends on the distance to be covered and the size of the target.
Nevertheless, there are important differences between the case of camera-based selection and the general case of 2D selection: * Area selection  instead of point selection.
Depending on the implementation, the complete target might have to be present in the camera image to be recognized by the system.
Depending on the granularity of visual features of the background surface, there is a certain distance range within which the phone can detect those features.
The user has to adapt selection distance accordingly.
When targets are observed through the display rather than directly on the background surface an additional delay is introduced by the recognition system.
This delay is detrimental to performance .
The upper limit of the movement velocity is bound not only by the user's motor capacity, but also by the limits of the recognition system.
The term magic lens is used here to denote augmented reality interfaces that consist of a camera-equipped mobile device being used as a see-through tool.
It augments the user's view of real world objects by graphical and textual overlays.
When the device is held above an object or surface, for example a map, visual features in the scene can be highlighted and additional information overlaid in real-time to objects on the device's display .
There are many applications envisioned and implemented.
For example, a map at a bus stop can show a graphical overlay depicting the current positions of busses.
In tourist guide and city applications, information on various sights and events can be accessed by moving the phone to the respective targets and observing the graphical overlays on the mobile device's display .
In gaming applications, a poster or paper can represent fixed portions of the game space, for example the goal frame in a soccer penalty shootout game, and the rest of the game is to be viewed and interacted with through the magic lens that recognizes its position and orientation on the fixed frame .
Whereas magic lens interfaces are based on the idea of realtime augmentation of the real world scene, peephole interfaces  denote a class of interfaces where the viewport of a mobile device is used as a window into a virtual workspace and no visual context is available outside the display.
Traditional static peephole interfaces move the virtual workspace behind the static peephole, whereas dynamic peephole interfaces move the peephole across a static workspace .
The latter require a spatial tracking method in order to compensate for the movement of the peephole, such that the workspace appears at a constant position in space.
Yee  presents several example applications, such as a drawing program and a personal information space anchored to the user's body as a frame of reference.
Magic lens pointing can be regarded as an extension of dynamic peephole pointing, in which additional visual context is provided in the background.
Both are ways of improving information navigation on handheld devices and overcoming the limitations of the display size.
Since typically only a small part of a document can be visualized on a handheld device display at a time, the user needs effective mechanisms to continuously navigate to different parts of a document in order to mentally create a holistic understanding.
Magic lens pointing appears to be a particularly promising kind of interaction, since it allows augmenting large scale information presentation with private and up-to-date information on the personal display.
The background surface can be a passive printed paper document or an active electronic display.
The large scale background surface allows the user to quickly and effectively acquire the global structure of a document and then examine a small focus area in detail.
A large scale city map, for example, allows for a much quicker orientation than the small device display alone.
The frame rate of the camera - and hence the update rate of the display - is limited.
It lies typically between 15 and 30 Hz on current devices.
Yet, this is sufficient for perception of a smooth movement.
In comparison to the original experiments of Fitts, the z-coordinate of the cursor position has an effect on the appearance  of the target.
Moreover, the target can be selected from a wider selection space than what is possible with many other pointing devices.
Taken together, these factors may lead to more variable selection trajectories and more variable involvement of muscle groups.
The phone shows an augmented view of the background, but the hand occludes part of the background.
The user has to decide whether to acquire information from the background or through the magic lens and has to move hands so as to not occlude required information.
In dynamic peephole interfaces the target can only be observed through the device display when the device is positioned over the target.
The target is not present in the physical world.
In this case the basic Fitts' law  MT = ao + bo ID with ID = log2  is expected to lead to a good prediction of movement times.
MT is the movement time that the model predicts.
ID is the index of difficulty , D is the distance from the starting point to the target, and W is the width of the target.
Lag and low frame rates increase the coefficients ao and bo compared to direct observation of the target .
Our hypothesis is that with magic lens pointing the situation is different because there is an initial phase in which targets can be directly observed and a second phase in which the view on the target is mediated through the device.
We try to justify this hypothesis in the analysis below.
The magic lens situation is depicted in Figure 2.
We denote the first phase of magic lens pointing as physical pointing: The target  can be directly observed in the physical world.
At some point during the movement towards the target, the target falls below the magic lens and can no longer be observed directly, but only through the magic lens.
With a screen width of S the split point is located at a distance of S/2 at which half of the target is visible on the screen and half of it can be directly observed.
If we postulate a virtual target of width S, centered at the real target T, the first phase can be modeled as : MTp = ap + bp log2.
At the split point, the second phase - virtual pointing - begins: The target can now only be observed through the device.
The second phase starts at a distance of S/2 and can be modeled as  MTv = av + bv log2.
As summarized in the introduction, these include delay, display update rate, maximum distance and movement speed at which targets are recognized.
Moreover, especially for small targets, jitter - noise in the cursor position - becomes an issue.
Delay in particular has a direct influence on the control loop that governs rapid aimed movements.
It has been found that movements longer than 200 ms are controlled by visual feedback .
The Fitts' law constants a and b can thus be interpreted in terms of a visual feedback loop or control loop that is assumed to underlie targeted movements.
The deterministic iterative corrections model  assumes that a complete movement is made up of a series of n ballistic submovements, each taking a constant time t and covering a constant amount 1- of the remaining distance.
Thus the first submovement starting at distance D ends at distance D, the second starts at D and ends at 2D, and so on, until a submovement ends within the target, i.e., nD  W/2.
Equation  can be rewritten in terms of the time needed to make a corrective submovement t and in terms of machine lag L if we write b and c as b =  t and c =   : MT = a +  t log2 +   log2  To empirically assess the two-part Fitts' law model derived in this analysis, we conducted two experiments.
The first experiment examined magic lens pointing, and the second dynamic peephole pointing.
The movement process starts with detecting the stimulus and initiating a ballistic movement.
In physical pointing , in which targets are directly visible and not mediated through the device, the control loop consists of perceiving the current distance to the target, planning the next ballistic micromovement, and effecting hand movement.
In their Model Human Processor  Card et al.
Hence the total duration for one cycle is t = P + C + M = 240 ms, which is in the range cited in .
The experiments were carried out utilizing the cyclical multi-direction pointing task paradigm of ISO 9241-9 .
Put briefly, there are nine targets visible; on a large background surface in Experiment 1 , and in the virtual plane in Experiment 2 .
One target at a time is marked in a circle of nine targets, and that target should be selected by moving the crosshair on the phone's display and pressing the joystick button.
Preferring the multi-directional over the one-directional task was natural, because in real world applications objects and areas are dispersed on a larger area surface.
Ware and Balakrishnan  analyze the contributions of lag and frame rate to the constant b in basic Fitts' law .
If the observation of the targets is mediated by the device - i.e., the targets are only visible through the device display - then a machine lag component is introduced into the control loop .
In both magic lens and dynamic peephole pointing, the integrated camera of the device is used as a position sensor.
Images are taken at regular intervals, for example with a frame rate of 15 Hz.
First, there is a delay m1 caused by the image acquisition hardware, i.e., when a frame reaches the tracking algorithm it shows the situation m1 milliseconds ago.
The time the algorithm needs to process the frame adds another component m2.
The time to render the result on the display is m3.
Hence when the sensed position becomes visible on the display it shows the situation D = m1 + m2 + m3 milliseconds ago.
Assuming a uniform distribution of the perception in the frame interval TF, the total machine lag is on average L = D + 0.5 TF.
With the devices and algorithms used in the experiments, the total machine lag amounted to 118 ms for Experiment 1  and 294 ms for Experiment 2 .
In the setup we used, the computational complexity of the dynamic peephole interface was higher than for the magic lens interface, which required a more powerful device.
Twelve subjects  were recruited, most from TU Berlin and the rest from collegelevel institutes.
Ten subjects were right-handed, one was left-handed and one ambidextrous.
Only two used the camera on their camera phone regularly.
The subjects were paid a small incentive for participation.
All subjects were healthy and had normal or corrected-to-normal vision.
The experiment was conducted on a custom-tailored system consisting of a PC communicating over Bluetooth with the mobile phone to control the highlighting of the target item on a large display .
A Nokia 6630  was utilized as the selection device.
Its camera frame rate is 15 fps, the resolution of the view finder is 160x120 pixels and the display area is 32x24 mm.
It showed the camera view finder stream and a crosshair in the center of the screen that indicated the cursor hotspot position.
The application also highlighted recognized visual markers in the camera image with yellow rectangles.
Users could select a recognized visual marker by pressing the phone's joystick button.
A Java application on the PC received user input via Bluetooth and updated the display between the trials accordingly.
The display center was positioned 1.5 m above the floor.
The display showed 9 visual markers in black on white background in a circular arrangement with an angular spacing of 40.
The to-be-selected target was presented with a red frame appearing around the visual code.
Standing position in front of the display was fixed by positioning a stopper on the floor to a distance where the subject could touch the screen with an extended arm.
To start the block, the subjects had to move the crosshair in the view finder on top of the target and press the joystick button.
If a target was missed, a brief beep sound was played.
In such a situation, subjects were instructed not to try to correct the error, but to continue to the next target.
After each block, there was a resting period of at least 15 seconds and, after the experiment, background information of the subject and verbal accounts of selection strategies were collected.
The experiment yielded 10692 data points .
Responses for which the system could not detect a marker  and first selection in a block  were not included in the movement time  analysis.
These removals left 9940 data points.
In the cyclical selection paradigm, targets always appear in the same order: starting from the top item, the next item is always opposite and slightly clockwise from the selected one.
One block consists of all nine items selected three times.
The subjects were instructed to select the highlighted item as quickly and accurately as possible.
Even though within a block the subjects know where the next target will be, they still have to perform a goal-directed movement as fast as possible.
As in the classic Fitts' law studies , we varied target width W and distance D. The obtainable W and D combinations were limited by the size of the plasma display and the minimal marker size that the system could recognize.
Distances between successive targets ranged from 55 to 535 mm.
For each target width, three distances were specified to cover a wide range of index of difficulty  values: The minimum distance such that the targets on the sphere would not overlap; the maximum such that all targets would fit on the large display; and a distance with ID computed as the mean of the above.
33 combinations of W and D were generated in this way.
Each W, D combination was held constant for three rounds , after which another W, D pair was selected.
Each participant was presented with a unique randomly generated permutation of the combinations.
Altogether 9 non-randomized practice blocks were carried out by each subject.
Thus, the total number of selections per subject was 9 blocks x 3 rounds per block x 9 selections per round = 243 selections for practice; and 33 blocks x 3 rounds x 9 selections = 891 selections for the actual experiment.
Collapsed over the experimental conditions, the mean MT was 1.22 sec  with a relatively high error rate of 7%.
As shown in Figure 5, the error rate was high for small targets only.
This is partly due to the limits of automatic marker recognition being pushed by hand jitter.
For targets greater than 40 mm, the error rate is below 4%.
This is quite comparable to reports of other input devices on mobile phones, such as joysticks .
Participants' performance improved during the experiment.
The slope and intercept of the regression line were -0.009 and 1.354, respectively.
The small slope implies that only minor learning effects occurred after the practice trials.
MT decreases with growing W, but levels off at about 50 mm.
Further increasing W does not decrease MT.
The effect of target distance D on MT is more complex due to interaction with W. We observed fast movement times and low error rates for large-enough ID values, enabling what was called the lineof-sight selection strategy.
In such a situation the silhouette of the device is used as a selection cue, enabling more attention to eye-hand-coordination than the display of the device.
The strategy leads to superior performance, but is only possible when the targets are not too densely spaced so that occlusion of the target by the device occurs only in the final phase of selection.
The size with which the target appears on the magic lens display depends on the vertical distance  between the camera lens and the background surface.
The closer the camera, the larger the target appears on the display; the further away, the smaller the target gets.
There was a linear relationship mapping target width W to zdistance .
On average, large targets were selected from a distance of 22 cm, small ones from 10 cm, medium-sized targets falling in between.
Figure 6 plots z-distance by target width W. The boxplots for each W show the 25% quartile, the median, and the 75% quartile of the z-distance, as well as the minimum and maximum values.
The three lines with different slopes are: * Blue line : The closest z-distance such that the complete target is contained in the camera image.
The target fills the whole display.
The target appears very small on the display.
The height of the target equals half the display height.
For the basic Fitts' law model we only used target distance D and target width W as independent variables, with ID = log2.
For all 33 combinations of D and W we computed mean MT values.
Each combination contains 289 selections on average.
In each group outliers of more than 2 SD from the mean were removed in this calculation.
We follow the reasoning  of Ware and Balakrishnan  and analyze the data in terms of the unmodified index of difficulty, using the real target width rather than the effective target width We .
We is computed post-hoc based on the standard deviation of the spread around the target.
The aim of the modified ID is to provide a more accurate measure of the rate of information processing.
The first reason to use the unmodified ID is that it accounts for more of the variance.
The second reason is that it can be used to predict actual performance in a particular situation, since it is based on the real target width.
This reflects the conditions in the experiment where targets were densely spaced and participants relied on a more displaybased strategy: The targets were observed through the magic lens for a longer time than for the larger ID values, i.e., the duration of the first phase  relative to the duration of the second phase  was shorter for small ID values than for larger ID values.
With increasing ID values, i.e., more widely spaced targets, the physical pointing phase was longer relative to the virtual pointing phase, enabled by the fact that the phone occludes the targets later in the pointing process.
The traditional case is cursor pointing, which involves translating a point cursor onto the target.
View pointing is defined as adjusting the view such that the target becomes visible and the view contains all parts of the target.
View pointing has been defined in the context of multiscale user interfaces, in which pointing involves navigation to the appropriate scale to make the target visible .
Fitts' law then becomes ID = log2, where W1 is the width of the view and W2 is the width of the target visible.
The border distance between cursor pointing and view pointing is reached when the target's height is half the height of the camera image.
Beyond that, the presence of the complete target on the display implies that the cursor is on the target.
The figure shows that participants preferred cursor pointing over view pointing, because the median z-
In Figure 8 the basic Fitts' law prediction is illustrated in a slightly different way.
For each of the  combinations it shows on the x-axis the movement time predicted by the model and on the y-axis the movement time actually measured.
For a perfect model, all data points would lie on the bisecting line.
For the basic Fitts' law model there is a particularly large spread of measured MT values  for a predicted MT of 1.04 sec.
These  combinations all have the same ID = log2 = 2.4 and denote the most densely spaced targets.
Twelve subjects  were recruited from the Helsinki University of Technology.
Eleven subjects were right-handed and one was left-handed.
Three had more than sporadic experience with camera phones.
The subjects were not paid for their participation.
All subjects were healthy and had normal or corrected-to-normal vision.
A Nokia N80  was utilized in the experiment.
It features a camera able of 15 fps and a 3.5x4.1 cm display with a resolution of 352x416 pixels.
As in Experiment 1, a crosshair in the center of the screen indicated the cursor hotspot position.
The targets were rendered on the screen according to 3D position recognized from the camera image.
Again, the target was highlighted with a red frame and the subject should select it by pressing the phone's joystick button.
All feedback  were provided on the mobile device.
The targets were again circularly arranged.
The circle was always centered at the middle of the tracking surface.
At the beginning of each block participants were instructed to move across the tracked area to learn the positions of the targets.
Following a camera view model, participants could get more overview by pulling back from the tracking surface to zoom out.
Beyond that, no visual aid was given during the trials.
A visual marker grid printed on a landscaped A0 paper sheet was used as the background surface for recognizing the position of the phone.
The size of the tracked area was the same as in Experiment 1.
In the analysis below we follow the same groupings  as in Experiment 1.
Again, in each group outliers of more than 2 SD from the mean are excluded, unless otherwise noted.
Collapsed over the experimental conditions, the mean MT was 2.13 sec  with an error rate of 5%.
As shown in Figure 10, the highest error rates again appear at low target widths.
For target widths W > 4.8 cm the error rate is below 4%.
Error rates are lower than for physical pointing, possibly because overall movement speed was slower in the dynamic peephole case.
Participants could not see the targets in the first phase of pointing and hence chose another point in the speed-accuracy tradeoff.
When including all outliers the resulting regression is: MT = -2.210 + 1.618 log2 + 1.176 log2, R2 = 0.96 That the same prediction accuracy can be reached with the two-part model is not surprising, since every Fitts' law task can be regarded as made up of smaller component Fitts' law tasks with identical characteristics in terms of delay.
We arrived at a model that includes just one additional parameter in comparison to the basic model.
This model predicts movement time in the magic lens case much better .
When treating the split point, i.e., the display size of the magic lens, as an additional parameter, the least squares parameter estimation predicts a display size close to the actual display size.
This supports the validity of the model.
Augmented reality interfaces project digital information on the real world scenery in real time.
In augmented reality interaction using camera-equipped mobile devices in particular, this layer is visualized through the narrow viewport of the device.
Since the digitally projected space does not fit in the display all at once, the user must actively move the "keyhole" to explore the space and its objects of interest, all the time relying on system feedback for the identity and location of overlaid objects.
The limits of the acuity of the human visual system, the physical size of displays in mobile devices, and the computational costliness of real-time processing of visual image data all speak for the claim that this problem cannot be expected to disappear for a while.
Although augmented reality interaction on mobile devices have this one characteristic in common, even a cursory examination of the numerous application ideas reveals that interaction types fall into two quite distinct categories: * the objects of interest are visible on the physical surface used for positioning or * the objects of interests do not map to the real world but exist only in the projected virtual space.
In the first case the augmented reality information is projected on real world objects, meaning that these physical objects unambiguously mark the location of the digital information.
In the second case only the real world space is utilized, all projected objects are "new" and there is no direct mapping to features of the environment.
Analyzing these two situations, we arrived at the conclusion that these two situations map to two different interaction tasks known in the literature: magic lens pointing and dynamic peephole pointing, respectively.
It has not been previously reported how users perform in these two situations and if our standard methods of modeling apply here.
To address this issue, we reported two controlled experiments utilizing the cyclic pointing paradigm.
The results indicate that there is a fundamental difference in the nature of the tasks themselves.
We found that the standard Fitts' law model predicts performance quite well in the dynamic peephole pointing task , but not in the magic lens pointing task .
The presumption of the standard model that the feedback loop is governed by constant processing times throughout the interaction is violated in the magic lens case.
Augmented reality interaction is crucially different in the two tasks we explored.
While our data comes from a specific setting, the model allows for speculatively exploring the effects of changing parameters.
The proposed model can be used directly to make hypotheses in similar interaction situations.
For example, by varying the parameters lens size S and machine lag L, we arrive at the following implications: * Increasing lens display size S means that the first logarithmic term in equation  becomes smaller and the second logarithmic term becomes larger, which results in a shorter physical phase relative to the virtual phase.
Since the multiplicative factor associated with the virtual phase is larger, the overall movement time should increase.
On the other hand, you cannot decrease S too much, because although it minimizes occlusion, it provides less screen real estate to display information.
It is thus critical that lag is minimized.
It is possible that performance on both task types could be significantly improved with advance cues that help guide movement before the target candidate is on the display.
Such cues can relate the location of the target - as in techniques utilizing halos  - and perhaps its identity, and they can give overviews or maps of the distribution of targets in the space - as in applications of focus+context techniques .
We analyzed target acquisition with camera phones as magic lenses and as dynamic peephole displays.
In the first case, some external visual context is augmented by the device.
In the second case the device is spatially tracked, but there is no visual context outside the device's display.
We have shown that dynamic peephole pointing can be modeled by Fitts' law.
In dynamic peephole pointing the whole interaction is mediated by the device in a uniform way - there are no distinguishable phases as in magic lens pointing.
By contrast, even though the magic lens had a shorter machine lag than the dynamic peephole interface, it was not adequately explainable by Fitts' law.
Since the device introduces some non-zero delay, the characteristics of visual feedback are different in the first and the second phase.
In the magic lens setup, this leads to a weak prediction of movement times when basic Fitts' law is used .
To more adequately model the situation of magic lens pointing we introduced a two-part model with three parameters  that led to more accurate predictions .
We expect that magic lens interaction will become more popular in the future, since a large range of applications are conceivable if robust camera-based tracking is available for camera-equipped mobile devices.
