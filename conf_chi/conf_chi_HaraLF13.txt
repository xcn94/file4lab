Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori.
In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk  to find, label, and assess sidewalk accessibility problems in Google Street View imagery.
We report on two studies: Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users; Study 2 investigates the comparative performance of turkers.
We show that turkers are capable of determining the presence of an accessibility problem with 81% accuracy.
With simple quality control methods, this number increases to 93%.
Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.
According to the most recent US Census , roughly 30.6 million individuals have physical disabilities that affect their ambulatory activities .
Of these, nearly half report using an assistive aid such as a wheelchair  or a cane, crutches, or walker  .
Despite aggressive civil rights legislation for Americans with disabilities , many city streets, sidewalks, and businesses in the US remain inaccessible .
The problem is not just that sidewalk accessibility fundamentally affects where and how people travel in cities but also that there are few, if any, mechanisms to determine accessible areas of a city a priori.
Indeed, in a recent report, the National Council on Disability noted that they could not find comprehensive information on the "degree to which sidewalks are accessible" across the US .
Traditionally, sidewalk assessment has been conducted via in-person street audits , which are labor intensive and costly , or via citizen call-in reports, which are done on a reactive basis.
As an alternative, we propose the use of crowdsourcing to locate and assess sidewalk accessibility problems proactively by labeling Google Street View  imagery .
We report on two studies in particular: a feasibility study  and an online crowdsourcing study using Amazon Mechanical Turk .
Because labeling sidewalk accessibility problems is a subjective and potentially ambiguous task, Study 1 investigates the viability of the labeling sidewalk problems amongst two groups of diligent and motivated labelers: three members of our research team and three "sidewalk accessibility experts"--in this case,
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
We use the results of this study to:  show that the labeling approach is reliable, with high intraand inter-labeler agreement within and across the two groups;  acquire an understanding of baseline performance--that is, what does good labeling performance look like?
For Study 2, we investigate the potential of using crowd workers on Mechanical Turk  to perform this labeling task.
We evaluate performance at two levels of labeling accuracy: image level, which tests for the presence or absence of the correct label in an image, and pixel level, which examines the pixel-level accuracies of the labels provided .
We show that, when compared to ground truth, turkers are capable of determining that an accessibility problem exists in an image with 80.6% accuracy  and determining the correct problem type with 78.3% accuracy .
Using a simple majority voting scheme with three turkers, this accuracy jumps to 86.9% and 83.8% respectively.
We also examine the effect of two quality control mechanisms on performance: statistical filtering and multilevel review .
Our findings suggest that crowdsourcing both the labeling task and the verification task leads to a better quality result.
We also demonstrate the performance/cost tradeoffs therein.
The primary contributions of this paper are threefold:  the first step toward a scalable approach for combining crowdsourcing and existing online map imagery to identify perceived accessibility issues,  measures for assessing turker performance in applying accessibility labels, and  strategies for improving overall data quality.
Our approach could be used as a lightweight method to bootstrap accessibility-aware urban navigation routing algorithms, to gather training labels for computer vision-based sidewalk assessment, and as a mechanism for city governments and citizens to report on and learn about the health of their community's sidewalks .
In the US, state and federal departments conduct and encourage road safety audits that can also include walkability and pedestrian access .
Less formally, community organizations organize "Walk Audits" to find and assess deficiencies such as missing sidewalks, curb ramps, and/or dangerous street crossings .
Participatory reporting of accessibility problems has also been accomplished through applications that allow citizens to report non-emergency neighborhood issues to local government agencies  or to share information on wheelchair accessibility of businesses ; however, these applications do not support remote, virtual inquiry and have not been shown to scalably collect data on accessible public rights-of-way.
Street and neighborhood audits are also conducted by researchers in public health, sociology, and urban planning with the goal of studying the built environment and its impact on human behavior .
Since physical audits are often time-consuming and expensive , some studies have explored more efficient methods including recording video while driving for later review , or using satellite imagery and other map tools .
Omnidirectional streetscape imagery such as that in GSV has recently been used to perform virtual audits .
Reported benefits include time-savings and the ability to monitor and analyze multiple cities from a central location .
As an emerging area of research, work thus far has focused on the robustness and reliability of such approaches.
Most importantly for our work, high levels of concordance have been reported between GSV vs. physical audit data for measures including pedestrian safety, traffic and parking, and pedestrian infrastructure .
Finally, most relevant to our work is the recent CrossingGuard paper by Guy and Truong , which focuses on navigation aids for visually impaired pedestrians and includes a small-scale study of GSV with three turkers.
While similar in spirit, Guy and Truong focus exclusively on intersections for the visually impaired while we examine sidewalks for people with mobility impairments.
More importantly, we ask turkers to mark perceived accessibility problems  while  asks turkers to "check-off" the existence of traffic objects .
Lastly, we ask turkers to directly label pixels, which helps establish an important initial baseline for collecting streetview accessibility training examples for computer vision.
We provide background on sidewalk accessibility and sidewalk audit methods, in addition to related work on crowdsourcing and image labeling.
The US Department of Transportation  and the US Access Board  describe common problems that inhibit pedestrian access, including:  no place to walk--paths are either non-existent or not well-connected to destinations such as schools and transit;  poor walking surfaces;  blocked pathways, either temporarily  or permanently ;  difficult street crossings ;  narrow sidewalks: wheelchair and scooter users require a wider path than ambulatory pedestrians, with most guidelines suggesting at least 60 inches.
Our image labeling task is analogous to that commonly performed in computer vision research for image segmentation, and object detection and recognition .
These tools differ in the level of information acquired about each image and their userbase.
For example, in von Ahn et al.
LabelMe  provides even more granular segmentation by allowing users to draw polygonaloutlines around objects, which are publically viewable and editable.
Finally, to our knowledge, Sorokin and Forsyth  were the first to experiment with "outsourcing" image labeling to Mechanical Turk.
In a series of experiments, they showed that a large number of high quality image annotations could be acquired relatively cheaply and quickly.
Others have successfully used Mechanical Turk for a variety of purposes including document editing , graphical perception experiments , and near real-time assistance with visual problems for blind people .
Other labeling techniques were explored in early prototypes .
The test dataset used in the labeling interface consists of 229 images manually scraped by the research team using GSV of urban neighborhoods in Los Angeles, Baltimore, Washington DC, and New York City.
We attempted to collect a balanced dataset.
Of the 229 images, 179 contained one or more of the aforementioned problem categories; 50 had no visible sidewalk accessibility issues and were used, in part, to evaluate false positive labeling activity.
Based on our majority-vote ground truth data , we determined the following composition: 67 images with Surface Problems, 66 images with Object in Path, 50 with Prematurely Ending Sidewalk, and 47 with Curb Ramp Missing.
This count is not mutually exclusive-- 48 images in total included more than one problem type.
The label Other was used 0.5% of the time in Study 1 and 0.6% in Study 2 and is thus ignored in our analyses.
We return to the potential issue of image age in the discussion.
To collect geo-labeled data on sidewalk accessibility problems in GSV images, we created an interactive online labeling tool in JavaScript, PHP and MySQL .
We also created a verification interface  where users could accept or reject previously collected labels.
Below, we describe the annotation interface and the primary dataset used in our studies.
We return to the verification interface in the Study 2 section.
For the annotation interface, labeling is a three-step process consisting of marking the location of the problem , categorizing it into one of five types, and assessing its severity.
For the first step, the user draws an outline around the perceived accessibility problem in the image .
A pop-up menu then appears with five problem categories: Curb Ramp Missing, Object in Path, Surface Problem, Prematurely Ending Sidewalk, and Other.
After a problem category has been selected, a five-point Likert scale appears, asking the user to rate the severity of the problem where 5 is most severe  and 1 is least severe .
The label is then complete.
After all identified sidewalk problems have been labeled in a given image, the user can select "submit labels" and another image is loaded.
Images with no apparent sidewalk problems can be marked as such with a button labeled "There are no accessibility problems in this image."
In this section, we provide an overview of the correctness measures used in our two studies.
Because this is a new area of research, we introduce and explore a range of metrics--many of which have different levels of relevancy across application contexts .
Assessing annotation correctness in images is complex.
To guide our analysis, we derived two spectra that vary according to the type and granularity of data extracted from each label: the localization spectrum and the specificity spectrum.
The localization spectrum describes the positioning of the label in the image, which includes two discrete levels of granularity: image level and pixel level.
For image level, we simply check for the absence or presence of a label anywhere within the image.
Pixel level is more precise, examining individual pixels highlighted by the label outline.
Our pixel-level analysis is analogous to image segmentation in computer vision and, indeed, our evaluation methods are informed from work in this space.
The specificity spectrum, in contrast, varies based on the amount of descriptive information evaluated for each label.
Figure 3: The verification interface used to experiment with crowdsourcing validation of turker labels--only one label is validated at a time in batches of 20.
At the finest level of granularity, we check for matches based on the five label categories as well as corresponding severity ratings: Object in Path, Prematurely Ending Sidewalk, Surface Problem, Curb Ramp Missing , and No Problem .
Note that Curb Ramp Missing and No Problem were exempt from severity ratings.
At the next level of granularity, we only examine problem types, ignoring severity ratings; we refer to this level as multiclass.
Finally, at the coarsest level of granularity we group all problem categories into a binary classification of problem vs. no problem.
As the first work in the area, these dimensions of analysis are important for understanding crowd worker performance across various measures of correctness.
Identifying an appropriate level of correctness may depend on the specific application context.
For example, because of the focal length and camera angles used in GSV imagery, simply identifying that an accessibility problem exists in an image  localizes that problem to a fairly small geographic area: a specific street side and sidewalk within a city block.
This level of geographic precision may be sufficient for calculating accessibility scores or even informing accessibility-aware routing algorithms.
Binary classification--whether at the image level or the pixel level--also helps mitigate the subjectivity involved in selecting a label type for a problem .
In other cases, however, more specific correctness measures may be needed.
Training computer vision algorithms to segment and, perhaps, automatically identify and recognize obstacles, would require pixel-level, multiclass granularity.
As a result, we incorporated a second set of correctness measures, which extend from work in information retrieval: precision, recall, and an amalgamation of the two, fmeasure.
All three measures return a value between 0 and 1, where 1 is better:
True positive here is defined as providing the correct label on an image, false positive is providing a label for a problem that does not actually exist in the image, and false negative is not providing a label for a problem that does exist in the image.
In this way, precision measures the accuracy of the labels actually provided  while recall measures the comprehensiveness of the correct labels provided .
For example, a precision score of 1.0 means that every label the turker added was correct but they could have missed labels.
A recall score of 1.0 means that the turker's labels include all of the actual problems in the image but could also include non-problems.
Given that algorithms can be tuned to maximize precision while sacrificing recall and vice versa, the f-measure provides a single joint metric that encapsulates both.
We use accuracy, precision, recall, and fmeasure to describe our image level results.
For image-level analysis, we computed two different correctness measures: a straightforward accuracy measure and a more sophisticated measure involving precision and recall.
For accuracy, we compare ground truth labels with turker labels for a given image and calculate the percentage correct.
For example, if ground truth labels indicate that three problem types exist in an image: No Curb Ramp, Object in Path, and a Surface Problem, but a turker only labels No Curb Ramp, then the resulting accuracy score would be 50% .
Pixel-level correctness relates to image segmentation work in computer vision.
Zhang  provides a review of methods for evaluating image segmentation quality, two of which are relevant here: the goodness method, which examines segmentation based on human judgment and the empirical discrepancy method, which programmatically calculates the difference between test segmentations and "ground truth" segmentations for a given image.
The goodness method can be advantageous in that it does not require ground truth; however, it is labor intensive because it relies on human judgment to perceive quality.
Thus, we also explored two empirical discrepancy methods: overlap   and, again, precision/recall combined with f-measure , which is similar to that explained above though applied at the pixel level rather than the image level.
For our first discrepancy method, overlap is defined as:
We explore agreement at both the image level and the pixel level across binary and multiclass classification.
Three wheelchair users were recruited via listservs and word-of-mouth: two males with spinal cord injury  and one male with cerebral palsy.
All three used motorized wheelchairs; one also used a manual wheelchair but rarely.
Each wheelchair user took part in a single labeling session at our research lab.
Participants were asked to label the images based on their own experiences and were instructed that not all images contained accessibility problems.
They were also asked to "thinkaloud" during labeling so that we could better understand the rationale behind their labeling decisions.
The sessions lasted for 2-3 hours and included a short, postlabeling interview where we asked about the participant's personal experiences with sidewalk/street accessibility and about potential improvements to our labeling tool.
In consideration of participant time and potential fatigue, only a subset of the total 229 image dataset was labeled: 75 in total.
These images were selected randomly from each of the four problem categories  plus an additional 15 randomly selected "no problem" images.
Participants were compensated $25-35 depending on session length.
Below, we report on evaluating agreement between the researchers, the wheelchair users, and the researchers compared to the wheelchair users.
For the latter calculation, we compare majority vote data from each group so N=2 rather than N=6.
We describe both image-level and pixel-level performance.
We computed inter-rater agreement on labels at the image level using Fleiss' kappa , which attempts to account for agreement expected by chance.
As this was an imagelevel analysis, we tested for agreement based on the absence or presence of a label in an image and not on the label's particular pixel location or severity rating.
Multiple labels of the same type were compressed into a single "binary presence" indicator for that label.
For example, if three individual Surface Problems were labeled in an image, for our analysis, we only considered the fact that a Surface Problem was detected and not how many occurrences there were exactly.
This helped control for different annotator tendencies--some who would provide one large label to cover contiguous problem areas and others who would provide separate labels.
Results are shown in Table 1 for both binary and multiclass classification .
Note that if the outline A is perfectly equal to the outline B, then Overlap=1.
If A and B are disjoint, then Overlap=0.
Although this metric is easy to understand, similar to the straightforward accuracy measure for image-level analysis, it fails to capture nuances in correctness.
Thus, for our second discrepancy metric we define precision, recall, and fmeasure at the pixel level.
From the image segmentation literature , precision is defined as the probability that a generated outline-fill pixel area correctly highlights the target object and recall is the probability that a true outlinefill pixel is detected.
Thus, in order to calculate precision and recall at the pixel level, we need to compute three different pixel counts for each image:
True positive pixels: number of overlapping pixels between the ground truth segmentation and the test segmentation; 2.
False positive pixels: number of pixels in the test segmentation not in the ground truth segmentation; 3.
False negative pixels: number of pixels in the ground truth segmentation not in the test segmentation.
Before calculating pixel-level correctness for any of the measures, we flatten all labels with equivalent type into the same layer and treat them as a single set of pixels.
This allows us to more easily perform pixel-by-pixel comparison between ground truth labels and test labels marked with the same problem type.
Labeling accessibility problems perceived in streetscape images is a subjective process.
As such, our first study focused on demonstrating that informed and well-motivated labelers could complete the labeling task and produce consistent results.
We had two additional goals:  to produce a vetted ground truth dataset that could be used to calculate turker performance in Study 2, and  to help contextualize Study 2 results .
We collected independently-labeled data from two groups: three members of our research team and three wheelchair users .
This is likely because Object in Path and Surface Problems are often less salient in images and because they are occasionally substituted for one another .
We emphasize that even 10-15% overlap agreement at the pixel level would be sufficient to confidently localize problems in images and highlight these areas in accessible map routing interfaces.
This level of consistency, however, may not be sufficient for training computer vision.
We return to this point in the discussion.
Calculating pixel-level agreement is more challenging.
Because no widespread standards exist for evaluating pixellevel agreement for human labelers, we followed the process prescribed by Martin et al.
We verify the labeling process by showing that pixel-level label overlap and f-measure scores are higher between labelers on the same image than across different images.
These scores will later act as a baseline for defining good pixel-level performance when evaluating turker labels.
To compare between the same images, 678 comparisons are required .
Because the wheelchair users only labeled 75 of the 229 images, their comparison count is correspondingly lower .
We ignore images for which all annotators labeled No Problems Found .
Our results are shown in Table 2.
From these results, we conclude that our pixel level annotations across labelers are reasonably consistent, although less so than for image level.
Unsurprisingly, agreement is higher for binary classification than for multiclass, though not substantially.
We emphasize, however, that pixel outlines for even the same object across labelers will rarely agree perfectly; the key then, is to determine what level of overlap and f-measure scores are acceptable and good.
Finally, now that we have shown the feasibility of the labeling task and found reasonably high consistency amongst labelers, we can use these Study 1 labels to produce a ground truth dataset for evaluating turker performance.
We consolidate the labeling data from the three researchers into four unified ground truth datasets: binary and multiclass at both the image and the pixel level Consolidating Image-Level Labels: To combine imagelevel labels across the three labelers, we simply create a majority-vote "ground truth" dataset.
Any image that received a label from at least two of the three researchers was assigned that label as "ground truth."
Consolidating Pixel-Level Labels: Combining labels from the three researchers at the pixel level is less straightforward.
The consolidation algorithm will directly impact the results obtained from our correctness measures.
For example, if we combine highlighted pixel areas across all three researchers , then turker precision is likely to go up but recall is likely to go down.
If, instead, we take the intersection across all three labelers, the ground truth pixel area will shrink substantially, which will likely increase turker recall but reduce precision.
Consequently, we decided to, again, adopt a majority vote approach.
To produce the majority vote pixel-level dataset, we look for labels from at least two of the three researchers that overlap by 15% of their unioned area.
The value of 15% was chosen because it is the lower-quartile cutoff using researcher overlap data.
For binary classification, the label type was ignored--thus, any labels that overlapped by 15% or more were combined.
For multiclass, the labels had to be of the same type.
To investigate the potential of using untrained crowd workers to label accessibility problems, we posted our task to Mechanical Turk during the summer of 2012.
Each turker new to the task was required to watch at least half of a 3-minute instructional video, after which the labeling interface automatically appeared.
Note: one task encompasses labeling one image.
We first describe high-level results before performing a more detailed analysis covering labeler count vs. accuracy, two quality control evaluations, and the best and worst performing images.
For the analysis below, we do not consider severity ratings.
Instead, we leave this for future work.
However, given that we found a high rate of false positives amongst the turker data, we did examine the effect of removing labels that received a severity rating of a 1  or a 2 .
Our findings did not change significantly as a result.
In all, we hired 185 distinct turkers who completed 7,517 image labeling tasks and provided a total of 13,379 labels.
When compared with our ground truth dataset, overall turker accuracy at the image level was 80.6% for binary classification and 78.3% for multiclass classification.
At the pixel level, average area overlap was 20.6% and 17.0% for binary and multiclass, respectively.
These numbers are reasonably close to the values of 27% and 23% that we saw for wheelchair users vs. researchers.
Because we have 28 turkers per image, we can run the analysis multiple times for each group size, average the results, and calculate error margins .
For example, when we set the majority vote group size to three, we randomly permute nine groups of three turkers.
In each group, we calculate the majority vote answer for a given image in the dataset and compare it with ground truth.
To compute the majority vote answer for each group size, we use the same label consolidation process as that used for the researcher majority vote labels.
Collecting multiple annotations per image helps account for the natural variability of human performance and reduces the influence of occasional errors; however, it also requires more workers .
Here, we explore accuracy as a function of turkers per image.
We expect that accuracy should improve as the number of turkers increases, but the question then, is by how much?
To evaluate the impact of the number of turkers on accuracy, we collected labels from 28 or more turkers for each of our 229 images.
We conducted this analysis at the image and pixel levels for binary and multiclass classification across our multiple correctness measures.
Results are shown in Figure 4  and Table 3 .
As expected, performance improves with turker count but these gains diminish in magnitude as group size grows.
For image-level multiclass, we see a similar trend.
At the pixel level, the binary area overlap measure improves from 20.6% to 30.3% with 5 turkers but only to 31.4% with 9 turkers.
Again, multiclass performance is similar .
Even though group sizes beyond 5 continue to improve results at both the image and pixel level, this benefit may not be worth the additional cost.
Note that for the pixel level, the recall score rises dramatically in comparison to other metrics.
This is because the consolidated majority vote pixel area tends to grow with turker count .
Figure 5:  Show the effect of increasingly aggressive turker elimination thresholds at the image- and pixel-levels based on average multiclass performance of 5 images.
Error bars are standard deviation  and standard error .
As the threshold increases, fewer turkers remain and uncertainty increases.
We explore two quality control approaches: filtering turkers based on a fixed threshold of acceptable performance and filtering labels based on crowdsourced validations collected through our verification interface.
In both cases, we perform our analyses offline, which allows us to simulate performance with a range of quality control mechanisms.
Statistical Filtering: For the first approach, we explored the effect of eliminating turkers based on their average multiclass performance at both the image and pixel level.
The goal was to uncover effective performance thresholds for eliminating poor quality turkers.
We assign measure of errors to image-level and pixel-level correctness by using a Monte Carlo-based resampling approach called Bootstrap .
We first eliminate all turkers from our dataset who had completed fewer than five tasks.
We then take samples of the remaining 142 turkers with replacement.
For each sampled turker we randomly select five tasks that s/he completed to measure their average multiclass accuracy  or multiclass overlap .
We shift our elimination threshold by increments of 0.01 and reject turkers if their average performance is lower than this threshold.
At each increment, we also calculate overall performance across all tasks among the remaining turkers.
We repeat this process independently at the image and pixel levels N=1000 times to calculate error bars.
Results are shown in Figure 5 .
In both figures, we see overall performance steadily increase as poor performing turkers get eliminated.
However, the threshold where elimination takes effect differs between the two mechanisms due to differences in difficulty.
For example, to achieve the same accuracy level as we would expect from majority vote with 3 turkers , the average performance elimination threshold needs to be 0.76 .
At that threshold, imagelevel multiclass accuracy amongst the remaining turkers goes up to 0.84, but at a cost of eliminating 51.2% of our workforce.
Thus, as expected, our results show accuracy gains with increasingly aggressive elimination thresholds; however, these accuracy gains come at a cost of reducing the effective worker pool.
We expect that future systems can use these results to identify poor performing turkers proactively during data collection via ground truth seed images , and either offer additional training, or, in the extreme case, rejecting the work outright and blacklisting the turker.
The threshold used depends on the accuracy needs of the application.
The Verification Interface: For the second quality control approach, we use our verification interface  to subjectively validate labels via crowdsourcing.
Here, turkers validate existing labels rather than provide new ones.
We ensured that the same turker did not label and validate the same image.
As the validation task is simpler than the labeling task, we batched 20 validations into a single hit at a cost of 5 cents.
We collected three or more validations per label across 75 images .
Whereas the median time to label an image was 35.2s, the median time to validate a label was 10.5s.
Thus, collecting validations is quicker and cheaper than collecting new labels.
We performed a series of analyses with the validation data, using both majority vote validation and zero tolerance validation.
For the latter, if any validator down-votes a label, that label is eliminated.
We compare these results to no quality control , the use of majority vote labels, and a combination of majority vote labels plus subjective validation.
Results are in Figure 5.
As before, performance improves with additional turkers--either as labelers or as validators.
The best performing quality control mechanism was 3 labelers  plus 3 validators  beating out 5 labelers .
This suggests that it is more cost effective to collect 3 labels with validation than 5 labels total per image, particularly given that validation requires less effort.
Figure 6: A selection of the bottom and top three performing images in our dataset based on multiclass pixel-level area overlap.
Top row: original GSV image; middle row: majority vote ground truth from researchers using 15% overlap; bottom row: turker labels.
Numbers show turker performance results for that image, from left to right: image-level binary, image-level multiclass; pixel-level binary, pixel-level multiclass.
Finally, to uncover what aspects of an image make it particularly easy or difficult to label, we sorted and visually inspected images in our dataset by multiclass pixel-level area overlap performance.
Figure 6 shows a selection of the bottom and top performing images .
For the worst performing images, there are many false positives: for example, utility poles and stop signs labeled as obstacles even though they are not in the sidewalk path.
Figure 6c highlights two additional common problems: first, problem types can have ambiguous categories--in this case, the ground truth label indicates Sidewalk Ending while many turker labels selected Surface Problem; second, it is unclear how much of the problem area should be highlighted.
For Sidewalk Ending, the ground truth labels highlight only the sidewalk termination point--some turkers, however, would label this section and any beyond it with no sidewalk .
Future interfaces could detect these mistakes and provide active feedback to the turker on how to improve their labeling.
In contrast, for the best performing images, the accessibility problems are, unsurprisingly, more salient and the camera angle provides a relatively close-angle shot.
An additional limitation relates to the GSV images themselves.
Image quality can sometimes be poor due to lighting conditions, which can often be auto-corrected, or blurriness.
More work is also needed to assess the degree to which sidewalk occlusion  is an issue in GSV images across different regions.
Finally, GSV image age is also a potential problem .
The following factors should mitigate this lattermost issue:  as noted earlier, virtual GSV neighborhood audits and physical audits have resulted in high concordance for pedestrian infrastructure data ;  GSV is already being treated as a valuable resource by the accessibility community--e.g., one of our mobility-impaired participants mentioned that he uses GSV to examine an area for traversability before leaving his house;  GSV imagery is often updated as the GSV technology improves or simply to ensure accuracy e.g., Google updated 250,000 miles of roads in early Oct 2012 .
Moreover, all of the above GSV limitations may be potentially resolved through other data sources such as high-resolution top-down satellite or fly-over imagery , volunteer-contributed geo-located pictures , or government 311 databases.
While we captured important accessibility characteristics of sidewalks, other problems may exist.
For example, the wheelchair users in Study 1 indicated that sidewalk narrowness can also reduce accessibility.
We did not have a means of measuring sidewalk width or assess narrowness.
Future work should look at the ability to calculate widths , which could, perhaps, be reconstructed via the multiple camera angles offered by GSV or derived from the 3D-point cloud data that modern GSV cars collect .
While this 3D data is not yet publicly available, it could also be useful in object detection for automatically identifying problems.
We intend to integrate computer vision  into our approach primarily for image triage, view selection,
We have shown that untrained crowdworkers could find and label accessibility problems in GSV imagery.
We also highlighted the effect of common quality-control techniques on performance accuracy.
Here, we discuss limitations of our study and opportunities for future work.
Our prototype labeling system relied on a manually curated database of images selected by the research team.
This approach was sufficient to demonstrate the feasibility of our idea but ignored important practical aspects such as locating the GSV camera in geographic space and selecting an optimal viewpoint.
These challenges clearly need to be solved to produce a scalable approach.
Doing so will allow for further scalability, for example, where turkers verify automatically generated labels.
However, while our current pixel-level results should be useful for localizing where problems exist in images, they may not be sufficient for training CV algorithms.
To capture higher quality training data for CV, a future labeling tool should provide finer granularity outlines, feedback to turkers about their performance, proactive quality control, and better training.
For quality control, future applications will be using images where ground truth is unknown.
Instead, "ground truth" seed images will need to be injected into the labeling dataset to actively measure turker performance .
Active monitoring will allow turkers to receive performance feedback, help assist them when they make common mistakes, and warn and, eventually, eliminate poor quality workers if they do not improve.
Beyond turkers, we also plan to build a volunteer-based participatory website to both visualize our results and highlight areas that need data collection.
In contrast to our current interface, we could allow for collaborative editing  and experiment with incentivizing volunteers .
Our general approach of collecting useful, street-level information in a scalable manner from GSV images has application beyond sidewalks.
We would like to expand our approach to assess the accessibility of building fronts, friction strips and stop lights at intersections , and non-accessibility related topics such as tracking and labeling bike lanes in roadways.
