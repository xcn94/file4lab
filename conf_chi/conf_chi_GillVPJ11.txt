Privacy is frequently a key concern relating to technology and central to HCI research, yet it is notoriously difficult to study in a naturalistic way.
In this paper we describe and evaluate a dictionary of privacy designed for content analysis, derived using prototype theory and informed by traditional theoretical approaches to privacy.
We evaluate our dictionary categories alongside privacy-related categories from an existing content analysis tool, LIWC, using verbal discussions of privacy issues from a variety of technology and non-technology contexts.
We find that our privacy dictionary is better able to distinguish between privacy and non-privacy language, and is less contextdependent than LIWC.
However, the more general LIWC categories are able to describe a greater amount of variation in our data.
We discuss possible improvements to the privacy dictionary and note future work.
While the use of ICTs  is recognized to play a beneficial role in our lives, its increasing ubiquity has been met with some concern.
One area that is persistently discussed in the field of HCI is that of privacy .
Users' privacy perceptions in relation to technology use have become a central question in a variety of contexts such as location tracking in families , social network use amongst friends  and smart homes for the elderly .
A number of methodologies have been used to study privacy.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
While this method helps identify the source of the offence and also reveals participants' judgments, its limitation is that it does not capture natural, moment-to-moment privacy practices.
A second approach builds on the hypothesis that privacy attitudes and concerns motivate privacy-related behaviors .
Critical problems however have been noted regarding this approach.
First, the reliability of the methods used to measure privacy concerns have been called into question.
The leading items included in most attitudinal questionnaires bias participants' responses , often resulting in inflated self-reports of privacy concerns that rarely translate to privacy protective behavior .
Moreover, when such questionnaires are used in experimental settings, they can prime behaviors.
For example, one study found that participants avoided answering sensitive questions after completing a privacy concern measure .
These limitations have motivated privacy researchers to contrive methods that will capture nuanced, inclusive and unbiased portrayals of users' concerns, needs and practices.
Nonetheless, in the absence of a question that can anchor participants' language around the concept of privacy, any subsequent coding and interpretive analyses can be highly subjective while nuanced privacyrelated language may end up being ignored in favor of more easily coded themes .
In light of these challenges, the development of novel methodologies for the unbiased analysis of privacy have been described as critical in advancing this field .
Indeed, natural language is both a reflection and a mediator of internal states, such as personality and emotion, and social situations ; words reveal attention patterns, thoughts, feelings, and provide a way of understanding our social worlds .
Previous research has developed and used a variety of automated content analysis techniques for the systematic measurement of language, the relevance of which is best understood through comparison to psychometric measures, whose use is established in the field of HCI.
The present paper builds on the former approach by contributing a new set of privacy-related categories that can be used with an existing, widely used content analysis program .
These new privacy categories constitute a "dictionary" as they encompass a number of words relevant in the semantic analysis of the privacy domain.
Social scientists aiming to bridge the gap between privacy attitudes and actual behavior, may want to use the linguistic features encapsulated by the categories in our dictionary to reveal subtle attitudinal and behavioral differences over time or across contexts as well as relate these to individual characteristics such as gender, personality and social class.
Designers concerned that a new version of software pushes the boundaries of privacy may use the dictionary to analyze users' opinions about the system in order to determine the precise privacy issues that users face.
This paper is organized as follows; first, we discuss definitions of privacy developed for both technology and non-technology environments.
In ensuring that the dictionary is applicable across contexts, we base it on a wide and context-inclusive definition.
We go on to describe the step-by-step procedure we followed to construct and develop the privacy dictionary.
To determine whether our new privacy categories are capturing unique aspects of privacy language, not tapped upon by previous tools, we also include in our analysis of texts existing categories from the LIWC dictionary.
Next, we present the dataset of interviews and focus groups from seven privacy-sensitive contexts against which we evaluated the dictionary.
We then outline the analysis and evaluation of the dictionary.
Our main finding is that the privacy dictionary captures unique linguistic features in privacy language.
By contrast, LIWC categories are found to measure general contextual differences, but do not reliably distinguish between privacy oriented and non-privacy oriented language.
In the final section we discuss future plans for improving the reliability and validity of the dictionary.
It is thus not surprising that theoretical and empirical privacy research has taken place within and across a wide range of disciplines.
This interdisciplinary work has converged on a number of descriptive features: privacy is achieved behaviorally through actions of control , it serves a number of positive psychological functions  and it is governed by social norms negotiated through our interactions with others .
At the same time, however, research has shown that particular privacy features gain importance in relation to the context under investigation.
Whereas the static behavioral patterns fostered in some environments have led scholars to propose that privacy is achieved through the withdrawal of a person in a state of solitude or isolation, privacy can also follow from small group intimacy and when among large groups, from a condition of anonymity or reserve .
HCI researchers have explored the notion of context by identifying the features of privacy that become most salient as a result of technological affordances .
In one such analysis, Palen and Dourish  argue that participation in technology requires some degree of disclosure.
The dialectics of privacy form a need for balancing what is shared and kept to oneself.
The same authors note that technology-mediated information sharing is less amenable to the user's direct control , while technologies reconfigure the temporal nature of identity by framing the present against past and future actions shaped, constrained or recorded by technologies .
When technological and physical environments intersect, the meaning of context can take further new forms.
This fusion activates new privacy features, bringing different sets of norms in conflict to ultimately shape how technology is perceived and used .
The discussion so far should illustrate that despite theorists' agreement over some of its features, context inevitably determines much of the way we conceptualize and study privacy.
The lack of a unifying theoretical account of privacy creates a challenge for our project: to build a dictionary that is sensitive to the semantics of privacy, our underpinning theoretical framework must encompass a wide and comprehensive set of privacy features.
We turn to theories of categorization that help explain the multivariate nature of privacy and go on to propose a feature-based definition that will motivate our work.
However, many natural language categories do not share a common set of defining features.
For example, card-games, board games and playing tennis bear a `family resemblance' structure.
Members characterized by more features of the family are better exemplars, thus making membership a matter of degree .
Prototype theory evolved from this perspective to propose that concepts, such as privacy, are organized through prototypes that represent the average member of a concept.
When new situations are encountered, we evaluate their similarity against the prototype to determine whether they belong to the concept and whether they are good or poor exemplars .
Context, in particular, has been shown to shift the prototype  rendering previously good exemplars into poor exemplars .
For instance, when outdoors, the word `games' is more likely to be interpreted as `sports'.
At a Halloween party, however, `role-playing', generally regarded to be a poor exemplar of games, becomes a more viable interpretation.
Privacy researchers  have proposed that the multifaceted nature of privacy can be explained through prototype theory.
Recently, this claim was empirically established .
When a concept is organized by a prototype, a wide range of features are reported, none of which are shared across all reports .
In , 146 participants reported an average of 6.6 features, a process that yielded a total of 82 privacy features.
It was then determined whether participants could reliably rate the features' importance or centrality with regards to the concept.
Once it is shown that features of the concept vary in their degree of centrality, exemplars of the concept can be directly derived from the features .
Using a 9-point scale , 118 participants were able to reliably rate the privacy features' centrality.
In a final step, 62 participants evaluated vignettes that contained either more central or peripheral features of the privacy concept.
The vignettes containing more central privacy features were recognized as better exemplars.
This can lead researchers to overlook important privacy language.
By including language that expresses both central and peripheral privacy features it is possible to address the danger of biased coding and interpretation .
An automated linguistic method built on the same heuristics can be faster than laborious human coding .
Despite the many motivations driving a prototype approach to privacy linguistics, as noted earlier, content analysis techniques have already been developed, albeit in other research contexts.
Using these techniques it has been possible to identify emotional states , predict deception  and detect differences in personalities , to give just a few examples.
Alongside, the custom privacy categories we created , we also apply these established methods to compare with our own approach.
To summarize our objectives, this research sets out to understand whether new categories based on the privacy prototype or existing linguistic categories from the LIWC dictionary, reveal specific characteristics within privacy language.
This is achieved by comparing privacy to nonprivacy language.
To determine how stable these categories are across different contexts, our analysis also focuses on the interactions between privacy language and context.
The next section describes the procedure used to construct the dictionary from the 82 privacy features.
It then details the choice of existing LIWC dictionaries used.
The findings reviewed above  reconcile the different views on privacy, while the features reported by participants provide a solid basis for constructing the privacy dictionary.
Four reasons motivate this approach:  From a theoretical perspective, the privacy prototype covers the entire gamut of psychological and behavioral components discussed in the literature .
Advantageously, a dictionary built on this foundation will not be representative of a single theoretical view.
This means that the dictionary could be used to provide a fair test of theory, without the danger of influencing results through theory-based methods.
The privacy dictionary is developed for use with the 2001 version of the LIWC software, since this is the most widely used dictionary for content analysis .
To construct the dictionary, iterative techniques, similar to those applied in the development of the LIWC dictionaries, were used .
A first version of the privacy dictionary was developed on the basis of the prototype feature list discussed earlier .
This list included phrases or words generated by 146 participants.
Features that were phrases were reduced to single words if possible so as to ensure maximal compatibility with the LIWC software .
For example, "having control over one's information" was broken down into two linguistic units: control and information.
This process yielded a total of 72 unique words.
These revised prototype words were then used as "seed words" over several iterations to generate additional synonyms and antonyms using traditional and computational semantic dictionaries and thesauri.
In a first step, two judges evaluated the consistency of the additional synonyms and antonyms with the original words, with consensus between judges determining a word's inclusion or exclusion.
This resulted in the selection of 573 dictionary words.
In a second step, `key word in context' analysis  was conducted on the dictionary words on a sample of data .
The output of this analysis provided contextual information of the occurrence of the dictionary words.
This step was necessary to ensure that the reduction of multi-word prototype features to single words did not capture unintended meanings from those originally envisaged by the judges in Stage 1.
Words regarded as inconsistent with the original intended meanings were excluded.
This process led to 185 words being excluded.
The final stage in the dictionary development was to construct theoretically sound categories of semantically similar words, which would form the basis of the output of the analyses carried out using the privacy dictionary.
This is necessary to enable the measurement of consistent and reliable categories that can provide theoretically meaningful results.
Two judges, one familiar with privacy theory and the other with linguistics and content analysis, worked together on this task.
The eight high-level categories presented in Table 1, are the result of discussion and unilateral consensus between these two researchers.
As noted earlier, the LIWC dictionary contains a large number of semantic categories with possible relevance to privacy research.
Therefore, rather than building new categories from scratch which have substantial overlap with the standard LIWC categories, we adopted 16 of the LIWC categories, which corresponded to the following prototype features: `Sexual life' , `Body' , `With people you feel close to' , `Personal space' , `Financial information' , `Concealing one's emotions' , `Personal' , `Involves a group of people and no one else' , `At home' .
In this section we detail the methodology applied to collect a dataset of one-to-one interviews and focus groups against which we evaluated the dictionary.
We chose seven offline and online contexts that previous research suggests are sensitive to privacy issues.
Project title Co-operation or Contest?
Available participant demographics 5 detainees  in two police custody areas  20  secondary school and sixth form students  3 managers in door-to-door financial services firms 5  school or college students or in unemployment  5  undergraduate and graduate students  9  participants over 65 living in deprived/mid-deprived areas  6 participants  with diverse health and illness experiences 
We then searched through the UK Data Archive  or contacted researchers who had worked on these topics to identify previously collected datasets.
The aim was to find qualitative data rich in privacy content, which had been generated by asking questions unrelated to privacy, in order to avoid methodological problems of priming in the responses.
Table 2 presents more details of the datasets included in our study.
The data included fully abided with participants' informed consent and the institutions' ethics approval procedures.
A team of five researchers who were knowledgeable in privacy theory selected appropriate transcripts using the following procedure.
Two researchers worked on each context.
The first surveyed the entire panel of transcripts made available in order to identify a maximum of five transcripts per context that involved a diversity of privacyrelated issues.
Focusing on one transcript at a time, the same assistant identified areas in the text where participants expressed privacy-related issues.
These segments were examined by the second assistant who raised any disagreements concerning the inclusion of a given privacy text.
A series of three-way  Restricted Maximum Likelihood  mixed effect models were calculated with the privacy dictionary categories as dependent variables.
This method was used since it is well suited to data consisting of repeated measures.
Privacy had two levels : The Privacy condition included utterances assessed by dataset coders to express privacy issues.
The Non-privacy condition contained all other language captured during each interview .
Context comprised of seven levels, one for each context in the data panel .
To control for the functionally different conversational role of interviewers and interviewees, a third independent variable, Speaker, was coded with two levels ; we do not consider these speaker results in detail in the current paper.
Table 3 displays the mean differences across conditions for the eight privacy categories.
The categories Intimacy and OpenVisible explained little variance within this dataset; indeed the negative R2 found for OpenVisible indicates that the fit curve of this model explains less variance in the data than a straight line placed at the mean.
We thus retain only the remaining six categories in the subsequent analyses and revisit the two excluded categories in the discussion.
Words belonging to six linguistic categories of the privacy dictionary were used significantly more frequently in the Privacy than the Non-privacy condition: PrivateSecret, NegativePrivacy, NormsRequisites, OutcomeState, Restriction and SafetyProtect.
This indicates that our privacy dictionary categories captured differences between privacy and non-privacy language.
Turning now to the role of Context: For the NormsRequisites category, we note a significant interaction between Privacy and Context.
We also note a main effect of Context on NormsRequisites, with Tukey post-hoc tests showing a similar pattern of Context more generally: NormsRequisites category words were used significantly more in the QOL context than in CHI, FIN, CUL and SNS contexts.
Taken together, the interaction and main effect for NormsRequisites demonstrate that the QOL context had an influence over participants' general language, an effect that was stronger when they spoke about privacy specifically.
We therefore note that the category NormsRequisites is to some extent sensitive to contextual issues.
Table 4 reports the main effects for Privacy, Context and Privacy*Context interactions.
We note that findings relating to Speaker are not presented in detail here, but instead are left to future work1.
Again, SpeakerID was included as a random effect in each model.
The 16 LIWC categories explained a larger amount of variance as shown by the R2 reported below.
Table 3 displays the mean differences across conditions for the 16 LIWC categories.
We found main effects of Privacy to be significant for Other, Other references, and You which were used more frequently in the Privacy than the Non-privacy condition.
Thus, from a total of 16 categories, six revealed differences between privacy and non-privacy language, three of which identified patterns specific to non-privacy discussions.
Findings relevant to Context are the interaction of Privacy and Context for Money, I, Home, Family, and You categories.
Main effects were found for 12 of the 16 LIWC variables with the following not showing a main effect of context: Posemo, Other references, Space, and Groom.
Given the complex relationships between these linguistic categories and the different contexts , we do not describe these in detail.
These findings suggest that despite their relevance to privacy, these categories are more sensitive to measuring general contextual features that frame privacy than discriminate specific privacy language.
This apparently ubiquitous usage of many LIWC categories is further demonstrated by the significant findings relating to speaker role, which we do not report in any further detail.
Thus, the greater explanatory power of these categories comes from their relative frequency and consistent use in language.
The privacy categories forming the output of our analysis found that participants within the Privacy condition described more negative privacy experiences, such as, feeling intruded upon, embarrassed, threatened, ; they made references to the norms and expectations of privacy, e.g., discretion, respect  and used language that expressed the realms that privacy protects e.g., data, secrets .
Participants detailed concerns about safety and protection e.g., security, safeguard , as well as discussed the various behavioral states through which they achieve privacy and its outcomes e.g., alone, quiet  or the behaviors applied to manage privacy e.g., control, hide .
In the LIWC analysis, with the exception of just a few variables, the majority of categories occurred more frequently between contexts, irrespective of whether participants were talking about privacy.
Thus, although these categories were considered by us to be directly relevant to privacy, they appear more suitable for capturing general contextual differences.
The three categories that captured patterns in privacy language showed that when talking about privacy, people made more references to others and third-person pronouns.
These categories reflect the relational and social nature of this construct .
However, in answering more specific theoretical questions about privacy, the applicability of these categories seems to be limited.
Thus, the remainder of the discussion explores in more detail the privacy dictionary.
We consider why some of the categories explained little variance, and discuss ways for the future development and validation of the dictionary.
Despite the possible concern that the category SafetyProtect represents a narrow and specific component of privacy , it was nevertheless used relatively frequently in privacy related discourse.
A small number of dictionary words in a category may not necessarily be an issue .
Indeed, the relatively low frequency of words within the privacy dictionary categories did not impair its function in general.
Nonetheless, in the development of the LIWC dictionary categories, that also formed the basis of our approach, safeguards were taken to ensure that words within categories achieved an acceptable frequency of usage .
Given the very specialized nature of the words within the privacy categories, we have to be more sympathetic to less-frequently occurring words.
One particular problem of using low-frequency words however is that of sparse data which is more likely to lead to skewed distributions.
This research developed and evaluated a privacy dictionary whose objective is to assist researchers in conducting automated content analysis of texts and transcripts, providing a valuable addition to the arsenal of tools available for the study of privacy.
The integrative theoretical approach used to build the dictionary can help reduce bias introduced by theory-based methods.
It can also provide a shared and common platform that allows meaningful comparisons of cross sectional data.
In practical terms, it can precede qualitative thematic coding by preidentifying language of interest that would otherwise require laborious coding .
The dictionary offers new prospects for future theoretical development.
The categories can be used to track privacy perceptions over time and changing conditions; for instance, users' privacy perceptions before and after the introduction of mandatory and possibly threatening technologies can be measured through language.
Furthermore, some contexts are governed by strong privacy norms, an aspect the privacy dictionary is able to capture, raising stakeholder awareness to prevent privacy threats from escalating into violations.
The dictionary can also drive the development of new measures.
The cumulative use of dictionary words can serve as a general privacy metric while analyses at the individual word level can inform the development of new psychometric measures by identifying context-specific privacy behaviors, to give one example.
People's language across comparable technological and non-technological privacy-sensitive contexts may assist in answering this question.
To summarize our findings, of the six privacy categories that were retained for analysis, the variance explained by privacy versus non-privacy language and context ranged from between 10 and 49 percent.
Out of these six categories, all exposed context-independent linguistic patterns in privacy language when compared to non-privacy language.
In future dictionary iterations, the issue of data sparsity due to the inclusion of low frequency items in dictionary categories must be balanced against the requirement to capture a narrow and focused portrayal of privacy.
Whilst the inclusion of a greater number of conceptually related words may improve the reliability and power of dictionary categories, further work will also need to test the validity of the privacy dictionary categories.
Therefore, we propose further internal and external evaluation of the privacy dictionary.
For example, internal validity can be improved by combining information from psychometric measures of privacy and multidimensional semantic space measures from computational linguistics .
Finally, the explanatory power of our dictionary could also be improved  through the inclusion of basic contextual rules enabled in more recent versions of the LIWC software, which could aid word-sense disambiguation and more accurate classification of words within dictionary categories.
We thank Fadhila Mazanderani, Anne-Marie Oostveen and Sacha Brostoff whose assistance was invaluable with the transcript analysis.
We gratefully acknowledge Cristina Soriano and the reviewers of this paper whose comments improved our work.
This research was funded by the EPSRC through the Privacy Value Networks project  and partially supported by the European Commission Future and Emerging Technologies programme FP7-COSI-ICT .
Acquisti, A. and Grossklags, J. Privacy attitudes and Privacy behavior: Losses, Gains, and Hyperbolic Discounting.
In Camp and Lewis: The Economics of Information Security Kluwer, 2004.
Privacy in multimedia communications: protecting users not just data.
The environment and social behavior.
Beach, S., Schulz, R., Downs, J., Matthews, J., Barron, B. and Seelman, K. Disability, Age, and Informational Privacy Attitudes in Quality of Life Technology Applications: Results from a National Web Survey.
Boyle, M. and Greenberg, S. The language of privacy: Learning from video media space analysis and design.
Christidi, S. and Rosenbaum-Elliott, R. Shared spaces and personal corners in social networking websites: the contracted and the cryptically revealed self.
Chung, C. and Pennebaker, J. W. The Psychological Function of Function Words.
Nursing older dying patients: findings from an ethnographic study of death and dying in elderly care wards.
DeCew, J. W. In Pursuit of Privacy: Law, Ethics, and the Rise of Technology.
DeCew, J. W. The Priority of Privacy for Medical Information.
Fehr, B. Prototype Analysis of the Concepts of Love and Commitment.
The Language of Emotion in Short Blog Texts.
Hancock, J. T., Curry, L., Goorha, S. and Woodworth, M. T. On lying and being lied to: A linguistic analysis of deception.
Harper, J. and Singleton, S. With a Grain of Salt: What Consumer Privacy Surveys Don't Tell Us.
Introna, L. D. and Pouloudi, A. Privacy in the information age: Stakeholders, interests and values.
Joinson, A. N. and Paine, C. Self-disclosure, Privacy and the Internet.
Joinson, A. N. Looking at, looking up or keeping up with people?
Motives and use of facebook.
In Proceedings of the twenty-sixth annual SIGCHI conference on Human factors in computing systems .
Joinson, A. N., Paine, C., Buchanan, T. and Reips, U. D. Measuring self-disclosure online: Blurring and nonresponse to sensitive items in web-based surveys.
Labov, W. The boundaries of words and their meaning.
Georgetown University Press, Washington DC, 1973.
Leyshon, A., Knights, D. and Burton, D. Delivering Financial Services in the Home.
Leyshon, A., Signoretta, P., Knights, D., Alferoff, C. and Burton, D. Walking with Moneylenders: The Ecology of the UK Home-collected Credit Industry.
Lowe, W. Content analysis and its place in the  scheme of things Qualitative Methods 2, 1 2004, 25-27.
Livingstone, S. and Bober, M. United Kingdom Children Go Online.
Mazanderani, F. and Brown, I.
Making things private: exploring the relational dynamics of privacy.
In Proceedings of the Computers, Privacy and Data Protection , 2010.
American Psychological Association Publications, Washington, DC.
Murphy, T. and Whitty, N. Risk and Human Rights in UK Prison Governance.
Oberlander, J. and Gill, A. J.
Language with character: A stratified corpus comparison of individual differences in e-mail communication.
Palen, L. and Dourish, P. Unpacking "privacy" for a networked world.
In Proceedings of the SIGCHI conference on Human factors in computing systems .
Patil, S., Romero, N. and Karat, J. Privacy and HCI: methodologies for studying privacy issues.
In Proceedings of the SIGCHI conference on Human factors in computing systems .
30.Pattenden, R. and Skinns, L. Choice, Privacy and Publicly Funded Legal Advice at Police Stations.
Pedersen, D. M. Model for types of privacy by privacy functions.
Linguistic Inquiry and Word Count : LIWC2001.
Petronio, S. Boundaries of privacy: Dialectics of disclosure.
State University of New York Press, Albany: NY, 2002.
Raento, M. and Oulasvirta, A.
Designing for privacy and self-presentation in social awareness.
Roen, K., Scourfield, J. and McDermott, E. Cultural Context of Youth Suicide: Identity, Gender and Sexuality.
Rosch, E. Principles of categorization.
Technology and Natural Death: a Study of Older People.
Skinns, L. Co-operation or Contest?
Inter-Agency Relationships in Police Custody Areas.
Stutzman, F. and Kramer-Duffield, J.
Friends only: examining a privacy-enhancing behavior in facebook.
In Proceedings of the 28th international conference on Human factors in computing systems .
Tausczik, Y. R. and Pennebaker, J. W. The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods.
Tavani, H. T. Philosophical theories of privacy: Implications for an adequate online privacy policy.
Terry, W., Olson, L. G., Wilss, L. and Boulton-Lewis, G. Experience of dying: concerns of dying patients and of carers.
Vasalou, A., Joinson, A., Houghton, D. A prototype analysis of privacy .
Westin, A. Privacy and freedom.
