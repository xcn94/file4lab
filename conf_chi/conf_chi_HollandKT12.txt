Automated detection of excessive visual search  experienced by a user during software use presents the potential for substantial improvement in the efficiency of supervised usability analysis.
This paper presents an objective evaluation of several methods for the automated segmentation and classification of ES intervals from an eye movement recording, a technique that can be utilized to aid in the identification of usability problems during software usability testing.
Techniques considered for automated segmentation of the eye movement recording into unique intervals include mouse/keyboard events and eye movement scanpaths.
ES is identified by a number of eye movement metrics, including: fixation count, saccade amplitude, convex hull area, scanpath inflections, scanpath length, and scanpath duration.
The ES intervals identified by each algorithm are compared to those produced by manual classification to verify the accuracy, precision, and performance of each algorithm.
The results indicate that automated classification can be successfully employed to substantially reduce the amount of recorded data reviewed by HCI experts during usability testing, with relatively little loss in accuracy.
The ISO 9241 standard defines usability as "the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use" .
Unfortunately, usability testing is often an expensive and time consuming process, requiring careful manual review and analysis of users' interaction with applications .
As a result, despite the integral nature of usability to the success of an application, usability testing is often neglected as part of the development process .
The primary shortcoming of usability testing is its qualitative nature, as described by several usability practices and guidelines .
By identifying and standardizing usability metrics, usability may be evaluated quantitatively, a process which lends itself to automation.
In this way, usability testing itself can be made more usable, by reducing the time and effort spent evaluating an interface.
In this paper, we consider the efficacy of eye movements as an indicator of software usability.
Specifically, we explore a number of techniques for the automated segmentation and classification of usability recordings of eye movement data.
Through quantitative analysis of basic eye movements  and the patterns they produce , we attempt to accurately and precisely locate time intervals in which the user experiences difficulty with a software interface.
There are a number of eye movement types identified by varying characteristics; of these, however, fixations and saccades are of particular importance to the field of humancomputer interaction .
Fixations occur when the eye is held in a relatively stable position such that the fovea remains centered on an object of interest, providing heightened visual acuity.
Saccades occur when the eye globe rotates quickly between points of fixation, with very little visual acuity maintained during rotation .
Various sources have described the usability implications of eye movements ; however, usability evaluation based on eye movements generally makes use of only scanpath  and fixation density overlays , discarding a wealth of information that may be gained from the complete eye movement record.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Final version of the paper will appear in the proceedings of CHI 2012.
Visual search occurs naturally as a means of obtaining information about our surroundings, and there are two primary types of information processing that occur during visual search, parallel and serial .
Serial search, exemplified in Figure 2, occurs when the target object is defined by more than one basic feature, requiring attentional shifts between objects until the target is located.
Poor interface layout, individual interface component sizes, coloring, and other usability/design issues may lead to prolonged or excessive visual search.
For the purposes of this paper and the corresponding research, excessive visual search  is defined as any onscreen search interval not directly related to task completion.
We begin by defining visual search and its properties, exploring previous research on its applicability to the field of human-computer interaction, and providing a general description of the way it may be identified by automated analysis.
We then present an overview of five segmentation algorithms used to divide the eye movement recordings into distinct intervals, along with seven classification algorithms used to identify intervals of excessive search.
Finally, we present the methodology used to verify the accuracy of each algorithm, a description of our manual classification process, and a discussion of the results.
Eye tracking was first employed as a usability metric in the 1950s by Fitts, et al.
Jacob and Karn  survey 21 usability studies incorporating the use of eye movement metrics as a primary usability metric.
A common trend among these studies is the noted use of fixation count and duration as primary indicators of software usability.
Eye movement metrics are often considered with little attempt to detect problem areas of a recording, instead attempting only to identify that usability problems exist within it .
It is this problem in particular that we attempt to address in the current work.
Poole and Ball  provide a thorough summary of a variety of quantitative eye movement metrics  and their implications towards the usability of a given interface.
For instance: fixations indicate areas of interest within an interface, and in the context of visual search short fixations and fixation clustering across a region may indicate difficulty identifying a target; regressive saccades and re-fixations are indicative of difficulty processing a target, often due to poor design or increased complexity; and increased pupil size is often indicative of cognitive effort and fatigue.
A scanpath is an aggregate of fixations and saccades directed from one target of interest to the next.
According to : "In a search task, an optimal scan path is viewed as being a straight line to a desired target, with relatively short fixation duration at the target."
This ideal, however, is often not the case, and visual search occurs frequently during the course of human-computer interaction due to the non-uniformity of basic tasks and the complex design patterns of modern user interfaces.
While there has been a substantial amount of research on visual search  and its implications on usability , to the best of our knowledge there has been very little progress in the automated identification of excessive visual search.
In previous work, we described and evaluated several techniques for the automated classification of excessive visual search under mouse/keyboard based event segmentation.
Final version of the paper will appear in the proceedings of CHI 2012.
Algorithm SEG-MK SEG-E SEG-EC SEG-F SEG-FC Metrics Mouse/keyboard events Eye position Eye position Fixation centroid Fixation centroid Region N/A Square Circular Square Circular Table 1.
Due to the serial nature of visual search within an interface, usability issues in the design and implementation may be identified through analysis of excessive visual search .
While automated analysis cannot interpret the placement or quality of individual interface components in the same manner as a human observer, the quantitative properties of eye movements make it possible to identify intervals of excessive visual search within an eye movement recording.
Automated identification of excessive search intervals makes it possible to reduce manual inspection of usability recordings, ignoring less relevant sections of the recording and focusing the attentions of a human observer on intervals of poor usability.
Visual search leading directly to task completion, idle search behavior, and off-screen behavior are not considered excessive.
Then, excessive visual search is identified according to the following basic algorithm, as shown in Figure 3, where specific eye movement metrics and threshold values vary: 1.
The eye movement recording is parsed and divided into distinct intervals.
An index value is generated for each interval based on the characteristics of specific eye movement metrics within the interval.
A threshold value is generated, either empirically or as a function of the average index.
The intervals with an index above or below the threshold are classified as excessive visual search.
Threshold values are empirically selected via manual inspection of performance data to provide greater accuracy, though these values may vary with application domain and environment.
The mouse/keyboard segmentation method  uses mouse/keyboard events to segment the eye movement recordings, as these represent conscious decisions by the user and imply that the user's target has been found.
Our first excessive search classification algorithms have been developed using this segmentation method, and as such it is used as a baseline for comparison with subsequent methods.
That is, the classification algorithm thresholds were set and fixed using the SEG-MK segmentation method, and during the development of subsequent segmentation methods these thresholds were not changed.
Final version of the paper will appear in the proceedings of CHI 2012.
Algorithm ES-F ES-S ES-P ES-SL ES-SA ES-SAI ES-SAID Metrics  Fixation count Average saccade amplitude Average pupil dilation Total saccade amplitude Convex hull area Convex hull area x Inflection count Convex hull area, Inflection count, Duration Table 2.
While mouse/keyboard events provide an acceptable segmentation of the recording timelines for automated classification, use of this technique requires access to an additional layer of information.
As well, it is our hypothesis that more detailed segmentation methods may improve the accuracy of the presented algorithms.
To further investigate, an eye movement based segmentation method  is developed using only the raw eye movement data to define event intervals as follows: 1.
Mark the first/current point in the eye movement recording as the reference point.
Continue through the eye movement record until a point that is more than D units horizontally or vertically from the reference point is found.
Mark this new point as the current reference point and add it to the event list.
Repeat steps 1-3 until all points in the eye movement record have been examined.
In comparison to the raw eye movement signal, fixations are often more directly indicative of attention and interest.
Fixation based segmentation followed the basic algorithm described previously for SEG-E, with the primary difference being the data set to which the algorithm is applied.
In comparison, SEG-E and SEG-EC operate on the raw eye movement signal, while the fixation based algorithms  operate on the fixations identified within the raw eye movement signal.
Essentially, this defines a rectangular region of interest and allows for a certain amount of overlap as the user's attention shifts between elements.
For our purposes, all offscreen points were ignored and D was empirically set to 200 pixels.
Nevertheless, this parameter is application dependent.
An additional segmentation method  is derived from the SEG-E variant, using the Euclidean distance between points for comparison to D, essentially defining a circular region of interest.
For this purpose, all off-screen points are again ignored, and in the current experiments D is empirically set to 275 pixels.
This results in two further segmentation methods: SEG-F, shown in Figure 4, using rectangular regions of interest with a threshold D of 75 pixels; and SEG-FC, using circular regions of interest using a threshold D of 175 pixels.
A velocity threshold of 30/sec is employed for the velocity threshold algorithm .
To examine whether additional accuracy could be gained by considering only fixation points, rather than the raw eye movement data, providing the segmentation that is based on strictly defined points of attention.
The eye movement based algorithms are modified to use the fixation points filtered by an I-VT algorithm .
Seven classification algorithms are presented as described previously in  and summarized in Table 2.
The ESF, ES-S, ES-P, and ES-SL algorithms rely on basic attributes of the human visual system, while the scanpath based algorithms ES-SA, ES-SAI, and ES-SAID incorporate more advanced aspects of visual search.
Usability testing is performed on the DeltaV process control software, utilizing an interface similar to common diagram editing applications , as part of a related but separate study for developing methods of objective usability evaluation.
Screen recordings and their corresponding scanpath/input event overlays are viewed for manual classification of visual search with Tobii Studio.
All algorithms and data analysis are implemented and performed in MATLAB.
Usability testing is conducted with the Tobii X120 eye tracker running at 120Hz.
DeltaV is run on a Dell Optiplex 745 with 4 GB of RAM, and displayed on a 19 inch flat panel monitor with a resolution of 1280x1024.
A velocity threshold  of 30/sec is used to reduce the eye movement data into the fixations and saccades .
The scanpath area algorithm  uses the area of the convex hull formed by fixation points within each interval as its index value, and the average across all intervals as the threshold.
The area of the convex hull is indicative of the total search area, with smaller values indicating efficient search behavior.
Search intervals with a convex hull area above the threshold are classified as ES.
The scanpath area/inflections algorithm  uses the area of the convex hull formed by fixation points multiplied by the number of times the scanpath changes direction  for each interval as its index value, and the average index value across all intervals as the threshold.
Inflections of the scanpath are indicative of attention shifts, with larger inflections counts suggesting increased visual search.
Search intervals with an index value above the threshold are classified as ES.
Four basic user behaviors are considered during the manual classification: task completion , in which the user is performing the operations necessary to complete a given task; excessive visual search , in which the user is experiencing prolonged difficulties finding the interface components necessary for task completion; idle , in which the user is waiting for the interface to respond after a specific action is performed; and off-screen , in which the user is reading task-related instructions presented outside the boundaries of the computer monitor.
Manual classification of the task recordings was performed by a trained research assistant using superimposed eye movement traces to build a classification baseline.
A full and thorough description of the manual classification process is described in .
The scanpath area/inflections/duration algorithm  uses multiple index values for each interval: the area of the convex hull formed by fixation points, the inflection count, and the total duration of the interval.
Longer intervals between subsequent mouse/keyboard events can indicate difficulties in locating the next target, therefore increasing the probability of the ES.
Search intervals with a convex hull area above the average, an inflection count greater than 5, or duration of more than 4 seconds are classified as ES.
Threshold values were selected empirically.
Participants are given a series of 15 tasks to complete in the process control application, during which screen recordings, eye movement records, and input logs are generated and synchronized for each task.
Tasks are similar to each other with specific interface components varied to reduce learning effects.
The automated ES classification algorithms are subsequently run using the various segmentation methods with the eye movement recordings and input logs of the manually identified recordings.
ES intervals generated by the automated analysis are compared to those provided by manual classification to determine the percentage of automatically identified search intervals that were correctly identified as excessive and the percentage of ES intervals missed or erroneously identified.
To determine the relative performance of the different segmentation/classification algorithms, each algorithm was then run separately across all eye movement records and computation times were measured in seconds.
Note that in all the figures, an asterisk  indicates statistical significance of p < 0.05 and a dagger  indicates statistical significance of p < 0.001, as determined by a oneway ANOVA test between the algorithms of a particular group.
For the segmentation algorithms, the comparison is performed across all classification algorithms, and vice versa, where F for segmentation algorithms and F for classification algorithms.
For example, the label "ES-F *" indicates that there is a significant main effect in the values produced by the ES-F algorithm when compared across segmentation algorithms.
Figure 6 presents a summary of the average percent of total time classified by each algorithm.
Assuming correct identification of ES intervals, a lower percent of total time classified as ES indicates more precise identification.
Averaged across all classification algorithms, segmentation by mouse and keyboard events  on average marks the smallest amount of the total recording time as ES , while segmentation based on fixations  marks the largest .
Across all segmentation algorithms, pupil based classification  on average marks the smallest amount of the total recording time as ES , while classification based on scanpath length  marks the largest .
Figure 7 presents a summary of the average percent of time correctly identified as excessive search by each algorithm.
Averaged across all classification algorithms, segmentation by mouse and keyboard events  has the highest average percent of correctly identified intervals , while segmentation based on fixations  has the lowest .
Across segmentation algorithms, scanpath based classification  has the highest average percent of correctly identified intervals , while pupil based classification  has the lowest .
Figure 5 presents a summary of the relative distribution of search behavior identified during manual classification.
Across the 21 eye movement recordings, task completion and offscreen behavior comprise roughly one third of the recording time each, while excessive visual search and idle search constitute the remaining duration.
The amount of idle search is relatively low, occurring only when the interface is unresponsive, and often resembling excessive search behavior.
The overall amount of non-task completion behavior is substantial, on average leaving 67% of the recording time as irrelevant.
Figure 8 presents a summary of the average percent of time erroneously classified by each algorithm.
Averaged across all classification algorithms, eye movement based segmentation  has the lowest average percent error , while fixation based segmentation  has the highest .
Across all segmentation algorithms, fixation based classification  has the lowest average percent error , while pupil based classification  has the highest .
Of the various segmentation methods, segmentation by mouse and keyboard events  provides the most stability, with less fluctuation in the results obtained between algorithms.
Classification algorithm thresholds are set using the SEG-MK algorithm, which may account for its relative stability; it is likely that the accuracy of the various segmentation methods could be improved by modifying these thresholds.
Mouse/keyboard events represent a conscious action by the user, often directly related to task completion.
This leads to some amount of overlap in behavioral intervals, and increases the likelihood that any given interval contains at least some amount of task completion behavior.
Eye movement based segmentation  obtains the best results overall, often having a higher percent of correctly identified intervals and a lower error than the other segmentation methods.
Eye movement based segmentation defines intervals according to approximated regions of interest, allowing the users' attention to determine the logical segments of the recording.
Fixation based segmentation  shows the least accuracy of the considered segmentation methods, generally having a higher percent of total time classified, a lower percent correct, and a higher percent error than the opposing segmentation methods.
Inaccuracy in the fixation based segmentation methods may be due, in part, to inaccuracies inherent in the I-VT algorithm used to identify fixations and the reduced specificity caused by the merging of individual data points into discrete fixations/saccades.
Eye movement based segmentation  may be the most useful algorithm, relying solely on the eye movement record without the need for extraneous information such as input events or application state.
Of the classification algorithms, fixation based classification  obtains the best results using eye movement based segmentation , with an average of 53% of total time classified, 60% correctly classified, and 25% erroneously classified.
Saccade based classification  obtains the best results using eye movement based segmentation , with an average of 49% of total time classified, 52% correctly classified, and 29% erroneously classified.
Pupil based classification  obtains the best results using segmentation by mouse and keyboard events , with an average of 53% of total time classified, 52% correctly classified, and 39% erroneously classified.
Classification based on scanpath length  obtains the best results using eye movement based segmentation , with an average of 65% of total time classified, 69% correctly classified, and 28% erroneously classified.
Classification based on scanpath area  obtains the best results using eye movement based segmentation , with an average of 56% of total time classified, 65% correctly classified, and 32% erroneously classified.
Classification based on a combination of scanpath area and inflections  obtains the best results using eye movement based segmentation , with an average of 42% of total time classified, 53% correctly classified, and 31% erroneously classified.
Classification based on a combination of scanpath area, inflections, and duration  obtains the best results using eye movement based segmentation , with an average of 61% of total time classified, 73% correctly classified, and 31% erroneously classified.
Classification based on a combination of scanpath area, inflections, and duration  may be the most useful algorithm, averaging the highest percent correctly identified and an error rate somewhere between the other algorithms.
Pupil based classification  is clearly the least effective of the considered algorithms, averaging the lowest percent correctly identified and the highest error rate.
As such, it seems to be a poor indicator from which to draw conclusions about usability in the context of this work.
Of the basic eye movement metrics, fixation count seems to be the most reliable indicator of excessive search behavior, with a greater number of fixations indicating more extensive visual search and processing among distractor elements in the interface.
Average saccade amplitude is less indicative of excessive search, as this reflects similarly the extent of visual search and processing, but is also largely affected by the size and location of interface elements.
Scanpath length and area indicate the overall span of attention within the interface, and are more accurate indicators of excessive search than the basic metrics.
Scanpath inflections indicate shifts in user attention and scanpath duration indicates the extent of processing; applying these in conjunction with scanpath area provides greater accuracy than any individual metric for the identification of excessive visual search.
Difficulty understanding the interface layout  often results in extensive scanning across multiple regions of the screen, with large saccade amplitudes, slightly longer fixation durations, and erratic inflections.
In addition, while its classification as excessive search is debatable, there are a number of occurrences of what could be deemed habitual search, these are immediately preceded and followed by off-screen behavior and generally consist of 3-7 short fixations not necessarily related to task completion.
It should be noted that while these are the most prevalent indicators of excessive visual search, they should not be considered comprehensive.
Classification error as defined in the previous section is a combination of the unclassified excessive search intervals and the intervals misclassified as excessive search.
Several factors contribute to this error.
The most substantial of these may be the accuracy of segmentation; manual classification is a relatively fine-grained approach, defining intervals of excessive search at the millisecond level, while automated segmentation provides a much coarser separation.
As such, the intervals defined by manual and automated classification are not exactly equivalent.
In this sense, a certain amount of error is unavoidable, but can be reduced by improving the accuracy of segmentation.
Additionally, excessive search is not clearly defined by a single eye movement pattern, and as a result it is difficult to pinpoint excessive search intervals using only a single metric .
Throughout the course of manual classification, we have noticed several eye movement patterns indicative of excessive visual search.
Difficulty selecting an interface element  generally results in a scanpath concentrated within a small region, with long fixation durations and small saccade amplitudes.
However, as eye tracking becomes more ubiquitous, and novel methods of producing cheap eye tracking equipment receive more attention , this limitation will inevitably be resolved.
Another issue is the applicability of the usability problems identified in the process control interface to the usability of software interfaces in a more general context.
For instance, an excessive amount of time spent reading a control list within a process control interface would not necessarily be considered an excessive amount of time while reading the contents of a web browser.
Unfortunately, the time, effort, and training required for manual classification is extremely prohibitive, and, as such, it was only possible to obtain manual classification data from a single source in the current work.
This issue may have introduced some amount of bias or inaccuracy into the performance comparison, as manual inspection is at least partially subjective and may vary from person to person.
Despite this, the strictest adherence to formulaic inspection of usability recordings was employed to ensure similarity in the manual intervals produced.
The primary shortcoming of usability testing is its qualitative nature, requiring detailed and time consuming analysis by a trained observer.
In our experience, each minute of recorded data required approximately one hour of manual review.
With average recording times of 3 minutes per subject, this equates to roughly 63 hours spent on manual classification of the 21 usability recordings considered in this paper.
In comparison, automated classification with the ES-SAID algorithm is able to correctly identify an average of 73% of the excessive search intervals while discarding roughly 40% of the total recording time across the 21 recordings in less than 5 minutes.
This is a substantial improvement, providing a potential savings of roughly 25 hours spent on manual classification.
In this paper we have described several previously considered techniques for the automated classification of excessive visual search, presented a variety of novel eye movement based segmentation methods, and provided an objective evaluation of the various segmentation and classification algorithms across and expanded data set.
The results indicate that automated classification of excessive visual search may be employed to substantially reduce the amount of recorded data reviewed during usability testing.
Of the considered algorithms, the segmentation algorithm SEG-E and classification algorithm ES-SAID provided the most accurate detection of excessive visual search, confirming that eye movement based segmentation is able to provide more accurate search intervals than segmentation based purely on mouse/keyboard events.
The current paper focuses on the identification of intervals of excessive visual search within the recording, allowing software developers to more easily inspect areas of concern; however, the range and scope of possible applications is much wider.
Based on classification statistics and assuming accurate classification it may be possible to provide further diagnostics.
Final version of the paper will appear in the proceedings of CHI 2012. search through a combined analysis of attentional span , attentional shifts , and processing complexity .
Future research in this area will likely involve improvements and innovations in both segmentation and classification, the eventual goal being to detect not only when usability problems occur, but to provide further analysis of the location and reason for these problems within the interface.
In addition to this, we hope to develop more complex algorithms for the automated detection of additional search behaviors, such as task completion and idle search.
A. Witold, et al., "Consolidating the ISO Usability Models," presented at the 11th International Software Quality Management Conference and 8th Annual INSPIRE Conference, 2003.
J. Rubin and D. Chisnell, Handbook of Usability Testing: How to Plan, Design, and Conduct Effective Tests, 2 ed.
L. Vukelja, et al., "Are engineers condemned to design?
J. S. Dumas and J. C. Redish, A Practical Guide to Usability Testing: Intellect Books, 1999.
R. J. Leigh and D. S. Zee, The Neurology of Eye Movements, 4 ed.
A. Duchowski, Eye Tracking Methodology: Theory and Practice, 2nd ed.
Ball, "Eye tracking in humancomputer interaction and usability research: current status and future prospects," in Encyclopedia of Human-Computer Interaction, C. Ghaoui, Ed., ed: Idea Group, 2005, pp.
Ball, et al., "Applying the Post-Experience EyeTracked Protocol  Method in Usability Testing," Interfaces, vol.
M. C. Russell, "Hotspots and hyperlinks: using eyetracking to supplement usability testing," Usability News, vol.
J. M. Wolfe, "What Can 1 Million Trials Tell Us About Visual Search?," Psychological Science, vol.
J. Shen, et al., "Distractor ratio influences patterns of eye movements during visual search," Perception, vol.
I. D. Gilchrist and M. Harvey, "Refixation frequency and memory mechanisms in visual search," Current Biology, vol.
Human factors in computing systems, Ft. Lauderdale, Florida, USA, 2003, pp.
O. V. Komogortsev, et al., "Eye movement driven usability evaluation via excessive search identification," in 14th International Conference on Human-Computer Interaction, 2011.
O. Komogortsev, et al., "EMA: Automated eyemovement-driven approach for identification of usability issues," in Design, user experience, and usability.
Theory, methods, tools and practice.
O. Komogortsev, et al., "Aiding usability evaluation via detection of excessive visual search," presented at the 2011 ACM SIGCHI Conference on Human Factors in Computing Systems , Vancouver, BC, Canada, 2011.
P. M. Fitts, et al., "Eye movements of aircraft pilots during instrument-landing approaches," Aeronautical Engineering Review, vol.
R. Jacob and K. Karn, "Commentary on Section 4.
Eye Tracking in Human-Computer Interaction and Usability Research: Ready to Deliver the Promises," in The Mind's Eye: Cognitive and Applied Aspects of Eye Movement Research, ed: Elsevier, 2003, pp.
F. T. W. Au, et al., "Automated usability testing framework," presented at the Proceedings of the ninth conference on Australasian user interface Volume 76, Wollongong, Australia, 2008.
Chevalier, "A Study of Automated Web Site Evaluation Tools," University of Washington, Department of Computer Science2002.
J. H. Goldberg and X. P. Kotval, "Computer interface evaluation using eye movements: methods and constructs," International Journal of Industrial Ergonomics, vol.
O. V. Komogortsev, et al., "Standardization of Automated Analyses of Oculomotor Fixation and Saccadic Behaviors," IEEE Transactions on Biomedical Engineering, vol.
W. Sewell and O. Komogortsev, "Real-time eye gaze tracking with an unmodified commodity webcam employing a neural network," in 28th of the international conference extended abstracts on Human factors in computing systems, Atlanta, Georgia, USA, 2010, pp.
J. S. Agustin, et al., "Low-cost gaze interaction: ready to deliver the promises," presented at the Proceedings of the 27th international conference extended abstracts on Human factors in computing systems, Boston, MA, USA, 2009.
