The hallmarks of multi-point interaction tasks are that the manipulations of the control points require a certain scale to show the scene in adequate detail, and that the user must navigate sequentially to each of the control points.
In addition, individual actions  are often mutually dependent - that is, the user must assess the effects of each action on the overall task, by checking the other control points or the overall image.
In rectangle selection, for example, the user must check to make sure that each adjustment still maintains all parts of the desired feature inside the selection rectangle.
The dual requirements of scale and navigation often conflict with one another: when the user is zoomed in on one control point, the other points are likely to be off-screen; if the user zooms out far enough to see all control points, then there is not enough detail to carry out the manipulation accurately.
As a result, users must carry out several zoom or pan actions to complete one multi-point task.
Multi-point interaction tasks involve the manipulation of several mutually-dependent control points in a visual workspace - for example, adjusting a selection rectangle in a drawing application.
Multi-point interactions place conflicting requirements on the interface: the system must display objects at sufficient scale for detailed manipulation, but it must also provide an efficient means of navigating from one control point to another.
Current interfaces lack any explicit support for tasks that combine these two requirements, forcing users to carry out sequences of zoom and pan actions.
In this paper, we describe three novel mechanisms for view control that explicitly support multipoint interactions with a single mouse, and preserve both visibility and scale for multiple regions of interest.
We carried out a study to compare two of the designs against standard zoom and pan techniques, and found that task completion time was significantly reduced with the new approaches.
The study shows the potential of interfaces that combine support for both scale and navigation.
Multi-point interaction tasks are those where the user interacts with several control points within a single compound task.
A control point is a location associated with an on-screen object which allows a user to adjust properties of that object through mouse input.
A common example of a multi-point interaction is adjusting a selection rectangle: in this task, the user must manipulate several different handles at the corners and sides of the rectangle before the selected area is judged to be correct .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
All three of the new techniques alter the view to maintain both scale and visibility for an arbitrary set of points in the workspace.
They also provide `guaranteed manipulability' - adequate detail and on-screen location - so that all points can be manipulated without the need for zooming or panning.
To test the effectiveness of the new approach, we carried out an experiment to compare two techniques  against standard zooming and panning.
With each technique, participants carried out several multipoint tasks in which they adjusted the two ends of a line in a simulated image editing tool.
We found that both of the new techniques were significantly faster than either zooming or panning.
In addition, subjects preferred the new techniques over the standard versions.
Our results suggest that applications incorporating both multi-point interactions and large workspaces should include explicit support for working at multiple locations, and for assessing the effects of those manipulations on the overall dataset.
Work dealing with multi-point interaction has largely focused on bi-manual manipulation.
This work is of significance, although it bears repeating that our work deals with manipulation via a single mouse and cursor.
Early research on bi-manual manipulation looked at the task of positioning and scaling graphical objects with a tablet and slider .
In relevance to our work, they make note of the necessity for a user of a single-mouse system to decompose drawing problems serially.
More recently, Latulipe et al.
The authors found that the symmetric manipulation technique outperformed the others.
Several techniques have been developed which enhance a user's ability to view or navigate to off-screen objects.
Baudisch's Halo technique  provides users with an explicit notification of object locations, but lacks any mechanism for navigation.
Irani and Gutwin  extended Baudisch's idea by providing a method of instantly hopping to the off-screen objects.
Their method was found to be more efficient at navigation than zooming and panning alone.
Frisbee  provides a widget showing a region remote from the user's actual workspace, and is intended for use on large displays.
Except for Frisbee, all these techniques require that the off-screen objects of interest be known to the system.
They do not allow for free discovery of, or navigation to, off-screen objects.
Relevant work looking at the importance of scale and how that can be used to govern zoom and pan operations was done by Bederson et al.
They described Pad++, a zoomable graphical sketchpad system which allows users to navigate smoothly through multi-scale space.
For a theoretical investigation on smooth zooming and panning, refer to van Wijk and Nuij .
They investigated the codependence of zoom and pan, and developed a model for optimal navigation.
Plumlee and Ware  also developed a model which predicted differences in performance for zoom and multiple window interfaces.
Recent work dealing with explicit zooming and panning includes that done by Bourgeois and Guiard .
They investigated zoom and pan parallelism over distances that are large relative to the graphical objects being dealt with, and found that parallel two-handed input outperforms one-handed input.
Interfaces which automatically perform zooming and panning based on some distinct yet related interaction have been investigated by a number of groups.
For example, researchers have looked at coupling zoom level with navigation speed , and have developed systems that adjust zoom based on scrolling speed.
Ware and Fleet  also looked at automatic speed modulation in flying interfaces, referred to as Depth Modulated Flying .
Another form of speed-coupled flying, involving navigation through 3D data, was shown by Tan et al.
Distinct from navigation to off-screen objects is navigation to on-screen, yet physically distant objects.
Bezerianos and Balakrishnan  described vacuum, a method for bringing distant objects closer using a suction metaphor.
Another technique is Drag-and-Pop by Baudisch et al.
A different approach using cursor extension was considered for TractorBeam , a stylusbased remote interaction technique for large tabletop displays.
The TractorBeam stylus uses an integrated 3DOF position tracker to determine the remote pointing location.
Focus-and-context as a general technique was first discussed by Furnas ; he attempted to formalize fisheye views, defining a `degree-of-interest' function which is dependent on a priori interest and distance parameters.
A broad review of early research into fisheye views was performed by Leung and Apperley .
Actual implementations of focus-and-context systems of note include those by Sarkar and Brown , and Carpendale and Montagnese .
Unusual applications of fisheye lenses also exist, such as the coupling of fisheye lenses to eyetracking devices .
While early work dealt mostly with laying the theoretical foundation and exploring implementations, more recent work has increasingly dealt with the usability of focus-andcontext systems.
For example, Gutwin and Skopik  found that users completed steering tasks faster using a fisheye presentation than with panning and radar techniques.
In contrast, Nekrasovski et al.
It is interesting to note that a clear view of where exactly fisheye views provide benefit and where they detract from usability has yet to develop.
In Figure 1, for example, following each change to a visible handle, the user is forced to adjust the view to perform an action on a different handle.
A combination of panning and zooming is commonly used for improving visibility of the workspace.
Both of these approaches can make a particular point of interest visible and accessible, but at the cost of either reducing the level of visible detail , or obstructing visibility of some other region in the workspace .
A multi-point interaction is one where, in order to complete a task, a user must manipulate several related control points .
During the task, the manipulation of an individual control point can potentially impact the task in such a manner that a previously manipulated control point must be revisited.
This dependence of control points on one another often means that the task cannot be reduced to a fixed set of steps.
A second requirement of multi-point interactions, which is closely coupled to control point visibility, is a user's need to work at a scale fine enough for the task.
When a user is interacting with a control point, or assessing the impact of a control point interaction, the user must be shown enough detail in the workspace to accurately determine the current state of the task.
Any interface supporting multi-point interactions must present the user with acceptable detail in regions relevant to the particular task at hand.
Zooming is the most common solution to the scale requirement.
A zoomed-in view provides greater detail, but can lead to problems with visibility, since the zoomed viewport shows less of the data than a zoomed-out view.
Guaranteed manipulability is closely related to the concept of guaranteed visibility, introduced by Munzner et al.
Guaranteed visibility is the idea of maintaining a visual representation for objects of interest on screen .
Guaranteed manipulability extends this idea to guarantee that objects can also be manipulated - which limits the visual representation to those that support the original manipulations of the object.
In the case of multi-point interactions, there is an advantage to guaranteeing the manipulability of control points.
Preserving manipulability allows a user to always access a control point directly, without having to adjust the view.
In this manner, sequential interactions with distinct control points need not be interleaved with view navigation tasks.
In examining zoom and pan, one can see that neither technique guarantees manipulability of all control points.
Performing a zoom can constrain the user's view such that one or more control points are off screen.
Similarly, performing a pan can move one or more control points off screen.
It is desirable for any interaction supporting a multipoint interaction to provide guaranteed manipulability.
Dependence on the state of multiple dependent points leads to two usability requirements that any interface must fill.
These requirements are control point visibility, and scale adequacy.
The nature of multi-point interaction tasks also highlights two further design factors that need to be considered: guaranteed manipulability, and visibility of intermediate regions.
The control points involved in a multi-point interaction may be any distance from one another in the virtual workspace.
Depending on the display parameters , the control points may be closer together or farther apart in screen coordinates.
While the user will not interact directly with intermediate regions, it can be necessary for a user to refer to these regions in order to evaluate past actions, or plan new actions.
It is important to note that the importance of intermediateregion visibility is largely task dependent.
Some tasks, where success can only be determined based on information in intermediate areas, absolutely require that the interaction provide visibility of intermediate regions.
Other tasks do not require this quality, but would nevertheless benefit from the additional spatial awareness.
One difficulty in implementing split scrolling relates to managing the corresponding merge operation.
It is desirable to allow the user to seamlessly manipulate the view such that the sub-regions merge back into a single viewport.
The merge can occur transparently if the user drags all the control points back to a single screen, but in other cases, the user will need to execute a command to join the views.
Developing this scrolling algorithm in such a way as to make interaction intuitive and robust was not done to our satisfaction in time for the user study.
For this reason, this technique was not included in the experiment.
We developed three novel interaction techniques designed specifically for supporting multi-point interactions on a system utilizing a single mouse-driven cursor.
All three techniques were designed with our two core requirements in mind: they preserve visibility and manipulability of all control points, and they show information in detail in the vicinity of the control points.
Two of our three techniques were selected for evaluation.
Note that while we discuss these techniques in the context of a two point line drawing interaction, they are generalizable for interactions involving more than two control points.
The split scrolling technique supports multi-point manipulation through automatic scrolling and splitting of the viewport.
Scale is preserved throughout the interaction, as is guaranteed manipulability of control points.
Initiation of the split scrolling action occurs automatically, using knowledge of the geometry being manipulated.
This knowledge is used to determine when a user's operation spans a region larger than the screen.
For the case of a line manipulation operation with two control points, normal auto-scrolling occurs as a user drags a control point; split scrolling occurs when the two points of the line can no longer fit on the screen at the current scale.
In this situation, the viewport splits into multiple sub-regions, one for each of the control points.
Each point can still be manipulated freely, with each sub-region scrolling in order to contain the control point.
The line separating the sub-regions is oriented such that it is orthogonal to the vector between the centers of the two sub-regions.
This provides the user with an indication of the relative positioning of the two subregions.
For both types of scrolling, the scroll rate is related directly to the mouse movement rate.
Split scrolling satisfies both requirements for a multi-point interaction technique: automatic splitting preserves control point visibility throughout the interaction, and scale adequacy is preserved because the scale remains unchanged.
Split scrolling falls short, however, in providing visibility of intermediate regions.
The splitting of the viewport causes regions between the control points to be hidden, which may be unacceptable for some tasks.
The Fisheye Auto-Zoom technique combines automatic panning, automatic zooming, and automatic creation of localized fisheye lenses.
The technique assumes, like the split scroll technique, that the user has initially adjusted the magnification such that there is adequate detail for all stages in the interaction.
Initially, the user is free to adjust control points in the visible area.
When the user drags one of the control points off the screen, the Fisheye Auto-Zoom technique causes the view to pan until the two points can no longer both fit on the screen.
As the user continues and drags a point off the screen, the view zooms out progressively to keep both points in view.
As zooming occurs, two lenses appear over the two control points.
This preserves the visible scale in the region immediately surrounding the control points.
As with the split scroll technique, the rate of scrolling  with the Fisheye Auto-Zoom is directly related to the rate of cursor movement.
We found in an informal usability study that users preferred this strategy to a constant scrolling rate.
There are several possibilities as to what to do once the interaction is complete.
First, the system could dismiss the lenses and keep the zoomed out view, providing the user with a contextual view in order to help in deciding the next course of action.
Another possibility would be to have the dismissing of the lenses trigger a zoom back to the original working scale.
The Fisheye Auto-Zoom technique was designed, like the split scroll technique, with the recognition of the importance of scale and visibility.
Scale is preserved, at least in the vicinity of the control points, and visibility is preserved through automatic panning and zooming.
The principle of guaranteed manipulability is also followed, with control points always being available for interaction on the screen.
Visibility of intermediate regions is also preserved, although much of the intermediate information is at a lower detailed scale than the user may want.
This is an advantage over the split scroll technique, but still may not be adequate for some tasks.
The Fisheye Overview technique employs localized fisheye lenses, but unlike the Fisheye Auto-Zoom technique, there is no support for automatic panning or zooming.
The assumption is that the user has initially navigated the view such that it spans the area over which the user plans to interact.
The technique then places fisheye lenses over each control point.
The user moves control points into the desired positions, and the interaction is complete.
The Fisheye Overview technique fills the requirements for control point visibility and scale adequacy.
Like the Fisheye Auto-Zoom technique, the fisheyes provide a detailed view of the data, and it is assumed that the system provided some means for the user to specify lens magnification level.
Control point manipulability is preserved, on the other hand, via the wide field of view.
Visibility of intermediate regions is also preserved to a degree, since these regions are shown, but not at a high level of detail.
It may be useful to note that this technique is limited in a sense compared to the others described so far, in that the user's region of interaction is limited to the initial field of view.
The seriousness of this limitation will depend on the user's intended course of action.
It may also be useful to note that the Fisheye Overview technique can be seen as a subset of the Fisheye Auto-Zoom technique, corresponding to the portion of the Fisheye Auto-Zoom technique occurring after the field of view has been established .
This does not necessarily detract from the usefulness of the Fisheye Overview: the technique is appropriate in cases where the user wants to initially choose the scope of the interaction, and it avoids the automatic zooming and panning of the other technique which, while powerful, may be confusing for some users.
A variation of the Fisheye Overview technique was briefly tried, which uses a dwell time to govern fisheye appearance, rather than relying on omnipresent fisheye lenses.
In the dwell time variation, the fisheye lenses appear over the control point after the user has clicked the control points and has had their mouse motionless for a preset amount of time.
Once the lens appears, it stays stationary,
Keeping the lens stationary is useful for combating the `fisheye overshooting effect', the exaggeration of mouse movements when using cursor-controlled fisheye lenses .
An informal usability study showed this variation to be unpopular, at least in its given form, so it was not pursued.
To test the effectiveness of new techniques against standard methods, we carried out a user study as described below.
For example, in a selection task, the user will know exactly what visual objects they wish to include in the selection.
There were four line-extension tasks used in the study, and participants carried out six identical trials with each task.
Participants saw the same tasks in each interface condition.
Four control-point interaction techniques were used in the study: Zoom, Pan, Fisheye Auto-Zoom, and Fisheye Overview.
Zoom and Pan were treated as two different cases, as a user performing the task would likely use one or the other where absolute scale differences are moderate.
For each of the conditions, we attempted to distill the interactions down to their simplest forms.
Each implementation was designed to require the minimum number of actions by the user to invoke each of the techniques, and provide the simplest means possible of invoking commands.
Since the techniques were implemented in isolation, rather than within a fully featured application, the complexity of embedding each of these techniques in an application workflow was avoided.
Users of the Zoom interface were required to left click for data interaction , but right click for zooming operations.
When zoomed in, a right click would zoom out  so that the entire data set was visible.
When zoomed out, a right click would zoom in, with the point clicked in the center of the screen.
The zoomed out view provided a contextual view, and the zoomed-in view allowed for fine point control .
The user was able to zoom during data interaction .
The Pan interface used a single level of magnification that was appropriate for fine control, equal to that of the zoomed-in view of the Zoom condition.
The cursor operated in two modes.
If the mouse was clicked while the cursor was hovering over a control point, then subsequent cursor movements resulted in displacement of the control point.
If the mouse was clicked while the cursor was not hovering over a control point, then subsequent cursor movements resulted in panning.
The panning rate had a 1:1 relationship to cursor movement, with data moving in the same direction as mouse movements.
Hover state was indicated by a change in the mouse cursor.
The Auto-Zoom interface varied the magnification level as described in the previous section dealing with the Fisheye Auto-Zoom interaction.
Initial scale was the same as in the zoom and pan conditions, with the line visible, but the targets offscreen.
The rate of automatic panning and zooming was governed directly by the speed of the cursor.
The Fisheye Overview interface was unique in that the viewport did not change during the interaction.
A custom study system was built with OpenGL and C++, using the Idelix PDT library for the fisheye lenses .
The system presented a series of control-point interaction tasks overlaid on an aerial photograph .
The study was conducted on a computer with a Pentium4 CPU running Windows XP, with an optical mouse and a 1024x768 pixel display.
The study used a simple instantiation of multi-point control - a line with two control handles.
The selectable region of each control point was 8 pixels across.
Participants were asked to move the two control points of the line onto two targets that were 50 pixels across at full zoom and that could only be selected at this level of magnification.
Each trial began with the line centered on the display.
For conditions starting at full zoom, the targets were off screen; however, targets were in line with the control points, which means that the task involved simply extending the line in both directions.
Participants were introduced to the study, given a demographics questionnaire, and randomly assigned to one of four order groups.
They then carried out practice trials with the system, and continued to the main tasks with the four different interfaces.
After each interface condition, participants filled out an effort questionnaire based on the NASA TLX survey , and completed an overall preference questionnaire at the end of the study.
Participants were instructed to work as quickly as possible; the system would not allow them to continue until each task had been successfully completed, so there was no possibility of errors.
As can be seen from Figure 7, Fisheye Overview was considerably faster than the other conditions: more than two seconds faster than Fisheye Auto-Zoom per trial, more than three faster than Zoom, and almost four seconds faster than Pan .
Fisheye Auto-Zoom was also considerably faster than Zoom and Pan .
As shown in Figure 8, performance in the Overview condition reached its maximum faster than in the other conditions.
The study used a within-participants one-way factorial design: the single factor was interaction technique, with four levels .
Participants carried out tasks with all four interfaces; order was balanced such that each technique was seen in each position an equal number of times.
The system collected completion time data for all trials; participants also filled out questionnaires as described above.
With 16 participants, each of whom carried out 24 test trials in each of 4 conditions, the system collected data from a total of 1536 trials.
A one-way ANOVA showed a significant main effect of Interaction Technique , and a follow-up Tukey HSD test showed that all conditions were significantly different from one another .
After testing with each interface condition, participants filled out an effort questionnaire based on the NASA Task Load Index .
The questionnaire asked participants to rate each interface on a scale of one to five, in terms of mental demand, physical demand, temporal demand, overall effort, and frustration level.
The results are shown in Figure 9.
The only consistent result in this data is that Pan was seen as requiring more effort than the other techniques.
As stated above, the study confirmed what we expected - that the number of operations necessary to carry out multipoint tasks with traditional interfaces takes considerable time.
However, we only tested multi-point tasks, and did not try a range of activities that might occur in a real-world situation.
Would our results hold in a real situation, and would the new approaches introduce any problems that would detract from their overall effectiveness?
The first question is whether real-world multi-point interaction tasks will see the same degree of improvement, when carried out with the new interface techniques.
We believe that if the tasks share characteristics with those studied here - mouse-based adjustment of handles or objects, in situations where only moderate scale differences are used, a real-world task should see the same improvement as seen in our study.
The total benefit to be gained, however, depends on the frequency of multi-point interaction tasks in the particular activity.
There is also a possibility that different implementations of zoom and pan , could provide better results than the ones we studied.
The second question is whether an interface like the ones introduced here would cause problems for other tasks.
There are possible situations in which the addition of the fisheye lens could cause difficulty: for example, if the user wishes to line up a control point with an object that is nearby, but not close enough to be in the flat  region of the lens, the fisheye distortion can make alignment difficult.
In most cases, the relevant objects from the workspace will be close enough to be undistorted; however, this example suggests the need for a simple control over the magnification and extents of the fisheye.
In terms of effects on other types of tasks, one factor that will reduce the potential harm is that the view alterations  only occur in very specific task situations - that is, when the user defines multiple control points.
The lenses need not appear when the user is in the middle of other types of tasks, and it should be fairly simple for the system to recognize the conditions where the new approach is viable .
One advantage that the Fisheye Auto-Zoom technique has in this regard over the Overview technique is that in the former, the user implicitly specifies the correct level of zoom for the control points .
The study showed that for simple multi-point interaction tasks, the `guaranteed manipulability' interfaces were significantly and substantially faster than the standard interfaces.
In the following sections, we consider reasons why these differences were found, discuss how the results will generalize to more realistic tasks, and discuss ways that the principles underlying the new approach can be used in future interfaces.
The clear and overriding difference between the techniques, and the obvious reason for the differences in completion time, was the number of operations required to complete the task.
The zoom and pan conditions required approximately twice as many operations as did the two fisheye conditions: * Zoom.
For each point, the zoom interface requires two zoom operations  and two drags .
For each point in our tasks, the pan interface required two or three pan motions  and a corresponding drag operation at each stage.
Each point requires a single drag motion, although the motion can be thought of as two parts - a general drag to find the target, and then a more exact motion to place the object.
Each point requires a single drag, and unlike the Auto-Zoom condition, the target is already in view when the user starts the motion.
It is interesting to note that although other researchers have shown that precise positioning of fisheye lenses is difficult , these effects did not appear to slow down the participants in our study, or dramatically reduce their preference for the fisheye techniques.
More generally, it is important for designers to think about interaction with the visual workspace from the perspective of higher-level tasks - tasks which may require combinations of several lower-level mechanisms .
The fisheye techniques demonstrated in the study can be adopted as described for many visual-workspace situations; however, designers can build other techniques that use the principles of guaranteed manipulability and visibility of intermediate regions.
In the longer term, designers may need to rethink the basic zoom-and-pan premise of most visual-workspace systems.
We were surprised at the effectiveness of the Fisheye Overview, and this technique's success argues for the possibility that many visual tasks could be carried out primarily through an overview representation.
Overviews have clear advantages for global awareness of the workspace, and with tools that provide context-sensitive magnification, people may be able to carry out most tasks without needing to use a full-window zoom.
Pan and zoom were chosen as comparators in the study because they are so common in real-world visual workspace systems.
However, our pan and zoom implementations are already considerably lower-effort than what is available in most commercial systems - in many cases , users must select a tool in order to carry out either type of navigation action.
A clear lesson in systems where navigation is frequent is to reduce the effort required for these basic techniques.
Fourth, we plan to explore other ways that systems can automatically recognize when a set of locations have been implicitly identified by the user, and adjust their behaviour accordingly.
This has already been done with search , but could also be based on frequently-visited locations, dependencies in the data, or other rules.
Last, we will investigate one of the remaining shortcomings of the novel interfaces described here - that data not in the proximity of control points is either shown at a less detailed scale, or is occluded entirely.
It would be useful to either modify these interfaces, or develop new interfaces, that overcome this limitation.
In this paper we investigated the problem of users of single mouse systems performing multi-point interactions.
We identified two critical requirements for any interface that supports multi-point interactions, namely visibility of points of interest, and access to data at adequate scales.
We proposed three novel interfaces: Split Scroll, Fisheye AutoZoom, and Fisheye Overview, which meet these requirements.
An experimental comparison of the two fisheye interfaces with zoom and pan in a representative task found that the new techniques performed significantly better than both standard methods.
In the future, we plan to investigate several avenues suggested by this study.
First, we will study more complex multi-point tasks that involve a larger number of points, additional task requirements for checking regions between the points, and stricter dependencies between the different manipulations.
Our intention is to use the image-selection scenario described at the beginning of the paper.
Second, we plan to refine the two fisheye techniques to allow for user control of the lenses .
These revised techniques can then be used in openended tasks that involve varying requirements for detail and context.
In addition, we will further develop other guaranteed-manipulability techniques, such as the split-scroll algorithm.
Third, we will investigate other types of tasks that involve combinations of targeting and scale changes.
For example, one relatively common task involves selecting a small object at magnification, moving to a distant location, and placing the object .
If the magnification level of the beginning and end of the task is maintained, scrolling will be time-consuming, and so a scale change is required.
