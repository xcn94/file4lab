Today many workers spend too much of their time translating their co-workers' requests into structures that information systems can understand.
This paper presents the novel interaction design and evaluation of VIO, an agent that helps workers translate request.
VIO monitors requests and makes suggestions to speed up the translation.
VIO allows users to quickly correct agent errors.
These corrections are used to improve agent performance as it learns to automate work.
Our evaluations demonstrate that this type of agent can significantly reduce task completion time, freeing workers from mundane tasks.
Called VIO, our agent takes on the role of a webmaster's assistant .
Requesters email requests  to the webmaster using natural language.
VIO preprocesses the requests and prefills website update forms with suggestions.
These prefilled forms are presented to the webmaster for approval.
The forms are an augmentation of a traditional directmanipulation interface that allow the webmaster to quickly recognize the task and repair mistakes made by VIO.
Our interaction design focuses on making repairs easy because  we accept that agents make errors, and  having an interface that lets webmasters correct errors by doing their regular work and without generating additional work allows VIO to be deployed with little or no training.
Through the process of repairing and approving forms, webmasters provide training data, allowing VIO to "learn in the wild," that is, directly from use.
This frees the webmaster to then focus on non-procedural tasks that require more human skill.
Today many workers in companies spend time translating requests into language and structures that information systems can understand.
Consider the task of transferring a student from a waitlist to a course.
The requester, a professor, has an intent that matches a common work task.
The professor expresses her intent in an email to the department coordinator with relevant information such as the student's and course's names.
The coordinator then logs in to the appropriate systems and makes the changes, translating the request into information the system can understand.
Organizations address translation tasks by assigning a human-service-agent, such as administrative assistants, webmasters, network administrators, purchasers, etc., who perform procedural translation tasks on behalf of coworkers or customers.
Procedural translation tasks are good candidates for automation because the input is easily captured, the output is structured, and the tasks are repeatedly executed.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Casting VIO as a webmaster's assistant is a first step to concretely test our ideas.
However, the design principles of VIO generalize to a much larger set of procedural tasks found within organizations.
The design of VIO raises several fundamental HCI research questions including:  How effective is a human-serviceagent collaborating with an agent that has had little training compared to a traditional direct manipulation interface?
In this paper we address all these questions.
We begin by describing our novel interaction method that combines natural language interaction--in this case the preprocessing of incoming email requests--with existing direct manipulation tools, and a feedback loop to the machine-learning algorithms.
The first is an empirical study where collaboration with VIO reduced task completion time by 17%.
The second is a Keystroke-Level Model  analysis that provides more detail on the benefits provided by VIO.
In addition, these evaluations provide insights into the cost of different types of agent errors.
Some researchers in the Human-Computer Interaction  community have championed the benefits of automation while others focus on the power of direct manipulation .
While the work on embodied agents has yet to demonstrate significant effects, the implementation of automation via machine learning in underlying systems has become core to both HCI research and development.
Our system tries to find a happy medium in the tension between full automation and direct manipulation.
Our presentation of the request in terms of a form builds on Malone et al.
In discussing future work they predict a graceful degradation between automatically and manually handled tasks.
Our interaction design follows this principle, allowing users to experience the graceful improvement of VIO.
Previous research has explored the use of natural language processing to automatically fill out forms .
Additionally research prototypes have been built that convert email requests into website updates .
However, these systems have not presented agents that perform perfectly, nor have they addressed how users handle agent errors.
In fact, previous work in this area presents no evidence of increased productivity through collaboration with an agent.
Out of the tension between automation and direct manipulation, the mixed-initiative community has arisen.
They focus on exploring how humans and AI systems can more effectively collaborate to solve complex tasks.
Hearst captures the essence of this when she notes that AI researchers want to model humans in order to make computers that act intelligently, while HCI researchers want to leverage computing to make it easier for users to make intelligent decisions .
The goal of the mixed-initiative community is to develop intelligent systems that allow for collaborative interaction to solve problems .
While most mixed-initiative systems have focused on solving complex planning tasks such as generating train schedules  we focus on how agents can free users from mundane tasks.
One mixed-initiative project that has focused on mundane tasks is the LookOut  system, which assists users with adding meetings to calendars.
Like VIO, the system processes incoming email and attempts to fill a form, in this case a calendar event.
While LookOut at a high level is quite similar, our design offers some advances.
Our system has been designed to be domain independent, where LookOut only addresses meetings.
LookOut interrupts the user and focuses on assessing whether a message warrants an interruption .
Our design instead follows an interaction model set by Letizia , where the assistance falls to the side of the locus of attention, allowing the user to easily ignore the assistance but to also benefit from it by having it close.
Finally, our system uses a deeper learning model that provides more effective help over time.
The Eager system , an early example of programming by demonstration, detected simple tasks that a user repeated twice during an interaction.
The system then used highlighting  to draw the user's attention to extracted information.
Eager would use the extracted information to construct a repetitive task for the user.
Eager, however, has no feedback mechanism for repairing errors.
Previous research on email use claims that email is a "habitat" where much project-management work is done .
They developed a "thrask"  interface to help users complete tasks that involve multiple email exchanges.
Their system does not employ machine learning, but instead addresses the presentation of emails.
We view our system as complementary in that VIO provides automation of mundane tasks so users can address more complex threaded tasks.
Finally, many commercial products exist to help organizations manage and automate their workflows.
Business process automation software such as Remedy  provides an abstraction layer between the application logic and the business practice to be automated.
This layer allows businesses to connect different systems together, but also creates environments where many workers must perform the form filling, translation tasks.
In summary, our system  integrates theories from mixedinitiative computing and natural-language research into a working system that has been demonstrated to improve human performance,  advances the design of humanagent interfaces by addressing the issues of agent error and learning, and  extends the research done on blending email applications with tasks and project-management.
The interaction design of VIO embraces the fact that agents make errors.
Instead of investing huge engineering efforts in an attempt to build perfect agents, our design allows an agent with little or no training to observe a task and begin making suggestions.
The interaction design allows users to repair agent errors without increasing the work they would have done without VIO.
As VIO learns, the interaction style allows its suggestions to significantly reduce task completion time.
Figure 2 illustrates VIO's functional architecture.
VIO modifies the incoming email by adding a ranked list of likely tasks followed by a structured list of all possible tasks .
The webmaster reviews the email and selects the appropriate task-form link.
This transitions the webmaster to a task-form showing all of the elements available for update with as many fields as possible completed .
The webmaster then adds any missing information and repairs any information VIO extracted incorrectly.
She then submits the form, causing the execution module to execute a database transaction, and by extension, update the website.
The results of the interaction are then forwarded to the learning module, which analyzes the entire interaction and improves VIO's performance.
The current task-form layout addresses this issue by placing the source message on the left.
This arrangement reduces scrolling and makes comparisons between the email and the form elements much easier.
The form elements use background color to help communicate actions.
Elements updated by VIO have an orange background, elements updated by the human-service-agent have a blue background, and elements that have not changed have a white background.
Additionally, an orange highlight in the email helps users see items that have been extracted.
In pilot studies VIO would occasionally make incomplete extractions that participants failed to see.
In one example, the incoming email asked the webmaster to add "School of Information Sciences, University of Portlandia" as an affiliated organization.
However, VIO extracted "Information Sciences University" as the organization.
Highlighting the extracted text in the email allows users to more easily notice partial extractions.
Figure 3 shows an incoming email request VIO has modified with a ranked and structured list of tasks.
An earlier design presented a completed form based on VIO's best guess instead of a list of tasks.
However, in pilot testing participants had difficulty recognizing when VIO had selected the wrong form.
To address this problem we borrowed a technique from information retrieval and used a ranked list.
This approach is a form of "soft fail" : an agent error that still moves the user closer to their goal.
In an earlier prototype the ranked list showed only the top three items; however, pilot testing revealed problems with this interaction.
When participants encountered a list where the appropriate task did not appear as one of the top three items, they hesitated before making a selection from the structured list.
Participants spent time re-examining the incoming email, apparently worried that they had not understood the requester's intent.
The current design addresses this hesitation by using a threshold value to determine the length of the ranked list.
When the VIO has high confidence in a single form, it lists only one item like in the example above .
When VIO has high confidence in several forms, it lists several forms.
When VIO has low confidence for all forms, it makes no suggestion and only the structured list appears.
Figure 4 displays a Modify Person task-form.
An earlier design placed the message text at the top of the task-form to communicate that users should start at the top and work down to the update button.
For form ranking, VIO logs the form selected for each message.
The history of selections is used as a label to the log of messages.
Messages are represented as a bag of words.
A k-way boosted decision tree algorithm is trained to generate the model for form ranking.
For the experiment described in the evaluation background section, this model has a mean reciprocal rank above .90.
For entity-instance selection, VIO logs the records selected for forms.
The record selected is the label for  a data set represented as the difference between the records and the words in the message.
A Naive Bayes classifier is trained to generate the model for entity instance selection.
This model has the mean reciprocal rank above .85.
For form-field completion, each field value v completed by the user in a form is logged.
A domestication algorithm searches the message for a string s similar to v and declares s a label for the field.
The labels are used for a corpus of messages represented as strings.
A conditional random field algorithm is trained to generate an extraction model.
Extraction model entity F1 performance ranges from 0.0 to 1.0.
Extraction performance depends most strongly on the number of labels for training and the length of v. To summarize, during use the user is assumed to select the appropriate form, to select the appropriate target instance of the form, and to complete the appropriate fields for a form.
VIO used the log of this interaction to label processed messages and train machine-learning models for form selection, instance selection, and form-field pre-filling.
The resulting models are accurate, but imperfect, predictions of user interactions.
In addition, we removed threaded discussions and split multiple requests into separate messages.
During this procedure, we were careful to preserve the original language.
This procedure produced a corpus of 228 messages sorted in roughly chronological order.
The first 200 messages were used to train VIO.
The remaining 28 messages were used for the evaluation.
During training, the appropriate form was selected, the appropriate target instance of the form was selected, and the appropriate fields were completed for each of the 200 messages.
VIO used the log of this interaction to label processed messages and train machine-learning models for form selection, instance selection , and form field pre-filling.
Note that in many domains human-service-agents process 200 requests every few days, so the VIO receives only a modest amount of training.
Task T1 Form Add person Task Create a new person record with first name, last name, email, organization, city, and state.
Add an image to an existing person record.
Add title, street address, and office location to an existing person record.
Add a new news record with headline and body text.
Add image to an existing person record.
Add a new news record with headline, body text and URL.
Fix a misspelled name in an existing person record.
For the empirical evaluation of the interaction, we selected one hypothesis to test: Does interacting with VIO significantly reduce the amount of time needed to complete website update requests?
In addition, we wanted to investigate  how errors committed by VIO impacted the participants' performance speed,  how errors made by VIO impacted errors participants committed to the database, and  general usability issues around the VIO interface.
Method Participants played the role of the webmaster in one of two conditions: CMS and VIO.
In the CMS condition participants completed tasks using a web interface designed to look like a traditional content management system .
In the VIO condition participants completed tasks using the same interface, augmented with VIO's suggestions for form selection, instance selection, and prefilled field values.
Participants first received instructions from the experimenter.
The experimenter walked participants through an example task to quickly familiarize them with the interface.
Next, participants completed 20 practice tasks.
The goal of this extensive practice was to train the participants to work more like skilled users.
While working on these tasks, participants could ask questions of the experimenter.
Following the practice questions, participants completed the 8 evaluation tasks .
The number of tasks was kept small due to the amount of time spent on training.
Following the completion of the final task, participants answered a survey on their perception of usability and were graded using a grading script that compared the final database state with the correct state.
Participants in the VIO condition first viewed an incoming email to the webmaster that had been modified with a prioritized and structured list of task-forms.
Participants selected one of these, transitioning them from the email client to a web browser that displayed the task-form.
Participants in the CMS condition followed a similar task flow.
They first viewed the incoming request in the email client.
Next they navigated to their browser, which displayed the "Task Picker" page, showing the same structured list of tasks that appeared in VIO's modified email .
Here they selected the appropriate task from a structured list of links to all task-forms.
Selecting a task caused the browser to transition to the task-form page.
This form used the same layout as the VIO task-form  with the following exceptions.
First, in the CMS condition, the task-form did not show the source email.
Second, in the CMS condition there was no orange background color to indicate actions VIO had taken, since VIO had taken no actions.
In the real world, software products do not make a connection between the requesting email and a website update task.
For incentives, participants were offered $30 to complete the experiment and penalized $5 for each mistake with a guaranteed minimum of $15.
In addition, a $20 bonus was given to the top 50% of all participants based on a combination of their speed and accuracy.
In this case accuracy entailed correctly making the requested update to the website.
Our intent was to motivate them to work quickly and accurately, like a real webmaster.
As participants worked, a logging application and a screencapture application ran in the background.
The logging software captured the title of the window in focus.
Participants opened each email message in its own window, performed the task, and closed the message window before moving on to the next one.
Task completion time was defined as the amount of time between subsequent openings of new email message windows.
Forty people  ranging in age from 18 to 35 with an average age of 22.62 participated.
All subjects had previous experience with building or updating websites.
Twenty were randomly assigned to each condition.
The far right column details the decrease in time for VIO as a percentage of the total time from the CMS condition.
The results show that across all questions, the VIO interface decreases the amount of time needed to complete a task by approximately 17%.
Task time was measured in milliseconds and is reported in seconds.
Because task time was positively skewed, we truncated extreme values at the mean + 2.5 SD for each task.
The number of times an email message was in focus for greater than 1 second during the completion of a single task was recored.
To investigate participants' perception of the VIO and CMS interfaces, we asked them to complete a survey based on the instrument for measuring usability developed by van Schaik et al.
Questions addressed the ease of use, usefulness, involvement, and flow control.
Prior to analysis, scores were inverted such that high scores indicated more positive assessments.
Survey responses were factor analyzed using Varimax rotation.
The solution indicated the presence of five factors that accounted for 77% of the variance.
Factor 1  was comprised of three questions about the ease of use of the system .
The three factors formed a reliable scale  and were averaged prior to analysis.
Factor 2  was comprised of four questions about the perceived usefulness of the software .
These four questions formed a reliable scale  and were averaged prior to analysis.
Factor 3  was comprised of two involvement questions .
These two questions were averaged prior to analysis .
Factor 4  was comprised of 4 questions about participant's feeling of control during the task .
Responses to these four questions were averaged prior to analysis .
We analyzed the results using a 2  by 8  repeated measures ANOVA, in which interface was a between-subjects factor and task was a within-subjects factor.
Table 2 details the average task completion times and standard deviation for these times for both conditions.
As can be seen, subjects in the VIO condition performed significantly faster than those in the CMS condition .
Agent Learning Errors For each learning problem, the agent can make two general types of errors.
A false negative error occurs when the agent fails to produce a valid suggestion.
A false positive error occurs when the agent produces an incorrect suggestion.
False negative errors represent a lost opportunity to help the user.
False positive errors require the user first to recognize and then correct an error.
Specifically, the following errors are possible: WF:  selected wrong taskform as its top choice, but placed the correct taskform on the prioritized list.
MF:  failed to list the correct form on the prioritized list.
WR:  selected the wrong record when displaying the task-form.
MR:  failed to suggest a record for a form.
WX:  extracted the wrong information and added it to the task-form.
MX:  failed to extract data included in the email needed in the task-form.
Table 3 lists errors VIO made.
Note that VIO never made an MF, MR or WX error during the test.
Column" VIO Ext" shows the number of items the VIO extracted from the email to the task-form.
Task T3 requires additional explanation.
T3's source email requests a modification to a person's title, address, and office location; however, VIO selected the wrong person.
VIO selected "David Rodgerson" instead of "David McCullar."
Additionally, VIO infers the requestor wishes to update his last name, and it extracts the last name "McCullar" of the actual requestor and overwrites "Rodgerson" with "McCullar."
Error Summary Suggests Modify Person  followed by Add Person .
Extracts wrong person's record--matches first name but not last.
Misses: office location and street address.
Misses: headline and body text.
This reduction comes from reduced navigation and from a reduced need to copy and paste.
Participants in the VIO condition reduced navigation in three ways.
First, they had the list of tasks appear in the email, eliminating the need to navigate to the task picker page.
Second, because they had the incoming email message displayed within the task-form, they did not need to toggle between the email and the task-form in order to copy and paste content.
Third, the VIO selected the correct record automatically for the participant.
To further understand the impact of placing the email next to the form we examined the participants' logs.
Table 5 shows the average number of times the task email window was in focus for greater than 1 second.
The VIO and CMS columns show the number of times participants viewed the source email in each condition, and the "decrease" column shows the percentage decrease in time to complete a task for the VIO condition as compared to the CMS .
Overall, the table indicates that the task email is viewed far less frequently in the VIO condition.
Thus, users save the labor of switching between the email and the form.
Participant Errors Participants introduced few errors into the web site database.
CMS participants introduced 12 errors and VIO participants introduced 15 errors.
In looking more closely at the errors, we can see that the error VIO made to task T3 caused 13 of the participants in the VIO condition to introduce an error.
Usability survey Participants' responses to our survey measures were averaged to create four factors: ease of use, usefulness of the software, feelings of involvement, and flow control.
Means for each scale are shown in Table 4.
Responses were analyzed using one-way analyses of variance.
For flow control, there was a borderline significant effect of condition , indicating that participants in the VIO condition perceived that they experienced less flow than participants in the CMS condition.
ANOVAs on the other three scales showed no significant effects .
VIO improved users' speed, yet it did not result in any loss in users' perceptions of ease of use, usefulness, or personal involvement.
A 2  by 8  repeated measures ANOVA, in which task was the repeated factor, indicated a significant main effect of condition , indicating that switches were much less common in the VIO condition.
There was also a significant effect of task  and a significant task by condition interaction , indicating that the switches were more common in some tasks than others, and that the benefits of VIO over CMS for task switches were larger for some tasks than others.
Interestingly, while tasks T2 and T6 showed the greatest reduction for the VIO condition, they both had very little difference in window toggling behavior.
These tasks both involved adding a photo, and the time reduction seems to come from reducing the number of steps in saving the file from the email and linking it to the task-form.
With respect to task T3, VIO clearly generated an error that many participants allowed to enter the dataset.
Two  of the 13 participants noticed the error but incorrectly compensated for it.
We propose three ways to address this problem of selecting the wrong record.
First, the algorithm that selects the record could use machine learning to improve its accuracy.
Second, VIO could be tuned to reduce the number of false positives at expense of reducing suggestions.
This solution would mean users select the record for more tasks; however, the slight increase in effort would be offset by reduced errors.
Third, when VIO has high confidence for more than one record, the record information could be included in the modified email suggestion.
For example, the email list of tasks might include "Modify Person: David Rodgerson," "Modify Person: David McCullar," "Add Person: David Rodgerson," and "Add Person: David McCullar."
Participants gave good usability scores for both conditions.
This result indicates that the experiment was unbiased with respect to the design of VIO and CMS forms, and that the time and performance differences were not due to qualitative usability differences.
In addition, the high scores for VIO indicate that it would likely be accepted by users.
When interacting with the pull-down menu, the skilled user types the first few letters to get to the correct item in the list, then types the enter key to select it.
For this KLM we used a reading time of 300 words per minute.
To apply the reading rate we considered the number of words in each email message, excluding blocks of text that are generally not read.
For example, add-news tasks sometimes included the text of the news story, which we excluded from the word count.
Text such as URLs and email addresses each counted as a single word.
Reading times for the tasks ranged from 4 to 10 seconds.
Design For this analysis we used the same eight tasks from the empirical study, under four conditions: * CMS: This KLM was based on our CMS interface.
This condition works as a benchmark for revealing the performance gains from the other conditions.
In this condition the incoming email had the structured list of tasks, but no prioritized list.
Also, in the task-form, no fields were updated by the VIO.
This condition measures interaction design effects without interference from VIO.
This condition provides a view of an agent with some training.
Also, this condition helps us see how accurately the KLM matches human performance, allowing us to investigate the time difference needed to recognize a VIO error.
This condition shows the maximum benefit the VIO can provide.
Results Table 6 shows the modeled task-completion times in seconds.
Column CMS shows the completion time for the CMS interface; column VIO-interface shows the completion time for VIO interface with no learning; column current-VIO shows completion time the VIO with the same level of learning as the empirical study; and column perfect-VIO shows the completion time for VIO with no errors.
The bottom row details the total time for all tasks in each condition and the percent decrease in task time for the different VIO conditions as compared to the CMS condition.
The VIO-interface-only condition reduces the task time by 15% compared to the CMS; the current-VIO condition reduces task time by 43% compared to the CMS; and the perfect-VIO condition reduces the task time by 71% compared to the CMS.
The KLM models the time needed to make corrections to agent errors, but it is not intended to model time taken for users to think about the message and examine the agent-
The empirical evaluation demonstrated that the agent with a little training can provide significant benefit.
However, this evaluation raised three questions:  How much does VIO action reduce time as compared to the interface design?
In order to gain some insight, we performed a Keystroke-Level Modeling  analysis.
This method predicts average performance for skilled users by assigning values to user and system actions.
We used CogTool , a software tool that allows interface designers to import their interface screens, to measure the sizes and map the locations of buttons and other clickable targets, and to provide a script of a user's actions such as button presses, reading time, pointing time, system processing time, etc.
The automation the tool provides significantly reduces the time and effort required to perform a KLM.
In both the CMS and the VIO interfaces there are multiple ways to perform each task.
For this KLM we chose to have the skilled user do what we observed most participants in the empirical study do: read the message, navigate to the task-picker page , select the correct form, toggle between the message window and the task-form for each field's content and paste it in , or copy and paste from the embedded message .
There are two exceptions: * T1 requests an update to the person's city and state to "Alexandria, VA." In the KLM the expert user pastes "Alexandria" into the city field, but simply types "VA" into the state field.
Assuming that the completion of the 20 training tasks aided some participants more than others, the ranked list can be viewed as a list of most-skilled to leastskilled users.
Surprisingly the chart shows a set of near parallel lines, revealing that VIO reduced task competition time by the same amount regardless of the skill level.
While the total amount of time reduced remains the same, as skill increases, and the total completion time gets small, the percentage of time saved by the agent as a factor of total task time increases.
Discussion The VIO-interface-only condition produced a 15% reduction in time over the CMS condition.
This 15% benefit comes exclusively from the interface because in this condition VIO uses no learning to automate the task.
The savings comes exclusively from eliminating the need to navigate to the task-picker page and select the correct form, and from eliminating the need to toggle between email and the browser by embedding the message in the task-form.
The perfect-VIO condition produced a 71% reduction in time compared to the CMS.
Subtracting the 15% benefit provided by the interface reveals the maximum benefit from the VIO's learning at 56%.
This number may be high, as users of the system would still need to check to see if an error occurred, something not modeled in the KLM.
The current-VIO condition produced a 43% reduction compared to the CMS, revealing a 28% reduction provided by VIO in addition to the interface.
The KLM prediction of human behavior clearly shows that as the agent learns, the users reduce task time by having more and more of the task automated.
However, this savings of 43% is much higher than the 17% found in the empirical study.
Two main factors help explain this difference: skilled performance and error recognition.
The 20 practice tasks participants completed before the timed tasks may not have provided enough training for them to perform at the level of an expert, particularly for the VIO interface, which requires interaction methods that are unfamiliar to participants.
In addition, KLM results do not take into account the time needed to recognize errors; they only take into account the amount of time to fix it.
Figure 5 provides some insight into the issue of skilled performance and error recognition.
The five fastest  CMS participants had an average total task completion time of 300.6 seconds, just 3% slower than the KLM prediction of skilled performance.
This data indicates that these participants may have been performing at skilled level after the 20 training tasks.
The five fastest VIO participants had an average total task competition time of 207 seconds, 25% slower than the KLM predicted.
From this data we can speculate that the additional time needed to recognize VIO errors is approximately 25% of the KLM prediction of total task time for skilled users.
In considering reduction in task time, while VIO reduced task time by 17% for all users, it reduced task time by 31% for the top five performers when compared to the top five performers in the CMS condition.
This paper presents a new mixed-initiative interaction design where users train an agent to automate completion of mundane, procedural update tasks.
The design embraces the idea that agents make mistakes through an interaction design that asks users to repair agent errors as a method of providing training data that improves the agent's learning.
The design allows users to simply perform their work in order to train the agent.
Our empirical evaluation demonstrates that this interface coupled with an agent that has very little training can significantly reduce the amount of time that workers spend on mundane, procedural tasks.
One main insight gained during the design and evaluation of VIO is the need to develop the machine-learning system and the interface simultaneously.
Co-development allowed for negotiation between the input requirements of the learning system and the actions required by the user to complete the work.
These studies provide evidence to support our research direction of using agents with little or no training to improve users' performance on procedural tasks, thus freeing workers to focus on tasks requiring more creative thinking.
VIO has recently been deployed to assist a real webmaster in the maintenance of a large project website.
In the current log of webmaster requests, approximately 50% of messages contain single tasks that can be executed with the existing system.
As a next step, we plan to add machine learning and interaction capabilities to handle multiple tasks in a single message.
In addition, we are now developing a new mixed-initiative interface where end-users can construct their own workflows, allowing them to design the tasks they wish the agent to learn to automate.
This material is based upon work supported by the Defense Advanced Research Projects Agency , through the Department of the Interior, NBC, Acquisition Services Division, under Contract No.
The authors wish to thank professors Susan Fussell, Robert Kraut, and Bonnie John for help with study design and data analysis.
We also thank the reviewers and Chris Scaffidi for detailed comments.
