The ability to detect and differentiate breakpoints during task execution is critical for enabling defer-to-breakpoint policies within interruption management.
In this work, we examine the feasibility of building models that are capable of detecting and differentiating three granularities  of perceptually meaningful breakpoints, without specifications of users' tasks.
We collected an ecological sample of task execution data, and then asked human observers to review the collected videos and identify perceived breakpoints and their type.
Statistical methods were applied to learn models that map features of the ongoing interaction to each type of breakpoint.
Results showed that the models were able to detect and differentiate breakpoints with reasonably high accuracy across tasks.
Among many uses, such models can enable interruption management systems to better realize defer-to-breakpoint policies for interactive, free-form tasks.
This limitation severely inhibits the ability to realize defer-to-breakpoint policies in practice, though these are known to be effective .
In this work, we seek to overcome this central limitation by understanding how to detect breakpoints and differentiate their granularity without requiring any task specification.
Granularity refers to the difference between the observable actions surrounding a breakpoint , and the ability to differentiate granularity is critical.
For example, this would allow systems to reason about whether to defer notifications until coarser breakpoints, which occur less often, but offer larger reductions in cost; or until finer breakpoints, which occur more often, but provide smaller reductions in cost .
A basic question is how many granularities of breakpoints are detectable and meaningful during task execution.
From studies of event perception  and task interruption , there is evidence for at least three perceptually meaningful granularities; Coarse, Medium, and Fine.
For example, when editing documents, Fine may be switching paragraphs; Medium may be switching documents; and Coarse may be switching to an activity other than editing.
We investigate how these three granularities of breakpoints are manifested during the execution of free-form tasks and examine the feasibility of building models that detect and differentiate them.
We collected ecological samples of task execution from three categories; document editing, image manipulation, and programming.
Utilizing methods used to study perception , observers reviewed collected videos, identified perceived breakpoints and their type, and entered rationale.
Breakpoints were thus detected based only on a user's observable interaction, not their internal state, similar to the data that would be available to a system in practice.
By aggregating and filtering the breakpoint data, we could identify `true' breakpoints, i.e., those with high agreement.
From observers' rationale, our own analysis of the data, and related work , we identified candidate features of the interaction that might indicate each type of breakpoint.
Predictive features were identified from the candidate set and statistical models that map these features to the true breakpoints were learned and evaluated.
A breakpoint is the moment between two meaningful units of task execution , and controlled studies have shown that deferring delivery of notifications until a breakpoint is reached can meaningfully reduce cost of interruption .
However, to implement these types of policies within a system for interruption management, we need to better understand how to efficiently and accurately detect breakpoints within interactive tasks.
One common method for detecting breakpoints is to match users' ongoing interaction to specifications of tasks defined a priori .
The benefit of our models is that they are able to detect and differentiate breakpoints using only features of the ongoing interaction in free-form tasks, without any specifications of those tasks.
This would, for example, enable systems to better realize defer-to-breakpoint policies in practice.
The ability to detect breakpoints during task execution has many useful applications.
For example, for interruption management, studies show that deferring the delivery of notifications until a breakpoint is reached can meaningfully reduce cost of interruption .
However, in these studies, specifications for primary tasks were determined in advance using modeling techniques such as GOMS , enabling interruptions to be cued at specific moments .
If breakpoints could be reliably detected in free-form tasks, then defer-to-breakpoint policies similar to those used in the controlled studies could be fully realized in practice.
Detection of breakpoints can also contribute to an emerging class of interactive tools that enables knowledge activities to be organized into reusable structures and shared .
A challenge in building these types of tools is being able to organize user activities without having to repeatedly solicit input .
Models that detect breakpoints could thus allow automated organization, reducing the burden on end users.
A breakpoint represents the moment between meaningful units of observable actions , and is reflective of internal transitions in perception or cognition .
For example, breakpoints are often used to study how people segment incoming sensory stimuli .
A consistent finding is that observers identify many of the same locations as breakpoints, showing that perception is segmented into discrete units and a shared cognitive schema is driving this process .
These results generally hold for tasks that are familiar and unfamiliar to observers .
Observers report that certain visual cues such as changes in the attended-to object, action on that object, or tempo of the action provide salient indicators of breakpoints .
This implies that it should be possible to build models  that utilize analogous cues to detect breakpoints within execution of interactive or other tasks.
Another relevant finding is that observers can dynamically modulate the granularity of segmentation between coarse and fine units of action , where granularity refers to the perceived difference between the previous and subsequent units.
Since part of the fine breakpoints typically align with coarse breakpoints, schemas driving perception and action are thought to have at least a two-level hierarchy .
This implies that models should differentiate at least two types of breakpoints - Coarse and Fine - during task execution.
Neuroscience studies also show that breakpoints identified by observers of actions are similar to those experienced by the person performing the same actions .
This is logical, since the person performing an action is also an observer of their own action as part of a closed loop system .
Our work leverages these methods to identify perceptually meaningful breakpoints in free-form computer tasks and to understand how to build models that detect them.
Several methods have been used for detecting breakpoints.
For interactive tasks, researchers have used the number of application windows selected within a sliding time window as an indicator of an activity switch  .
Results of this method were highly variable, ranging from 20% to 90% accuracy rates, and only one type of breakpoint could be detected.
A second method is to specify the execution structure of tasks in advance and then monitor those tasks at runtime .
Although this allows breakpoints to be detected in tasks that have fairly prescribed sequences, it is very difficult to use these types of static specifications to detect breakpoints in free-form tasks, which are much more common.
A third method is to instrument a virtual desktop window manager and detect when a user switches "rooms" .
This method could only detect a single type of breakpoint and would force users to use this type of window manager.
A related thread of research has produced statistical models of interruptibility for interactive tasks .
One explanation as to why these models work is that they implicitly detect a user's time to breakpoint.
Indeed, the authors reported that they informally observed users deferring acceptance of a cued interruption until they reached the next breakpoint.
In the area of mobile devices, Ho and Intille  used data from multiple accelerometers attached to a person's body.
Based on analyzing the data signatures in various physical postures and movements, they were able to detect moments when users were in physical transition .
Results from a study showed that deferring an interruption until this transition reduced cost of interruption.
The software was configured to record screen interaction at a low, but adequate frame rate  using the Camtasia SDK and logged mouse, keyboard, and other relevant system events using the Windows Hooks API.
Users were asked to activate the recording software the next time that they would be primarily focused on performing any task within the relevant category for at least an hour.
We emphasized that they should perform the task, with the interleaving of any other tasks, as usual.
To avoid recording sensitive data, users were reminded that they could pause/ restart the software at any time.
Once at least an hour of data was recorded , the user notified the experimenter, who collected it and removed the software.
For task content, for DE, one user was writing a research paper while the other was writing study guides for exams.
For IM, one user was touching up personal photos from a recent vacation while the other was developing icons and other graphics for a software application.
For Programming, one user was developing an interface for a research project while the other was writing code for a course assignment.
Applications included Word, Photoshop, and Eclipse.
To develop effective and efficient models for detecting and differentiating task breakpoints, our process was to: x Collect representative samples of users' task execution, in the form of screen interaction videos and event logs.
To facilitate collection and analysis of the breakpoint data, we developed several new software tools.
Activity Recorder records a user's screen interaction and logs system events; Breakpoint Annotator enables observers to review videos, identify breakpoints, and enter linguistic explanations; and Breakpoint Analyzer supports interactive analysis of the data.
Our tools can be utilized to reduce the effort required to collect and analyze similar data, e.g., data in .
The next step was to determine the locations of perceived breakpoints and their type within the task execution data.
24 observers were recruited, 8 per category, and were asked to review the two videos from an assigned category, mark the location and type of each perceived breakpoint, and enter a brief description as to why they felt this was a breakpoint.
Observers were asked to detect and differentiate three types of breakpoints, guided by the following descriptions: x Coarse.
The moment when the largest meaningful and natural unit of execution ends and the next one begins.
The moment when the smallest meaningful and natural unit of execution ends and the next one begins.
The moment when a natural and meaningful unit of execution, which is smaller than Coarse but larger than Fine, ends and the next one begins.
Inclusion of Coarse and Fine breakpoints, along with their descriptions, is consistent with research on event perception .
Medium was included since prior work has shown that there are three classes of interruption cost , ostensibly tied to 3 levels of breakpoints, and results from a pilot study showed that users were able to differentiate the three types of breakpoints within the data .
Using observers to identify breakpoints in another user's tasks is effective because research has shown that the same schema used to chunk a person's goal-directed actions are also used to chunk their perception when observing another person performing those same actions .
Task execution data was collected from three general task categories; Document Editing , Image Manipulation  and Programming .
These categories were selected because they are often performed by many users, comprise diverse subtasks, and require varying engagement.
Using several categories would allow better understanding of the similarities and differences among breakpoints across tasks.
For each category, two users  were recruited and screened to ensure that they had adequate experience in the category they selected and would be comfortable having their interaction data recorded and viewed by others.
Each user received $20 for participating in this study.
We wanted to collect samples of users' own personal or work tasks, performed in their own environment, ensuring a high degree of ecological validity.
Our recording software was thus installed on users' own machines and they were informed of what data it was recording and how to control it.
For example, the software allows recording to be started, paused, or stopped at any time using keyboard shortcuts and shows its current status through an icon in the system tray.
Once questions were answered, the observer began annotating the first video.
When a breakpoint was detected, the observer selected a button indicating the type of breakpoint .
In response, the video was paused, a tick mark was shown on the relevant timeline, and a textbox was activated for entering an explanation.
The observer could review the video and modify breakpoints as desired.
The observer annotated both videos within an assigned category, but since annotation required about two hours, the process was split across 2 days.
The order of videos in a category was counter-balanced.
Observers received $20 for participating.
If those cues could be identified, then models could be built  that automate a similar process.
For procedure, observers came to our lab and were asked to review videos of task execution and identify moments at which they felt that one unit of execution ended and another began; using cursor movements, interaction sequences, and state of the task as cues.
The different types of breakpoints were explained using the previous descriptions.
The overall methodology was consistent with prior work .
Our Breakpoint Annotator tool  was used to assist the observer in the annotation process.
As expected, Fine breakpoints were the most frequent while Coarse breakpoints were the least frequent =128.9, p<0.0001; offering evidence that perceptual processing of free-form tasks also occurs in a hierarchical manner .
Temporal distances between breakpoints are summarized in Table 2.
The average distance between breakpoints ranges from about 1.5 min  to 10.7 min , with the overall average between any two breakpoints being about 3.8 min.
These results support and extend the data reported in .
This data is important because it provides some of the first ecological estimates of how long an interruption reasoning system would need to defer delivery of information in order to reduce interruption cost.
These values could also inform the design of interfaces that allow users to specify how long they would be willing to wait for different types of information .
From observers' explanations, Coarse breakpoints typically corresponded to a switch in high-level activity, indicated by switching to other application judged to be unrelated to the main task, e.g., changing to a music player, checking email, or reading news online.
A Coarse breakpoint was also often indicated by returning back to the main application.
Medium breakpoints were tied to switching to applications judged to be relevant to the primary task or to a large shift in focus within the content of the application.
For example, for DE, this included starting to edit a paragraph in another section of the document, saving the document, and opening another document.
For IM, this included loading another image, starting to edit a different region or visual feature of the image, and saving the current image.
For Programming, this included starting to edit a new class in the file, saving the current source file, switching to another source file, and switching between the code and debug windows.
Fine breakpoints were usually tied to actions on the content within an application.
For example, for DE, this included completing formatting commands, searches, and copy/paste sequences; and starting to edit another paragraph near the current insertion point.
For IM, this included completing layer manipulations, resize of canvas, and operations such as color adjustments, blending, cropping, and selection.
For Programming, this included starting a new method, closing a method, completing a compile, completing the check in/out of a file; and completing definition of class variables.
Interestingly, observers did not identify lower-level units, such as completing a specific sentence or line of code, or moving between fields in a dialog, as Fine breakpoints.
The commonly cited reason, clearly evident in the videos, was that editing at the level of a sentence, line of code, or area of pixels exhibited rapid interleaving of pointing, typing, erasing, selecting, scrolling, etc.
Thus, attempting to detect breakpoints at this level of detail is probably not warranted, consistent with earlier empirical findings .
Overall, this data offers some of the first evidence as to where and how often breakpoints occur within interactive tasks, and offers insight into the types of features that might be useful in models for detecting and differentiating them.
A screenshot of our tool that allows breakpoints to be aggregated and interactively analyzed.
The aggregated breakpoints are visualized in the top window.
When a breakpoint is selected, the video  is positioned at the corresponding temporal location.
Candidate features are listed at the right and allow each breakpoint or other bin within the video to be efficiently coded.
We first needed to divide the interaction data into discrete bins, which is necessary since there is natural variance in the temporal locations that refer to the same breakpoint, e.g., some observers may take different amounts of time to decide whether a breakpoint had just occurred.
Our goal was to select a bin size large enough such that slightly different locations referring to the same breakpoint would fall into the same bin, but small enough such that locations referring to different breakpoints would not.
Whether a marked location referred to the same breakpoint was determined by analyzing observers' explanations and the corresponding parts of the interaction videos and logs.
From testing a number of bin sizes, between 1s and 20s, we found that a bin size of 10s best met our goal and that this value achieved our goal for each type of breakpoint.
This is slightly larger than bin sizes used in prior work , but our tasks were of much longer duration, on the order of hours as opposed to minutes.
Table 3 shows the number of bins for each task, and how many of those bins contained each type of breakpoint.
If a bin had multiple types of breakpoints, it was counted multiple times.
We then had to establish the minimum number of observers who needed to have indicated that a breakpoint was within a bin before being able to conclude that that bin contained a "true" breakpoint.
The third step was to combine the breakpoint data across observers and identify breakpoints that had high agreement.
This would remove "noise" from the data set and provide the ground truth for the model building process.
Figure 2 shows a screenshot of our interactive tool that was used to facilitate analysis and coding of the breakpoint data.
Also, independent sample t-tests confirm that more observers had detected a breakpoint in a breakpoint bin than in the other bins across tasks and breakpoint type .
What is perhaps most intriguing about this result is that the observers, all of whom had annotated the videos separately, identified many of the same moments as breakpoints.
This occurred because observers were likely perceiving similar cues in the interaction videos.
This implies that it should be possible to build models that leverage those same cues to detect and differentiate breakpoints for free-form tasks.
Though there were fewer breakpoint bins due to filtering, the average temporal distances were similar to those listed in Table 2 and ranged from 1.4 min to 11.9 min, with the average between any two breakpoint bins being 4.3 min.
Next, we needed to identify features that could be used to detect and differentiate breakpoints during task execution.
Candidate features were determined based on an analysis of observers' explanations and event logs, our own analysis of the task data, lessons reported in prior work , and whether values could be realistically computed in a system.
For Coarse breakpoints, observers were very consistent in describing them as a switch to another activity that was not related to the main task .
However, this abstract description does not yield any specific, usable features and a model would not be able to know what a user's main task was without prior knowledge.
Based on detailed inspection of video segments corresponding to Coarse breakpoints, we observed that they were frequently tied to switches among various types of applications or content, e.g., music players, e-mail and instant messaging, or online shopping and news.
Our observations are also consistent with results derived from an analysis of users' activity data, as reported in .
We thus created a set of application categories including Entertainment, Communications, and Web; with the latter being further categorized based on whether it is a common news or shopping site based on its URL; and those already being used as part of this work .
Under the assumption that various applications could be mapped to these categories, features were created for the number of switches between them.
Also, the number of applications started, exited, and moved were included, as these have also been argued to indicate switches in high-level activity .
Though our approach offers a reasonable starting point and extends prior work for detecting Coarse breakpoints, future work should explore the value of including features tied to the degree of similarity among application content, e.g., using techniques in .
Note that overcoming challenges of applying such techniques within the domain of interactive applications is well beyond the scope of our current work.
Our approach, following , was to compute the average number of breakpoints per bin, considering only those bins with at least one breakpoint; add 1.65 standard deviations; and round.
This process establishes an alpha=.05 threshold , and this threshold was calculated for each task and breakpoint type.
A bin with a number of breakpoints  greater than the computed threshold was considered to contain a true breakpoint, or breakpoint bin.
Table 4 shows the decision thresholds used in this filtering process.
The number of breakpoints meeting the thresholds was 445 , and are summarized in Table 5.
Inspection of the table shows that the filtering was fairly uniform.
Medium and Fine breakpoints typically occurred during the interaction within an application.
Our approach here was to bind features to independent actions at the application interface level, following work in .
If the first two occurred within a bin, then this would likely indicate Medium; whereas if the latter two occurred, then this might indicate Fine, e.g., due to switching paragraphs.
For Coarse breakpoints, we identified 20 features that were independent of any one application.
For Medium and Fine, we identified 33 features for DE, 33 for IM, and 42 for Programming, with some overlap.
Samples of the features  are provided in Table 6.
A coding agenda was developed, comprising a description, example, and rule for each feature .
For each breakpoint bin , values for the features were computed by applying the agenda to corresponding parts of the videos.
We also computed values for the features for a sample of bins that had no breakpoint , enough to compose 25% of the total training cases.
The coding was validated by having an independent coder compute values for the candidate features for 10% of the bins, randomly selected from the training cases.
Cohen's Kappa showed satisfactory agreement between them .
Coarse breakpoints were deemed independent of any one application while Medium and Fine were more dependent.
Training cases were organized accordingly, but Medium and Fine cases from each task category were included as part of NAB cases for Coarse, helping to minimize overlap between the models.
Given this organization of the training cases , the predictive features were extracted using Correlated Feature Selection  with a Greedy Stepwise search .
CFS was chosen since some candidate features may have been correlated.
Predictive features are shown in bold in Table 6.
The last step was to learn models that map the predictive features to the breakpoint types and NAB.
A multilayer perceptron  was leveraged to learn each mapping, as it does not assume independence of features and has been used to learn similar models in prior work .
The model for Coarse breakpoints had two outputs  while the models for each category of task had three outputs .
All models had one hidden layer.
For input, the model for Coarse used only those features that were independent of the task  while inputs for the other models corresponded to features tied to the application, in addition to the general features.
Mappings were learned using back propagation, and a 10fold cross validation was used to evaluate the models.
Table 7 shows results for predicting Coarse and NAB.
The model yielded an overall accuracy of 87.1%, which is much better than the baseline =72.3, p<0.001; baseline =51%, where baselines were calculated as the accuracy of always predicting the most common outcome.
The high accuracy can likely be attributed to the model's features detecting a switch between certain application categories that often indicated a switch between unrelated activities.
More sophisticated analysis of the similarity between the content of applications may yield further improvements.
Tables 8a-c show results for detecting and differentiating Medium, Fine, and NAB for the three task categories.
Before predictive features could be extracted, we needed to decide how the models would be built.
Our approach was to create one application independent model for predicting Coarse/NAB and a set of application-specific models for predicting Medium/Fine/NAB for the task categories, giving a total of 4 models.
The model was slightly less accurate for differentiating between Medium and Fine.
However, the most egregious type of error, detecting either type of breakpoint when none existed, was low .
This model was able to effectively differentiate Medium and Fine, and Medium and NAB.
However, the model would sometimes predict Fine when the actual was NAB.
This could be due to the mouse movements being less predictive of users' intents or there being less visible structure in this particular task category.
The model was slightly less effective at differentiating Fine and NAB, but it was very effective at differentiating Medium and NAB, and Medium and Fine.
Our models were developed using breakpoints identified by observers who did not share users' internal understanding of their tasks.
As a final metric, we thus wanted to test how well our models could predict breakpoints identified by the users themselves.
We asked users whose interaction data were annotated by observers to identify breakpoints in their own data, and then tested the accuracy of our models on it.
The accuracy of the model for Coarse breakpoints ranged from 40-100%, with an average of 76.5% across users .
Other than for P1, these results show that our models were able to accurately predict breakpoints identified by the users, even though a number of these breakpoints did not intersect with those identified by the observers.
This validates that our models can predict breakpoints independent of the knowledge of the task.
Overall, even though there were some errors, our results demonstrate that it is feasible to build models that detect and differentiate breakpoints within free-form tasks with fairly high accuracy.
This ability to detect a majority of the breakpoints should be more than sufficient to allow useful functionality, e.g., to enable defer-to-breakpoint policies.
This research sought to further understand different types of breakpoints across various tasks and examine the feasibility of building models that could detect and differentiate them.
Our work has produced several important findings.
First, we were able to identify interactions that characterize each type of breakpoint.
For example, a switch in high-level activity corresponds to a Coarse breakpoint, a switch in the current source object  of an application corresponds to Medium, and a switch in the action on the current object corresponds to Fine.
This shows that there is a perceivable structure within free-form tasks, which models should be able to detect.
Interestingly, these characteristics closely parallel those found to indicate breakpoints within physical tasks .
Second, we found that temporal distances between types of breakpoints ranged from about 1 to 10 min, with an average of about 4 min.
Our results support previous work showing that users repeatedly multi-task , but also show that this multi-tasking occurs at multiple levels of detail.
Third, we reported which features of user interaction were found to be predictive of each breakpoint type.
Though our set of features should by no means be considered final, they do provide deeper insights into the range of features that should be included in similar models deployed in practice.
Finally, our models had an overall accuracy of 69% to 87%.
We believe these are very positive results, especially since no specifications of the tasks were used.
As our work has now shown that building models for detecting breakpoints is feasible, more robust models could be built by applying the methodology in this work on a much larger sample of interaction data.
Finally, the cognitive duration of a breakpoint is not known, but would be important for certain applications of our work, e.g., for interruption management where cost may not be reduced if information delivery exceeds this duration after a breakpoint.
Future work should try to empirically determine this duration, which may inform the bin size for the models.
To deploy similar models in practice, one must consider  how to instrument applications, which breakpoints to detect and how to train the models, and what bin size to use.
Instrumentation of applications is needed to send relevant events to a model.
Such instrumentation can be achieved by leveraging existing research efforts such as , intercepting application events by writing plug-ins , or adapting the underlying UI toolkit .
Regardless of the method used, our work provides valuable insights as to the type and level of detail of the instrumentation needed.
Our work shows that three types of breakpoints can be detected, but this does not mean that models must detect all three in practice.
For example, in interruption management, if users are able and willing to have information deferred up to 4-5 min on average, then a system may only need to utilize the one model for detecting Coarse breakpoints.
Default models could be deployed, but, if needed, their accuracy may be improved by refining them per user .
This could be achieved by leveraging toolkits for generating models on-the-fly , assuming users would be willing to provide needed input.
Further improvements would require identifying and integrating additional predictive features.
Finally, to detect breakpoints, a model must typically assess interaction within a fixed time window.
Our work suggests a window of about 10s, but an implementation may need to experiment with different values, considering the tradeoff between computational overhead and discriminatory power.
The ability to detect and differentiate breakpoints represents an emerging need within at least interruption management, e.g., to enable defer-to-breakpoint policies.
Our work has made several contributions addressing this need.
First, we leveraged work in psychology to better understand the concept of breakpoints and leveraged unit identification methodology to identify three granularities of perceptually meaningful breakpoints during task execution.
Second, we provided insights into the characteristics of interaction that indicate each breakpoint type and evidence as to how often each type of breakpoint naturally occurs in practice.
Finally, our current models were able to detect from 69% to 87% of each type of breakpoint.
Our work has thus demonstrated the feasibility of building models that are able to detect and differentiate breakpoints during free-form tasks, without any pre-defined specifications of those tasks.
Beyond addressing the limitations previously discussed, our main direction of future work is to build similar models as part of a broader system for interruption management.
The models would enable various policies to be programmed for deferring delivery of non-critical notifications, e.g., defer until next Coarse breakpoint or until the next breakpoint of any type, and tested with various categories of tasks.
There are several limitations to our work.
First, our work investigated breakpoints within categories of tasks that all required the generation or manipulation of content.
Future work should thus study models for detecting breakpoints in other tasks, e.g., those that stress information-seeking.
Second, we analyzed about one hour of task execution data from each of six users.
The Effects of Interruptions at Different Moments within Task Execution.
Proceedings of the ACM Conference on Human Factors in Computing Systems, 2004, 271-278.
Baeza-Yates, R.A. and B. Ribeiro-Neto Modern Information Retrieval.
A Framework for Specifying and Monitoring User Tasks.
On the Need for Attention Aware Systems: Measuring Effects of Interruption on Task Performance, Error Rate, and Affective State.
Card, S., T. Moran and A. Newell The Psychology of Human-Computer Interaction.
Lawrence Erlbaum Associates, Hillsdale, 1983.
Czerwinski, M., E. Cutrell and E. Horvitz.
Instant Messaging: Effects of Relevance and Timing.
Czerwinski, M., E. Horvitz and S. Wilhite.
A Diary Study of Task Switching and Interruptions.
Proceedings of the ACM Conference on Human Factors in Computing Systems, 2004, 175-182.
Dietterich, K. Johnsrude, M. McLaughlin, L. Li and J.L.
Tasktracer: A Desktop Environment to Support Multi-Tasking Knowledge Workers.
Proceedings of the International Conference on Intelligent User Interfaces, 2005, 75-82.
Using Context-Aware Computing to Reduce the Perceived Burden of Interruptions from Mobile Devices.
Proceeding of the ACM Conference on Human Factors in Computing Systems, 2005, 909-918.
Horvitz, E., P. Koch and J. Apacible.
Busybody: Creating and Fielding Personalized Models of the Cost of Interruption.
Proceedings of the ACM Conference on Computer Supported Cooperative Work, 2004, 507510.
Investigating the Effectiveness of Mental Workload as a Predictor of Opportune Moments for Interruption.
Leveraging Characteristics of Task Structure to Predict Costs of Interruption.
Proceedings of the ACM Conference on Human Factors in Computing Systems, 2006, 741-750.
The Goms Family of User Interface Analysis Techniques: Comparison and Contrast.
Frequency-Based Detection of Task Switches.
Newtson, D. Attribution and the Unit of Perception of Ongoing Behavior.
Newtson, D. and G. Engquist.
The Perceptual Organization of Ongoing Behavior.
Newtson, D., G. Enquist and J. Bois.
The Objective Basis of Behavior Units.
Rizzolatti, G., L. Fadiga, V. Gallese and L. Fogassi.
Premotor Cortex and the Recognition of Motor Actions.
Shen, J., L. Li, T. Dietterich and J. Herlocker.
A Hybrid Learning System for Recognizing User Tasks from Desktop Activities and Email Messages.
Proceedings of the International Conference on Intelligent User Interfaces, 2006, 86-92.
Perceiving, Remembering, and Communicating Structure in Events.
Event Structure in Perception and Conception.
Examining the Robustness of Sensor-Based Statistical Models of Human Interruptibility.
Proceedings of the ACM Conference on Human Factors in Computing Systems, 2004, 207-214.
Examining Task Engagement in Sensor-Based Statistical Models of Human Interruptibility.
Proceeding of the ACM Conference on Human Factors in Computing Systems, 2005, 331-340.
Correlation-Based Feature Selection for Discrete and Numeric Class Machine Learning.
Hanson, C. and W. Hirst.
On the Representation of Events: A Study of Orientation, Recall, and Recognition.
Rooms: The Use of Multiple Virtual Workspaces to Reduce Space Contention in a Window-Based Graphical User Interface.
