Mobile interaction can potentially be enhanced with welldesigned haptic control and display.
However, advances have been limited by a vicious cycle whereby inadequate haptic technology obstructs inception of vitalizing applications.
We present the first stages of a systematic design effort to break that cycle, beginning with specific usage scenarios and a new handheld display platform based on lateral skin stretch.
Results of a perceptual device characterization inform mappings between device capabilities and specific roles in mobile interaction, and the next step of hardware re-engineering.
By nature, haptics is a "private" medium that provides for unobtrusive device interaction.
Because touch receptors can be found all over the body, it is usually possible to find a suitable location to provide a haptic stimulus without environmental interference.
In addition to challenges related to use context, there are recurrent problems in mobile interaction design stemming from the ever-increasing functionality demanded of devices with limited screen and keypad space.
For example, due to practical limits on the amount of information that can be accessed by scrolling, mobile content is often organized in deep hierarchies that present navigational challenges for the user.
Also, indicators of system status must compete with active content for precious screen real estate.
Use of the haptic modality has potential for offloading  screen communication, and increasing perceptual bandwidth available for interaction with a mobile information appliance.
Haptic, or touch, interaction offers many potential benefits for the use of mobile devices, such as mobile phones, PDAs and portable media players.
These devices are designed to be worn or carried wherever the user goes, and therefore must be usable in a wide range of use contexts.
Often there is environmental noise and distraction, and users must multiplex their visual, auditory, and cognitive attention between the environment and the information device .
Additionally, it may not be appropriate to use certain modalities in some contexts - for example, audio in a quiet meeting, or information-rich visual displays while driving.
In this paper we present the first stages of a systematic design effort to match the potentials of haptic technology to the challenges of contemporary mobile interaction design .
The aim is to explore how tactile technology can meet user needs in ways that are not currently met by visual and auditory interfaces alone.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
In the next step of our process, we discover the platform's expressive capabilities through a user-based perceptual characterization to aid in appropriately mapping haptic signals to our usage scenarios.
Finally, we reconsider the device applications originally envisioned in light of the characterization results, and discuss how our findings will guide further iterative development of the haptic stimuli, user interface hardware and applications.
Piezoelectric actuators may be configured in a way that also produces non-vibrotactile skin stimulation .
When the user places his/her finger on actuators which collectively comprise a multi-element tactile display, the relative motion of the individual piezo tips stretches the skin locally, activating skin mechanoreceptors.
Applying specific patterns of distributed skin deformation can create the illusion of touching small-scale shapes and textures.
A device based on this technology, called the Virtual Braille Display  , has been used to render legible Braille dots using only lateral stretching of the skin.
Similar sensations can be achieved using technologies that push into the skin , but the lateral skin-stretch configuration is mechanically simpler and makes the most efficient use of the range of motion of commercially available piezoelectric bending motors , resulting in favorable power, size, and weight profiles.
Such a configuration also provides internal mechanical grounding, as forces are generated between adjacent piezo elements.
We thus chose lateral skin-stretch as the most promising configuration for our next stage of design.
Our approach uses the same basic principle as the VBD, but miniaturized and embedded in a handheld form factor wherein the skinstretch site is displayed to the user's thumb, and mounted on a slider.
The device is described in further detail later.
While there is promise for the use of haptics on a mobile device, there are few examples of functioning implementations.
Some underlying difficulties are listed below.
Applying low-frequency forces to a user requires a fixed mechanical ground.
In a mobile context, the forces could be created relative to the user, which imposes constraints on the physical design and force output capabilities.
An alternative is tactile display, which generates no net force on the user, but consequently limits the scale of sensations transmitted.
Use of a conventional motor for forcefeedback introduces a significant impact on all three.
The most common occurrence of haptic feedback in mobile devices today is the ubiquitous mobile phone or pager vibrator.
Patterns of vibration are typically used to indicate various alerts, such as an alarm or incoming call.
Recently there has also been commercial and research interest in putting vibration in more sophisticated applications .
Generally, vibrotactile stimuli are produced globally  and with only two levels , and generally do not afford bidirectional interaction in the sense of the user actively exploring information through movement and the sense of touch .
Devices that are capable of delivering grounded forces to the user have the potential for greater expressive capacity than vibration.
Designs are restricted to minimal degrees of freedom  , yet must create enough added value to justify the power, size, and weight trade-offs.
Piezoelectric actuation offers significant promise for mobile applications because it can achieve a smaller form factor without coils and magnets.
In the case of a touch screen , the user typically experiences the illusion of local actuation although the entire screen moves.
Creating true multiple loci of actuation on a small scale is significantly more complicated using vibrotactile signals .
With the piezoelectric skin-stretch technology in mind, we developed several initial application concepts through brainstorming, storyboarding, and low-fidelity form mockups.
These informed an iterative progression to four key application areas for further investigation, and mechanical evolution of our hardware platform.
Grasping her phone inside her purse, she explores the ringer mode menu by moving the selection highlight while receiving tactile feedback.
Each menu item feels unique, like touching objects with different shape and texture, and she recognizes the sensation of the "silent" menu item because she has used this function before.
She selects the "silent" mode and receives tactile feedback as confirmation.
The scenario illustrates one way we can employ haptic icons , or tactons  - brief, artificial tactile stimuli - to provide assistance in making selections.
A unique tactile stimulus is assigned to each item in a list menu; with repeated feedback, users quickly associate functional meanings to abstract haptic icons .
The piezo tactile display technology described previously is capable of displaying small simulated surface features, such as bumps and gratings, with arbitrary motion relative to the user's finger.
It promises a rich vocabulary of haptic icons, which are later characterized in this paper.
By mounting the tactile display on a slider that is also sensitive to thumb pressure, it becomes an input device.
The user can select items in a vertical list menu by moving the display up and down.
As the selection highlight is moved, the haptic icon associated with the selected list item is felt.
Kinesthetic awareness of finger position allows the user to operate the device without looking, and to make a selection using the tactile display.
Different page elements, such as headings, images, and links, can be rendered as haptic icons that are played when the user scrolls over them.
Thus, each page has an associated "haptic map" that reflects its structure.
Users learn to recognize familiar pages and can quickly scroll to desired sections or links.
Improvements in scrolling efficiency would encourage user behaviors such as scanning to understand page structure and context, and increase the amount of information that can practically be presented on a page.
He didn't like using his old mobile phone's browser for this because he had to scroll around a lot to view content, which made him often lose his place.
Bob accesses a sports website using his new hapticallyenabled phone and scrolls down into a news story.
He feels the sensation of his finger sliding over a textured surface while the text of the story moves up the screen.
As he continues to scroll, he feels the headline of the next story  and some links .
All the stimuli move smoothly past his finger in sync with the scrolling movement.
Having scanned the page, Bob scrolls back up and quickly locates his area of interest  aided by the memory of what that part of the page feels like.
Small-screen mobile devices typically require more scrolling and/or selection actions to navigate a deep rather than wide information layout.
Both place demands on the user's visual attention.
Haptic augmentation as vibrotactile feedback has been shown to improve performance in a handheld scrolling task .
However, a compact multiple-element tactile display offers additional capabilities such as smooth tactile flow rendering .
Her location- and orientationaware mobile device helps her find the shop with an active map and directions.
The device also provides haptic feedback so she doesn't have to constantly look at the screen, keeping her eyes and ears on her surroundings.
Mary holds the device discreetly at her side, with her thumb resting on the tactile display and pointing forward.
The tactile display repeatedly strokes her thumb in the reverse direction , indicating that the device is pointed in the opposite direction from her destination.
As she turns around, the sensation gradually weakens, then begins to build again in the opposite, forward direction; she is now correctly oriented.
Mary starts walking while continuing to hold the device.
The stroking becomes faster, indicating that she is approaching her destination.
Any application that assists the user in finding a spatial target could utilize an expressive tactile display to convey a direction cue.
On a macro scale, this includes vehicle-based or walking navigation tasks, where the user must travel to a destination.
On a small scale, the user could receive haptic assistance to orient a mobile device camera so imagerecognition software can read a barcode or scene.
Vibrotactile stimulation at distributed points of the body has been considered for navigation , but in a non-intrusive handheld form factor, the display of tactile flow can be used to indicate 1-D direction.
Albert is composing a text message to a buddy.
His fingers are busy entering text, but occasionally he places his thumb on the tactile display to move the cursor, and feels a subtle repeating haptic icon that indicates his friend Steve has come online.
Albert can continue with his task, aware of the status change.
Later, Albert is on the phone when his friend Dean goes offline.
Albert feels a different haptic icon and is aware of Dean's status without having to interrupt his conversation or to remove the phone from his ear to look at the display.
Haptic alerts are commonly used on mobile devices, signaling events such as incoming or dropped calls .
Simple, high-amplitude signals such as vibration can be perceived through clothing and on various areas of the body, but more expressive tactile stimulation requires direct contact with sensitive parts of the skin, such as the fingertips.
Therefore, it is best suited to situations where the device is being actively used and held in the hand, where the haptic feedback provides background status information.
If the active application also makes use of haptics, the stimuli used for background notification must be distinct from the foreground application's haptic signals.
Examples such as this underscore the importance of designing haptic icons in the larger context of their anticipated usage, and employing empirical data relating to their group perceptual characteristics.
As appropriate for our stage of concept exploration, we retained a tether in this version, concentrating our energies on the initial steps of perceptual characterization and basic interaction scenarios rather than mobile field studies.
The THMB's tactile display exploits an actuation technique similar to the one used for the VBD.
As shown on Figure 3, the tactile display consists of eight piezoelectric benders  stacked together and separated by small brass rods .
By inducing bending motion to the piezo actuators, local regions of skin compression and skin stretch are generated across the thumb tip, resulting in a dynamic tactile sensation.
Users perceive relative motion of adjacent piezo actuators, rather than individual piezo element activation.
Longitudinal bending of an individual piezo-actuator is produced by applying a differential voltage between a central and two external electrodes.
Eight 1-byte control signals, one per piezo-actuator, are generated by a 1.5 GHz PC host running Linux, and are fed to an electronics module comprising an FPGA chip and a set of custom filters and amplifiers.
The resulting control voltages range from 50V and produce a no-load deflection of approximately 0.2 mm at the display surface .
Control signals are updated at 3125 frames/sec.
An individual piezo can thus be driven at up to 1562 Hz, which approaches the vibratory bandwidth humans require to perform skillful manipulation tasks .
This framework supports a wide range of tactile stimuli that differ in complexity and degrees of expressiveness.
Typical tactile experiences, as described by users, range from simple "buzzing" to the sensation of a pattern sliding under the finger.
The system allows for the creation of haptic icons in three basic modes, which may be combined as necessary: * Distributed vibration mode: The high bandwidth of piezo actuators allows them to stimulate skin at high frequency.
All eight THMB piezo actuators can be moved independently, generating a 1D spatial tactile pattern.
To explore the application concepts, we designed and built a handheld prototype.
The Tactile Handheld Miniature Bimodal  interface consists of a plastic casing containing a tactile display for the thumb with an active surface area of 6.4 x 8.7 mm, mounted on a slider with travel of ~11 mm, a 2.5 inch diagonal LCD screen, and electronics interfacing the device to a PC server.
The THMB enclosure affords a PDA-like device that fits in the palm of the left hand, though it is tethered and is slightly thicker than typical mobile handheld devices .
The tactile display protrudes about 1 mm from the left side of the casing, where the thumb of the holding hand is naturally positioned.
The user controls the slider position by flexing the first thumb phalanx, and triggers a push-button with light inward pressure on the tactile display.
The THMB's tactile display is evolved from that of the Virtual Braille Display  .
This is similar to the method used by , but each THMB actuator can be moved at a different frequency.
In the same way visual objects seem to move across the screen in a movie, tactile percepts can "travel" seamlessly across the display.
The development of the initial application concepts and handheld tactile display hardware was guided by an understanding of the general capabilities of the lateral skinstretch technology, and ideas for how it could address user needs in mobile contexts.
To proceed to the next stage of more detailed application design, we needed to quantify how users perceive the haptic signals generated by the new hardware.
We then mapped some of the regions of the haptic "vocabulary" , allowing us to assess suitability of the envisioned applications, and what stimuli would best match the roles specified in our concept designs.
We used a similar approach to perceptual characterization as .
The core stimulus salience quantification method utilized multidimensional scaling , a tool for analyzing perception in complex stimulus spaces .
Given a range of stimuli, MDS analysis produces maps of how the perceptual space is organized.
Our new hardware can generate moving stimuli, but the range of detectable movement speeds was not known.
We therefore performed a study to estimate this range.
This enabled us to select speeds for icons for later MDS analysis.
Examples of stimuli used for the speed study.
The highlighted area represents one tactile frame in which there is the sensation of stretching and compression at opposite ends of the display.
The independent variables were: speed ; direction ; and wave type .
The dependent variables, measured with a forced-choice method, were: perceived direction , yielding an accuracy measure when compared to the actual direction, and confidence level .
The purpose of the speed study was to determine the available perceptual bandwidth in one possible dimension that could be used as a parameter for haptic icon design.
The question we sought to answer was: "What is the upper limit on the movement speed of a virtual `shape' that people are able to perceive?"
To estimate the range of useable stimulus speed we hypothesized that the users' ability to perceive the movement direction would decrease as speed increased.
The trials were conducted on a Linux PC with the tactile device attached.
On each trial, the computer selected random values for each independent variable.
The user pressed a GUI button labeled "Play" to feel the stimulus, which was repeated three times with an intervening delay of 0.7 second.
The user was then required to input the perceived direction and confidence level before proceeding to the next trial.
There were five "training" trials where the user was informed of the actual direction via a modal dialog box just after entering their responses, followed by 40 "test" trials where the user received no notification.
We used a simple moving stimulus consisting of a square waveform that was "tweened" across the tactile display to achieve a sense of motion .
Two waveforms were used, producing either a moving region of skin expansion  followed by compression , or compression followed by expansion.
The maximum stimulus speed was limited by the sampling frequency to 3.40 m/s .
8 right-handed volunteers  participated in the user study.
Each user took approximately 5-10 minutes to run the study.
The overall accuracy results from the speed study are shown in Figure 5.
The relationship of accuracy and speed was statistically significant with , supporting the experimental hypothesis.
Accuracy fell to approximately chance levels at the maximum speed of 3.40 m/s, but approached 90% at 0.34 m/s using a polynomial regression.
While likely due to random variation, this observation is being further investigated.
At the higher speeds, users reported that the stimulus felt like a "click" or small vibration and that direction was difficult to ascertain.
The results from the speed study show that the device is capable of signaling the direction of stimulus movement over a large range of speeds.
The sensation experienced is comparable to sliding one's finger across a surface with a small bump.
It thus seems feasible to use a directional "tactile flow" signal in applications such as assisted navigation.
In addition, the results suggest that speeds lower than approximately 0.34 m/s would be appropriate for designing abstract haptic icons that convey the sense of motion.
The purpose of the haptic icon discrimination experiment was to assess the range and distribution of perceivable difference of some specific haptic icons rendered with this device.
The multidimensional scaling  technique was used to map the organization of the stimulus space.
The stimuli were selected according to a 5 waveforms x 2 amplitudes x 3 speeds factorial combination, resulting in 30 haptic stimuli .
These factors roughly correspond to stimulus components used in prior studies for tactile displays .
The waveforms were chosen to represent qualitatively different tactile experiences based on first-pass experimentation with different signals, and included both repeating and non-repeating waveforms.
For the speed parameter, we chose a range that produced an accuracy rate approaching 90% in the prior speed study.
The participants completed five stimulus-sorting blocks in a method similar to that used in  and .
The sorting method is a way to efficiently measure perceptual similarity between pairs of stimuli.
Participants were seated at a workstation and operated the mouse with the right hand while holding the device in their left hand with the thumb resting on the tactile display.
Participants used a GUI that presented the 30 stimuli in a grid of approximately 1 cm2 tiles.
They could trigger stimulus playback by clicking a tile with the left mouse button, and used the right mouse button to pick up, move, and drop the tiles into approximately 7 cm2 regions on the screen, which represented clusters.
On the first block, they could adjust the number of clusters using onscreen +/buttons.
In subsequent blocks, they were required to produce 3, 6, 9, 12, or 15 clusters, presented in random order; the number of clusters closest to the user-selected for the first block was excluded.
Additionally, because the experimental paradigm uses relative perceptual data, the dominance of the repeating / non-repeating waveform difference may obscure subtle differences among the non-repeating waveforms .
A closer examination of the graph suggests that duration and amplitude may also be salient perceptual dimensions, but their organization in the overall MDS graph is not consistent.
However, when subsets of the data were analyzed one waveform at a time, most of the graphs exhibited clear duration and amplitude structure along the x- and y-axes.
Because the data was collected in a task where users were required to sort all stimulus factors at once, we hypothesized that because the less salient dimensions are perceived qualitatively differently depending on waveform, a global MDS solution was unable to represent them all consistently.
We therefore performed an additional experiment to determine the validity of the subset analysis.
Ten right-handed individuals  participated in the study, and were compensated CAD $10.
All subjects completed the tasks within one hour.
We performed an MDS analysis on the data obtained from the sorting task.
Stimuli that are sorted together into a cluster were assigned pairwise similarity scores proportional to the total number of clusters in a given sort, because it is reasoned that when a user has more clusters from which to choose, the significance of placing two stimuli together in a cluster is increased.
The results from a two-dimensional MDS1 performed with ordinal, untied data are shown in Figure 7.
Analyses in 3-D and higher dimensions did not yield any additional structural information about the data.
The graph clearly indicates that users tend to structure the stimulus space in terms of waveform, with the tri stimuli clearly distinguished, and roll stimuli also being separated from the non-repeating waveforms bump, edge, and saw.
The stimuli formed by the three non-repeating waveforms - bump, edge, and saw - were less clearly distinguished on the graph, indicating that users did not consistently sort them separately from one another.
This suggests that the We recently performed a detailed validation of the MDS technique and analyses using the data set presented here.
For more information, please refer to .
The subgroup MDS experiment consisted of four trials: a control trial similar to the first MDS experiment, and three subgroup trials where users performed sorting tasks using individual waveform subgroups.
We chose the tri, roll, and edge waveforms for further analysis because the earlier MDS analysis and qualitative reports indicated that they were judged to be the least similar.
For the waveform subgroup trials using 6 stimuli, after the first trial the clusters were 2, 3, or 4 clusters using the same presentation and exclusion criteria.
The control trial was presented first, followed by the three waveform subgroup trials in random presentation order.
All other data collection methods were the same as in the first MDS experiment.
Many participants reported that they preferred them to their mobile phone's vibration mode.
A variety of reasons were given, including quiet operation and moderate stimulus amplitudes.
Since the sliding function was not used in the perceptual characterization studies, it is not known whether this report would be affected by using the slider for input.
Five right-handed people  participated in the subgroup experiment.
None had participated in a previous experiment with the device.
Participants were paid CAD $20 for a 90-minute session.
The subgroup MDS results confirmed the findings from the earlier subset analysis, with duration and amplitude being clearly employed by users to organize the stimulus space.
Figure 8 indicates no clearly discernible duration/amplitude organization in the control trial graph with all 30 stimuli, but when individual waveforms were tested separately, the organization became apparent.
In the subgroup graphs, duration is aligned vertically and amplitude horizontally.
Additionally, the data from the control trial exhibited the same overall structure as the data from the first MDS study, providing further confirmation of the original results and the robustness of the technique despite differences in the number of clusters used in the sorting task.
Taken together, the results indicate that duration and amplitude, while secondary to some differences between waveforms, are nevertheless discernible and useful as salient parameters for haptic icon design in this environment.
The results from the three perceptual characterization studies suggest that users are capable of distinguishing a wide variety of stimuli produced by the hardware prototype.
Direction, certain waveforms, duration, and amplitude are salient parameters that may be used in designing haptic icons for use in applications.
The three-way grouping we observed among waveforms was especially interesting, because it empirically suggests how our first-pass parameterization model of haptic icons could be improved; for example, instead of treating waveform as a single parameter, in subsequent designs one could consider nonperiodic versus periodic waveforms, and further subdivide the periodic group into different wave shapes .
Judging from the results of the perceptual characterization, haptic icons designed along the dimensions of waveform , duration, and direction are candidates for distinguishing items in a list.
Because the most salient parameters are the direction and speed of the stimulus, it is important to decouple this rendered motion from illusions of relative stimulus motion generated as a result of the voluntary thumb movements to produce control input to the system.
One way of avoiding this confound is to signify a discrete command such as scrolling an item up or down with a larger but mechanically grounded gesture that incorporates pressing the slider against an end-stop.
As originally envisioned, the browsing application uses rendered speed and direction parameters to provide haptic feedback to the user about the movement of the point of focus within the page.
Haptic shape  is the only parameter available to provide information about the selected item 
However, the two MDS studies suggest that the user's ability to discriminate haptic shape with this device may be somewhat limited when using non-periodic signals.
It is possible to build and test the browser application using the currently identified set of haptic icons, but its usefulness may be limited by the relatively narrow choices of icons.
We have enabled several opportunities for further development through this approach.
With the data provided by the perceptual characterization studies, it is possible to design and select appropriate haptic icons for the applications originally envisioned, and to prototype the applications and to use more conventional usability testing methods to iterate and improve their designs.
By continuing perceptual characterization experiments it will be possible to achieve a more complete understanding of the expressive capabilities of the device.
Additional parameters for haptic icons are available, such as complex motion, noise, superposition of waveforms, etc.
It may also be possible to gain additional headroom in perceived stimulus amplitude by more carefully designing waveforms that achieve maximal amounts of skin stretch between adjacent piezo elements.
Focused studies could determine whether subtle differences between stimuli are being masked by highly salient stimuli, such as the tri waveform group in the present study, as was found in .
Finally, the process yields information about how to effectively improve the hardware to best suit the intended applications, as mentioned in the Discussion above - for example, stronger actuation could improve the range of available salient haptic icons for the browser application.
Other form factors may also be useful in further explorations; for example, a steering wheel that provides directional tactile feedback as in our navigation scenario.
Further hardware miniaturization as well as un-tethering of power and control will require engineering effort, but seems feasible given current technological trends.
In summary, we have described how a single iteration of task scenario development, hardware design and perceptual characterization has forged a connection between the mobile application space and a tactile display concept, and directly informed the hardware re-engineering process.
By taking the first steps to identify a primitive haptic vocabulary and guaranteeing perceptual comprehension of the stimuli, the process enables further development to concentrate on interaction design, thus boot-strapping the creation of haptic prototypes that are likely to function effectively when deployed as mobile tools.
The location-finding application concept relies on the tactile display's ability to convey direction information to the user.
The user studies confirmed that direction of tactile flow is clearly distinguishable across a useful range of speeds.
Intensity, waveform and rhythm of repeating stimuli may be used to provide additional information about the distance to the target, status, or movement of the target.
Our results thus encourage prototyping and usability testing for this application according to the original design concept.
User feedback obtained during interviews following the perceptual characterization sessions indicated strong potential for using the device for alerts, based on the judgment that it would be pleasant and non-intrusive compared to currently available vibrotactile displays.
Data from the perceptual characterization suggests a hierarchy of salience that could be mapped to the relative importance or urgency of an alert.
For example, a periodic signal would be useful for important alerts due to its high saliency.
Less important changes in background status, such as the movement of passively monitored "buddies", could be conveyed with non-repeating signals.
Finally, if background status indicators are to be multiplexed with other haptic signals generated by the foreground  application, one of the dimensions identified in the user studies could be allocated for this display.
For example, if the speed dimension was allocated to background status indicators, slow moving stimuli could be used for the foreground application, while fast-moving stimuli could indicate background alerts.
The present work represents the first cycle of an iterative design process through which we seek to extend mobile user interfaces by sidestepping a vicious cycle typical to the introduction of novel interaction techniques and technology.
Limited deployment of sophisticated haptic hardware has impeded field-demonstrated applications; likewise, there is minimal user familiarity with basic interaction principles to support conventional usability testing.
This makes it difficult to build a value proposition and impedes further investment in pioneering hardware.
We wish to thank members of the Centre for Intelligent Machines, McGill University, for their work on developing the device, especially Qi Wang, Don Pavlasek, and Jozsef Boka, and the UBC SPIN Research Group, for their contributions to the perceptual characterization process.
This work was supported in part by the BC Innovation Council.
On Haptic Interfaces for Virtual Environment and Teleoperator Systems  .
MacLean, K.E., and Enriquez, M. Perceptual Design of Haptic Icons.
A., Whittaker, S., and Bradner, E. Interaction and outeraction: instant messaging in action.
Oulasvirta, A., Tamminen, S., Roto, V., & Kuorelahti, J. Interaction in 4-second bursts: The fragmented nature of attentional resources in mobile HCI.
Perceptual analysis of haptic icons: an investigation into the validity of cluster sorted MDS.
On Haptic Interfaces for Virtual Environment and Teleoperator Systems , IEEE .
Poupyrev, I., Maruyama, S., and Rekimoto, J. Ambient Touch: Designing Tactile Interfaces for Handheld Devices.
Pouyrev, I., and Maruyama, S. Tactile Interfaces for Small Touch Screens.
Finger Force and Touch Feedback Issues in Dexterous Telemanipulation.
Design and performance of a tactile display using RC servomotors.
Ward, L. Multidimensional scaling of the molar physical environment.
