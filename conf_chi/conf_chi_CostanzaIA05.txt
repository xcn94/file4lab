Using a mobile device in a social context should not cause embarrassment and disruption to the immediate environment.
Interaction with mobile and wearable devices needs to be subtle, discreet and unobtrusive.
Therefore, we promote the idea of "intimate interfaces": discrete interfaces that allow control of mobile devices through subtle gestures in order to gain social acceptance.
To achieve this goal, we present an electromyogram  based wearable input device which recognizes isometric muscular activity: activity related to very subtle or no movement at all.
In the online experiment reported, the EMG device, worn on an armband around the bicep, was able to reliably recognize a motionless gesture without calibration or training across users with different muscle volumes.
Hence, EMG-based input devices can provide an effective solution for designing mobile interfaces that are subtle and intimate, and therefore socially acceptable.
Furthermore, it has been suggested that wearable input devices should be "as natural and  unnoticeable as possible" if they are meant to be adopted in everyday and public situations .
Our research extends this concept: we believe that not only the devices should be unnoticeable and natural, but also the interaction with them needs to be subtle, discreet and unobtrusive.
Therefore, we promote the idea of "intimate interfaces": discrete interfaces that allow control of mobile devices through subtle gestures in order to gain social acceptance.
Most of the interaction with mobile devices takes place when surrounded by other people, such as on buses and trains.
The design of interaction techniques has to take into account the social context where the interaction will occur.
Using a mobile device in a social context should not cause embarrassment and disruption to the immediate environment.
The replacement of ring-tones with vibrating alerts in mobile phones constitutes an example of a widespread subtle interface to improve social acceptance.
Unfortunately, this idea has not yet been generalized to other parts of the interface.
In fact, this type of device is converging with wearable computers being designed and developed in universities; the latest version of MIThril  for example is based on a commercial PDA.
Recent technological developments in eyeglass displays  make them interesting candidates for the next generation of mobile devices.
The display technology is integrated in standard eyeglass frames and lenses and it is barely noticeable to observers.
The computational power of contemporary mobile and wearable devices is advancing rapidly, however their interfaces continue to mimic those of desktop computers.
Yet the conditions of use at the desk are very different from those in a mobile context.
Consequently, mobile interfaces recently received much attention from the CHI community .
Important issues of the mobile context include:
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Eyeglass displays create a virtual semitransparent screen in front of the user, with resolution between 320x240 and 640x480 pixels allowing the display of around 20 lines of text.
However, the decoupling of device and display reduces the effectiveness of interaction techniques that associates the manipulation and display of information.
Examples include touch-screen displays often embedded in PDAs, tilt-based interfaces to navigate content in handheld devices , and tangible interfaces where interaction with digital content is performed through manipulation of physical icons virtually linked to it .
Therefore, eyeglass displays require a new interaction paradigm that is not so strongly relying on physically manipulating the mobile device.
An alternative to display-centric interfaces is an interface centred on the user's body.
Gestural interaction has been proposed as an interaction technique for mobile devices, and it appears a suitable way of interaction with systems based on the eyeglass displays.
The user performs gestures to issue commands, and the results are displayed on the virtual screen.
In previous research, gestures are typically sensed by accelerometers , capacitive techniques  or proximity sensors worn on different parts of the body .
These techniques require the users to noticeably move their limbs, which can be inconvenient and socially unacceptable.
On the contrary, electromyographic  signals can convey information about isometric muscular activity: activity related to very subtle or no movement at all.
Hence it allows the definition of a class of "subtle" or "motionless gestures" that can be used to design discreet, intimate mobile interfaces.
In mobile computing biosignals have been used for context awareness  and inference of the user affective state .
In contrast, in this paper, we focus on explicit control of the interface.
EMG is a biosignal related to muscle contraction .
Studies on the use of EMG for gesture recognition have been reported, but none of them takes explicit advantage of its subtlety, the fact that commands can be issued without the generation of observable movements.
This paper describes the design of a novel controller for mobile devices based on EMG.
Previous research on the use of EMG for human-computer interfaces is reviewed in the "Related Work" section.
In the "Motivation" section we present a rationale for our approach and how it differs from prior art.
We discuss our iterative design process, focussing on pilot studies and how those impacted on the system we developed in the "Design Process" section.
Subsequently we describe a user study that we performed and we discuss the results.
The electromyogram  is an electrical signal generated by muscle contraction.
It can be recorded noninvasively using surface electrodes in differential pairs, each pair constituting a channel.
Methods for effective recording and computer aided analysis of EMG signals have been the object of study in the field of biomedical engineering for the last three decades .
The typical biomedical analysis for diagnosis applications involves envelope detection, energy measurement  and frequency characterization .
Control applications generally involve signal acquisition from a number of differential electrodes, feature extraction and real-time pattern classification.
The first examples of EMG based real-time control systems can be found in the field of prosthesis control and functional neuromuscular stimulation.
The system must be specifically calibrated for each subject and uses EMG signals from two channels.
A number of studies focused on the use of EMG for computer interfaces targeted at users with physical disabilities.
Putnam and Knapp  developed a reconfigurable system to control generic graphical user interfaces.
The system incorporates a "continuous control" mode where the amplitude of the contraction is mapped to a parameter swing  and a "gesture recognition" mode that discriminates between two gestures and can be used for discrete selections.
The gesture recognition is performed on a dedicated digital signal processing  board and it is based on neural networks.
It requires training for each user of the system.
Spectral features of EMG signals are analysed, in addition to the amplitude, to increase performance.
The system is not reported to require individual calibration for each user and it is implemented on a DSP board.
Other examples of EMG based CHI include a number of interfaces for musical expression.
For this type of application the signal is used in a continuous fashion , the amplitude is mapped to a variety of sound synthesis parameters.
The systems presented in this context are often wearable and allow movement of the performer on stage, yet they are not  designed for the mobile  context.
Knapp and Lusted  present a generic battery powered platform to interface EMG signals to MIDI systems.
Tanaka and Knapp  complement EMG data with inertial sensor information, so that both isometric  and isotonic  activity can be monitored.
Their system requires calibration for every user.
Recent studies focus on the use of EMG for the recognition of an alphabet of "discrete" gestures.
Fistre and Tanaka  propose a system that can recognise six different hand gestures using two EMG channels on the forearm.
The device is designed to control consumer electronics and is described as portable.
Testing in a mobile context has not been reported.
Wheeler and Jorgensen  report the development and successful testing of a "neuroelectric joystick" and a "neuroelectric keypad".
Using EMG signals collected from four and eight channels on the forearm they successfully recognise the movement corresponding to the use of a virtual joystick and virtual numeric keypad.
Gestures mimicking the use of physical devices are successfully recognised using hidden Markov models.
The system is proposed as an interface for mobile and wearable devices, but an embedded implementation is not reported, nor is testing in a mobile context.
In a different fashion, but still in the context of CHI, EMG signals have been used in conjunction with other physiological signals  to detect the affective state of the user .
The system we propose is a small wireless armband controller, with a form factor in the tradition of Bodymedia , which can be worn under clothes to make it unnoticeable.
The armband device is completely selfcontained; it senses, amplifies and analyzes the EMG signals from the bicep.
If a gesture is recognised the device transmits via Bluetooth an appropriate message containing the device ID and the parameters describing the gesture .
When the device senses muscular activity not corresponding to the defined gestures it remains "silent"; i.e.
The Bluetooth message can be received by devices compliant to this standard, such as many commercially available PDAs and mobile phones, and easily mapped to an interface.
The armband device is designed to potentially fit any user, it does not require calibration or training to the muscles of individual people.
Social acceptance is a key factor for mobile devices.
Rekimoto  points out that any mobile technology needs to be as unnoticeable and natural as possible to be considered usable in everyday situations.
Lumsden and Brewster  question the social acceptance of speechbased and gesture-based interaction.
We believe that not only the technology itself should be unnoticeable and natural, but also the user interaction with the mobile device needs to be subtle, discreet and unobtrusive.
Speech recognition has been criticised as an interaction technique for mobile devices because verbal communication is the most common form of interpersonal communication, so in many situations it would be awkward and inappropriate to start "talking" to a computer.
Hand and body gestures are also important part of human to human communication.
Therefore, the same criticism moved to speech applies to interfaces based on evident gestures.
We believe that EMG's greatest potential for mobile interfaces is its ability to sense muscular activity not related to movement.
This characteristic allows the definition of a class of "subtle" or "motionless gestures" that can be used to design discreet, intimate mobile interfaces.
Yet previous studies on the use of EMG for human computer interaction  do not explicitly consider subtlety, leading to a different approach.
Tanaka and Knapp  report as a limitation the fact that in EMG signals muscular activity and movement are not always related.
They remedy this by complementing EMG with inertial sensor  data in a multimodal fashion.
Minimal computational complexity is essential to implement the processing in a low power embedded device, such as an ARM7 microcontroller.
The EMG controller does not occupy the user's hands, and does not require them to operate it, hence it is "hands free".
When combined with eyeglass displays, and/or audio output, it forms a closed loop "hands free" system.
This can be highly advantageous in a number of everyday situations: for example when the user is carrying objects.
It can also be useful in specific fields, such as maintenance, where the user's hand are needed to perform a principal task, and uses the mobile computing system for assistance.
If augmented with tactile actuators, the armband device can form an integrated multimodal i/o system.
When compared to other types of sensing EMG presents a number of difficulties due to the need for contact electrodes and their placement .
However, its advantages provided related to subtlety make it worth studying.
Moreover, solid gel and dry electrodes can be used in place of wet electrodes.
Recent studies on non-contact sensing for EMG  are even more encouraging.
The system design was carried out as an iterative process centred on users.
A number of exploratory informal user studies were performed to insure that the system would be natural and easy to use.
Figure 1 summarizes the process.
From the hardware point of view the system is quite straightforward and not very different from other EMG controllers described in the literature.
The device includes an high input impedance amplifier connected to electrodes, an anti-aliasing filter, a microcontroller to sample and process the EMG signal and a Bluetooth communication module to transmit the processing results.
All of the processing takes place in the microcontroller: when a gesture is recognised, an appropriate message is transmitted via Bluetooth.
For the preliminary experiments, however, the wearable device streamed the amplified sampled signal via Bluetooth, and the processing took place on a standard Windows PC.
The design of the recognition algorithm and the definition of the gesture were carried out in parallel to satisfy two requirements: the gesture should be  natural for people to perform, and  different enough from "normal" muscle activity to avoid misclassification .
The process started with a pilot study to select one muscle and subtle isometric contractions that could serve the definition of "motionless gesture".
The subjects for this pilot were chosen so that a range of different muscle volumes were tested.
Initial candidates for the muscle selection were: the temporalis, the bicep, the triceps, the forearm, the abdominals and the calf.
The test revealed the bicep as the best candidate because it lies superficially making the signal fairly immune to activity generated by other muscles, and it is well defined even in non-athletes.
The gesture was defined as a brief contraction, such that it could be performed without being noticed, while the arm is unfolded, parallel to the body while the user is standing.
A second informal study was conducted to refine the definition of the subtle gesture and create a model and algorithm for its detection.
New subjects participated in the study and were chosen for a variety of muscle volumes.
EMG signals were recorded from subjects performing the selected contraction and compared with the signals generated by other types of muscle activity, such as moving in an indoor space, lifting objects of various weights and gesticulating while talking.
The subjects were informed that the purpose of the study was to define a subtle gesture that could be used to control mobile devices.
With this procedure we wanted to see whether such a definition of "brief contraction" would be consistent across individuals, and to ensure that the gesture definition would be, to a certain extent, natural to perform, rather than defining a gesture a priori and ask/force the users to learn it.
The model resulting from the second study, depicted in Figure 2, is based on the standard deviation of the EMG signal, calculated with a sliding window of duration 0.2s overlapping for 75% of its duration.
The standard deviation was chosen to smooth the data and emphasize discontinuities in the energy of the electromyogram.
The window size was selected to be the longest possible without filtering out interesting features.
A mathematical model and a recognition algorithm for the brief contraction were then created heuristically from the observation of the data.
Given the noise-like characteristics of the EMG signal , standard peakdetection techniques could not be employed.
Such peaks were rather modelled as follows: an interval "beginning" of duration TB of low activity  followed by an interval "middle" of high activity of duration TM and then again low activity for an interval "end" of duration TE.
High activity and low activity were defined respectively as the standard deviation of the signal being above a threshold H and below a threshold L. To allow some tolerance in the model, the condition on the history is imposed on the average of its values.
The condition on the middle needs to be satisfied by 50% of the samples, and the condition on the end by 70% of the samples.
The model definition is stricter on the duration of the contraction rather than its intensity.
This is because the preliminary study showed that the duration was more consistent than the intensity across users, despite the fact that no specific indication was given about either.
One disadvantage of this model is that it requires the gesture to be completed before the recognition can take place.
The recognition could be made faster by removing the "end condition" for the closure of the gesture; however, this would cause an increase in the number of false positives.
The tuning of the five parameters of the model required a third informal study.
New and returning users were informally asked to test the system.
The testing was conducted to stress the system to produce false positives and false negatives.
The iterations recurred until the number of false positives approached zero and the system recognised contractions performed by any user.
Once the recognition worked robustly on one gesture, the possibility of creating a gesture alphabet of two gestures was explored.
The gestures were defined as two short subtle contractions of different durations.
This corresponded to varying the value of TM in the model together with its tolerance.
The results obtained at this point were then validated with the formal user studies reported in the next section.
Each of the four contraction tasks was preceded by a short familiarization session.
While walking, participants navigated 24 meter laps around obstacles setup in a regularly trafficked walkway in Media Lab Europe, see Figure 3.
This setup was similar to the one reported by Pirhonen et al.
Participants were given written instructions that informed them the study was assessing EMG as a subtle interface for mobile devices and they would control the system using their bicep while walking using a subtle contraction that could be performed with their arm relaxed at their side.
Participants were also instructed the contraction recognised has a minimum and maximum duration and a minimum strength requirement.
No further instructions were given for the subtleness of the contractions, thus it was subjective to the participant to define subtly.
We refer to the four contraction tasks the participants performed as `generic', `short', `long', and `mixed' contractions.
During these tasks participants were prompted to contract with an audio stimulus in the form of a MIDI piano tone delivered through wireless headphones.
In the generic task participants attempted to consistently make contractions that the system would recognise.
In the short task they attempted to consistently make the shortest contraction the system would still recognise.
In the long task they attempted to consistently make the longest contraction the system would recognise.
In the mixed task they attempted to make both long and short contractions when given corresponding stimuli.
Each task was preceded by a short familiarization session.
During the familiarization sessions participants stood and only heard an auditory feedback when the system recognised a contraction.
No coaching or further feedback as to the amplitude or duration of the contraction was given to the participants, so they were unaware of why the algorithm was or was not recognizing the contraction.
They were only aware if the contraction was recognised.
This was also true for the walking tasks.
An experiment was conducted to assess the usability of EMG as a subtle interaction technique for mobile devices.
The experiment evaluated the system usability in a mobile context.
The study was conducted using a setup similar to the one successfully employed by Pirhonen et al.
For participants 1, 2, and 4 the reference and ground positions were swapped because the inner reference pressed against their bodies while walking causing deflection artefact.
After the electrodes were applied, the wireless EMG device was mounted to the upper arm with an elastic band between the electrodes and elbow.
The wireless EMG device streamed 10 bit values at 80 Hz over serial Bluetooth to a 2 GHz Pentium 4 PC running Windows XP.
The BCI2000 software framework , running on the PC, was used for signal processing and stimulus presentation.
While the contraction detection algorithm was simple enough to run on the device's micro-controller, it was implemented under BCI2000 to allow real-time monitoring of the EMG signal and classification output for the experiment.
If the system detected a muscle contraction participants were given auditory feedback in the form of a MIDI trumpet tone delivered through wireless headphones.
No further feedback was given, thus when performing contractions participants were quantitatively unaware of the contraction's duration.
This minimal feedback was given to establish if the users could learn to use the feedback without specific training.
Subjective workload was measured with the NASA TLX  scales after each walking contraction task to assess demands imposed by the EMG controller and the different contraction types.
Workload is important because in a mobile environment users have less attention to spend on the interface and interaction technique because they are monitoring and navigating their surroundings , an additional complexity is introduced when the interaction technique uses the same body parts used while mobile.
Therefore, an interface and interaction technique with a lower workload will be more successful in a mobile context .
For the system setup, the participant's skin was first prepared with an abrasive gel to ensure signal quality.
In pilot studies, it was found the pre-gelled electrodes did not require skin abrasion unless users used skin creams or lotions earlier in the day.
For consistency, all participants were abraded in this formal study.
After abrasion, disposable, sold-gel, self adhering, Ag/AgCl 9mm disc surface electromyogram  electrodes were applied in three positions around the upper arm of the subject's dominant hand such that the input electrode was centred on the bicep brachii, reference was on the inside middle of the upper arm below the bicep, and ground was placed on the middle outside of the upper arm see Figure 4.
All subjects participated in all tasks: within-subjects design.
The tasks proceeded in order as follows, however, the short and long contraction tasks were performed in counterbalanced order, such that a participant randomly performed short or long first.
The tasks in detail are: 1.
Walking, No Contractions While wearing the wireless EMG device participants were instructed to walk ten laps at their preferred walking speed.
Standing, Familiarization, Generic Contractions Participants were given the wireless headphones, and told to briefly contract their bicep freely in order to familiarize themselves with the system.
The familiarization ended when either the participant was confident interacting with the system or a fifteen minute time limit was reached.
If participants could not confidently use the system after the time limit they were verbally given feedback as to why their contractions were not controlling the system.
This was only necessary for participants 2, 9, and 10; who were only told once to shorten their contractions, and then they were quickly able to control the system.
Walking, Stimulus-Response, Generic Contractions Subject's walked the obstacle course and attempted to contract when they heard an audio stimulus through the wireless headphones.
Participants were randomly presented 15  stimuli.
Standing, Familiarization, Short Contractions Similarly to generic contraction familiarization, participants stood and only heard an auditory feedback when the system recognised the contraction.
The system recognised the same contraction duration as in the first two tasks; it was subjectively up to the participant to define the short contraction.
Participants were again instructed the system recognised a contraction of certain duration and they should explore the limits of the system.
Walking, Stimulus-Response, Short Contractions Participants walked the obstacle course again and attempted their short contraction when they heard an audio stimulus through wireless headphones.
Participants were randomly presented 15  stimuli.
Standing, Familiarization, Long Contractions Participants stood and only heard an auditory feedback when the system recognised the contraction.
The system recognised the same contraction as in the previous tasks, it was subjectively up to the participants to define the long contraction.
When they were comfortable making the longest contraction they thought the system would still recognise the experiment continued with the next task.
Walking, Stimulus-Response, Long Contractions Participants walked the obstacle course again and attempted their longest contraction when they heard an audio stimulus through the headphones.
Participants were randomly presented 14  stimuli.
Walking, Stimulus-Response, Mixed Long and Short Contractions Finally, participants were instructed to walk the obstacle course again and make both short and long contractions when they heard either a high pitched MIDI piano tone  or low pitched MIDI piano tone  stimulus.
As with the original algorithm, only the first recognition was counted, any additional recognition was ignored until the next stimuli.
Applying this new short-long detection algorithm to the mixed contraction data resulted in an overall accuracy of 51%, with 55% shorts recognised and 47% longs recognised.
The misclassification rate for shorts as longs was 33%, and the misclassification rate for longs as shorts was 11%.
Users were able to control the system consistently with only the feedback that a contraction was recognised.
The generic contraction's accuracy of 96% indicates EMG can be used successfully as a controller.
The recognition of short and long contractions offline using the mixed data set was fairly low.
This may have occurred because the online algorithm recognised a small range of contraction durations; therefore the longs may not have been sufficiently different from the shorts for the participants to accurately produce them.
The range of contraction durations was set from pilot studies which indicated most false positives occur from very long muscle contractions, therefore a trade-off between reproducibility of long and short contractions and increased false positives may occur if the range is widened.
It is important to note the durations of the short and long contractions are subjective because the participants were not given feedback to their actual durations.
Therefore the participants trained themselves on what they considered were long and short contractions.
If the participants were given feedback for their contraction durations, they may learn to consistently make different long and short contractions.
There were no significant differences in the subjective workload tests between contraction tasks .
No false positives were detected while online during the first walking task.
In addition, the online recognition rates for the four contraction walking tasks were: generic 96%, short 97%, long 94%, and mixed 87%.
In the first familiarisation task, participants were able to control the system in an average of 3.75 minutes , excluding the three participants who reached the fifteen minute time limit and required additional feedback.
The participants given feedback , all had the same difficulty that their contractions were too long.
They were told once to make their contractions shorter and then they were able to control the system in 11.75, 1.78 and 5.48 minutes respectively.
As mentioned in the Design Process, offline analysis was performed on the data from the short and long contraction walking tasks to determine if short and long contractions are separable into two gestures for control.
Figure 5 shows the mean and standard deviations for the short and long contraction durations.
In addition, we noticed the three participants that required feedback in the first familiarization task became frustrated when they could not make the system recognise their contractions; however, at the end of the experiment they were comfortable using the system.
Workshop on Real World User Interfaces, a workshop at the Mobile HCI Conference 2003.
Barreto, A., Scargle, S., Adjouadi, M. Hands-off human-computer interfaces for individuals with severe motor disabilities.
Physiology and Mathematics of Myoelectric Signals.
IEEE Transactions on Biomedical Engineering, Vol.
Surface Electromyography: Detection and Recording.
Proc the 7th IEEE International Symposium on Wearable Computers, 2003.
A Wireless, Networked-based Biosensor Interface for Music.
International Computer Music Conference, 2002.
Real Time EMG Gesture Recognition for Consumer Electronics Device Control.
G., Ishii, H., and Buxton, W. A. S. Laying the Foundations for Graspable User Interfaces.
Geelhoed, E., Falahee, M., Latham, K. Safety and Comfort of Eyeglass Displays.
An exploration of manipulative user interfaces.
11.Hart, S. G., Wickens C. Workload Assesment and Prediction.
12.Headon, R., Coulouris, G. Supporting Gestural Input for Users on the Move.
13.Healy, J., Picard, R. Digital Processing of Affective Signals.
The Electromyogram  as a Control Signal for Functional Neuromuscular Stimulation-Part I: Autoregressive Modeling as a Means of EMG Signature Discrimination.
15.Hinckley, K., Pierce, J., Sinclair, M., Horvitz, E. Sensing techniques for mobile interaction.
This paper has shown that an EMG based wearable input device can be effectively employed in a mobile context for subtle and intimate interaction.
The system presented is able to reliably recognize a motionless gesture without calibration or training across users with different muscle volumes.
During the experiment we observed that the participants performed the gesture subtly and inconspicuously, which indicates motionless EMG gestures can be utilized as a socially acceptable alternative for mobile device interaction.
As proposed, in combination with eyeglass displays and/or audio output, an EMG controller forms a closed loop "hands free" system that is advantageous in situations where the principal task occupies the hands or environments where obtrusive interaction techniques would be annoying to surrounding people.
Further user studies should test the system in more complex "real world" scenarios, such as lifting objects or interacting socially, and have viewers rate the subtlety from the social perspective.
The simplicity of the gesture recognition algorithm illustrates that it is possible to obtain interesting results even without calibration.
Further development should include the use of more advanced analysis techniques, such as autoregressive modelling, which has been reported to be successful in some EMG literature .
We are working on expanding the EMG gesture alphabet for increased levels of control.
In general, this studied showed EMG based input devices can be effectively used to design mobile interfaces that are subtle and intimate.
The authors would like to acknowledge Alberto Perdomo and Juanjo Andres Prado in our group for hardware design and support.
Our thanks also go to the MINDGAMES group for extra support with hardware.
Thanks to Ian Oakley, Media Lab Europe, for the invaluable suggestions and Justen Hide at the University of York for the constructive comments.
Proc Fourth International Symposium on Wearable Computers .
21.Knapp, B., Lusted, H. A Bioelectric Controller for Computer Music Applications.
22.Lumsden, J., Brewster, S. A paradigm shift: alternative interaction techniques for use with mobile & wearable devices.
24.Pirhonen, A., Brewster, S.A., Holguin, C. Gestural and Audio Metaphors as a Means of Control in Mobile Devices.
25.Poupyrev, I., Maruyama, S., Rekimoto, J. Ambient touch: designing tactile interfaces for handheld devices.
The Use of the Electromyogram in a Man-Machine Interface.
27.Rekimoto, J. GestureWrist and GesturePad: Unobtrusive Wearable Interaction Devices.
5th IEEE International Symposium on Wearable Computers, 2001.
28.Rekimoto, J. Tilting Operations for Small Screen Interfaces.
30.Tanaka A., Knapp B. Multimodal Interaction in Music Using the Electromyogram and Relative Position Sensing.
Matthews, B. Hibbs, A.D. Matthews, R. Krupka, M. Multimodal Neuroelectric Interface Development.
IEEE Transactions on Neural Systems and Rehabilitation Engineering, June 2003, Vol.
Gestures as Input: Neuroelectric Joysticks and Keyboards.
