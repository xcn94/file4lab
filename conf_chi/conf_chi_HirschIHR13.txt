The 8D prototype allows for glasses-free display of 3D content, whilst simultaneously capturing any incident light for re-illumination and interaction.
From left to right: A user shines a light source  at the 3D display, and the rendered 3D model is correctly re-illuminated.
Virtual shadows can be cast by placing a finger between the display and light source.
This allows any light source to act as an input controller, for example to allow intuitive interaction with a medical data set.
8D works by simultaneously capturing and displaying a 4D light field .
We present an 8-dimensional  display that allows glasses-free viewing of 3D imagery, whist capturing and reacting to incident environmental and user controlled light sources.
We demonstrate two interactive possibilities enabled by our lens-array-based hardware prototype, and realtime GPU-accelerated software pipeline.
Additionally, we describe a path to deploying such displays in the future, using current Sensor-in-Pixel  LCD panels, which physically collocate sensing and display elements.
While researchers have considered such displays, or prototyped subsets of these capabilities, we contribute an interactive 8-dimensional  display which simultaneously captures and displays a 4D light field.
We describe the design of our lens-array based, projectorcamera 8D display prototype, and GPU-based pipeline for real-time rendering and capture of 4D light fields.
Our prototype provides horizontal and vertical parallax as a user moves within the viewing region, without the need for user instrumentation or tracking.
Additionally, our display simultaneously captures the incident 4D light field from environmental or user controlled light sources.
We demonstrate the use of such a display in interactive scenarios: allowing for realistic relighting of virtual 3D objects, as well as the unique ability to use any light source as a user input controller, for example to visualize medical imaging data.
With the advent of sensor-in-pixel  LCD displays, we propose a clear path to implementing thin, portable, 8D displays in the future.
Imagine a display that behaves like a window.
Glancing through it, viewers perceive a virtual 3D scene with correct parallax, without the need to wear glasses or track the user.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Light fields have long been a valuable tool in rendering , and more recently in developing next-generation cameras  and displays .
They have also seen limited use for HCI, where depth information and gesture can be extracted from a light field captured through a prototype SIP screen and combined mask .
Glasses-free 3D parallax barrier  and lens-array  displays have existed for over 100 years.
BRDF displays can simulate flat surfaces with a particular Bi-Directional Reflectance Distribution Function .
6D displays that demonstrate 4D relighting of 2D images have been shown in both active 
The GPU pipeline and associated data.
The pipeline is conceptually divided into three stages: deinterlacing, light field rendering, and interlacing.
A 4D input light field is combined with a 4D output light field, synthesized from 3D model geometry.
From left to right, images captured through the lens array are deinterlaced using lens center positions determined by calibration.
Each view of the captured light field is projected onto 3D model data.
Output light field views are generated by a virtual camera array, pre-filtered, and interlaced for display on the lens array.
A recently shown 7D display  tracks a single light point as input.
In a closely related work, Cossairt et al.
Our work contributes a hardware approach to real-time 8D display that is compatible with emerging display technologies and a new GPU rendering and capture pipeline to make simultaneous, interactive 4D lighting and 4D capture feasible.
Our display offers interesting possibilities for interacting through light.
Light pens and widgets have been previously used for interaction .
In recent years, lighting widgets have been integrated into tabletop computing systems , and novel optics and computer vision have been used for interaction with screens, tables, and physical surfaces over a screen .
We are proposing the first interaction system capable of fully capturing and displaying arbitrary light transport within a volume above the display.
The projector is calibrated using the mori e magnifier  effect.
A hexagonal grid of red bars on a black background is projected at the expected lens center locations.
The scale and rotation of the projected image are adjusted until the central view above the lens sheet is solid red.
See the supplementary video for details on both techniques.
We propose to implement an 8D display by placing an array of microlenses on a SIP LCD screen.
Due to the pixel pitch limits of existing SIP hardware, we implement an equivalent projector-camera system .
This optically simulates the orthographic light field produced by the SIP display.
Our prototype uses a grayscale Point Grey Gazelle 2048 x 2048, 120f ps camera with a 50mm Schneider Xenoplan lens as a sensor.
A Sanyo PLV-Z800 1920 x 1080 projector is modified by shifting lens forward 4mm, allowing it to create a focused 325dpi image, matching the width of the hexagonal lens sheet.
The projector and camera share an optical path through a 40/60 beamsplitter.
We prevent cross-talk by multiplexing both through crossed linear polarizers.
The 8-Dimensional nature of our relightable 3D display is apparent within our GPU pipline implementation.
Our main 8D rendering pipeline depicted in Figure 2 and described in this section, generates both the input and output light fields using off-screen rendering to texture arrays.
Implemented in HLSL and DirectX 11, the GPU pipeline runs in real-time on an Nvidia GTX 470 GPU.
Our draw loop consists of N x M renderings of our 3D scene, one pass for each of the N x M light field views shown on our display .
Each view is observed using an off-axis oblique camera projection , corresponding to the view angle through the lens sheet of our 8D display prototype, and then rendered into a slice of a 2D texture array.
We implement a simplified version of the rendering equation, neglecting BRDFs.
To capture a 4D light field we deinterlace images recorded from the back of the lens array in our GPU pipeline .
For each render pass, we project P x Q captured input views onto the scene using projective texture mapping.
In practice, we use 5 x 5 views for both input and output, as a limited number of texture look-up coordinates can be passed between the shader stages.
After the 8D rendering is completed, two additional render passes with associated shader stages implement two 4D spatio-angular filtering operations, and hexagonal interlacing/deinterlacing .
Our 8D display prototype is subject to optical and computational limitations on its performance.
Our prototype supports 7 x 7 views optically.
However, due to limitations of our GPU pipeline, we are able to support only 5 x 5 views in real time.
Though our output images are resampled onto a hexagonal grid in our GPU pipeline in order to accommodate the hexagonal lens array, the approximate equivalent rectilinear resolution of our display is 274 x 154 per view.
With a 3mm focal length, the lens array offers a 19 field-of-view.
Sampling theory for automultiscopic displays, which predicts the inherent spatio-angular resolution trade-off shared by all such designs, is characterized by Zwicker et al.
Their frequency domain analysis explains the depth of field exhibited by automultiscopic displays.
Following Zwicker, for a display with angular sampling rate v , and spatial sampling t rate t, objects at distance |z | >  v will be blurred.
Defining the t plane at the focal point of the lens sheet, and the v plane at unit distance, t = 0.547mm and v = 0.026mm.
Given the above equation for z , objects up to 21mm from the display are reproduced without blur.
Empirical depth of field characterization shows satisfactory reproduction for objects extending up to 3cm.
This can be improved with increased angular resolution.
As is apparent in the video, direct reflection and scattering from the lens sheet competes with the light emitted from the screen of 8D display.
Anti-reflection coatings can reduce such reflections to less than 1% of incident light, and are common in commercial optics.
For example, in Figure 3 , details of the face and brain are obscured in our lower spatial resolution display prototype.
Even at these low resolutions, our prototype illustrates the potential of such light-based interactive displays.
A hand-held light  reveals either a patient's skin and facial features, or brain tissue .
This non-physical interpretation of lighting intensity leads to a novel interface for viewing medical data sets.
The captured incident light field  is projected onto a head and brain model, modulating the skin transparency by incident light intensity.
Our goal in creating an 8D Display is to take a step towards displays that can produce the convincing illusion of physical reality .
A key aspect of this goal will be the ability of these displays to realistically react to incident environmental lighting.
Figure 1 shows the 8D display prototype rendering a virtual 3D model.
When viewing the display, the model appears to be 3D, with full parallax.
In this example, the user also moves a lamp over the display, and the 3D model responds to the incident light and is correctly re-illuminated.
In a second demonstration, depicted in Figure 3, more intense light acts like a virtual x-ray, revealing inner structure in MRI data.
The examples demonstrated in this work only begin to touch on the possibilities enabled when light transport is modeled by a display in this way.
Abundant interactive possibilities include: using an off-the-shelf light source as 6DoF input controller, direct manipulation of physical lights to cause relighting of 3D scenes, augmented-reality mirrors, accurately mimicking surfaces with exotic BRDFs, and applying nonrealistic physics to real lighting sources.
Beyond our demonstrated x-ray example, one could imagine implementing noneuclidean optics, with multiple 8D displays.
8D Displays of sufficient intensity can be used to computationally illuminate objects in the environment, for interaction, user guidance, or advanced shape scanning and acquisition.
Much of the potential impact of this research is predicated on the existence of Sensor-In-Pixel  LCDs.
In recent years LCD manufacturers have introduced semiconductor technologies that combine light sensitive elements into the LCD driver matrix .
In combination with collocated, thin, optical capture and display elements, such as those provided by a SIP LCD, this work suggests a straightforward route to achieving a low-cost, commercially realizable, real-time, 8D display.
The 40in diagonal Samsung SUR40 Sensor-In-Pixel display has 1920 x 1080 resolution, yielding a pixel pitch of 55dpi.
One goal of this work is to inspire manufacturers to increase these numbers.
We have presented a glasses-free 3D display capable of reacting to real-world environmental and user-controlled lighting in real-time.
This work paves the way to creating displays that can produce physically convincing illusions that participate optically in the environment in which they are rendered.
It is our hope that this work will inspire follow-on investigations in the HCI community to more fully explore the great potential of 8-dimensional displays.
