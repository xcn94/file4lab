When designing questionnaires there is a tradition of including items with both positive and negative wording to minimize acquiescence and extreme response biases.
Two disadvantages of this approach are respondents accidentally agreeing with negative items  and researchers forgetting to reverse the scales .
The original System Usability Scale  and an all positively worded version were administered in two experiments  across eleven websites.
There was no evidence for differences in the response biases between the different versions.
A review of 27 SUS datasets found 3  were miscoded by researchers and 21 out of 158 questionnaires  contained mistakes from users.
We found no evidence that the purported advantages of including negative and positive items in usability questionnaires outweigh the disadvantages of mistakes and miscoding.
It is recommended that researchers using the standard SUS verify the proper coding of scores and include procedural steps to ensure error-free completion of the SUS by users.
Researchers can use the all positive version with confidence because respondents are less likely to make mistakes when responding, researchers are less likely to make errors in coding, and the scores will be similar to the standard SUS.
Designers of attitudinal questionnaires  are trained to consider questionnaire response styles such as extreme response bias and acquiescence bias .
In acquiescence bias, respondents tend to agree with all or almost all statements in a questionnaire.
The extreme response bias is the tendency to mark the extremes of rating scales rather than points near the middle of the scale.
To the extent that these biases exist, the affected responses do not provide a true measure of an attitude.
Acquiescence bias is of particular concern because it leads to an upward error in measurement, giving researchers too sanguine a picture of whatever attitude they are measuring.
A strategy commonly employed to reduce the acquiescent response bias is the inclusion of negatively worded items in a questionnaire , , .
Questionnaires with a mix of positive and negatively worded statements force attentive respondents to disagree with some statements.
Under the assumption that negative and positive items are essentially equivalent and by reverse scoring the negative items, the resulting composite score should have reduced acquiescence bias.
More recently, however, there is evidence that the strategy of including a mix of positively and negatively worded items creates more problems than it solves .
Such problems include lowering the internal reliability , distorting the factor structure , ,  and increasing interpretation problems with cross-cultural use .
The strategy of alternating item wording has been applied in the construction of most of the popular usability questionnaires, including the System Usability Scale  , SUMI , and QUIS .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The System Usability Scale is likely the most popular questionnaire for measuring attitudes toward system usability , .
Its popularity is due to it being free and short--with 10 items that alternate between positive and negative statements about usability .
It has also been the subject of some recent investigations ,  , , which makes it a good candidate to manipulate to study whether the benefits outweigh the potential negatives of alternating item wording.
The ten traditional SUS items are shown in Exhibit 1.
This suggests participants tended to agree slightly more with negatively worded items and to disagree slightly more with positively worded items .
The magnitude of the difference was modest, with the average absolute deviation from the average score of .19 of a point and the highest deviation on item 4 
Finstad  compared a 7-point version to the original 5point version of SUS and found users of enterprise systems interpolated significantly more on the 5-point version than on the 7-point version; however, there was no investigation on the effects of changing item wording.
Based on difficulties observed with non-native speakers completing the SUS, Finstad  recommended changing the word cumbersome in Item 8 to awkward - a recommendation echoed in  and .
In 2008 a panel at the annual Usability Professionals Association conference entitled Subjective ratings of usability: Reliable or ridiculous?
On the panel were two of the originators of the QUIS and SUMI questionnaires.
As part of the panel presentation, an experiment was conducted on the effects of question wording on SUS scores to investigate two variables: item intensity and item direction .
For example, the extreme negative version of the SUS Item 4 was I think that I would need a permanent hot-line to the help desk to be able to use the web site.
Volunteer participants reviewed the UPA website.
After the review, participants completed one of five SUS questionnaires -- an all positive extreme, all negative extreme, one of two versions of an extreme mix , or the standard SUS questionnaire .
Sixty-two people in total participated, providing between 10-14 responses per condition.
Even with this relatively small sample size the extreme positive and extreme negative items were significantly different from the original SUS  = 6.90, p < .001 and represented a large effect size .
These results were consistent with one of the earliest reports of attitudes in scale construction .
Research has shown that people tend to agree with statements that are close to their attitude and disagree with all other statements .
I think that I would like to use this system frequently.
I found the system unnecessarily complex.
I thought the system was easy to use.
I think that I would need the support of a technical person to be able to use this system.
I found the various functions in this system were well integrated.
I thought there was too much inconsistency in this system.
I would imagine that most people would learn to use this system very quickly.
I found the system very cumbersome to use.
I felt very confident using the system.
I needed to learn a lot of things before I could get going with this system.
The proper scoring of the SUS has two stages: 1.
Subtract one from the odd numbered items and subtract the even numbered responses from 5.
This scales all values from 0 to 4 .
Add up the scaled items and multiply by 2.5 .
Much of the research applied to the design of rating scales for usability attitudes comes from disciplines other than usability, typically marketing and psychology.
In other fields, items can include more controversial or ambiguous topics than is typical of system usability.
Although many findings should still apply to usability questionnaire design, it is of value to the design of future usability questionnaires to review research related specifically to the analysis of rating scales used in usability--especially the SUS.
Likewise, only respondents who passionately disfavored the usability agreed with the extremely negatively questions--resulting in a significant higher average score.
The results of this experiment confirmed that extreme intensity can affect item-responses towards attitudes of usability, so designers of usability questionnaires should avoid such extreme items.
Due to the confounding of item intensity and direction, however, the results do not permit making claims about the effects of alternating positive and negatively worded items.
The improperly scaled scores are still acceptable values, especially when the system being tested is of moderate usability .
A researcher may only become aware of coding errors after subjecting the results to internal reliability testing and obtaining a negative Cronbach's alpha - a procedure that few usability practitioners routinely practice.
In fact, this problem is prevalent enough in the general practice of questionnaire development that the makers of statistical software  have included it as a warning The  value is negative due to a negative average covariance among items.
This violates reliability model assumptions.
You may want to check item codings .
We are able to estimate the prevalence of the miscoding error from two sources.
First, in 2009, eight of 15 teams used the SUS as part of the Comparative Usability Evaluation-8  workshop at the Usability Professionals Association annual conference .
Of the eight teams, one team improperly coded their SUS results.
Second, as part of an earlier analysis of SUS, we  examined 19 contributed SUS datasets.
Two were improperly coded and needed to be recoded prior to analysis.
Considering these two sources, three out of 27 SUS datasets  had negative items that weren't reversed.
Assuming this to be a reasonably representative selection of the larger population of SUS questionnaires, we can be 95% confident that miscoding affects between 3% and 28% of SUS datasets.
Hopefully, future research will shed light on whether this assumption is correct.
Despite published concerns about acquiescence bias, there is little evidence that the common-wisdom of including both positive and negatively worded items solves the problem.
To our knowledge there is no research documenting the magnitude of acquiescence bias in general, or whether it specifically affects the measurement of attitudes toward usability.
The goals of this paper are to determine whether an acquiescence bias exists in responses to the SUS, and if so, how large is it?
Does the alternating wording of the SUS provide protection against acquiescence and extreme response biases?
Further, does its alternating item wording outweigh the negatives of misinterpreting, mistaking and miscoding?
The major impetus for alternating scales is to control acquiescent response bias .
The alternation of positive and negative items also provides protection against serial extreme responders - participants who provide all high or all low ratings - a situation that could be especially problematic for remote usability testing.
For example, when items alternate, responses of all 1's make no sense.
When items do not alternate, responses of all 1's could represent a legitimate set of responses.
Disadvantages for alternating question items Despite the potential advantages of alternation, we consider three major potential disadvantages.
Misinterpret: Users may respond differently to negatively worded items such that reversing responses from negative to positive doesn't account for the difference.
As discussed in the previous section, problems with misinterpreting negative items include creating an artificial twofactor structure and lowering internal reliability, especially in cross-cultural contexts.
Mistake: Users might not intend to respond differently, but may forget to reverse their score, accidentally agreeing with a negative statement when they meant to disagree.
We have been with participants who have acknowledged either forgetting to reverse their score or commenting that they had to correct some scores because they forgot to adjust their score.
Miscode: Researchers might forget to reverse the scales when scoring, and would consequently report incorrect data.
Despite there being software to easily record user input, researchers still have to remember to reverse the scales.
The internal reliability of both versions of the questionnaires was high and nearly identical.
For the original SUS questions with 51 cases Cronbach's alpha was .91.
For the positively worded SUS with 110 cases Cronbach's alpha was .92.
To look for an overall effect between questionnaire types, we conducted a t-test using the scaled SUS scores, the average of the evenly numbered items, and the average of the odd-numbered items.
The means and standard deviations appear in Tables 1-3.
In April 2010, 51 users  performed two representative tasks on one of four websites .
Examples of the tasks include making reservations for a car or flight, locating items to purchase, finding the best interest rate and locating store hours and locations.
At the end of the test users answered the standard 10 item SUS questionnaire.
There were between 12 and 15 users for each website.
In August 2010, a new set of 110 users  performed the same tasks on one of four websites tested four months earlier.
There were between 20 and 30 users for each website.
The testing protocol was the same except the new set of users completed a positive-only version of the SUS as shown in Exhibit 2.
Note that other than replacing system with website, the odd items are identical to those of the standard SUS but the even items are different - worded positively rather than negatively.
I think that I would like to use the website frequently.
I found the website to be simple.
I thought the website was easy to use.
I think that I could use the website without the support of a technical person.
I found the various functions in the website were well integrated.
I thought there was a lot of consistency in the website.
I would imagine that most people would learn to use the website very quickly.
I found the website very intuitive.
I felt very confident using the website.
I could use the website without having to learn anything new.
There was a significant difference for the odd items  = -2.61 p < .02 - the items not changed between versions of the questionnaire.
There was a statistically significant difference between the odd and even-numbered items for the original SUS questionnaire  = 4.26, p <.001 and the all positive SUS questionnaire  = 2.60, p <.02, suggesting the even items elicit different responses than the odd items in both questionnaires.
Our second approach was to examine responses to the most highly correlated negative and positive item which, according to 's large SUS dataset were items 2 and 3 .
The correlation between those items from this experiment was also high and significant .
For this assessment, we counted the number of times respondents provided a response of a 4 or 5 to both items 2 and 3.
The overall SUS scores between the standard and all positive versions of the SUS were not significantly different, which suggests that changing the wording of the items in this way does not appear to have a strong effect on the resulting SUS measurements.
There was no difference in the even numbered item averages .
However, the oddnumbered item averages  were significantly different, with slightly lower scores for positive and slightly higher scores for the negative versions of the items.
To say the least, we did not expect this result, and found it difficult to explain.
In examining the difference by website, the bulk of the differences came from two of the four websites .
Investigating systems in the wild can be tricky because researchers have no control over the timing of system changes .
It is possible that changes to the websites somehow affected only the odd numbered questions, but that is pure speculation.
To minimize the potential confounding effects of website changes and item wording, we conducted another experiment with the questionnaires tested simultaneously rather than asynchronously.
We also planned for a larger sample size to increase the power of the experiment.
To assess acquiescence bias, we counted the number of agreement responses  to the odd numbered  items in both questionnaires.
To measure extreme response bias, we counted the number of times respondents provided either the highest or lowest response option  for both questionnaire types for all items.
We used two approaches to assess the magnitude of the potential mistake problem.
First, we looked for internal inconsistencies within questionnaires by comparing the number of times respondents agreed  to negatively worded items and also agreed to positively worded items  --an approach similar to .
We considered a questionnaire to contain mistakes if there were at least 3 responses indicating agreement to positively and negatively worded items or 3 responses with disagreement to positively and negatively worded items.
In August and September 2010, 213 users  performed two representative tasks on one of seven websites .
The tasks included finding the best price for a new car, estimating the trade-in value of a used-car and finding information about mutual funds and minimum required investments.
At the end of the study users randomly completed either the standard or the positively worded SUS.
There were between 15 and 17 users for each website and questionnaire type.
The internal reliability of both questionnaires was high - Cronbach's alpha of .92  for the original and .96  for the positive.
To look for an overall effect between questionnaire types, we conducted a t-test using the scaled SUS scores, the average of the evenly numbered items and the average of the odd-numbered items.
The means and standard deviations appear in Tables 4-6.
As in Experiment 1, we assessed  the magnitude of mistaken responses: internal inconsistencies in at least 3 questions, and  the consistency of responses to items 2 and 3.
There continued to be a statistically significant difference between the odd and even-numbered items for the original SUS questionnaire  = 3.09, p <.01 and the all positive SUS questionnaire  = 2.32, p < .03.
To assess acquiescence bias, we counted the number of agreement responses  to the odd numbered  items in both questionnaires.
To measure extreme response bias, we counted the number of times respondents provided either the highest or lowest response option  for both questionnaire types for all items.
Because that finding did not replicate in Experiment 2, it was very likely a consequence of having collected the data asynchronously.
It could be that the websites changed or the type of users who participated were in some way different.
In almost every way, the data collected in Experiment 2 with the standard and positive versions of the SUS were similar, indeed, almost identical.
There were no significant differences in total SUS scores or the odd or even averages.
Cronbach's alpha for both versions was high .
The differences in the factor structures  were very likely due to the relatively small sample sizes.
There was little evidence of any differences in acquiescence or extreme response biases between the original SUS questionnaire and the all positive version in either experiment.
Using the more conservative of the two estimation methods for mistaken responses, there were 3 out of 51 from Experiment 1 and 18 out of 107 in Experiment 2 for a total of 21 out of 158 questionnaires which contained at least 3 internal inconsistencies.
This would suggest 13.3%  of SUS questionnaires administered remotely contain mistakes.
For miscoding errors, three out of 27 SUS datasets  were improperly coded resulting in incorrect scoring.
We did not find any evidence for a strong acquiescence or extreme response bias.
Even if strong evidence were found, the recommendation by  to reverse scale responses instead of item wording would not correct the problems of mistakes and miscoding.
The data presented here suggest the problem of users making mistakes and researchers miscoding questionnaires is both real and much more detrimental than response biases.
There is little evidence that the purported advantages of including negative and positive items in usability questionnaires outweigh the disadvantages.
This finding certainly applies to the SUS when evaluating websites using remote-unmoderated tests.
It also likely applies to usability questionnaires with similar designs in unmoderated testing of any application.
Future research with a similar experimental setup should be conducted using a moderated setting to confirm whether these findings also apply to tests when users are more closely monitored.
Researchers interested in designing new questionnaires for use in usability evaluations should avoid the inclusion of negative items.
Researchers who use the standard SUS have no need to change to the all positive version provided that they verify the proper coding of scores.
In unmoderated testing, it is more difficult to correct the mistakes respondents make, although it is reassuring that despite these inevitable errors, the effect is unlikely to have a major impact on overall SUS scores.
Researchers who do not have a current investment in the standard SUS can use the all positive version with confidence because respondents are less likely to make mistakes when responding, researchers are less likely to make errors in coding, and the scores will be similar to the standard SUS.
