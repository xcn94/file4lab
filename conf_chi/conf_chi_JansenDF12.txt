We explore the use of customizable tangible remote controllers for interacting with wall-size displays.
Such controllers are especially suited to visual exploration tasks where users need to move to see details of complex visualizations.
In addition, we conducted a controlled user study suggesting that tangibles make it easier for users to focus on the visual display while they interact.
We explain how to build such controllers using off-the-shelf touch tablets and describe a sample application that supports multiple dynamic queries.
Previous work has mainly focused on input issues without considering the types of tasks that might be actually carried out on a WSD.
These tasks are likely to be essentially visual in nature, since they will benefit the most from moving from desktop environments to high-resolution WSDs.
Examples are visual exploration tasks such as in information visualization, and they constitute the main motivation for this work.
Since a key benefit of high-resolution WSDs is the ability to get an overview of complex visuals from a distance and see details up-close, it is important to support locomotion.
For users to be able to comfortably carry out visual tasks, interaction devices must also be easy to operate eyes-free.
In this article we discuss five such requirements and how well they are supported by existing WSD interaction techniques.
We present a categorization of techniques that uncovers a promising area in the design space that combines mobility and tangibility.
Here we propose to explore the use of customizable tangible remote controllers such as illustrated in Figure 1 for carrying out visual exploration tasks on WSDs.
We explain how to build such devices using multitouch tablets and capacitive tangible controls.
Since there is a practical cost in building and employing such controls , they must provide clear benefits compared to similar solutions based on touch or pen input alone .
We report on a controlled user experiment taking the slider as an example to show that tangible controls can indeed facilitate interaction with vertical displays.
We finally illustrate our approach with a prototype system that lets users explore multidimensional datasets on a highresolution WSD by performing multiple dynamic queries with tangible range sliders .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Here we discuss the requirements for interaction techniques to conveniently support visual exploration tasks on large displays, and how well they are met by existing approaches.
We then motivate the use of tangible remote controllers, and discuss previous evaluations of tangible user interfaces to show why more studies are needed to validate our approach.
Mid-air or freehand techniques are techniques where the user's hand is free to move in 3-D space  .
The large majority of these techniques support mobility .
They combine hand or handheld device tracking with interaction techniques such as ray-casting and hand posture recognition to let users perform tasks such as selecting objects or panning-and-zooming .
Other mid-air techniques need to be operated from a desk   and sensing technologies also exist that only work close to the display  .
Mid-air interaction supports mobility  - although techniques such as ray casting are not well-suited to up-close interaction - but it is inherently limited in accuracy and often subject to fatigue.
Eyes-free operation is possible  but most mid-air techniques require additional visual feedback on the display .
Although pointing is the most common task, a large design space exists for techniques that are more task-specific, especially 3-D tasks for which mid-air input is particularly well-suited.
For precise 2-D or 1-D input, however, other approaches exist that seem more adequate.
We derive the five following requirements for WSD interaction techniques in a context of visual exploration: R1 Little visual attention: the interaction devices should demand as little visual attention as possible.
For example, a scientist who is tuning the parameters of a simulation should be able to observe the results directly on the large display without having to constantly switch her attention between the display and the input device.
R2 Little visual clutter: since the primary focus of the task is the visual information displayed, the technique should not require too much visual feedback on the WSD.
For example, a pop-up menu can occlude the data being visualized and disrupt the exploration process or disturb other users.
R3 Mobility: the technique should allow for locomotion.
This is especially important on high-resolution WSDs, where users can get an overview of a complex visualization when standing at a distance, while moving towards the screen provides them with previously illegible details .
R4 Task-specificity: the appropriate input technique depends on the task.
Input devices for WSDs have been categorized by their degree of guidance into 3-D, 2-D and 1-D , and guidance should best match the number of degrees of freedom of the task .
Although scientific visualization applications exist that need 3-D positional input, most information visualization applications heavily rely on 2-D object selection and 1-D dynamic queries .
R5 Genericity: while devices and techniques should be ideally targeted towards applications, many visual exploration applications exist depending on the domain.
For practical reasons we therefore need to access a wide range of applications with the same technology and interaction styles.
There is a natural tension between task-specificity and genericity, which on standard desktop computers has been partly addressed by the use of a generic input device  and a set of logical tools that are half-way between generic and specific .
But it is not clear yet how this tension should be addressed in WSD environments.
In surface-based techniques, the hand's motion is constrained by a 2-D surface  .
A first type of approach uses the WSD itself as the constraining surface , and this mostly includes pen-based and touch interaction.
A problem is that users need to move in front of objects to select them, and although a number of reaching techniques have been proposed , touch- and pen-based input unavoidably require users to be close to the display .
A second type of approach uses horizontal constraining surfaces separate from the wall, such as a desk and a mouse  or touch-sensitive tabletops .
But this type of approach, as well as solutions such as chair-mounted touch tablets , restricts locomotion .
A third type of approach uses handheld surfaces such as PDAs and touch tablets  .
Users are free to move around as if they were using a remote controller .
This approach opens up a wide range of possibilities, such as deporting the application's widgets to the handheld device, eliminating the need for visual feedback on the wall .
However, this also implies switching users' focus from the display to the input device, something which should be best avoided for visual exploration tasks .
We discuss existing approaches for WSD interaction with respect to our requirements.
We classify them according to their input range and the type of input they involve .
The first two types of input are from .
We focus on manual input; other modalities can be quite useful as they often allow for mobility and in some cases eyes-free operation, but they are best used as complements to manual input for tasks that require a minimum amount of input bandwidth.
Physical controls such as buttons, sliders and knobs are widely used in everyday life and can be useful in WSD environments as well .
Physical controls can be placed at a fixed location, either at a distance from the display  or on the display's structure .
However, the best solution for mobility are remote controllers .
One simple example are hand-held devices used to navigate in slides during presentations, and a more sophisticated version could be devised for navigating in visualizations.
One advantage of physical controls is that they provide richer tactile feedback than flat 2-D surfaces and therefore they might be easier to find and operate eyes-free .
In addition, continuous controls such as sliders or knobs are especially well-suited to 1-D tasks since they are path constrained  .
But despite the fact that some controllers exist that are programmable, classical remote controllers are not flexible enough to accommodate a wide range of tasks .
Based on the previous requirements and classification of techniques, we propose to use tangible remote controllers to perform visual exploration tasks on a WSD.
Remote controllers fully support mobility  and compared to early PDA-based systems , tangible controls should be easier to acquire and operate when the display requires full user attention .
Technically speaking, we propose a hybrid solution similar to SLAP .
Compared to a display-less configurable control board approach , this approach supports flexible relabeling  and 2-D positional input.
Both phone and tablet form factors are commercially available, the latter being especially suited to input-rich applications.
Like GUI widgets, tangible controls can be built that are between generic and task-specific .
Since widgets are already widely used in desktop applications - including in information visualization applications - this approach can facilitate both the porting of existing applications to WSDs and knowledge transfer among end users.
Tangibles have already started to be combined with portable touch devices.
One of them, the Fling Joystick2 , is commercially available.
Our tangible remote controller prototype uses a similar technology.
However, with the exception of the Fling Joystick and Clip-On buttons , existing systems require the tablet to lie flat so that the tangibles do not glide off it, which defeats the mobile nature of tablets .
Also, no previous system has been designed to be used as remote controllers.
In isolation, these augmented tablets are of limited use since tangibles and visual output from applications compete for the same  space.
Using them with WSDs is, we believe, a more compelling application, but it poses technical and design problems that we address in this article.
Tangible user interfaces  employ physical rather than virtual objects for interacting with applications .
In contrast with the previous category, most tangibles are reconfigurable in space .
Although TUIs have been rarely used with WSDs, we include in our classification techniques that can potentially be employed .
We make a distinction between physical props, i.e., objects that can be moved around and whose primary purpose is to specify positions and orientations, and controls, i.e., objects with moving parts such as buttons and sliders that are usually fixed.
Most existing TUI systems are not mobile  .
Early systems involve application-specific props .
Systems with generic controls have also been proposed that include configurable control boards  and hybrid systems  where controls are laid out on horizontal touch displays.
Hybrid systems have the advantage of supporting both 2-D surface and tangible interaction , and allow dynamic control relabelling .
Other techniques have been designed for vertical surfaces, and include sticky or magnetic props and controls .
These, like other techniques on the left column of Table 1, have the drawback of requiring close-up interaction .
So far no system combining tangible interaction and mobility has been proposed for wall interaction , but hybrid tangible systems have started to be built for touch-phones and tablets.
We review them in more detail in the next section.
Our approach can be evaluated in different ways.
Concerning mobility, commercial touch phones and tablets are designed to support it, so provided controllers stick to the display and can be conveniently carried when unused, tangible systems built with these devices should inherently support mobility.
A less clear claim is that tangibles bring advantages compared to similar approaches that use touch only .
Since studies have been already carried out on general-purpose TUIs, we first review them.
Fitzmaurice and Buxton  devised a task where various tangible objects had to be manipulated to match their virtual counterparts on a screen.
Baseline conditions involved multiple pens & pucks or a unique pen & puck .
Although tangibles were found to be more effective, touch input was not tested.
Tangible props outperformed direct touch but in contrast to the original study, input and output spaces coincided so there was no eyes-free component to the task.
They found the slider to outperform the scrollbar, which proves the superiority of two-handed input but not of tangible input per se.
One of the SLAP widgets , the rotary knob, was evaluated in a task where subjects had to reach back-and-forth between buttons placed near a video player requiring sustained attention and a knob placed far away.
The tangible knob was found to outperform its touch counterpart.
However, these results cannot be easily generalized to our setup where the user is free to move the tablet with respect to the visual display.
Overall these studies provide some support for the idea of augmenting remote controller tablets with tangibles, but they apply to situations that significantly differ from ours.
Tuddenham's study involves an interactive table where input and output coincide, a very different setup from ours which requires operating a hand-held device while observing visual feedback on a wall display.
The SLAP widgets study did separate input and visual output, yet its setup differs from ours since it lacked a physical frame of reference and the controls could not be moved with respect to the display, both of which can make touch input harder.
Also, Fitzmaurice and Tuddenham evaluate compound actions for which tangibles are known to be especially suited and SLAP evaluates a touch widget that is known to be difficult to operate eyes-free .
We therefore propose a new study to address these concerns.
Another contribution of our study lies in clean performance metrics.
Tuddenham and Fitzmaurice only measure overall error and completion time.
Device reacquisition is not controlled for and reacquisition time is only inferred by comparing global user idle times per trial.
We set out to determine not only if tangibles are more effective overall but also when and why.
We therefore measure control acquisition performance and control operation performance separately, as well as how subjects allocate their visual attention.
In this section we present the design of our remote controller prototype.
Similar to recent work , we lay out tangible widgets on off-the-shelf capacitive touch tablets, as these are now widely available and support wireless communication necessary for WSD interaction.
In addition, they provide 2-D touch and display capabilities.
In contrast to previous work, we specifically designed our system to be used as a remote controller for WSDs.
This involves technical requirements, some of which directly derive from our general requirements for WSD techniques.
Because users need to be able to move with the remote controller and operate it in any orientation , the tangible controls should stick to the screen and be kept small to save screen real estate.
Users should be able to control as many applications as possible , hence tangible controls should be kept generic and ideally mimic widgets that are commonly found in existing GUI toolbars .
Additionally the remote controller should be fully reconfigurable since users might need to control different applications or might need different control sets depending on the current task .
This requires support for dynamic control reassignment, and also requires tangibles to be easy to remove and to replace on the tablet surface.
We explain how to build a tangible remote controller that addresses these requirements.
We focus here on the hardware aspects; remote communication with WSD applications and mechanisms for mapping controls to applications will be discussed in the section Sample Application.
Our tangible controls are tracked using capacitive sensing.
Most multi-touch tablets use mutual capacitance between row and column electrodes to detect touch: each row / column crossing exhibits a baseline capacitance that increases when a conductive object gets close enough .
Small conductive items such as metal paper clips typically produce small changes which cannot be discriminated from the sensor noise, whereas bigger objects like scissors or human bodies produce measurable changes.
On a capacitive tablet, a touch event is typically produced when the capacitance exceeds a threshold.
The body of our tangible controls has been built from lasercut non-conductive material such as cardboard and acrylic .
Our final prototypes are built from transparent acrylic to allow visual information to be displayed underneath as in SLAP Widgets .
All controls are based on a fixed frame, which is entirely coated with conductive ink such that users are free to touch them wherever they prefer while preserving their transparency.
The frames are directly connected to the feet to forward touch events.
Moving parts are also coated with conductive ink and each part, i.e., button plate, inner knob, and slider thumbs, creates its own touch event.
Figure 3 illustrates the mechanism for the range slider.
In contrast to previous systems like TUIC  we need to support mobile use and therefore require that the controls stick to the tablet.
They should also be easy to remove and reposition and not require special modifications to the tablet.
Consequently, we decided against magnets  as they require a ferromagnetic backing of the tablet.
Suction cups as used by the Fling Joystick are better suited but take up considerable space or restrict the tangibles to the edge of the screen.
We found that two to three small surfaces of about 20 mm2 are enough to provide a tight grip on tablets while being easy to remove without leaving traces.
Accordingly, tangibles can in principle be designed so that they are constantly tracked as in vision-based systems.
However, most tablets do not give access to raw input, making the detection of small tangibles impossible or unreliable.
Also, the number of simultaneous touches that can be tracked is limited, which limits the number and complexity of tangibles that can be used.
We therefore chose to have our tangibles detected and tracked only while touched.
This allows to fit more controls and works well in practice, since our tangibles stick to the tablet and their moving parts are unlikely to move unless they are touched.
To have tangibles detected and tracked only when touched, we used non-conductive bodies with a conductive coating that forwards user touch to the capacitive screen .
For rapid prototyping we experimented with electrostatic bags, graphite spray paint, copper tape and conductive foam .
We use capacitive markers so that the tablet can track the geometry  of tangible controls and their moving parts.
We opted for passive rather than active markers since an active design would require more space for electronics and batteries .
We decided against encoding individual control IDs in order to save space for the markers.
Once a control has been placed on the tablet, its type, location and orientation are registered.
Since controls stick in place, their identity and the identity of their moving parts can be subsequently retrieved from the position of touch events.
Since it might be useful to let users interact with the available tablet surface in addition to operating the tangible controls, we need to discriminate between tangible operation and actual touch events.
Our current prototype uses Apple iPads whose current API  only exposes information on touch location and the radius of its major axis.
We therefore use a marker encoding scheme that uses distance between single touch points and their major axis radii.
For sliders we use two 3x10 mm feet, while buttons and knobs use three 5x5 mm feet.
This proved enough to allow reliable detection of tangibles without false positives and false negatives.
Because tangibles have a practical cost - they need to be built and carried around - their benefits need to be clearly demonstrated to justify their use.
Although intuition and common sense suggests that tangible controls should always outperform touch, some studies show that it is not always the case .
In fact, it is hard to build reliable and effective tangible controls, and other negative results likely exist that have never been published due to reporting bias.
Other studies suggest that touch devices can be operated very effectively, so the need for augmenting them with tangibles is unclear.
For example, Buxton et al.
We set out to determine whether tangible controls provide benefits when using a tablet as a remote controller.
Our goal was to keep the experiment design as simple as possible to make our findings independent from the task and get a better understanding of the causes of differences in performance.
We chose to examine interaction using a single control and opted for a slider because it is widely used and is neither too simple nor too complex.
We decided to test it on a low-level tracking task, a task upon which we believe depends the performance of many higher-level visual exploration tasks, and added a secondary task to test control acquisition.
Furthermore, since we were not interested in user locomotion per se, we used a simplified setup where subjects stood in front of a wall-mounted display .
A repeated measures full-factorial within participants design was used.
One factor was the input technique, i.e., touch or tangible input.
A second factor was the task, i.e., either pure tracking  or tracking with a secondary task that forced subjects to release and reacquire the slider .
12 participants were randomly assigned to one of four groups according to a balanced latin square 2 x 2 design.
Like in , the target moved at a constant speed, "darted off" at random intervals and changed direction when it reached an extremum.
We used a target speed of 0.15 units per second, a dart-off distance of 0.3 units and a dart-off interval between 2 and 4 seconds.
A unit equals the slider's total range.
During the tracking + reacquisition task, the left side of the vertical display lit up red -- which we refer to as the tap stimulus, see Figure 4 -- at pre-defined random intervals between 2 and 6 seconds .
When this happened, subjects were instructed to tap the corresponding area of the tablet as soon as they could, then promptly return to the tracking task.
The tap area was large enough to allow for confident eyes-free acquisition and its only purpose was to have participants release then reacquire the slider's thumb.
In case the tap stimulus was ignored , each new tap stimulus was queued and displayed immediately after the reacquisition of the slider.
Subjects were told to be as fast and accurate as possible throughout the experiment and that the tracking and tapping tasks were equally important.
They were instructed to stand, hold the tablet in their non-dominant hand, and perform the taps with the index finger of their dominant hand in order to enforce the need for releasing the slider.
Subjects could otherwise hold the tablet and use the slider as they wished.
Subjects were given an initial practice session of 90 seconds per technique with the tracking + reacquisition task.
They were warned that holding the tablet could yield fatigue and were allowed to rest between trials.
Subjects were videotaped during the experiment and upon its completion, were given a questionnaire and were asked to comment on the techniques.
We ran the experiment on a MacBook Air connected to a 121x68 cm  display mounted on a wall at a 125 cm height , and communicating wirelessly with an iPad 2.
A table prevented subjects from getting closer than 230 cm.
In the touch condition, the iPad displayed a virtual slider of 1.2x9.5 cm with an active touch area of 1.2x8.0 cm .
In the tangible condition, a 1.2x9.5x0.6 cm physical slider , designed as explained in the previous section, was placed on top of the virtual slider.
As shown in Figure 5, the two sliders had a similar appearance.
The primary task for this experiment was loosely designed after the tracking task used by Fitzmaurice and Buxton .
Participants held the tablet, stood in front of a wall-mounted display and had to continuously readjust the slider to track a target presented on the vertical display.
The display showed a larger slider with both the current thumb position and the desired position  .
A red bar connected the two to highlight the error.
Subjects were instructed to keep this error as small as possible at all times.
The target followed a precomputed 1-D path among four different paths whose order was randomized within each block.
Our three main dependent variables were: * Tracking Error, the distance between the thumb and target as a percentage of the slider range, averaged over samples.
An overall means comparison shows it was smaller with tangible  than with touch .
For the pairwise comparison the effect is only significant under the tracking + reacquisition task , which supports our hypothesis H1.
The averages are plotted in Figure 6 left.
When comparing the two techniques under the tracking task, the effect is not significant but shows a trend , which weakly supports our hypothesis H4.
All the following analyses are reported for the condition tracking + reacquisition only .
Glances were accompanied with a clear downward head motion and were counted by viewing video recordings.
We additionally included the two following measures: * Tap Reaction Time, the average time between the onset of the tap stimulus on the vertical display and the moment the subject taps this area on the tablet.
These criteria were determined from event and video logs and effectively discriminated between misacquisitions and accidental touches .
We conjectured that the tangible slider would be faster to reacquire and that we would hence observe lower tracking errors overall during tracking + reacquisition tasks than using touch.
We also speculated that users would be more confident and therefore would look less often at the tablet.
However, we did not expect the tangible slider to bring much benefit while users operate them, hence we supposed there would be no measurable difference between the techniques in the pure tracking task.
Therefore, our hypotheses were: * For the tracking + reacquisition task: H1.
The tangible condition yields a lower tracking error than the touch condition.
The tangible slider is faster to reacquire than the touch-operated slider.
The tangible condition yields less glances to the tablet than the touch condition.
Input technique has no effect on tracking error.
Subjects used 3 types of strategy for managing their attention: * 7 subjects looked at the tablet at almost every tap / reacquisition under the touch condition  and almost never looked with the tangible , * 3 subjects tried not to look at the tablet under both techniques , * 2 subjects consistently looked at the tablet under both techniques .
Most subjects who switched their attention to the tablet at each reacquisition did so while acquiring the tap area.
During the touch and tracking + reacquisition condition, two subjects held the iPad in front of them for a few seconds then renounced, and one subject did so during a whole 90s trial.
In the tangible condition, two subjects reported using their whole hand to find the slider's thumb.
Some subjects reported using a strategy consisting in using the edge of the iPad as a frame of reference , especially for the touch condition.
User feedback further suggests that operating the tangible slider once acquired provides a better user experience compared to dragging on the iPad's touch surface.
As for performance, we did not expect the tangible slider to be more effective to operate once acquired, but we did find a trend in the pure tracking task that deserves further examination.
Overall these findings provide another strong case for the use of tangibles.
And yet, although our tangible underwent several iteration cycles and its design has been validated by our experiment, it still does not measure up to commercial physical controls.
One subject felt the slider was "flimsy" and did not move smoothly enough and another felt its resistance was too low.
Tracking errors also occurred, mainly due to a misplacement of the tangible slider.
This is because the virtual slider had to be at a fixed position for the purpose of the experiment, but in practice it would align itself to the tangible.
Furthermore, it focuses on acquisition and manipulation of controls and does not provide any information on how easy it is to lay them out on the tablet and map them to functions, as we illustrate in the next section.
When asked about their preference, 10 subjects replied they preferred the tangible slider and 2 subjects reported having no preference.
Subjects' comments and answers to our questionnaire were consistent with our objective measures.
Two subjects commented that the iPad was too heavy to hold for such a long period of time .
7 subjects commented negatively on the feeling provided by the touch screen while operating the slider  and/or commented positively on the feeling provided by the tangible slider: "the screen was a bit sticky", "after doing the task for a while I felt that the surface of the iPad was too sticky in a way", "at one point my finger was getting sweaty and it was not that easy to move the slider"; "no  friction with the tangible slider.
Our findings strongly suggest that using a tangible rather than a virtual slider on a touch device facilitates interaction with a vertical display that requires visual attention.
The tangible slider was faster to reacquire , which yielded a significantly smaller tracking error in the dual-task condition.
This is despite the fact that the active area of the touch slider spanned the whole height of the slider and thus was larger than the physical thumb.
At the same time, subjects could have used their hand like an area cursor to first find the physical thumb, then switch to a precise grip based on the tactile information they got.
Most importantly, users looked much less often at the tablet when they were using the physical slider .
Note that no critical event could be missed if subjects briefly switched their attention down, and they could easily catch up with the tracking task as they looked back at the target.
Despite the relatively low cost of looking down, most subjects did so only when this was necessary to prevent frequent misacquisitions .
Although most subjects reported feeling more effective when they could maintain their visual attention on the vertical display, it is likely that they kept looking at the display also because it was simply more natural and comfortable.
One subject did report feeling uncomfortable switching back and forth between the two displays in the touch condition and mentioned neck discomfort at the end of the experiment.
We built a prototype system to explore how a tangible remote controller can be used to interact with an actual application on a WSD .
We chose the information visualization system ivtk  because it contains a large number of GUI controls to perform visual exploration tasks .
The WSD server runs a modified version of ivtk that allows the external control of its GUI elements through TUIO .
Upon connection ivtk sends an interface description of the GUI controls for its current visualization as an XML stream to the tablet.
Each GUI control is given a TUIO ID that is listed in the description and then used by the tablet to send update events to ivtk.
Although full spatial multiplexing is often the most effective , it can require an amount of space that is unrealistic for mobile usage.
In some cases, a specific configuration will be used intensively over a period of time.
In the house search scenario, for example, the user might end up with too many or too few results that depend on attributes for which she has no clear preference .
In that case, the user might need to go back-and-forth between these attributes to filter them and observe how this affects the results.
Given the findings from our user study, we expect tangible controls to facilitate this type of task.
Because controls can be freely laid out on the tablet surface, users can experiment with different spatial layouts and chose the ones that are the most meaningful to them.
Although applications already exist that let users customize the layout of toolbars, they require specific interaction techniques and mode switches that are arguably not as natural to end users as simply moving physical controls around.
For example, on Figure 1, the topmost and leftmost sliders are used to change the attributes of the X and Y axes of a scatterplot, and are laid out in a way that maximizes the degree of compatibility between actions and visual responses .
Tangible controls can also be grouped by function to leverage spatial memory or to facilitate their concurrent control.
In this section we focus on end-users who are experts and already know the application's functionalities.
When such a user starts the system, the WSD shows an overview of the dataset and the tablet screen is initially empty.
To start exploring the data the user can take one of the tangible controls available  and stick it to the tablet's screen.
A menu then pops up and shows which functions are available that are compatible with the control .
Our current prototype gives users access to any of the dataset dimensions necessary for visual variable mapping and dynamic queries .
A larger number of functions would require the use of hierarchical menus.
Once assigned, the control displays its function and possibly additional visual information through the display .
The user can then operate the physical control or add new ones.
Each new control only shows those functions that are not already mapped to a control.
In a multi-user setting, the TUIO client keeps track of controls that are already mapped and notifies all connected tablets about changes.
Finally, our controller is wireless and portable, and can therefore be operated from any location in the room, depending on users' needs and on what they deem the most comfortable.
This includes sitting on a chair and resting the controller on their lap or on a table, although rich visualizations on ultra high-resolution WSDs typically encourage users to move forward to see more details .
In addition to "locomotion-based zoom and pan", our remote controller is compatible with location-aware interaction techniques  and can be sequentially combined with a number of surface interaction techniques.
For example, a user operating tangible controls on a tabletop  might need to perform selections on a touch-sensitive wall while keeping access to some of the parameters she was controlling.
With our system she could move some of the tangibles on the tablet and carry the tablet with her as she walks to the wall.
Each physical control can be reassigned to another function by pulling the mapping menu with a swipe gesture.
This support for dynamic remapping of tangible controls allows users to make only those functions available that are of immediate interest to them for the task at hand.
For example, users searching in a multidimensional dataset typically start with the most discriminant attributes , then progressively refine their search with secondary attributes .
A dynamic query control can therefore be mapped to a new attribute once the user is fully satisfied with the settings for the current attribute.
Such a remapping mechanism lies somewhere between fully temporal multiplexing mechanisms where a single control is used to access many functions and fully spatial multiplexing schemes where each function has a physical counterpart .
Our information visualization control system is an initial prototype that has been developed for exploration purposes and to illustrate the feasibility of the approach.
We plan to add extensive support for ivtk functionalities and support non-expert use by adding two mapping modes: one that shows the most important functions on the iPad as virtual widgets on top of which can be added tangible controls, and one where the complete list of functions is displayed in a dedicated scrollable area of the tablet.
We also plan to add support for object selection and search, two other fundamental task in visual exploration, by allowing users to use any free area on the tablet for 2-D pointing and by building a tangible mini-keyboard.
Tangible remote controllers have unique properties that make them well-suited to visual exploration tasks on high-resolution WSDs.
With respect to our requirement R1, they demand little visual attention, especially compared with touch-only solutions.
Our user experiment confirms that, at least for sliders and for low-level tracking tasks, tangible controls are easier to operate when users' attention is focused on a vertical display.
Two other key tasks are object selection  and search  and we plan to support them in the future.
They make it possible to deport to the tablet GUI widgets from existing applications, which can facilitate porting and user knowledge transfer.
Tangible remote controllers can also be customized by end-users, who can choose the controls that are relevant to their task and lay them out in a way that is meaningful to them.
We already mentioned some avenues for future work.
First of all, our experiment provided us with useful findings but more studies must be conducted to generalize and refine our results.
Also, our information visualization control system is currently only a prototype for illustrating the approach.
We are currently improving it in order to conduct observational studies on end users, including in collaborative settings.
Ahlberg, C., Williamson, C., and Shneiderman, B.
Dynamic queries for information exploration: an implementation and evaluation.
Balali Moghaddam, A., Svendsen, J., Tory, M., and Branzan Albu, A.
Integrating touch and near touch interactions for information visualizations.
Ball, R., and North, C. The effects of peripheral vision and physical navigation on large scale visualization.
In Proceedings of graphics interface 2008, Canadian Information Processing Society, 9-16.
Beaudouin-Lafon, M. Instrumental interaction: an interaction model for designing post-wimp user interfaces.
Beaudouin-Lafon, M. Lessons learned from the wild room, a multisurface interactive environment.
Bezerianos, A., and Balakrishnan, R. The vacuum: facilitating the manipulation of distant objects.
Boring, S., Baur, D., Butz, A., Gustafson, S., and Baudisch, P. Touch projector: mobile interaction through video.
Buxton, W., Hill, R., and Rowley, P. Issues and techniques in touch-sensitive tablet input.
A. Slidebar: analysis of a linear input device.
Elmqvist, N., Dragicevic, P., and Fekete, J.-D. Rolling the dice: Multidimensional visual exploration using scatterplot matrix navigation.
Fitzmaurice, G. W., and Buxton, W. An empirical evaluation of graspable user interfaces: towards specialized, space-multiplexed input.
Fitzmaurice, G. W., Ishii, H., and Buxton, W. A. S. Bricks: laying the foundations for graspable user interfaces.
Greenberg, S., and Fitchett, C. Phidgets: Easy development of physical interfaces through physical widgets.
Hinckley, K., Pausch, R., and Proffitt, D. Attention and visual feedback: the bimanual frame of reference.
Johanson, B., Fox, A., and Winograd, T. The interactive workspaces project: Experiences with ubiquitous computing rooms.
Kaltenbrunner, M., Bovermann, T., Bencina, R., and Costanza, E. Tuio: A protocol for table-top tangible user interfaces.
Khan, A., Fitzmaurice, G., Almeida, D., Burtnyk, N., and Kurtenbach, G. A remote control interface for large displays.
Kratz, S., Westermann, T., Rohs, M., and Essl, G. Capwidgets: Tangible widgets versus multi-touch controls on mobile devices.
The design of a gui paradigm based on tablets, two-hands, and transparency.
Leitner, J., and Haller, M. Geckos: combining magnets and pressure images to enable new tangible-object design and interaction.
Malik, S., Ranjan, A., and Balakrishnan, R. Interacting with large displays from a distance with vision-tracked multi-finger gestural input.
A., Stiel, H., and Gargiulo, R. Collaboration using multiple pdas connected to a pc.
Nancel, M., Wagner, J., Pietriga, E., Chapuis, O., and Mackay, W. Mid-air pan-and-zoom on wall-sized displays.
A multiple device approach for supporting whiteboard-based interactions.
Rekimoto, J. Smartskin: an infrastructure for freehand manipulation on interactive surfaces.
Sanneblad, J., and Holmquist, L. E. Ubiquitous graphics: combining hand-held and wall-size displays to interact with large images.
Smith, G. M., and schraefel, m. c. The radial scroll tool: scrolling support for stylus- or touch-based document navigation.
Streitz, N. A., Geissler, J., Holmer, T., Konomi, S., M uller-Tomfelde, C., Reischl, W., Rexroth, P., Seitz, P., and Steinmetz, R. i-land: an interactive landscape for creativity and innovation.
Tuddenham, P., Kirk, D., and Izadi, S. Graspables revisited: multi-touch vs. tangible input for tabletop displays in acquisition and manipulation tasks.
Ullmer, B., Ishii, H., and Jacob, R. Tangible query interfaces: Physically constrained tokens for manipulating database queries.
Villar, N., and Gellersen, H. A malleable control structure for softwired user interfaces.
Vogel, D., and Balakrishnan, R. Distant freehand pointing and clicking on very large, high resolution displays.
SLAP widgets: bridging the gap between virtual and physical controls on tabletops.
Yu, N., Tsai, S., Hsiao, I., Tsai, D., Lee, M., Chen, M., and Hung, Y. Clip-on gadgets: expanding multi-touch interaction area with unpowered tactile controls.
