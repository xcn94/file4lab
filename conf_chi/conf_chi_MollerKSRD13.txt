Self-reporting techniques, such as data logging or a diary, are frequently used in long-term studies, but prone to subjects' forgetfulness and other sources of inaccuracy.
We conducted a six-week self-reporting study on smartphone usage in order to investigate the accuracy of self-reported information, and used logged data as ground truth to compare the subjects' reports against.
Subjects never recorded more than 70% and, depending on the requested reporting interval, down to less than 40% of actual app usages.
They significantly overestimated how long they used apps.
While subjects forgot self-reports when no automatic reminders were sent, a high reporting frequency was perceived as uncomfortable and burdensome.
Most significantly, self-reporting even changed the actual app usage of users and hence can lead to deceptive measures if a study relies on no other data sources.
With this contribution, we provide empirical quantitative long-term data on the reliability of self-reported data collected with mobile devices.
We aim to make researchers aware of the caveats of self-reporting and give recommendations for maximizing the reliability of results when conducting large-scale, long-term app usage studies.
By contrast, systems that interact with the environment, social software, location-based services and ubiquitous computing systems must in many cases be evaluated `in the wild' in order to get an impression of how they work and how users work with them.
Researchers are often interested in usage patterns, adaptation processes and learning curves - briefly, in users' behavior with a system in context.
Long-term studies, lasting over weeks, months, or even years, are required to answer such questions Researchers have just begun to carry out `Research in the Large' , to be able to deploy and test systems, e.g.
Among the numerous established techniques for acquiring information in user studies , data logging, the experience sampling method  and the diary particularly have proven useful for long-term data gathering, as no experimenter needs to be present for those methods.
With data logging, a device collects data or context information automatically and without user intervention .
This technique can record information which is difficult to gather otherwise efficiently in cost and time .
Examples are all sorts of quantitative measures like usage data of applications, or fine-grained context information.
Its unobtrusiveness  entails high data validity .
However, researchers cannot apprehend users' intentions through logging alone , so that interpretation of the recorded data and the combination with other techniques is often required.
With experience sampling, participants actively collect insitu data upon request , that are scheduled randomly, time-based, or triggered by specific events .
Data can comprise photos, videos, audio recordings, sensor readings, but also annotations and questionnaires that clarify subjects' thoughts.
However, experience sampling can be highly interruptive and burdensome if the sampling rate is high and the study is conducted over a long time.
A diary diminishes this problem, as it allows users to decide on their own when to record data .
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
While diaries can be unstructured or structured , they bear the risk that subjects do not remember all events throughout the day if they, e.g., only make one entry every evening, or that they might refrain from writing down certain events, such as, e.g.
A structured exploration of how reliable self-reported information is under different conditions has not been conducted so far to our knowledge.
In our work, we systematically investigate the reliability of self-reporting, in comparison to logged data as ground truth.
In a six-week study, participants reported on their usage of Facebook and Mail on the smartphone .
We chose mobile app usage as a scenario for our investigation, since ground truth data can here easily be obtained.
We thereby extend current research trends to conduct research in the large e.g.
We add to this quantitative data on the accuracy of self-reported data.
We were thereby interested not only in the overall accuracy of self-reported data , but also in time-dependent effects .
We compared different self-reporting intensities with 3 groups, from voluntary reports over daily reminders to regularly presented automated questionnaires, and thereby provide first quantitative results for the reliability of self-reporting.
The paper is structured as follows: We begin with an overview on related work, focusing on self-reporting and logging with smartphones.
We then introduce SERENA, our questionnaire framework, and subsequently describe our study in which we compare the accuracy of self-reporting and logging.
The results are presented and discussed in detail, before we summarize the lessons learned and give an outlook to future work.
As an additional contribution to the community, we offer the software we developed for our experiment, SERENA, as free-to-use framework .
It consists of a study app and a web service, that support both user data logging and self-reporting through questionnaires.
We hope that other researchers find our tools handy and can use them in their own research and studies.
Mobile Probes  added the possibility to include images to experience sampling records.
As introduced earlier, self-reporting potentially does not represent the actual behavior of subjects and can be subject to various distorting effects.
For example, Lester et al.
Therefore, self-reporting is often combined with data logging.
The authors found significant biases in selfreports, e.g.
Other scenarios where logging has been employed are mobile device interaction , and usage patterns  or life logging .
Although logging potentially provides information with higher accuracy than selfreporting, it can easily be perceived as a privacy threat by subjects  or result in a useless amount of log data.
Smartphones have evolved to a convenient tool for selfreporting, as many people use them on a regular basis and have them readily available all the time.
A meta analysis conducted by Hufford and Shields  revealed that electronic diaries result in a higher compliance with subjects and in better results.
Meanwhile, there exists a battery of research applications for collecting user-reported data.
MyExperience  is an in situ data capturing tool for experience sampling.
It logs over 140 event types related to the context, the environment and the usage of the phone.
Based on previously specified triggers, it is able to make screenshots or to display questionnaires.
Through the observation of the user's context, questionnaires can be prevented in inconvenient situations, and correlations between response behavior and certain events can be observed.
Momento  is a mobile application that allows both study participants and experimenters to gather diary or experience sampling data.
Collected information  can be sent over the web, or using SMS/MMS to the desktop counterpart of the system.
The mobile client communicates bidirectionally with participants and can receive surveys.
However, no simple way to design questionnaires is integrated into the tool.
With the ContextPhone platform , developers can build context-sensing applications out of pre-existing modules.
The tool turned out helpful to researchers for the creation of study applications, e.g.
The commercial application droid Survey1 displays questionnaires to participants that can be generated with different templates.
It offers the additional functionality to record videos and to ask 13 different question types.
It is offered for Android and Windows Mobile.
EpiCollect4 is a free data collection tool for Android and iOS.
It offers gathering data through questionnaires and to view it online or on the phone.
The tool offers GPS functionality and four question types.
The entries made can be reviewed on the phone.
Moreover, a possibility to communicate with participants via Google Talk was added.
We developed a toolbox supporting data collection in longterm studies that we call SERENA .
SERENA combines selfreporting through questionnaires and automated data logging in a singular smartphone application.
The Android app can be customized exactly to the experimental needs through remote survey management, adaption of logging and automated data upload to a backend.
Each participant just needs to install SERENA on his or her smartphone at the beginning of the experiment, which greatly simplifies setting up studies.
While we initially built SERENA for the study described later in this paper, we designed it flexible enough to support a variety of possible study designs and anticipate to use it in future work as well.
The SERENA software and a brief tutorial is available at our website at https://vmi.lmt.ei.tum.de/serena.
By making SERERA available to the community, we encourage also other researchers to use it for their work.
We assume that the required consensus for data acquisition is obtained a priori by the researchers using this tool.
Group IDs also allow running completely independent studies in parallel.
Moreover, event logging can be activated.
Currently, SERENA records app usage and location information.
A commaseparated list of Android app activity and package names can be specified .
Thereby, we can adjust logging on sub-application level and limit observations to applications that are interesting for the study, preserving subjects' privacy as much as possible.
The configuration tool generates and exports a model describing the setup and questionnaires, which can either be used to build a ready-to-use app that is distributed to participants, or to remotely configure app instances that are already installed on subjects' devices.
We used an extensible XML-based description format to specify SERENA's functionality.
Prior to the experiment, the experimenter can create sets of questionnaires that will be used to collect information in the study.
Each questionnaire consists of a series of pages that can have one of seven types .
Questionnaires are configured to be voluntary, interval-based or event-based.
While voluntary questionnaires serve, e.g., for ESM-like methods, interval-based surveys facilitate also the conduction of diary studies.
Additionally, the experimenter can assign a group ID to each questionnaire and specify in which timespan it is valid .
With these features, SERENA supports multiple conditions in within-subjects and between-subjects study designs.
A within-subjects study can be realized by multiple questionnaires with different time intervals.
For example, participants receive questionnaire A in the first part and questionnaire B in the second part of the study.
For a between-subjects study, questionnaires can be assigned different group IDs.
From the main menu of the SERENA smartphone application, users can view and answer questionnaires which are currently active.
Questionnaires that are added or removed by the experimenter during the study appear or disappear automatically in this list.
Interval-triggered questionnaires are presented to the user in regular, previously specified intervals  using a notification that appears on the top of the screen.
Upon a click on the message, the questionnaire opens up.
With the notification, the user is given the possibility to postpone answering the questionnaire in order to minimize disturbance in an ongoing task.
Event-triggered questionnaires are shown automatically after a certain event has occurred, e.g.
The questionnaire pops up directly afterwards.
In the pilot, we identified and fixed some technical and usability issues in the first version of SERENA.
For example, we decided to keep all log files on the device in addition to uploading them to the server, since logs of one subject had arrived incomplete on the server.
The Free Text and Range questionnaire pages were redesigned for better usability.
The backend of our self-reporting toolkit where questionnaires can be created and managed.
The experimenter can, e.g., specify the interval after which subjects are notified to answer a questionnaire, and specify filters for event logging on the smartphone.
Questionnaires can then be sent to the mobile SERENA app .
A background service logs the currently active app package and activity name with timestamp information, according to the previously specified filters by the researcher.
Log files are saved to the SD card and are regularly uploaded to the server.
In case an application to be logged is not installed on the device, an Intent to the application's download page in the Google Play store is sent, so that the user has the possibility to download it.
If communication with the server fails, the user is notified to restore connectivity and to upload questionnaires manually using a menu item within the app.
All logs are saved locally, so that the researcher can collect the data from participants' phones manually in case automatic upload is not possible at all.
For each user, a unique ID is created.
This ID  allows the experimenter to uniquely identify from which device data was received.
Questionnaire replies and logged information can thereby be related to individuals without revealing the identity of the person.
Since smartphone usage can very well be assessed automatically through logging , we chose this scenario to gain reliable reference data that we used to compare against self-reported information.
While SERENA is not limited to any specific app, we constrained our analysis to two applications, in order to make self-reporting not too excessive.
We chose Facebook and Mail, since we assumed that they are frequently used by the majority of smartphone owners.
Self-reports were collected through Facebook questionnaires and Mail questionnaires, which were filled out according to the study condition.
In our study, self-reports were used only as vehicle to assess subjects' reporting behavior, so that we kept the questions simple.
Subjects were asked to estimate how long they had just used Facebook or Mail, and how often they had used Facebook or Mail without filling out a questionnaire.
Given that a user has started e.g.
Facebook three times and answered three questionnaires on those usages, we call those direct self-reports.
If she only fills out a questionnaire after the third usage, indicating that she has used Facebook three times, the former two app usages are considered to be reported indirectly; we hence call them indirect self-reports.
The study consisted of three conditions  that correspond to different `intensity levels' of self-reporting.
The backend is built with the Python Pyramid framework5 and a MySQL database.
The mobile SERENA app is implemented in Android, supporting API level 10 .
App and server communicate using JSON over HTTP.
In the Voluntary condition, users were reminded only once prior to the study to report on their application usage.
Every time they used either Facebook or Mail, they were instructed to fill out a short questionnaire.
However, they were never actively reminded throughout the study  to do so.
In this condition, subjects were, similar to Voluntary, not actively reminded to fill out a questionnaire after each application use.
However, a reminder notification appeared once a day .
The reminder only showed up if reporting has been missed at least once since the previous reminder.
Approximately one second  after returning to the home screen, a questionnaire opened automatically, why we call this the event-based condition.
In some cases, no questionnaire appeared.
This was the case when a user did not quit the application using the home button, but started another application using the application switcher, when another application was started immediately after quitting the previous one , or when another application was started automatically from Facebook or Mail using an Android Intent .
We required that participants own an Android smartphone and use email and the Facebook application with it.
30 subjects between 18 and 32 years , most of them students, participated in the study.
8 were females, 22 were males.
Participants were randomly assigned to one of the three conditions , so that n = 10 for each condition .
Participants were asked to install SERENA on their personal smartphone and to use the phone as usual during the study period.
Participants could come into our lab for installation assistance.
All other communication throughout the study was conducted via email.
Prior to the study, participants filled out a short questionnaire regarding their own smartphone usage.
Another questionnaire was filled out at the end of the study.
All questionnaires were sent to the experimenters through SERENA and could be matched to individual participants only through a unique ID.
In the course of the six-week study period, a reminder email was sent every two weeks, thanking subjects for their participation so far and indicating how long the study would still last.
At the end of the study, participants could choose a small gift for compensation .
We deliberately decided for a modest compensation in order not to influence results through the incentive.
In all three conditions, SERENA logged participants' actual app usage, filtered by the package names of the official Android Facebook application  and the name of the mail application the subject used.
Since a variety of different mail clients exist on Android  we asked prior to the study which application subjects use in order to include them to the logging whitelist.
If a participant used multiple mail clients on his or her device , both were aggregated and referred to as Mail usage.
In case subjects switched back and forth between applications within a short period of time, we aggregated subsequent usages of the same app and counted them as singular app usage, summing up individual usage times.
We assumed a single task if users returned to the original app within 60 seconds.
This was the case, e.g., when subjects were composing an email, looked something up in another app, and switched back to finish the email.
Often, those other applications were launched programmatically using an Intent, e.g.
Hypotheses 1 and 2 address the reliability of self-reporting compared to the actual behavior of subjects, with relation to different self-reporting conditions.
We thereby look at the self-reporting ratio, i.e., the ratio of reported and actually occurred app usages, and the estimated app usage durations of subjects in relation to actual durations.
Furthermore, there might be an effect with relation to time.
There are different possible patterns, such as an increase or decrease of self-reporting rates, or a decrease at the beginning and a rise towards the end .
Reporting could also influence actual app usage, i.e., subjects could use apps less because they have to answer a questionnaire afterwards.
This leads us to the following hypotheses.
Finally, we address users' perception and acceptance of the self-reporting process with the following hypotheses:
In total, 3,631 Mail usages and 3,181 Facebook usages were logged during the study.
For the following statistics, we do not average over participants, but look at individual usages.
Figure 3 illustrates the ratios of self-reported app usages in relation to the logged usages.
The bottom, darker-colored portions of the columns in Figure 3 represent direct reports .
The top, light-colored portions illustrate indirect reports, so that the columns in total represent the amount of all reported usages.
Facebook usages were in total reported at 37.6% in Voluntary, at 63.8% in Interval and at 54.3% in Event.
As previously described, not every application usage in the Event condition entailed a questionnaire notification.
Ratio of self-reported and measured app usage in the conditions Voluntary, Interval and Event.
AutoOpen denotes the amount of filled out questionnaires that have been opened automatically in the Event condition.
Direct self-reports represent the number of filled out questionnaires, while direct and indirect self-reports also comprise app usages that have been caught up in a subsequent questionnaire.
Of those AutoOpen questionnaires, 95.9% were answered for Facebook and 92.8% for Mail.
The portion of indirect self-reports was higher for Facebook than for Mail, and it was particularly high in the Interval condition.
Presumably, Facebook is more often used at a more unconscious level and has so much passed into subjects' habits and naturally integrated in their phone interaction that subjects did not remember filling out a questionnaire right afterwards.
This also explains why particularly in Interval, where only one reminder a day was sent, so many Facebook reports were forgotten.
A Student's t-test  showed no significant differences between the reported ratios in Voluntary, Interval, and Event.
Although the number of self-reporting reminders rose from Voluntary over Interval to Event, the number of actually reported events did not.
In Interval, subjects reported more than in Voluntary, which we hypothesized in Hypothesis 1 .
Interestingly, they reported less in Event than in Interval, which does not confirm Hypothesis H1a.
However, Hypothesis H1a could be accepted if we look only at the report rate where the questionnaire had appeared automatically.
Subjects overestimated the usage duration in their self-reports in all conditions.
Mail usage durations were overestimated even further.
For Mail, reported and logged durations were significantly different in all conditions.
We hypothesized that subjects overestimate their actual app usage when reporting on their behavior, which was already suggested by previous findings .
In fact, subjects overestimated app usage durations mostly by more than 100%, so that Hypothesis H2 is likely to be correct.
Figure 5 illustrates the self-reporting behavior in the course of the study.
The diagrams illustrate the direct self-report ratios  and direct plus indirect self-report ratios , aggregated for each week.
The general trend of the self-reporting ratio is decreasing in all conditions, except for Mail usage reports in the Interval condition.
We have no explanation for this exception, compared to the other conditions.
In the first week, subjects filled out a questionnaire at between 38.6%  and at 61.4%  of all usages.
Considering also indirect reports, the highest reporting rate is 78.1% .
In the second week, self-reporting rates decrease in average by 9.4% and remain almost constant between week 2 and 3 .
Figure 4 summarizes logged and reported Facebook and Mail usage durations.
Facebook has generally been used at least twice as long as Mail.
In Voluntary, Facebook has been used averagely for 1:29 minutes and Mail for 33 seconds.
In Interval, Facebook has been used averagely for 1:29 minutes at a time, Mail only for 37 seconds.
The average usage times in Event were 1:22 minutes for Facebook and 35 seconds for Mail.
Amount of self-reported and logged application usage for each week in the conditions Voluntary, Interval and Event.
Direct self-reports represent the number of filled out questionnaires, while direct and indirect self-reports also comprise app usages that have been caught up in a subsequent questionnaire.
Subjects mostly liked the way of voluntary self-reporting.
P3 found it "fast and playful" and liked that it is "low effort and can be filled out any time".
P6 said that "it's simple and is actually uncomplicated.
There's not much interface necessary."
Some subjects would have preferred some kind of automation, e.g.
P5: "I'd have preferred to be asked automatically, once or several times per day, to fill out a survey.
Logged App Usage Cause I was in a hurry or I forgot it fairly often, I didn't fill 400 out the questionnaire each time.
A daily notification would 300 have been sufficient for me."
For five subjects, usage habits 200 did not change.
However, five reported to use apps shorter or 100 less frequently.
P1 stated to "use apps more consciously, only when I really wanted to use them and not just started them be0 Ohne Titel 1 Ohne Titel 4 cause there was nothing to do".
According to P10, the effect was "no endless surfing any more and reduced usage".
In the last week, reporting rates range between 21.4%  and 67.2%  for direct self-reports, and 74.5%  when indirect self-reports are considered as well.
As already outlined in Figure 3, reporting rates start and remain lowest in the Voluntary condition.
They begin at a comparable level in Interval and Event, but rates decrease more in the Event condition that in Interval.
The tendencies in the results coincide with Hypothesis H3, where we hypothesized that self-reports decrease in the course of the study.
Similar to Voluntary, most participants in this group stated to like this way of reporting.
One subject  even would have favored Event, stating that "questionnaires should pop up by default after the app".
Some participants found the effect of self-reporting interesting.
P13 said that it was "interesting to yourself how often you open the apps" and found the effort "acceptable".
P17 admitted to have become sloppy with the time, but did not perceive questionnaires annoying.
However, six of ten participants in this group stated to have changed their behavior, e.g.
P12 stated to "not have looked at every single mail, and moved Facebook usage to the PC".
Figure 6 summarizes the actual number of Facebook and Mail sessions determined by logging.
Between week 1 and 2, usages decreased in all conditions, partly by more than 50%.
This trend partly continues until week 4; however, towards the end of the study, app usages rise again in all conditions.
In fact, subjects stated that their behavior was influenced by self-reporting.
This effect was stronger in Interval than in Voluntary, and stronger in Event than in Interval.
Comments from participants in this condition were rather critical.
P23 said: "Too time-intensive and complicated.
Periodically answering the same questions over and over again is annoying.
Sometimes, you even avoid using those apps."
Another user suggested to use less clicks in the questionnaire to make self-reporting more convenient.
P26 did not like event-based reporting because it was "...too annoying" and he would "prefer background logging".
Nine out of ten subjects also indicated that they used Facebook and Mail differently or at least thought about it.
P21 said: "I partly looked up e-mails at my PC when I was too lazy to fill out the questionnaire on the mobile".
Other participants reported to have used the apps significantly less, especially towards the end of the study .
P23 stated to "often have read only the notification but not started the app any more".
Unlike most other participants in the Event condition, P28 was happy about the increased awareness of app usage: "I now know how much time I've wasted with that!
I should waste my time with other things."
By week 3, the amount rose to 30%, by week 4 to 47%, and in week 5, more than half of participants  admitted to not regularly fill out questionnaires any more.
In the last week, the estimation was slightly lower again; 43% of subjects stated to have missed questionnaires.
While the rise of self-reports in week 6 can not be confirmed by the measured data, the estimation matches the decrease in the measured self-report ratio at the beginning of the study.
It is normal that app usages vary over time, depending on diverse factors .
However, in light of subjects' statements to actually have used applications less because of the study, there is evidence that selfreports can actually have influenced subjects' behavior.
Hypothesis H4 is therefore likely to be rejected.
Regarding Hypothesis H5, we can state that the effort to answer questionnaires was actually perceived higher in Event than in Interval, and in Interval higher than in Voluntary, as we expected.
However, the differences are smaller as anticipated, which matches also subjects' feedback that they felt reports burdening already in Interval or even Voluntary.
At the end of the study, subjects answered a final questionnaire in which they indicated how satisfied they were with self-reporting during the study.
Questions were answered on a Likert scale ranging from 1  to 5 .
The results are summarized in Figure 8.
Self-reporting was perceived less burdening in Voluntary than in Interval and Event.
Subjects responded to the statement "Answering the questionnaire was low effort" in Voluntary with averagely 4.1 , in Interval with 3.8 , and in Event with 3.7  and 3.4 .
Subjects responded above average that they always filled out the questionnaire after using Facebook or Mail.
The effect on application usage habits was rated differently depending on conditions.
In Voluntary, subjects agreed that answering questionnaires changed usage habits below average with 2.3 .
In Event, the estimation that reporting changed usage habits was highest: average agreement was 3.5 for Facebook and 2.9 for Mail.
Our study showed that self-reports on application usage can generally not be considered as accurate.
Depending on the condition, only approximately 40% to 70% of actual app usages were reported by subjects within six weeks.
Subjects responded to more than 90% of all questionnaires that automatically popped up in the Event condition, but this frequency of automated questionnaires would most likely not be feasible in a long-term study, since it would annoy users too much.
Already in our setup, where only in 49% of app usages in the Event condition a questionnaire appeared, subjects found reporting too time-intensive and annoying.
While some participants in Interval wished for automated surveys to prevent that they forget to report , subjects in Event felt overly burdened and wished for a larger interval between reports, or for logging.
Thus, there are probably two explanations for the rather low reporting rates in all conditions: in Voluntary, subjects forgot about reporting; in Event, they deliberately did not answer questionnaires because it was too much effort to them.
Based on our findings, we summarize some lessons learned, which can guide further usage of self-reporting in user studies and outline directions for future work.
Researchers must not blindly trust self-reported data, but take into account that this data can be unreliable.
Self-reports are a valuable data collection method in long-term studies and should be employed, but researchers should take into account that a corrective factor might be necessary when analyzing and interpreting the results.
For example, estimated usage times in our study have shown that subjects overestimate the duration of app sessions, and these results stand in line with earlier research .
Reliability also differs according to the scenario.
In our study, in particular many Facebook usages were missed by subjects' self-reports, which might be due to the more unconscious nature of mobile Facebook usage as of today.
Some subjects confirmed that they were not aware of their usage frequency before, and that only through self-reporting they initially became conscious of how often they log in to Facebook.
The data collection method should thus carefully be adapted to the scenario and the actual data to be gathered.
If, e.g., just random experiences or impressions should be collected, it can even be preferential when users do not report too often, because researchers can then learn which moments are salient to subjects.
However, when quantitative data or "instances" should be captured, self-logging can provide unreliable and incomplete data.
Often, studies strive for both quantitative and qualitative data.
For smartphone usage, logging can capture a variety of usage information in an unobtrusive way.
This is, however, not possible in all scenarios, for which self-reporting can then be an option to obtain data.
In order to assess the accuracy of self-reports as a qualitative method, we chose app usage as a criterion that can be compared to quantitative logs as ground truth.
Diary reports might not reflect entirely reliable usage frequencies, but this does not make them less reliable for the assessment of the experiences recorded.
Self-reports have their unique advantage for gaining additional insights which cannot be obtained in an automated way.
We showed that self-reporting can even influence actual behavior, in our case the usage frequency of the observed apps .
If possible, researchers should therefore consider a combination of self-reports and logged data to achieve additional certainty.
As of today, where self-reports are often recorded with smartphones, automated collection and logging of information is in many cases a small extra effort.
We have compared the reliability of self-reporting methods regarding smartphone app usage with different reporting intervals , using log data as ground truth.
To our knowledge, we provide the first quantitative analysis of the accuracy of self-reporting with mobile phones in long-term studies.
Our six-week experiment showed that self-reports do not provide a complete image of actual application usage and that subjects misestimate durations of application sessions.
The self-reporting interval thereby plays a less important role than hypothesized.
A lower required report rate can, on the contrary, even lead to better results .
Results also strongly differ on the task; Facebook usage was e.g.
Nonetheless, self-reporting is an important data collection technique, in particular in scenarios when no automated logging can be employed.
It is a substantial method for recording subjective and qualitative experiences with an application.
The selective nature of self-reports also helps to identify what is important to users.
Researchers should, however, be aware of the potential inaccuracy, dependent on the scenario, when using self-reports.
Future work thus should deeper investigate dependencies between reporting conditions and task types, in order to maximize reporting reliability.
Since inconvenient self-reporting modalities can influence the behavior that is to be logged, participants' satisfaction is crucial to successful self-reporting.
Do not use an overly high self-reporting intensity or interval.
In our study, report rates already started below 70% and decreased from the second week on, why dense self-reports from participants are hard to justify.
If the burden is too high, participants will get annoyed so that they refrain from reporting, or more severely, the alter their actual behavior.
Subjects stated to have reduced the usage of applications in order to reduce the logging effort.
Further analyses are necessary how self-reporting can be designed to be convenient from the beginning in order to keep users engaged.
Our results show that reporting rates in Interval were in average higher than in the more demanding Event condition, suggesting that less `pressure' can lead to even more satisfying results.
Data collected by SERENA could help here in future research.
The reporting rate over six weeks tended to decrease in general.
However, it is also notable that a relative increase in the second half of the study could be observed, which might correspond to the reminder emails sent after week 2 and 4, and with a guilty conscience of participants who had neglected reporting and now had a stronger sense of duty towards the end.
Subsequent work could systematically investigate how reminders can be adapted  to cause significant effects.
Barkhuus, L., and Polichar, V. E. Empowerment through Seamfulness: Smart Phones in Everyday Life.
An Introduction to Computerized Experience Sampling in Psychology.
Conference on Human Computer Interaction with Mobile Devices and Services, ACM , 47-56.
Bolger, N., Davis, A., and Rafaeli, E. Diary Methods: Capturing Life as it is Lived.
When Participants do the Capturing: the Role of Media in Diary Studies.
Carter, S., Mankoff, J., and Heer, J. Momento: Support for Situated Ubicomp Experimentation.
Conducting In Situ Evaluations for and with Ubiquitous Computing Technologies.
Deane, F. P., Podd, J., and Henderson, R. D. Relationship between Self-Report and Log Data Estimates of Information System Usage.
Demumieux, R., and Losquin, P. Gather Customer's Real Usage on Mobile Phones.
Conference on Human Computer Interaction with Mobile Devices and Services, ACM , 267-270.
Eagle, N., Pentland, A. S., and Lazer, D. Inferring Friendship Network Structure by using Mobile Phone Data.
Falaki, H., Mahajan, R., Kandula, S., Lymberopoulos, D., Govindan, R., and Estrin, D. Diversity in Smartphone Usage.
A. MyExperience: A System For In Situ Tracing and Capturing of User Feedback on Mobile Phones.
Conference on Mobile Systems, Applications and Services, ACM , 57-70.
Hartley, C., Brecht, M., Pagerey, P., Weeks, G., Chapanis, A., and Hoecker, D. Subjective Time Estimates of Work Tasks by Office Workers.
Henze, N., Rukzio, E., and Boll, S. Observational and Experimental Investigation of Typing Behaviour Using Virtual Keyboards for Mobile Devices.
Hufford, M. R., and Shields, A. L. Electronic Diaries: Applications and What Works in the Field.
Hulkko, S., Mattelm aki, T., Virtanen, K., and Keinonen, T. Mobile Probes.
Jansen, B. J., Taksa, I., and Spink, A. Handbook of Research on Web Log Analysis.
User segmentation & UI Optimization Through Mobile Phone Log Analysis.
Conference on Human Computer Interaction with Mobile Devices and Services, ACM Press , 495-496.
K arkk ainen, T., Vaittinen, T., and  n V aa anen-Vainio-Mattila, K. I Don't Mind Being Logged, But Want to Remain in Control: A Field Study of Mobile Activity and Context Logging.
Krumm, J. Ubiquitous Computing Fundamentals.
Lazar, J., Feng, J. H., and Hochheiser, H. Research Methods in Human-Computer Interaction.
Lester, J., Choudhury, T., and Borriello, G. A Practical Approach to Recognizing Physical Activities.
Mankoff, J., and Carter, S. Crossing Qualitative and Quantitative Evaluation in the Domain of Ubiquitous Computing.
In CHI2005 Workshop "Usage analysis: Combining Logging and Qualitative Methods" .
M oller, A., Michahelles, F., Diewald, S., Roalter, L., and Kranz, M. Update Behavior in App Markets and Security Implications: A Case Study in Google Play.
Workshop on Research in the Large.
Palen, L., and Salzman, M. Voice-Mail Diary Studies for Naturalistic Data Capture Under Mobile Conditions.
Raento, M., Oulasvirta, A., Petit, R., and Toivonen, H. ContextPhone: A Prototyping Platform for Context-Aware Mobile Applications.
Robinson, J. P., and Godbey, G. Time For Life: The Surprising Ways Americans Use Their Time.
Penn State University Press, 1997.
An Evaluation of Food Items Input Into an Electronic Food Monitoring Application.
