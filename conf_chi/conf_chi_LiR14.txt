Annotating documents has long been a widely used strategy for distilling important contents and externalizing related thoughts and ideas in context.
No one has studied the activity of annotating dynamic texts, such as online chat, although online conversation is an important communication media for global companies.
In this paper, we investigate Instant Annotation , a real-time annotation-enhanced chat tool.
We contrast the use of the enhanced chat tool to a standard chat tool for multilingual groups doing a brainstorming and decision-making task.
Results show that group satisfaction and perceived control of the conversation are enhanced for the participants who used IA.
We also report new patterns of annotation use and discuss design implications for group chat tools.
They found that users were not very good at multitasking between tagging and active participation in the meetings, presumably because of the extra cognitive demands.
In a previous work , we described a different approach to collaborative annotation - a design concept called Instant Annotation .
The IA design features a collaborative annotation sidebar that can be used to react to the text-based utterances of a group online chat.
In the group discussion snippet shown in Figure 1, two collaborators are discussing a shared research project, and the annotations mark an important piece of content , a meeting proposal, an idea and reaction, and a to-do item.
Collaborative annotation tools provide users the ability to connect text-based comments or other reactions to an anchoring context in the source material; the annotations may also offer an additional space for interaction among users.
Studies of collaborative annotation tools suggest that serious challenges arise for designers when the annotation features must be integrated with real-time communication and collaboration .
Because synchronous interaction already demands considerable attention, adding annotation as a subtask may be difficult; even more demand arises from the implied need to attend to and interact with others via the resulting annotations.
For instance, Kelkar et al.
Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted.
To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Screen capture of the IA prototype; annotations are in the side bar on the left.
Each annotation section in the IA sidebar is anchored to its context  in the main chat window next to it.
Users simply click on a section to make it active and type in an annotation.
If a user wants to attach a second annotation to the same context, it is stacked immediately below the existing one.
At any time, users can scan annotations currently in view or scroll up in the chat log to see previous annotations.
For long or stacked  annotation lists, the user simply mouses over the annotation section to expand it and show the full listing.
In this paper, we report whether and how a working version of the IA design might enhance brainstorming and decisionmaking tasks for small online groups.
Specifically, we used the prototype to explore three research questions: How is an IA chat tool used?
How is user experience affected by the IA functionality?
What are the costs of IA in group chats?
We collected a mix of dependent measures, including the types of annotations made, ratings of discussion quality and satisfaction, and comments from a post-task interview.
Participant nationality  was a second research variable.
In an earlier study  we found that in multilingual group chats , the imbalances in English proficiency led to communication problems.
Reasoning from participants' reactions to a design sketch of the IA concept , we hypothesized that the IA space might provide a second communication channel in online chats, and that it might compensate for some of the communication problems of non-native speakers.
We did open coding on annotations and interview transcripts to find common themes.
A second pass over the interview transcripts' open codes was performed using axial coding to look for data that could help to explain interesting results in the quantitative analysis.
The number of annotations generated by the four IA groups varied from 12 to 23, for a total of 90.
Thus participants did indeed use the IA function, but there was also considerable variability in how much groups employed the feature.
Table 1 summarizes the ways in which annotations were used, organized into five major use patterns.
We adapted a task from Freiermuth and Douglas .
Four participants formed a group to discuss how to spend $5000 to support environmental sustainability; they were told to generate at least eight ideas in 15 minutes.
At the end they were asked to choose the three best ideas.
In one condition , participants used AIM, which is a standard chatting tool.
In the other , participants used the IA tool depicted in Figure 1.
After the task, participants completed a post-experiment survey.
Finally, individuals were interviewed separately about their experiences.
The post-experiment survey included 10 Likert-style rating scales adapted from a prior study of online group discussion ; scale values ranged from 1= Strongly Disagree to 5=Strongly Agree.
Six scales assessed communication quality ; four others assessed satisfaction .
Each condition included five groups of four participants; each group included two native and two non-native speakers.
The groups in the two conditions were constructed to be similar in terms of education level, gender, and language proficiency.
This enabled a valid comparison of individuals' group chat behavior and reactions when using two different chat tools, for native and non-native speakers.
Note that even though IA was designed as a side channel next to the main chat, people sometimes used the space to expand on an idea or propose a new idea.
To better understand why users might add information in the IA space rather than in the main chatting window, we carefully examined the five cases of Add Information.
In one case, a new idea was proposed in the side bar.
The new idea pertained to the planning theme that was under discussion at that point in the main chatting window.
In this group, group members had first identified several themes, then went back to develop ideas within each theme.
The annotation was one such idea, suggesting that it was a convenient place to inject new ideas into what was already in the chat.
In other cases, Add Information annotations extended an idea that appeared in the chat; they were positioned right next to the original idea.
These examples suggest that users' motivation for adding information in the IA space was to organize discussion content by topics or themes, perhaps with the result that these points of discussion also became more visually salient.
Looking more generally at the kinds of annotations produced it is clear that the most popular use was Summarize, which accounted for 71% of the annotations.
Using the IA space, she could add to any point at any time, even after the group moved on to other topics.
When other people saw her annotation, they might move back to the discussion point she commented on.
This happened twice in one group, where a participant suggested that the intent of a comment like `Nice idea!'
However, in the IA space, such a comment could be posted next to an idea, causing the group to further discuss the idea that had been called out as a good one.
A design implication from this observation is that tools for group brainstorming should provide secondary channels for users who are slower in expressing ideas, to help them contribute to rapid discussions.
To explore user experience impacts of the IA tool, we analyzed participants' ratings of their discussion experiences, focusing particularly on judgments of perceived quality of the communication process and their satisfaction with the discussion.
To better understand differences in these ratings we also draw from the individual post-task interviews.
After confirming satisfactory internal reliability , we aggregated the six rating scales assessing quality of communication into the QC construct.
A two-way ANOVA with Tool and Speaker-type as between-subject factors revealed a main effect of Speaker =6.11, p<.02, but no effect of Tool or interaction of Tool and Speaker.
Native speakers provided generally higher QC ratings  than non-native speakers .
Although there was no main effect of Tool on overall quality of communication, inspection of the individual items comprising the QC scale suggested that the one item focused on conversation control  was sensitive to the presence of the IA feature.
For the non-IA condition, the mean ratings for native and non-native speakers were 4.11 and 2.60; when using IA, the means for both speaker types were higher .
Even though non-native speakers felt less control in general, speakers who used the IA tool felt that they could control the conversation more than those who did not use the IA tool.
Given this pattern, we re-examined the interview transcripts, which provided three explanations about how and why the IA feature may have enhanced conversation control.
We also derive design implications from this qualitative analysis, which are discussed in corresponding sections.
IA can be seen as a support for distributed cognition among team members via collective annotations of ideas and reactions during chat.
For example, one participant stated that she put complementary information in the IA space with the goal of keeping side points on the side, "I want to comment on that one point, and I felt that was a better place in the annotation than in the chat sequence.
Because it's more of a supporting statement, I wanted to put in like `To do:' what need to be done to get that idea going.
And I just didn't feel that it wants itself to be in the discussion, it's more like a side note of ok heads up, we need to do this for this idea."
In fact, Summarize, Communicate, Vote, To Do, and Add Information annotations are five different instruments for supporting group members' distributed cognition.
With the IA space, it is easy to externalize one's understanding of or reactions to the conversation, thereby sharing it with other team members.
The expectation is that the group as a result might enjoy a richer shared understanding; this in turn can explain why participants using IA felt more control over the conversations.
A design implication from this observation is that online chat tools should have features to help group members coordinate shared cognition through external representations like the five types of annotations.
The interviews suggest that IA may mitigate the "attenuation blocking" problem common in group meetings; this problem was discussed in the seminal work of Nunamaker , who emphasized the downsides of participants not being able to contribute comments at the time they felt to be most relevant.
One of our participants shared her frustration in traditional online chatting and how she used IA to overcome it, "I ended up, you know, I started typing something, and then I wiped it out, because somebody else already raised the issue.
Although IA did not change performance, it seems to promote perceptions of a better balance in conversations, which helps to address issues stemming from differences in language fluency.
In the Non-IA condition, when asked whether there was a leader in their group, all groups reported that native speakers assumed leader roles in their multilingual groups.
However, in the IA condition, no groups reported having a leader.
In fact, all of the IA groups emphasized that the member contributions of ideas and annotations were well balanced.
Even talkative people who tend to dominate conversations found a way to balance their input.
They waited a bit before talking, and spent time instead doing annotations for their group.
One native speaker shared this strategy of balancing the conversation in his group, "I was the one who was doing all the annotations.
I had a couple of ideas, but I kind of slipped and let other people talk, while I was annotating.
And once it slowed down, I added my own ideas."
Their satisfaction scores as well as perceived capability of controlling the conversation were higher.
The qualitative interview data extended our understanding of IA from previous work .
Users' comments suggested that IA might enhance feelings of controllability by mitigating attenuation blocking, supporting distributed cognition and helping to balance conversations.
Based on this analysis, we proposed some design implications for designing lightweight secondary conversation streams for group chat.
In the future it will be interesting to extend the use of IA to real-time text chats for larger groups, where richer and more flexible representations of the secondary channel may be needed.
Other features that may help with larger groups are labels to aid awareness of the greater number of participants; notifications of new annotations as they appear; and nested replies for more complex concepts and annotations.
Further discussion of related design ideas can be found in an earlier paper that first introduced the IA concept .
The post-task questionnaire also included four rating scales that assessed satisfaction with the group.
After confirming satisfactory internal reliability , we aggregated the scales to form the SAT construct.
A two-way ANOVA on SAT revealed a main effect of Tool = 5.92, p<0.05, but no effect of Speaker and no interaction, suggesting that satisfaction with one's group differs in the two conditions.
Specifically, participants who chatted using the IA tool were more satisfied with their group  than participants who chatted without the IA tool .
This might be due at least partially to their increased perceptions of conversation control.
Finally, the post-task questionnaire included two subconstructs designed to evaluate the cost of using the IA tool.
Two 5-point rating scales assessed ease of using IA ; three others probed the perceived cognitive cost of IA .
The average values for both constructs were moderately positive, with a mean of 4.0 for EIA and 3.93 for CIA.
We also conducted independent t-tests to contrast the perceptions of native and non-native speakers with respect to these measures but found no significant differences.
This suggests that even for users with relatively high cognitive loads , annotations combine well with chatting.
Our interviews also probed perceptions of difficulty in using the IA tool.
Users' comments were consistent with the rating scale data, suggesting that people can multitask between annotations and the main chatting task.
Our research has shown that adding instant annotations during real-time collaboration is both possible and has promising consequences for chat participants.
In terms of cost, participants seemed able to multitask well between the main chatting window and the IA side bar.
An analysis of the use patterns of the IA tool suggested diverse and subtle ways in which it can assist group communication.
